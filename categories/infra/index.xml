<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Infra on Mario Blog</title><link>https://codemario318.github.io/categories/infra/</link><description>Recent content in Infra on Mario Blog</description><generator>Hugo -- gohugo.io</generator><language>ko</language><lastBuildDate>Fri, 21 Apr 2023 14:00:47 +0900</lastBuildDate><atom:link href="https://codemario318.github.io/categories/infra/index.xml" rel="self" type="application/rss+xml"/><item><title>쿠버네티스: 5. 클러스터 아키텍처</title><link>https://codemario318.github.io/post/kubernetes_5/</link><pubDate>Fri, 21 Apr 2023 14:00:47 +0900</pubDate><guid>https://codemario318.github.io/post/kubernetes_5/</guid><description>&lt;img src="https://codemario318.github.io/post/kubernetes_5/kubernetes_cover.webp" alt="Featured image of post 쿠버네티스: 5. 클러스터 아키텍처" />&lt;h2 id="노드">노드&lt;/h2>
&lt;p>쿠버네티스는 컨테이너를 파드내에 배치하고 노드에서 실행함으로 워크로드를 구동한다. 노드는 클러스터에 따라 가상 또는 물리적 머신일 수 있다. 각 노드는 컨트롤 플레인에 의해 관리되며 파드를 실행하는데 필요한 서비스를 포함한다.&lt;/p>
&lt;p>일반적으로 클러스터에는 여러개의 노드가 있으며, 학습 또는 리소스가 제한되는 환경에서는 하나만 있을 수도 있다.&lt;/p>
&lt;p>노드의 컴포넌트에는 &lt;code>kubelet&lt;/code>, 컨테이너 런타임 그리고 &lt;code>kube-proxy&lt;/code>가 포함된다.&lt;/p>
&lt;h3 id="관리">관리&lt;/h3>
&lt;p>API 서버에 노드를 추가하는 두가지 주요 방법이 있다.&lt;/p>
&lt;ol>
&lt;li>노드의 &lt;code>kubelet&lt;/code>으로 컨트롤 플레인에 자체 등록&lt;/li>
&lt;li>사용자(또는 다른 사용자)가 노드 오브젝트를 수동으로 추가&lt;/li>
&lt;/ol>
&lt;p>노드 오브젝트 또는 노드의 &lt;code>kubelt&lt;/code>으로 자체 등록한 후 컨트롤 플레인은 새 노드 오브젝트가 유효한지 확인한다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="line">&lt;span class="cl">&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;kind&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;Node&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;apiVersion&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;v1&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;metadata&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;name&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;10.240.79.157&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;labels&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;name&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;my-first-k8s-node&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>쿠버네티스는 내부적으로 노드 오브젝트를 생성(표시)한다. 쿠버네티스는 &lt;code>kubelet&lt;/code>이 노드의 &lt;code>metadata&lt;/code>,&lt;code>name&lt;/code>필드와 일치하는 API 서버에 등록이 되어있는지 확인한다. 노드가 정상이면(필요한 모든 서비스가 실행중인 경우) 파드를 실행할 수 있게 된다. 그렇지 않으면, 해당 노드는 정상이 될때까지 모든 클러스터 활동에 대해 무시된다.&lt;/p>
&lt;blockquote>
&lt;p>💡 쿠버네티스는 유효하지 않은 노드 오브젝트를 유지하고, 노드가 정상적인지 확인한다. 상태 확인을 중지하려면 사용자 또는 &lt;a class="link" href="https://kubernetes.io/ko/docs/concepts/architecture/controller/" target="_blank" rel="noopener"
>컨트롤러&lt;/a>에서 노드 오브젝트를 명시적으로 삭제해야 한다.&lt;/p>
&lt;/blockquote>
&lt;p>노드 오브젝트의 이름은 유효한 DNS 서브 도메인 이름이어야 한다.&lt;/p>
&lt;h2 id="노드-이름-고유성">노드 이름 고유성&lt;/h2>
&lt;p>이름은 노드를 식별한다. 두 노드는 동시에 같은 이름을 가질 수 없다. 쿠버네티스는 또한 같은 이름의 리소스가 동일한 객체라고 가정한다. 노드의 경우, 동일한 이름을 사용하는 인스턴스가 동일한 상태와 노드 레이블과 같은 동일한 속성을 갖는다고 암시적으로 가정한다. 인스턴스가 이름을 변경하지 않고 수정된 경우 이로 인해 불일치가 발생할 수 있다. 노드를 대폭 교체하거나 업데이트 해야 하는 경우, 기존 노드 오브젝트를 먼저 API 서버에서 제거하고 업데이트 후 다시 추가해야 한다.&lt;/p>
&lt;h3 id="노드-상태">노드 상태&lt;/h3>
&lt;p>kubectl을 사용해서 노드 상태와 기타 세부 정보를 볼수 있다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="line">&lt;span class="cl">&lt;span class="err">kubectl&lt;/span> &lt;span class="err">describe&lt;/span> &lt;span class="err">node&lt;/span> &lt;span class="err">&amp;lt;insert-node-name-here&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="주소">주소&lt;/h3>
&lt;p>주소 필드는 클라우드 제공 사업자 또는 베어메탈 구성에 따라 다양하게 사용된다.&lt;/p>
&lt;ul>
&lt;li>&lt;code>HostName&lt;/code>: 노드의 커널에 의해 알려진 호스트명이다. &lt;code>-hostname-override&lt;/code> 파라미터를 통해 치환될 수 있다.&lt;/li>
&lt;li>&lt;code>ExternalIP&lt;/code>: 일반적으로 노드의 IP 주소는 외부로 라우트 가능 (클러스터 외부에서 이용 가능) 하다 .&lt;/li>
&lt;li>&lt;code>InternalIP&lt;/code>: 일반적으로 노드의 IP 주소는 클러스터 내에서만 라우트 가능하다.&lt;/li>
&lt;/ul>
&lt;h3 id="컨디션">컨디션&lt;/h3>
&lt;p>conditions필드는 모든 Running 노드의 상태를 기술한다.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>노드 컨디션&lt;/th>
&lt;th>설명&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Ready&lt;/td>
&lt;td>노드가 상태 양호하며 파드를 수용할 준비가 되어 있는 경우 True, 노드의 상태가 불량하여 파드를 수용하지 못할 경우 False, 그리고 노드 컨트롤러가 마지막 node-monitor-grace-period (기본값 40 기간 동안 노드로부터 응답을 받지 못한 경우) Unknown&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DiskPressure&lt;/td>
&lt;td>디스크 사이즈 상에 압박이 있는 경우, 즉 디스크 용량이 넉넉치 않은 경우 True, 반대의 경우 False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MemoryPressure&lt;/td>
&lt;td>노드 메모리 상에 압박이 있는 경우, 즉 노드 메모리가 넉넉치 않은 경우 True, 반대의 경우 False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PIDPressure&lt;/td>
&lt;td>프로세스 상에 압박이 있는 경우, 즉 노드 상에 많은 프로세스들이 존재하는 경우 True, 반대의 경우 False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NetworkUnavailable&lt;/td>
&lt;td>노드에 대해 네트워크가 올바르게 구성되지 않은 경우 True, 반대의 경우 False&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="용량과-할당가능">용량과 할당가능&lt;/h3>
&lt;p>노드 상에 사용 가능한 리소스를 나타낸다.&lt;/p>
&lt;ul>
&lt;li>CPU&lt;/li>
&lt;li>메모리&lt;/li>
&lt;li>스케줄 되어질 수 있는 최대 파드 수 등&lt;/li>
&lt;/ul>
&lt;p>용량 블록 필드는 노드에 있는 리소스의 총량을 나타낸다. 할당가능 블록은 일반 파드에서 사용할 수 있는 노드의 리소스 양을 나타낸다.&lt;/p>
&lt;h3 id="정보">정보&lt;/h3>
&lt;p>노드에 대한 일반적은 정보가 기술된다. kubelet이 노드에서 수집하여 쿠버네티스 API 로 전송한다.&lt;/p>
&lt;ul>
&lt;li>커널 버전&lt;/li>
&lt;li>쿠버네티스 버전&lt;/li>
&lt;li>컨테이너 런타임 상세 정보 등&lt;/li>
&lt;/ul>
&lt;h2 id="하트비트">하트비트&lt;/h2>
&lt;p>쿠버네티스 노드가 보내는 하트비느느 ㄴ클러스터가 개별 노드가 가용한지를 판단할 수 있도록 도움을 주고, 장애가 발견된 경우 조치를 할 수 있게 한다.&lt;/p>
&lt;ul>
&lt;li>노드의 .status 에 대한 업데이트&lt;/li>
&lt;li>kube=-node-lease 네임 스페이스 내의 Lease오브젝트&lt;/li>
&lt;/ul>
&lt;p>노드의 &lt;code>.status&lt;/code>에 비하면, 리스는 경량의 리소스이다. 큰 규모의 클러스터에서는 리스를 하트비트에 사용하여 업데이트로 인한 성능 영향을 줄일 수 있다.&lt;/p>
&lt;p>kubelet은 노드의 &lt;code>.status&lt;/code> 생성과 업데이트 및 관련된 리스의 업데이트를 담당한다.&lt;/p>
&lt;ul>
&lt;li>kubelet은 상태가 변경되거나 설정된 인터벌보다 오래 업데이트가 없는 경우 노드의 &lt;code>.status&lt;/code>를 업데이트한다. 노드의 &lt;code>.status&lt;/code> 업데이트에 대한 기본 인터벌은 접근이 불가능한 노드에 대한 타임아웃인 40초 보다 훨씬 긴 5분이다.&lt;/li>
&lt;li>kubelet은 리스 오브젝트를 (기본 업데이트 인터벌인) 매 10초마다 생성하고 업데이트한다. 리스 업데이트는 노드의 &lt;code>.status&lt;/code> 업데이트와는 독립적이다. 만약 리스 업데이트가 실패하면, kubelet은 200밀리초에서 시작하고 7초의 상한을 갖는 지수적 백오프를 사용해서 재시도한다.&lt;/li>
&lt;/ul>
&lt;h3 id="노드-컨트롤러">노드 컨트롤러&lt;/h3>
&lt;p>노드 &lt;a class="link" href="https://kubernetes.io/ko/docs/concepts/architecture/controller/" target="_blank" rel="noopener"
>컨트롤러&lt;/a>는 노드의 다양한 측면을 관리하는 쿠버네티스 컨트롤 플레인 컴포넌트이다.&lt;/p>
&lt;p>노드 컨트롤러는 노드가 생성되어 유지되는 동안 다양한 역할을 한다.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>등록 시점에 (CIDR 할당이 사용토록 설정된 경우) 노드에 CIDR 블럭을 할당하는 것이다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>노드 컨트롤러의 내부 노드 리스트를 클라우드 제공사업자의 사용 가능한 머신 리스트 정보를 근거로 최신상태로 유지하는 것이다.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>클라우드 환경에서 동작 중일 경우, 노드상태가 불량할 때마다, 노드 컨트롤러는 해당 노드용 VM이 여전히 사용 가능한지에 대해 클라우드 제공사업자에게 묻는다.&lt;/li>
&lt;li>사용 가능하지 않을 경우, 노드 컨트롤러는 노드 리스트로부터 그 노드를 삭제한다.&lt;/li>
&lt;/ul>
&lt;ol start="3">
&lt;li>노드의 동작 상태를 모니터링하는 것이다.&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>노드가 접근 불가능(unreachable) 상태가 되는 경우, 노드의 &lt;code>.status&lt;/code> 필드의 &lt;code>Ready&lt;/code> 컨디션을 업데이트한다. 이 경우에는 노드 컨트롤러가 &lt;code>Ready&lt;/code> 컨디션을 &lt;code>Unknown&lt;/code>으로 설정한다.&lt;/li>
&lt;li>노드가 계속 접근 불가능 상태로 남아있는 경우, 해당 노드의 모든 파드에 대해서 &lt;a class="link" href="https://kubernetes.io/ko/docs/concepts/scheduling-eviction/api-eviction/" target="_blank" rel="noopener"
>API를 이용한 축출&lt;/a>을 트리거한다. 기본적으로, 노드 컨트롤러는 노드를 &lt;code>Unknown&lt;/code>으로 마킹한 뒤 5분을 기다렸다가 최초의 축출 요청을 시작한다.&lt;/li>
&lt;/ul>
&lt;p>기본적으로, 노드 컨트롤러는 5 초마다 각 노드의 상태를 체크한다. 체크 주기는 &lt;code>kube-controller-manager&lt;/code> 구성 요소의 &lt;code>--node-monitor-period&lt;/code> 플래그를 이용하여 설정할 수 있다.&lt;/p>
&lt;h3 id="축출-빈도-제한">축출 빈도 제한&lt;/h3>
&lt;p>대부분의 경우, 노드 컨트롤러는 초당 &lt;code>--node-eviction-rate&lt;/code>(기본값 0.1)로 축출 속도를 제한한다. 이 말은 10초당 1개의 노드를 초과하여 파드 축출을 하지 않는다는 의미가 된다.&lt;/p>
&lt;p>노드 축출 행위는 주어진 가용성 영역 내 하나의 노드가 상태가 불량할 경우 변화한다. 노드 컨트롤러는 영역 내 동시에 상태가 불량한 노드의 퍼센티지가 얼마나 되는지 체크한다(&lt;code>Ready&lt;/code> 컨디션은 &lt;code>Unknown&lt;/code> 또는 &lt;code>False&lt;/code> 값을 가진다).&lt;/p>
&lt;ul>
&lt;li>상태가 불량한 노드의 비율이 최소 &lt;code>-unhealthy-zone-threshold&lt;/code> (기본값 0.55)가 되면 축출 속도가 감소한다.&lt;/li>
&lt;li>클러스터가 작으면 (즉 &lt;code>-large-cluster-size-threshold&lt;/code> 노드 이하면 - 기본값 50) 축출이 중지된다.&lt;/li>
&lt;li>이외의 경우, 축출 속도는 초당 &lt;code>-secondary-node-eviction-rate&lt;/code>(기본값 0.01)로 감소된다.&lt;/li>
&lt;/ul>
&lt;p>이 정책들이 가용성 영역 단위로 실행되어지는 이유는 나머지가 연결되어 있는 동안 &lt;strong>하나의 가용성 영역이 컨트롤 플레인으로부터 분할되어 질 수도 있기 때문&lt;/strong>이다. 만약 클러스터가 여러 클라우드 제공사업자의 가용성 영역에 걸쳐 있지 않는 이상, 축출 매커니즘은 영역 별 가용성을 고려하지 않는다.&lt;/p>
&lt;p>노드가 가용성 영역들에 걸쳐 퍼져 있는 주된 이유는 &lt;strong>하나의 전체 영역이 장애가 발생할 경우 워크로드가 상태 양호한 영역으로 이전될 수 있도록 하기 위해서&lt;/strong>이다. 그러므로, 하나의 영역 내 모든 노드들이 상태가 불량하면 노드 컨트롤러는 &lt;code>--node-eviction-rate&lt;/code> 의 정상 속도로 축출한다.&lt;/p>
&lt;p>모든 영역이 완전히 상태불량(클러스터 내 양호한 노드가 없는 경우)한 경우이다. 이러한 경우, 노드 컨트롤러는 컨트롤 플레인과 노드 간 연결에 문제가 있는 것으로 간주하고 축출을 실행하지 않는다. (중단 이후 일부 노드가 다시 보이는 경우 노드 컨트롤러는 상태가 양호하지 않거나 접근이 불가능한 나머지 노드에서 파드를 축출한다.)&lt;/p>
&lt;p>또한, 노드 컨트롤러는 파드가 테인트를 허용하지 않을 때 &lt;code>NoExecute&lt;/code> 테인트 상태의 노드에서 동작하는 파드에 대한 축출 책임을 가지고 있다. 추가로, 노드 컨틀로러는 연결할 수 없거나, 준비되지 않은 노드와 같은 노드 문제에 상응하는 &lt;a class="link" href="https://kubernetes.io/ko/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank" rel="noopener"
>테인트&lt;/a>를 추가한다. 이는 스케줄러가 비정상적인 노드에 파드를 배치하지 않게 된다.&lt;/p>
&lt;h2 id="리소스-용량-추적">리소스 용량 추적&lt;/h2>
&lt;p>노드 오브젝트는 노드 리소스 용량에 대한 정보: 예를 들어, 사용 가능한 메모리의 양과 CPU의 수를 추적한다. 노드의 &lt;a class="link" href="https://kubernetes.io/ko/docs/concepts/architecture/nodes/#%EB%85%B8%EB%93%9C%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9E%90%EC%B2%B4-%EB%93%B1%EB%A1%9D" target="_blank" rel="noopener"
>자체 등록&lt;/a>은 등록하는 중에 용량을 보고한다. &lt;a class="link" href="https://kubernetes.io/ko/docs/concepts/architecture/nodes/#%EC%88%98%EB%8F%99-%EB%85%B8%EB%93%9C-%EA%B4%80%EB%A6%AC" target="_blank" rel="noopener"
>수동&lt;/a>으로 노드를 추가하는 경우 추가할 때 노드의 용량 정보를 설정해야 한다.&lt;/p>
&lt;p>쿠버네티스 &lt;a class="link" href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" rel="noopener"
>스케줄러&lt;/a>는 노드 상에 모든 노드에 대해 충분한 리소스가 존재하도록 보장한다. 스케줄러는 노드 상에 컨테이너에 대한 요청의 합이 노드 용량보다 더 크지 않도록 체크한다. 요청의 합은 kubelet에서 관리하는 모든 컨테이너를 포함하지만, 컨테이너 런타임에 의해 직접적으로 시작된 컨 테이너는 제외되고 kubelet의 컨트롤 범위 밖에서 실행되는 모든 프로세스도 제외된다.&lt;/p>
&lt;p>&lt;strong>참고: 파드 형태가 아닌 프로세스에 대해 명시적으로 리소스를 확보하려면, &lt;a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved" target="_blank" rel="noopener"
>시스템 데몬에 사용할 리소스 예약하기&lt;/a>을 본다.&lt;/strong>&lt;/p>
&lt;h2 id="노드-토폴로지">노드 토폴로지&lt;/h2>
&lt;p>&lt;strong>기능 상태:&lt;/strong> &lt;code>Kubernetes v1.18 [beta]&lt;/code>&lt;/p>
&lt;p>&lt;code>TopologyManager&lt;/code> &lt;a class="link" href="https://kubernetes.io/ko/docs/reference/command-line-tools-reference/feature-gates/" target="_blank" rel="noopener"
>기능 게이트(feature gate)&lt;/a>를 활성화 시켜두면, kubelet이 리소스 할당 결정을 할 때 토폴로지 힌트를 사용할 수 있다. 자세한 내용은 &lt;a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/" target="_blank" rel="noopener"
>노드의 컨트롤 토폴로지 관리 정책&lt;/a>을 본다.&lt;/p>
&lt;h2 id="그레이스풀graceful-노드-셧다운shutdown">그레이스풀(Graceful) 노드 셧다운(shutdown)&lt;/h2>
&lt;p>&lt;strong>기능 상태:&lt;/strong> &lt;code>Kubernetes v1.21 [beta]&lt;/code>&lt;/p>
&lt;p>kubelet은 노드 시스템 셧다운을 감지하고 노드에서 실행 중인 파드를 종료하려고 시도한다.&lt;/p>
&lt;p>Kubelet은 노드가 종료되는 동안 파드가 일반 &lt;a class="link" href="https://kubernetes.io/ko/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination" target="_blank" rel="noopener"
>파드 종료 프로세스&lt;/a>를 따르도록 한다.&lt;/p>
&lt;p>그레이스풀 노드 셧다운 기능은 &lt;a class="link" href="https://www.freedesktop.org/wiki/Software/systemd/inhibit/" target="_blank" rel="noopener"
>systemd inhibitor locks&lt;/a>를 사용하여 주어진 기간 동안 노드 종료를 지연시키므로 systemd에 의존한다.&lt;/p>
&lt;p>그레이스풀 노드 셧다운은 1.21에서 기본적으로 활성화된 &lt;code>GracefulNodeShutdown&lt;/code> &lt;a class="link" href="https://kubernetes.io/ko/docs/reference/command-line-tools-reference/feature-gates/" target="_blank" rel="noopener"
>기능 게이트&lt;/a>로 제어된다.&lt;/p>
&lt;p>기본적으로, 아래 설명된 두 구성 옵션, &lt;code>shutdownGracePeriod&lt;/code> 및 &lt;code>shutdownGracePeriodCriticalPods&lt;/code> 는 모두 0으로 설정되어 있으므로, 그레이스풀 노드 셧다운 기능이 활성화되지 않는다. 기능을 활성화하려면, 두 개의 kubelet 구성 설정을 적절하게 구성하고 0이 아닌 값으로 설정해야 한다.&lt;/p>
&lt;p>그레이스풀 셧다운 중에 kubelet은 다음의 두 단계로 파드를 종료한다.&lt;/p>
&lt;ol>
&lt;li>노드에서 실행 중인 일반 파드를 종료시킨다.&lt;/li>
&lt;li>노드에서 실행 중인 &lt;a class="link" href="https://kubernetes.io/ko/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#%ED%8C%8C%EB%93%9C%EB%A5%BC-%EC%A4%91%EC%9A%94-critical-%EB%A1%9C-%ED%91%9C%EC%8B%9C%ED%95%98%EA%B8%B0" target="_blank" rel="noopener"
>중요(critical) 파드&lt;/a>를 종료시킨다.&lt;/li>
&lt;/ol>
&lt;p>그레이스풀 노드 셧다운 기능은 두 개의 &lt;code>[KubeletConfiguration](https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/)&lt;/code> 옵션으로 구성된다.&lt;/p>
&lt;ul>
&lt;li>&lt;code>shutdownGracePeriod&lt;/code>:
&lt;ul>
&lt;li>노드가 종료를 지연해야 하는 총 기간을 지정한다. 이것은 모든 일반 및 &lt;a class="link" href="https://kubernetes.io/ko/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#%ED%8C%8C%EB%93%9C%EB%A5%BC-%EC%A4%91%EC%9A%94-critical-%EB%A1%9C-%ED%91%9C%EC%8B%9C%ED%95%98%EA%B8%B0" target="_blank" rel="noopener"
>중요 파드&lt;/a>의 파드 종료에 필요한 총 유예 기간에 해당한다.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>shutdownGracePeriodCriticalPods&lt;/code>:
&lt;ul>
&lt;li>노드 종료 중에 &lt;a class="link" href="https://kubernetes.io/ko/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#%ED%8C%8C%EB%93%9C%EB%A5%BC-%EC%A4%91%EC%9A%94-critical-%EB%A1%9C-%ED%91%9C%EC%8B%9C%ED%95%98%EA%B8%B0" target="_blank" rel="noopener"
>중요 파드&lt;/a>를 종료하는 데 사용되는 기간을 지정한다. 이 값은 &lt;code>shutdownGracePeriod&lt;/code> 보다 작아야 한다.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>예를 들어, &lt;code>shutdownGracePeriod=30s&lt;/code>, &lt;code>shutdownGracePeriodCriticalPods=10s&lt;/code> 인 경우, kubelet은 노드 종료를 30초까지 지연시킨다. 종료하는 동안 처음 20(30-10)초는 일반 파드의 유예 종료에 할당되고, 마지막 10초는 &lt;a class="link" href="https://kubernetes.io/ko/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#%ED%8C%8C%EB%93%9C%EB%A5%BC-%EC%A4%91%EC%9A%94-critical-%EB%A1%9C-%ED%91%9C%EC%8B%9C%ED%95%98%EA%B8%B0" target="_blank" rel="noopener"
>중요 파드&lt;/a>의 종료에 할당된다.&lt;/p>
&lt;p>**참고:
그레이스풀 노드 셧다운 과정에서 축출된 파드는 셧다운(shutdown)된 것으로 표시된다. &lt;code>kubectl get pods&lt;/code> 명령을 실행하면 축출된 파드의 상태가 &lt;code>Terminated&lt;/code>으로 표시된다. 그리고 &lt;code>kubectl describe pod&lt;/code> 명령을 실행하면 노드 셧다운으로 인해 파드가 축출되었음을 알 수 있다.&lt;/p>
&lt;p>&lt;code>Reason: Terminated Message: Pod was terminated in response to imminent node shutdown.&lt;/code>**&lt;/p>
&lt;h2 id="논-그레이스풀-노드-셧다운">논 그레이스풀 노드 셧다운&lt;/h2>
&lt;p>&lt;strong>기능 상태:&lt;/strong> &lt;code>Kubernetes v1.24 [alpha]&lt;/code>&lt;/p>
&lt;p>전달한 명령이 kubelet에서 사용하는 금지 잠금 메커니즘(inhibitor locks mechanism)을 트리거하지 않거나, 또는 사용자 오류(예: ShutdownGracePeriod 및 ShutdownGracePeriodCriticalPods가 제대로 설정되지 않음)로 인해 kubelet의 노드 셧다운 관리자(Node Shutdown Mananger)가 노드 셧다운 액션을 감지하지 못할 수 있다. 자세한 내용은 위의 &lt;a class="link" href="https://kubernetes.io/ko/docs/concepts/architecture/nodes/#graceful-node-shutdown" target="_blank" rel="noopener"
>그레이스풀 노드 셧다운&lt;/a> 섹션을 참조한다.&lt;/p>
&lt;p>노드가 셧다운되었지만 kubelet의 노드 셧다운 관리자가 이를 감지하지 못하면, 스테이트풀셋에 속한 파드는 셧다운된 노드에 &amp;lsquo;종료 중(terminating)&amp;rsquo; 상태로 고착되어 다른 동작 중인 노드로 이전될 수 없다. 이는 셧다운된 노드의 kubelet이 파드를 지울 수 없어서 결국 스테이트풀셋이 동일한 이름으로 새 파드를 만들 수 없기 때문이다. 만약 파드가 사용하던 볼륨이 있다면, 볼륨어태치먼트(VolumeAttachment)도 기존의 셧다운된 노드에서 삭제되지 않아 결국 파드가 사용하던 볼륨이 다른 동작 중인 노드에 연결(attach)될 수 없다. 결과적으로, 스테이트풀셋에서 실행되는 애플리케이션이 제대로 작동하지 않는다. 기존의 셧다운된 노드가 정상으로 돌아오지 못하면, 이러한 파드는 셧다운된 노드에 &amp;lsquo;종료 중(terminating)&amp;rsquo; 상태로 영원히 고착될 것이다.&lt;/p>
&lt;p>위와 같은 상황을 완화하기 위해, 사용자가 &lt;code>node.kubernetes.io/out-of-service&lt;/code> 테인트를 &lt;code>NoExecute&lt;/code> 또는 &lt;code>NoSchedule&lt;/code> 값으로 추가하여 노드를 서비스 불가(out-of-service) 상태로 표시할 수 있다. &lt;code>kube-controller-manager&lt;/code>에 &lt;code>NodeOutOfServiceVolumeDetach&lt;/code>&lt;a class="link" href="https://kubernetes.io/ko/docs/reference/command-line-tools-reference/feature-gates/" target="_blank" rel="noopener"
>기능 게이트&lt;/a> 가 활성화되어 있고, 노드가 이 테인트에 의해 서비스 불가 상태로 표시되어 있는 경우, 노드에 매치되는 톨러레이션이 없다면 노드 상의 파드는 강제로 삭제될 것이고, 노드 상에서 종료되는 파드에 대한 볼륨 해제(detach) 작업은 즉시 수행될 것이다. 이를 통해 서비스 불가 상태 노드의 파드가 빠르게 다른 노드에서 복구될 수 있다.&lt;/p>
&lt;p>논 그레이스풀 셧다운 과정 동안, 파드는 다음의 두 단계로 종료된다.&lt;/p>
&lt;ol>
&lt;li>매치되는 &lt;code>out-of-service&lt;/code> 톨러레이션이 없는 파드를 강제로 삭제한다.&lt;/li>
&lt;li>이러한 파드에 대한 볼륨 해제 작업을 즉시 수행한다.&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>참고:
• &lt;code>node.kubernetes.io/out-of-service&lt;/code> 테인트를 추가하기 전에, 노드가 완전한 셧다운 또는 전원 꺼짐 상태에 있는지 (재시작 중인 것은 아닌지) 확인한다.
• 사용자가 서비스 불가 상태 테인트를 직접 추가한 것이기 때문에, 파드가 다른 노드로 옮겨졌고 셧다운 상태였던 노드가 복구된 것을 확인했다면 사용자가 서비스 불가 상태 테인트를 수동으로 제거해야 한다.&lt;/strong>&lt;/p>
&lt;h3 id="파드-우선순위-기반-그레이스풀-노드-셧다운">파드 우선순위 기반 그레이스풀 노드 셧다운&lt;/h3>
&lt;p>&lt;strong>기능 상태:&lt;/strong> &lt;code>Kubernetes v1.23 [alpha]&lt;/code>&lt;/p>
&lt;p>그레이스풀 노드 셧다운 시 파드 셧다운 순서에 더 많은 유연성을 제공할 수 있도록, 클러스터에 프라이어리티클래스(PriorityClass) 기능이 활성화되어 있으면 그레이스풀 노드 셧다운 과정에서 파드의 프라이어리티클래스가 고려된다. 이 기능으로 그레이스풀 노드 셧다운 시 파드가 종료되는 순서를 클러스터 관리자가 &lt;a class="link" href="https://kubernetes.io/ko/docs/concepts/scheduling-eviction/pod-priority-preemption/#%ED%94%84%EB%9D%BC%EC%9D%B4%EC%96%B4%EB%A6%AC%ED%8B%B0%ED%81%B4%EB%9E%98%EC%8A%A4" target="_blank" rel="noopener"
>프라이어리티 클래스&lt;/a> 기반으로 명시적으로 정할 수 있다.&lt;/p>
&lt;p>위에서 기술된 것처럼, &lt;a class="link" href="https://kubernetes.io/ko/docs/concepts/architecture/nodes/#graceful-node-shutdown" target="_blank" rel="noopener"
>그레이스풀 노드 셧다운&lt;/a> 기능은 파드를 중요하지 않은(non-critical) 파드와 중요한(critical) 파드 2단계(phase)로 구분하여 종료시킨다. 셧다운 시 파드가 종료되는 순서를 명시적으로 더 상세하게 정해야 한다면, 파드 우선순위 기반 그레이스풀 노드 셧다운을 사용할 수 있다.&lt;/p>
&lt;p>그레이스풀 노드 셧다운 과정에서 파드 우선순위가 고려되기 때문에, 그레이스풀 노드 셧다운이 여러 단계로 일어날 수 있으며, 각 단계에서 특정 프라이어리티 클래스의 파드를 종료시킨다. 정확한 단계와 단계별 셧다운 시간은 kubelet에 설정할 수 있다.&lt;/p>
&lt;p>다음과 같이 클러스터에 커스텀 파드 &lt;a class="link" href="https://kubernetes.io/ko/docs/concepts/scheduling-eviction/pod-priority-preemption/#%ED%94%84%EB%9D%BC%EC%9D%B4%EC%96%B4%EB%A6%AC%ED%8B%B0%ED%81%B4%EB%9E%98%EC%8A%A4" target="_blank" rel="noopener"
>프라이어리티 클래스&lt;/a>가 있다고 가정하자.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>파드 프라이어리티 클래스 이름&lt;/th>
&lt;th>파드 프라이어리티 클래스 값&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>custom-class-a&lt;/td>
&lt;td>100000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>custom-class-b&lt;/td>
&lt;td>10000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>custom-class-c&lt;/td>
&lt;td>1000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>regular/unset&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;a class="link" href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration" target="_blank" rel="noopener"
>kubelet 환경 설정&lt;/a> 안의 &lt;code>shutdownGracePeriodByPodPriority&lt;/code> 설정은 다음과 같을 수 있다.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>파드 프라이어리티 클래스 값&lt;/th>
&lt;th>종료 대기 시간&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>100000&lt;/td>
&lt;td>10 seconds&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10000&lt;/td>
&lt;td>180 seconds&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1000&lt;/td>
&lt;td>120 seconds&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>60 seconds&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>이를 나타내는 kubelet 환경 설정 YAML은 다음과 같다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="line">&lt;span class="cl">&lt;span class="err">**shutdownGracePeriodByPodPriority**:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="err">-&lt;/span> &lt;span class="err">**priority**:&lt;/span> &lt;span class="mi">100000&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="err">**shutdownGracePeriodSeconds**:&lt;/span> &lt;span class="mi">10&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="err">-&lt;/span> &lt;span class="err">**priority**:&lt;/span> &lt;span class="mi">10000&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="err">**shutdownGracePeriodSeconds**:&lt;/span> &lt;span class="mi">180&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="err">-&lt;/span> &lt;span class="err">**priority**:&lt;/span> &lt;span class="mi">1000&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="err">**shutdownGracePeriodSeconds**:&lt;/span> &lt;span class="mi">120&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="err">-&lt;/span> &lt;span class="err">**priority**:&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="err">**shutdownGracePeriodSeconds**:&lt;/span> &lt;span class="mi">60&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>위의 표에 의하면 &lt;code>priority&lt;/code> 값이 100000 이상인 파드는 종료까지 10초만 주어지며, 10000 이상 ~ 100000 미만이면 180초, 1000 이상 ~ 10000 미만이면 120초가 주어진다. 마지막으로, 다른 모든 파드는 종료까지 60초가 주어질 것이다.&lt;/p>
&lt;p>모든 클래스에 대해 값을 명시할 필요는 없다. 예를 들어, 대신 다음과 같은 구성을 사용할 수도 있다.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>파드 프라이어리티 클래스 값&lt;/th>
&lt;th>종료 대기 시간&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>100000&lt;/td>
&lt;td>300 seconds&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1000&lt;/td>
&lt;td>120 seconds&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>60 seconds&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>위의 경우, &lt;code>custom-class-b&lt;/code>에 속하는 파드와 &lt;code>custom-class-c&lt;/code>에 속하는 파드는 동일한 종료 대기 시간을 갖게 될 것이다.&lt;/p>
&lt;p>특정 범위에 해당되는 파드가 없으면, kubelet은 해당 범위에 해당되는 파드를 위해 기다려 주지 않는다. 대신, kubelet은 즉시 다음 프라이어리티 클래스 값 범위로 넘어간다.&lt;/p>
&lt;p>기능이 활성화되어 있지만 환경 설정이 되어 있지 않으면, 순서 지정 동작이 수행되지 않을 것이다.&lt;/p>
&lt;p>이 기능을 사용하려면 &lt;code>GracefulNodeShutdownBasedOnPodPriority&lt;/code> &lt;a class="link" href="https://kubernetes.io/ko/docs/reference/command-line-tools-reference/feature-gates/" target="_blank" rel="noopener"
>기능 게이트&lt;/a>를 활성화해야 하고, &lt;a class="link" href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/" target="_blank" rel="noopener"
>kubelet config&lt;/a>의 &lt;code>ShutdownGracePeriodByPodPriority&lt;/code>를 파드 프라이어리티 클래스 값과 각 값에 대한 종료 대기 시간을 명시하여 지정해야 한다.&lt;/p>
&lt;p>&lt;strong>참고: 그레이스풀 노드 셧다운 과정에서 파드 프라이어리티를 고려하는 기능은 쿠버네티스 v1.23에서 알파 기능으로 도입되었다. 쿠버네티스 1.26에서 이 기능은 베타 상태이며 기본적으로 활성화되어 있다.&lt;/strong>&lt;/p>
&lt;p>&lt;code>graceful_shutdown_start_time_seconds&lt;/code> 및 &lt;code>graceful_shutdown_end_time_seconds&lt;/code> 메트릭은 노드 셧다운을 모니터링하기 위해 kubelet 서브시스템에서 방출된다.&lt;/p>
&lt;h2 id="스왑swap-메모리-관리">스왑(swap) 메모리 관리&lt;/h2>
&lt;p>&lt;strong>기능 상태:&lt;/strong> &lt;code>Kubernetes v1.22 [alpha]&lt;/code>&lt;/p>
&lt;p>쿠버네티스 1.22 이전에는 노드가 스왑 메모리를 지원하지 않았다. 그리고 kubelet은 노드에서 스왑을 발견하지 못한 경우 시작과 동시에 실패하도록 되어 있었다. 1.22부터는 스왑 메모리 지원을 노드 단위로 활성화할 수 있다.&lt;/p>
&lt;p>노드에서 스왑을 활성화하려면, &lt;code>NodeSwap&lt;/code> 기능 게이트가 kubelet에서 활성화되어야 하며, 명령줄 플래그 &lt;code>--fail-swap-on&lt;/code> 또는 &lt;a class="link" href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration" target="_blank" rel="noopener"
>구성 설정&lt;/a>에서 &lt;code>failSwapOn&lt;/code>가 false로 지정되어야 한다.&lt;/p>
&lt;p>&lt;strong>경고: 메모리 스왑 기능이 활성화되면, 시크릿 오브젝트의 내용과 같은 tmpfs에 기록되었던 쿠버네티스 데이터가 디스크에 스왑될 수 있다.&lt;/strong>&lt;/p>
&lt;p>사용자는 또한 선택적으로 &lt;code>memorySwap.swapBehavior&lt;/code>를 구성할 수 있으며, 이를 통해 노드가 스왑 메모리를 사용하는 방식을 명시한다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">memorySwap&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">swapBehavior&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">LimitedSwap`&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>swapBehavior&lt;/code>에 가용한 구성 옵션은 다음과 같다.&lt;/p>
&lt;ul>
&lt;li>&lt;code>LimitedSwap&lt;/code>: 쿠버네티스 워크로드는 스왑을 사용할 수 있는 만큼으로 제한된다. 쿠버네티스에 의해 관리되지 않는 노드의 워크로드는 여전히 스왑될 수 있다.&lt;/li>
&lt;li>&lt;code>UnlimitedSwap&lt;/code>: 쿠버네티스 워크로드는 요청한 만큼 스왑 메모리를 사용할 수 있으며, 시스템의 최대치까지 사용 가능하다.&lt;/li>
&lt;/ul>
&lt;p>만약 &lt;code>memorySwap&lt;/code> 구성이 명시되지 않았고 기능 게이트가 활성화되어 있다면, kubelet은 &lt;code>LimitedSwap&lt;/code> 설정과 같은 행동을 기본적으로 적용한다.&lt;/p>
&lt;p>&lt;code>LimitedSwap&lt;/code> 설정에 대한 행동은 노드가 (&amp;ldquo;cgroups&amp;quot;으로 알려진) 제어 그룹이 v1 또는 v2 중에서 무엇으로 동작하는가에 따라서 결정된다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>cgroupsv1:&lt;/strong> 쿠버네티스 워크로드는 메모리와 스왑의 조합을 사용할 수 있다. 파드의 메모리 제한이 설정되어 있다면 가용 상한이 된다.&lt;/li>
&lt;li>&lt;strong>cgroupsv2:&lt;/strong> 쿠버네티스 워크로드는 스왑 메모리를 사용할 수 없다.&lt;/li>
&lt;/ul>
&lt;p>테스트를 지원하고 피드벡을 제공하기 위한 정보는 &lt;a class="link" href="https://github.com/kubernetes/enhancements/issues/2400" target="_blank" rel="noopener"
>KEP-2400&lt;/a> 및 &lt;a class="link" href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md" target="_blank" rel="noopener"
>디자인 제안&lt;/a>에서 찾을 수 있다.&lt;/p></description></item><item><title>쿠버네티스: 4. 쿠버네티스 오브젝트 관리</title><link>https://codemario318.github.io/post/kubernetes_4/</link><pubDate>Fri, 21 Apr 2023 13:57:47 +0900</pubDate><guid>https://codemario318.github.io/post/kubernetes_4/</guid><description>&lt;img src="https://codemario318.github.io/post/kubernetes_4/kubernetes_cover.webp" alt="Featured image of post 쿠버네티스: 4. 쿠버네티스 오브젝트 관리" />&lt;h2 id="관리기법">관리기법&lt;/h2>
&lt;blockquote>
&lt;p>⛔ 쿠버네티스 오브젝트는 하나의 기법만 사용하여 관리해야 한다. 동일한 오브젝트에 대해 여러 기법을 혼용하면 오동작이 발생할 수 있다.&lt;/p>
&lt;/blockquote>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>관리기법&lt;/th>
&lt;th>대상&lt;/th>
&lt;th>권장 환경&lt;/th>
&lt;th>지원하는 작업자 수&lt;/th>
&lt;th>학습 난이도&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>명령형 커맨드&lt;/td>
&lt;td>활성 오브젝트&lt;/td>
&lt;td>개발 환경&lt;/td>
&lt;td>1+&lt;/td>
&lt;td>낮음&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>명령형 오브젝트 구성&lt;/td>
&lt;td>개별 파일&lt;/td>
&lt;td>프로덕션 환경&lt;/td>
&lt;td>1&lt;/td>
&lt;td>보통&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>선언형 오브젝트 구성&lt;/td>
&lt;td>파일이 있는 디렉터리&lt;/td>
&lt;td>프로덕션 환경&lt;/td>
&lt;td>1+&lt;/td>
&lt;td>높음&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="명령형-커맨드">명령형 커맨드&lt;/h2>
&lt;p>사용자가 클러스터 내 활성 오브젝트를 대상으로 직접 동작시킨다. 실행할 작업을 인수 또는 플래그로 kubectl 커맨드에 지정한다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="l">kubectl create deployment nginx --image nginx&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="트레이드-오프">트레이드 오프&lt;/h3>
&lt;p>&lt;strong>장점&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>커맨드는 하나의 동작을 나타내는 단어로 표현됨&lt;/li>
&lt;li>클러스터를 수정하기 위해 단 하나의 단계만을 필요로 한다.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>단점&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>커맨드는 변경 검토 프로세스와 통합되지 않는다.&lt;/li>
&lt;li>변경에 관한 감사 추적(audit trail)을 제공하지 않는다.&lt;/li>
&lt;li>활성 동작 중인 경우를 제외하고는 레코드의 소스를 제공하지 않는다.&lt;/li>
&lt;li>새로운 오브젝트 생성을 위한 템플릿을 제공하지 않는다.&lt;/li>
&lt;/ul>
&lt;h2 id="명령형-오브젝트-구성">명령형 오브젝트 구성&lt;/h2>
&lt;p>kubectl 커맨드로 작업, 선택적 플래그, 파일 이름을 지정하여 실행한다.&lt;/p></description></item><item><title>쿠버네티스: 3. 쿠버네티스 오브젝트 이해하기</title><link>https://codemario318.github.io/post/kubernetes_3/</link><pubDate>Fri, 21 Apr 2023 13:53:47 +0900</pubDate><guid>https://codemario318.github.io/post/kubernetes_3/</guid><description>&lt;img src="https://codemario318.github.io/post/kubernetes_3/kubernetes_cover.webp" alt="Featured image of post 쿠버네티스: 3. 쿠버네티스 오브젝트 이해하기" />&lt;blockquote>
&lt;p>쿠버네티스 오브젝트는 &lt;strong>하나의 의도를 담은 레코드&lt;/strong>이다.&lt;/p>
&lt;/blockquote>
&lt;p>쿠버네티스 오브젝트는 쿠버네티스 시스템에서 영속성을 가지는 오브젝트이다. 쿠버네티스는 클러스터의 상태를 나타내기 위해 &lt;code>.yaml&lt;/code>로 작성된 오브젝트에 상세 내용을 기술한다.&lt;/p>
&lt;ul>
&lt;li>어떤 컨테이너화된 애플리케이션이 동작 중인지(어느 노드에서 동작중인지)&lt;/li>
&lt;li>해당 어플리케이션이 이용할 수 있는 리소스&lt;/li>
&lt;li>해당 애플리케이션이 어떻게 재구동 정책,&lt;/li>
&lt;li>업그레이드&lt;/li>
&lt;li>내고장성과 같은 것에 동작해야 하는지에 대한 정책&lt;/li>
&lt;/ul>
&lt;p>오브젝트를 생성하게 되면, 쿠버네티스 시스템은 오브젝트에 담긴 상태를 보장하기 위해 지속적으로 동작하게 된다.&lt;/p>
&lt;h2 id="오브젝트-명세spec과-상태status">오브젝트 명세(spec)과 상태(status)&lt;/h2>
&lt;p>거의 모든 쿠버네티스 오브젝트는 spec 오브젝트와 status 오브젝트로 구성된다.&lt;/p>
&lt;h3 id="spec">spec&lt;/h3>
&lt;p>spec을 가지는 오브젝트는 오브젝트를 생성할 때 리소스에 원하는 특징(의도한 상태)에 대한 설명을 제공하여 설정한다.&lt;/p>
&lt;h3 id="status">status&lt;/h3>
&lt;p>쿠버네티스 시스템과 컴포넌트에 의해 제공되고 업데이트된 오브젝트의 현재 상태를 설명한다.
컨트롤 플레인은 모든 오브젝트의 실제 상태를 사용자가 의도한 상태와 일치시키기 위해서 끊임없이 능독적으로 관리한다.&lt;/p>
&lt;p>&lt;a class="link" href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md" target="_blank" rel="noopener"
>https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md&lt;/a>&lt;/p>
&lt;h2 id="쿠버네티스-오브젝트-기술하기">쿠버네티스 오브젝트 기술하기&lt;/h2>
&lt;p>쿠버네티스에서 오브젝트를 생성할 때, 오브젝트에 대한 기본적인 정보와 더불어, 의도한 상태를 기술한 오브젝트 spec을 제시해야 한다.&lt;/p>
&lt;p>대부분의 경우 정보를 .yaml 파일로 kubectl에 제공한다. kubectl은 API 요청이 이루어질 때, JSON 형식으로 정보를 변환시켜준다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">apps/v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Deployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nginx-deployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">selector&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">matchLabels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nginx&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c"># tells deployment to run 2 pods matching the template&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">template&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nginx&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nginx&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nginx:1.14.2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">ports&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">containerPort&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">80&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="요구되는-필드">요구되는 필드&lt;/h3>
&lt;p>생성하고자 하는 쿠버네티스 오브젝트에 대한 &lt;code>.yaml&lt;/code> 파일 내, 다음 필드를 윟나 값들을 설정해 줘야한다.&lt;/p>
&lt;ul>
&lt;li>apiVersion: 해당 오브젝트를 생성하기 위해 사용하고 있는 쿠버네티스 API 버전&lt;/li>
&lt;li>kind: 오브젝트의 종류&lt;/li>
&lt;li>metadata: 오브젝트를 유일하게 구분지어 줄 데이터(이름 문자열, UID, 선택적인 네임스페이스 등)&lt;/li>
&lt;/ul>
&lt;p>spec에 대한 정확한 포맷은 모든 쿠버네티스 오브젝트마다다르고, 그 오브젝트 특유의 중첩된 필드를 포함한다.&lt;/p>
&lt;p>&lt;a class="link" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/" target="_blank" rel="noopener"
>https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/&lt;/a>&lt;/p></description></item><item><title>쿠버네티스: 2. 쿠버네티스 컴포넌트</title><link>https://codemario318.github.io/post/kubernetes_2/</link><pubDate>Fri, 21 Apr 2023 13:42:47 +0900</pubDate><guid>https://codemario318.github.io/post/kubernetes_2/</guid><description>&lt;img src="https://codemario318.github.io/post/kubernetes_2/kubernetes_cover.webp" alt="Featured image of post 쿠버네티스: 2. 쿠버네티스 컴포넌트" />&lt;h2 id="쿠버네티스-클러스터">쿠버네티스 클러스터&lt;/h2>
&lt;p>&lt;img src="https://codemario318.github.io/post/kubernetes_2/kubernetes_2_1.svg"
loading="lazy"
>&lt;/p>
&lt;p>컨테이너화 된 애프릴케이션을 실행하는 노드라고 하는 워커 머신의 집합으로, 모든 클러스터는 최소 한 개의 워커 노드를 가진다.&lt;/p>
&lt;p>워커 노드는 애플리케이션의 구성요소인 파드를 호스트하며, 컨트롤 플레인은 워커 노드와 클러스터 내 파드를 관리한다.&lt;/p>
&lt;p>프로덕션 환경에서는 일반적으로 컨트롤 플레인이 여러 컴퓨터에 걸쳐 실행되고, 클러스터는 일반적으로 여러 노드를 실행하므로 내결함성과 고가용성이 제공된다.&lt;/p>
&lt;h2 id="컨트롤-플레인-컴포넌트">컨트롤 플레인 컴포넌트&lt;/h2>
&lt;p>컨트롤 플레인 컴포넌트는 클러스터에 관한 전반적인 결정 (스케줄링 등)을 수행하고 클러스터 이벤트를 감지하고 반응한다.&lt;/p>
&lt;p>컨트롤 플레인 컴포넌트는 클러스터 내 어떠한 머신에서도 동작할 수 있으나, 간결성 유지를 위해 &lt;strong>구성 스크립트는 보통 동일 머신 상에 모든 컨트롤 플레인 컴포넌트를 구동&lt;/strong>시키고, 사용자 컨테이너는 해당 머신 상에 동작시키지 않는다.&lt;/p>
&lt;h3 id="kube-apiserver">kube-apiserver&lt;/h3>
&lt;blockquote>
&lt;p>API 서버는 쿠버네티스 API를 노출하는 쿠버네티스 컨트롤 플레인 컴포넌트이다. API 서버는 쿠버네티스 컨트롤 플레인의 프론트 엔드이다.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a class="link" href="https://coffeewhale.com/apiserver" target="_blank" rel="noopener"
>쿠버네티스 API서버는 정말 그냥 API서버라구욧&lt;/a>&lt;/p>
&lt;p>kube-apiserver는 마스터 노드의 중심에서 모든 클라이언트, 컴포넌트로 부터 오는 요청들을 받는 REST API 서버이다.&lt;/p>
&lt;p>&lt;code>kubectl&lt;/code>와 &lt;code>Kubernetes SDK&lt;/code>를 이용할 수 있지만, 직접 REST API 호출로 쿠버네티스와 통신할 수 있다.&lt;/p>
&lt;p>쿠버네티스 API를 사용하여 애플리케이션을 작성하는 경우 클라이언트 라이브러리 중 하나를 사용하는 것이 좋다.
&lt;a class="link" href="https://kubernetes.io/ko/docs/reference/using-api/client-libraries/" target="_blank" rel="noopener"
>https://kubernetes.io/ko/docs/reference/using-api/client-libraries/&lt;/a>&lt;/p>
&lt;h3 id="etcd">etcd&lt;/h3>
&lt;blockquote>
&lt;p>모든 클러스터 데이터를 담는 쿠버네티스 뒷단의 저장소로 사용되는 일관성 및 고가용성 키-값 저장소.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a class="link" href="https://tech.kakao.com/2021/12/20/kubernetes-etcd/" target="_blank" rel="noopener"
>Kubernetes 운영을 위한 etcd 기본 동작 원리의 이해&lt;/a>&lt;/p>
&lt;p>kubernetes는 기반 스토리지(backing storage)로 etcd를 사용하고 있고, 모든 데이터가 etcd에 보관된다(클러스터에 어떤 노드가 몇 개나 있고 어떤 파드가 어떤 노드에서 동작하고 있는지등 ). 동작중인 클러스터의 etcd 데이터베이스가 유실된다면 컨테이너 뿐만 아니라 클러스터가 사용하는 모든 리소스는 미아가 된다.&lt;/p>
&lt;blockquote>
&lt;p>따라서 쿠버네티스 클러스터에서 etcd를 뒷단 저정소로 사용한다면, 이 데이터를 백업하는 계획을 필수적으로 갖추어야 한다.&lt;/p>
&lt;/blockquote>
&lt;h3 id="kube-scheduler">kube-scheduler&lt;/h3>
&lt;blockquote>
&lt;p>노드가 배정되지 않은 새로 생성된 파드를 감지하고, 실행할 노드를 선택하는 컨트롤 플레인 컴포넌트.&lt;/p>
&lt;/blockquote>
&lt;p>아래 항목들에 대해 고려하여 스케줄링이 이루어진다.&lt;/p>
&lt;ul>
&lt;li>리소스에 대한 개별 및 총체적 요구 사항&lt;/li>
&lt;li>하드웨어/소프트웨어/정책적 제약&lt;/li>
&lt;li>어피니티(affinity) 및 안티-어피니티(anti-affinity) 명세&lt;/li>
&lt;li>데이터 지역성&lt;/li>
&lt;li>워크로드간 간섭&lt;/li>
&lt;li>데드라인&lt;/li>
&lt;/ul>
&lt;h3 id="kube-controller-manager">kube-controller-manager&lt;/h3>
&lt;p>컨트롤러 프로세스를 실행하는 컨트롤 플레인 컴포넌트.&lt;/p>
&lt;p>각 컨트롤러는 논리적으로 분리된 프로세스이지만, 복잡성을 낮추기 위해 모두 단일 바이너리로 컴파일되고 단일 프로세스 내에서 실행된다.&lt;/p>
&lt;ul>
&lt;li>노드 컨트롤러: 노드가 다운되었을 때 통지와 대응에 관한 책임을 가진다.&lt;/li>
&lt;li>잡 컨트롤러: 일회성 작업을 나타내는 잡 오브젝트를 감시한 다음, 해당 작업을 완료할 때까지 동작하는 파드를 생성한다.&lt;/li>
&lt;li>엔드포인트 컨트롤러: 엔드포인트 오브젝트를 채운다.(서비스와 파드를 연결시킨다.)&lt;/li>
&lt;li>서비스 어카운트 &amp;amp; 토큰 컨트롤러: 새로운 네임스페이스에 대한 기본 계정과 API 접근 토큰을 생성한다.&lt;/li>
&lt;/ul>
&lt;h3 id="cloud-controller-manager">cloud-controller-manager&lt;/h3>
&lt;blockquote>
&lt;p>클라우드 별 컨트롤 로직을 포함하는 쿠버네티스 컨트롤 플레인 컴포넌트이다.&lt;/p>
&lt;/blockquote>
&lt;p>클라우드 컨트롤러 매니저를 통해 클러스터를 클라우드 공급자의 API에 연결하고, 해당 클라우드 플랫폼과 상호 작용하는 컴포넌트와 클러스터와만 상호 작용하는 컴포넌트를 구분할 수 있게 해준다.&lt;/p>
&lt;p>클라우드 제공자 전용 컨트롤러만 실행한다. 자신의 사내 또는 PC 내부의 학습 환경에서 쿠버네티스를 실행 중인 경우 클러스터에는 클라우드 컨트롤러 매니저가 없다.&lt;/p>
&lt;p>kube-controller-manager와 마찬가지로 cloud-controller-manager는 논리적으로 독립적인 여러 컨트롤 루프를 단일 프로세스로 실행하는 단일 바이너리로 결합한다. 수평으로 확장(두 개 이상의 복제 실행)해서 성능을 향상시키거나 장애를 견딜 수 있다.&lt;/p>
&lt;ul>
&lt;li>노드 컨트롤러: 노드가 응답을 멈춘 후 클라우드 상에서 삭제되었는지 판별하기 위해 클라우드 제공 사업자에게 확인하는 것&lt;/li>
&lt;li>라우트 컨트롤러: 기본 클라우드 인프라에 경로를 구성하는 것&lt;/li>
&lt;li>서비스 컨트롤러: 클라우드 제공 사업자 로드밸런서를 생성, 업데이트 그리고 삭제하는 것&lt;/li>
&lt;/ul>
&lt;h2 id="노드-컴포넌트">노드 컴포넌트&lt;/h2>
&lt;p>노드 컴포넌트는 동작 중인 파드를 유지시키고 쿠버네티스 런타임 환경을 제공하며, 모든 노드 상에서 동작한다.&lt;/p>
&lt;h3 id="kubelet">kubelet&lt;/h3>
&lt;p>클러스터의 각 노드에서 실행되는 에이전트. &lt;code>kubelet&lt;/code>은 파드에서 컨테이너가 확실하게 동작하도록 관리한다.&lt;/p>
&lt;p>다양한 메커니즘을 통해 제공된 파드 스펙(PodSpec)의 집합을 받아 컨테이너가 해당 파드 스펙에 따라 건강하게 동작하는 것을 확실히 한다.&lt;/p>
&lt;p>kubelet은 쿠버네티스를 통해 생성되지 않는 컨테이너는 관리하지 않는다.&lt;/p>
&lt;h3 id="kube-proxy">kube-proxy&lt;/h3>
&lt;p>클러스터의 각 노드에서 실행되는 네트워크 프록시로, 크버네티스의 서비스 개념 구현부이다.&lt;/p>
&lt;p>노드의 네트워크 규칙을 관리한다. 네트워크 규칙이 내부 네트워크 세션이나 클러스터 바깥에서 파드로 네트워크 통신을 하도록 해준다.&lt;/p>
&lt;p>운영체제에 가용한 패킷 필터링 계층이 있는 경우, 이를 사용한다. 그렇지 않으면, kube-proxy는 트래픽 자체를 포워드한다.&lt;/p>
&lt;h3 id="컨테이너-런타임">컨테이너 런타임&lt;/h3>
&lt;p>컨테이너 런타임은 컨테이너 실행을 담당하는 소프트웨어이다.&lt;/p>
&lt;p>쿠버네티스는 containerd, CRI-O와 같은 컨테이너 런타임 및 모든 Kubernetes CRI(컨테이너 런타임 인터페이스) 구현체를 지원한다.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>CRI란?&lt;/strong>&lt;br>
클러스터 컴포넌트를 다시 컴파일하지 않아도 Kubelet이 다양한 컨테이너 런타임을 사용할 수 있도록 하는 플러그인 인터페이스이다.&lt;/p>
&lt;/blockquote></description></item><item><title>쿠버네티스: 1. 쿠버네티스란 무엇인가?</title><link>https://codemario318.github.io/post/kubernetes_1/</link><pubDate>Fri, 21 Apr 2023 13:19:47 +0900</pubDate><guid>https://codemario318.github.io/post/kubernetes_1/</guid><description>&lt;img src="https://codemario318.github.io/post/kubernetes_1/kubernetes_cover.webp" alt="Featured image of post 쿠버네티스: 1. 쿠버네티스란 무엇인가?" />&lt;h2 id="쿠버네티스란">쿠버네티스란?&lt;/h2>
&lt;p>쿠버네티스는 컨테이너화된 워크로드와 서비스를 관리하기 위한 이식성이 있고, 확장 가능한 오픈소스 플랫폼이다.&lt;/p>
&lt;ul>
&lt;li>컨테이너화된 워크로드와 서비스를 관리하기 위한 이식성이 있다.&lt;/li>
&lt;li>확장가능한 오픈소스 플랫폼이다.&lt;/li>
&lt;li>선언적 구성과 자동화를 모두 용이하게 해준다.&lt;/li>
&lt;li>크고 빠르게 성장하는 생태계를 가지고 있다.&lt;/li>
&lt;li>쿠버네티스 서비서, 기술 지원 및 도구는 어디서나 쉽게 이용할 수 있다.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;strong>컨테이너 장점&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>기민한 애플리케이션 생성과 배포&lt;/strong>&lt;br>
VM 이미지를 사용하는 것에 비해 컨테이너 이미지 생성이 보다 쉽고 효율적임.&lt;/li>
&lt;li>&lt;strong>지속적인 개발, 통합 및 배포&lt;/strong>&lt;br>
안정적이고 주기적으로 컨테이너 이미지를 빌드해서 배포할 수 있고 (이미지의 불변성 덕에) 빠르고 효율적으로 롤백할 수 있다.&lt;/li>
&lt;li>&lt;strong>개발과 운영의 관심사 분리&lt;/strong>&lt;br>
배포 시점이 아닌 빌드/릴리스 시점에 애플리케이션 컨테이너 이미지를 만들기 때문에, 애플리케이션이 인프라스트럭처에서 분리된다.&lt;/li>
&lt;li>&lt;strong>가시성(observability)&lt;/strong>&lt;br>
OS 수준의 정보와 메트릭에 머무르지 않고, 애플리케이션의 헬스와 그 밖의 시그널을 볼 수 있다.&lt;/li>
&lt;li>&lt;strong>개발, 테스팅 및 운영 환경에 걸친 일관성&lt;/strong>&lt;br>
랩탑에서도 클라우드에서와 동일하게 구동된다.&lt;/li>
&lt;li>&lt;strong>클라우드 및 OS 배포판 간 이식성&lt;/strong>&lt;br>
Ubuntu, RHEL, CoreOS, 온-프레미스, 주요 퍼블릭 클라우드와 어디에서든 구동된다.&lt;/li>
&lt;li>&lt;strong>애플리케이션 중심 관리&lt;/strong>&lt;br>
가상 하드웨어 상에서 OS를 실행하는 수준에서 논리적인 리소스를 사용하는 OS 상에서 애플리케이션을 실행하는 수준으로 추상화 수준이 높아진다.&lt;/li>
&lt;li>&lt;strong>느슨하게 커플되고, 분산되고, 유연하며, 자유로운 마이크로서비스&lt;/strong>
애플리케이션은 단일 목적의 머신에서 모놀리식 스택으로 구동되지 않고 보다 작고 독립적인 단위로 쪼개져서 동적으로 배포되고 관리될 수 있다.&lt;/li>
&lt;li>&lt;strong>리소스 격리&lt;/strong>&lt;br>
애플리케이션 성능을 예측할 수 있다.&lt;/li>
&lt;li>&lt;strong>리소스 사용량&lt;/strong>&lt;br>
고효율 고집적.&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="쿠버네티스가-왜-필요하고-무엇을-할-수-있나">쿠버네티스가 왜 필요하고 무엇을 할 수 있나&lt;/h2>
&lt;p>&lt;img src="https://codemario318.github.io/post/kubernetes_1/kubernetes_1_1.svg"
loading="lazy"
>&lt;/p>
&lt;p>기존 &amp;ldquo;전통적인 배포&amp;rdquo;, &amp;ldquo;가상화된 배포&amp;quot;를 거치며 &amp;ldquo;컨테이너를 통한 배포&amp;rdquo; 까지 발전해왔다.&lt;/p>
&lt;p>컨테이너를 통한 개발 환경은 애플리케이션을 포장하고 실행하는 좋은 방법이지만, 프로덕션 환경에서는 애플리케이션을 실행하는 컨테이너를 관리하고 가동 중지 시간이 없는지 확인해야 하는 등 여러 작업이 필요하게 된다.&lt;/p>
&lt;p>쿠버네티스는 분산 시스템을 탄력적으로 실행하기 위한 프레임 워크를 제공한다. 애플리케이션의 확장과 장애 조치를 처리하고, 배포 패턴 등을 제공한다. 예를 들어, 쿠버네티스는 시스템의 카나리아 배포를 쉽게 관리 할 수 있다.&lt;/p>
&lt;h3 id="서비스-디스커버리와-로드-밸런싱">서비스 디스커버리와 로드 밸런싱 &lt;/h3>
&lt;p>쿠버네티스는 DNS 이름을 사용하거나 자체 IP 주소를 사용하여 컨테이너를 노출할 수 있다. 컨테이너에 대한 트래픽이 많으면, 쿠버네티스는 네트워크 트래픽을 로드밸런싱하고 배포하여 배포가 안정적으로 이루어질 수 있다.&lt;/p>
&lt;h3 id="스토리지-오케스트레이션">스토리지 오케스트레이션&lt;/h3>
&lt;p>쿠버네티스를 사용하면 로컬 저장소, 공용 클라우드 공급자 등과 같이 원하는 저장소 시스템을 자동으로 탑재 할 수 있다.&lt;/p>
&lt;h3 id="자동화된-롤아웃과-롤백">자동화된 롤아웃과 롤백 &lt;/h3>
&lt;p>쿠버네티스를 사용하여 배포된 컨테이너의 원하는 상태를 서술할 수 있으며 현재 상태를 원하는 상태로 설정한 속도에 따라 변경할 수 있다. 예를 들어 쿠버네티스를 자동화해서 배포용 새 컨테이너를 만들고, 기존 컨테이너를 제거하고, 모든 리소스를 새 컨테이너에 적용할 수 있다.&lt;/p>
&lt;h3 id="자동화된-빈-패킹bin-packing">자동화된 빈 패킹(bin packing)&lt;/h3>
&lt;p>컨테이너화된 작업을 실행하는데 사용할 수 있는 쿠버네티스 클러스터 노드를 제공한다. 각 컨테이너가 필요로 하는 CPU와 메모리(RAM)를 쿠버네티스에게 지시한다. 쿠버네티스는 컨테이너를 노드에 맞추어서 리소스를 가장 잘 사용할 수 있도록 해준다.&lt;/p>
&lt;h3 id="자동화된-복구self-healing">자동화된 복구(self-healing)&lt;/h3>
&lt;p>쿠버네티스는 실패한 컨테이너를 다시 시작하고, 컨테이너를 교체하며, &amp;lsquo;사용자 정의 상태 검사&amp;rsquo;에 응답하지 않는 컨테이너를 죽이고, 서비스 준비가 끝날 때까지 그러한 과정을 클라이언트에 보여주지 않는다.&lt;/p>
&lt;h3 id="시크릿과-구성-관리">시크릿과 구성 관리 &lt;/h3>
&lt;p>쿠버네티스를 사용하면 암호, OAuth 토큰 및 SSH 키와 같은 중요한 정보를 저장하고 관리 할 수 있다. 컨테이너 이미지를 재구성하지 않고 스택 구성에 시크릿을 노출하지 않고도 시크릿 및 애플리케이션 구성을 배포 및 업데이트 할 수 있다.&lt;/p></description></item><item><title>0. 컨테이너와 도커</title><link>https://codemario318.github.io/post/docker_2/</link><pubDate>Sat, 15 Apr 2023 16:30:25 +0900</pubDate><guid>https://codemario318.github.io/post/docker_2/</guid><description>&lt;img src="https://codemario318.github.io/post/docker_2/docker_cover.png" alt="Featured image of post 0. 컨테이너와 도커" />&lt;p>컨테이너는 애플리케이션, 실행 라이브러리, 시스템 도구, 시스템 라이브러리 등을 포함하여 애플리케이션과 그 애플리케이션을 실행하는 환경을 패키징하여 이식성이 뛰어난 소프트웨어 패키지로 만든것이다.&lt;/p>
&lt;ul>
&lt;li>컨테이너화를 통해 더욱 신속하게 작업을 진행할 수 있다.&lt;/li>
&lt;li>효율적으로 소프트웨어를 배포할 수 있다.&lt;/li>
&lt;li>매우 높은 수준의 확장성을 확보할 수 있다.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;strong>서버 가상화&lt;/strong>
컴퓨터의 성능을 효율적으로 사용하기 위해 가상화 기술이 등장하였다.
서버 관리자 입장에서 리소스 사용률이 적은 서버들은 낭비라고 생각할 수 있다. 그렇다고 모든 서비스를 한 서버 안에 올린다면 안정성에 문제가 생길 수 있다. 이에 따라 안정성을 높이며 리소스를 최대한 활용할 수 있는 방법으로 고안된 방법이 서버 가상화이다.&lt;/p>
&lt;/blockquote>
&lt;h2 id="컨테이너-정의">컨테이너 정의&lt;/h2>
&lt;p>컨테이너는 소프트웨어 서비스를 실행하는 데 필요한 특정 버전의 프로그래밍 언어 런타임 및 라이브러리와 같은 종속 항목과 애플리케이션 코드를 함께 포함하는 경량 패키지이다.&lt;/p>
&lt;p>컨테이너는 운영체제 수준에서 CPU, 메모리, 스토리지, 네트워크 리소스를 쉽게 공유할 수 있게 해주며 컨테이너가 실제로 실행되는 환경에서 애플리케이션을 추상화 할 수 있는 논리 패키징 매커니즘을 제공한다.&lt;/p>
&lt;h2 id="컨테이너의-이점">컨테이너의 이점&lt;/h2>
&lt;h3 id="책임-분리">책임 분리&lt;/h3>
&lt;p>컨테이너화를 통해 책임을 깔끔하게 분리할 수 있다.&lt;/p>
&lt;p>개발자는 애플리케이션의 로직과 종속 항목에 집중하고, IT 운영팀은 특정 소프트웨어 버전 및 구성과 같은 애플리케이션의 세부 요소 대신 배포 및 관리에 집중할 수 있다.&lt;/p>
&lt;h3 id="워크로드-이동성">워크로드 이동성&lt;/h3>
&lt;p>컨테이너는 Linux, Windows, Mac 등 운영체제를 가리지 않고, 가상머신 물리적 서버, 개발자 컴퓨터, 데이터 센터, 온프레미스 환경, 퍼블릭 클라우드 등 사실상 어느 환경에서나 구동되므로 개발 및 배포가 크게 쉬워진다.&lt;/p>
&lt;h3 id="애플리케이션-격리">애플리케이션 격리&lt;/h3>
&lt;p>컨테이너는 운영체제 수준에서 CPU, 메모리, 스토리지, 네트워크 리소스를 가상화 하므로 개발자에게 다른 애플리케이션으로부터 논리적으로 격리된 OS 환경을 제공한다.&lt;/p>
&lt;h2 id="컨테이너와-vm의-차이">컨테이너와 VM의 차이&lt;/h2>
&lt;p>VM은 기본 하드웨어에 대한 엑세스 권한을 갖는 호스트 운영체제 위에서 Linux또는 Windows 같은 게스트 운영체제를 실행하기 때문에 컨테이너와 비교되는 경우가 많다.&lt;/p>
&lt;p>컨테이너는 가상 머신과 마찬가지로 애필리케이션을 관련 라이브러리 및 종속 항목과 함께 패키지로 묶어 소프트웨어 서비스 구동을 위한 격리 환경을 마련해준다. 그러나 컨테이너를 사용하면 훨씬 작은 단위로 업무를 수행할 수 있어 이점이 훨씬 많다.&lt;/p>
&lt;ul>
&lt;li>VM 보다 훨씬 더 가볍다.&lt;/li>
&lt;li>OS 수준에서 가상화되고, VM은 하드웨어 수준에서 가상화된다.&lt;/li>
&lt;li>OS 커널을 공유하며 VM에서 필요한 것 보다 훨씬 적은 메모리를 사용한다.&lt;/li>
&lt;/ul>
&lt;h2 id="컨테이너의-용도">컨테이너의 용도&lt;/h2>
&lt;p>애플리케이션을 실제 구동 환경으로부터 추상화할 수 있는 논리 패키징 메커니즘을 제공함. 이러한 분리를 통해 어떤 환경에서도 컨테이너 기반 애플리케이션을 쉽게 지속적으로 배포할 수 있음.&lt;/p>
&lt;h3 id="민첩한-개발">민첩한 개발&lt;/h3>
&lt;p>컨테이너를 사용하면 개발자가 종속 항목과 환경에 미치는 영향을 신경쓰지 않고 훨씬 더 빠르게 개발을 진행할 수 있다.&lt;/p>
&lt;h3 id="효율적인-운영">효율적인 운영&lt;/h3>
&lt;p>컨테이너는 경량이며 필요한 컴퓨팅 리소스만 사용하면 된다. 따라서 애플리케이션을 효율적으로 구동할 수 있다.&lt;/p>
&lt;h3 id="폭넓은-구동-환경">폭넓은 구동 환경&lt;/h3>
&lt;p>컨테이너는 거의 모든 곳에서 구동할 수 있어 환경에 영향 없이 사용할 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="컨테이너-기술">컨테이너 기술&lt;/h2>
&lt;h3 id="namespaces">Namespaces&lt;/h3>
&lt;p>VM에서는 각 게스트 머신별로 독립적인 공간을 제공하고 서로가 충돌하지 않도록 하는 기능을 갖추고 있다.&lt;/p>
&lt;p>리눅스에서는 이와 동일한 역할을 하는 namespaces 기능을 커널에 내장하고 있다.&lt;/p>
&lt;ul>
&lt;li>mnt(파일시스템 마운트): 호스트 파일 시스템에 구애받지 않고 독립적으로 파일 시스템을 마운트하거나 언마운트 가능&lt;/li>
&lt;li>pid(프로세스): 독립적은 프로세스 공간을 할당&lt;/li>
&lt;li>net(네트워크): namespace간 network 충돌 방지(중복 포트 바인딩 등)&lt;/li>
&lt;li>ipc(SystemV IPC): 프로세스간의 독립적인 통신통로 할당&lt;/li>
&lt;li>uts(hostname): 독립적인 hostname 할당&lt;/li>
&lt;li>user(UID): 독립적인 사용자 할당&lt;/li>
&lt;/ul>
&lt;p>namespaces를 지원하는 리눅스 커널을 사용하고 있다면 다음 명령어를 통해 바로 namespace를 만들어 실행할 수 있다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ sudo unshare --fork --pid --mount-proc bash
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># ps aux&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">root &lt;span class="m">1&lt;/span> 4.0 0.0 &lt;span class="m">17656&lt;/span> &lt;span class="m">6924&lt;/span> pts/9 S 22:06 0:00 bash
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">root &lt;span class="m">2&lt;/span> 0.0 0.0 &lt;span class="m">30408&lt;/span> &lt;span class="m">1504&lt;/span> pts/9 R+ 22:06 0:00 ps aux
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>PID namespace에 실행한 bash가 PID 1로 할당되어 있고, 바로 다음 실행한 ps aux 명령어가 PID 2를 배정 받았다.&lt;/p>
&lt;h3 id="cgroups---control-groups">cgroups - Control Groups&lt;/h3>
&lt;p>cgrups는 자원(resources)에 대한 제어를 가능하게 해주는 리눅스 커널 기능으로 아래와 같은 자원들을 제어할 수 있다.&lt;/p>
&lt;ul>
&lt;li>메모리&lt;/li>
&lt;li>CPU&lt;/li>
&lt;li>I/O&lt;/li>
&lt;li>네트워크&lt;/li>
&lt;li>device 노드 (&lt;code>/dev/&lt;/code>)&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">// &lt;span class="s2">&amp;#34;dhlee&amp;#34;&lt;/span> 유저가 소유하하며,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">// 메모리를 제어할 그룹 &lt;span class="s2">&amp;#34;testgrp&amp;#34;&lt;/span> 생성
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ sudo cgcreate -a dhlee -g memory:testgrp
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ ls -alh /sys/fs/cgroup/memory/testgrp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">합계 &lt;span class="m">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">drwxr-xr-x &lt;span class="m">2&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 .
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">dr-xr-xr-x &lt;span class="m">8&lt;/span> root root &lt;span class="m">0&lt;/span> 7월 &lt;span class="m">7&lt;/span> 15:30 ..
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 cgroup.clone_children
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--w--w--w- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 cgroup.event_control
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 cgroup.procs
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.failcnt
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--w------- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.force_empty
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.kmem.failcnt
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.kmem.limit_in_bytes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.kmem.max_usage_in_bytes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-r--r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.kmem.slabinfo
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.kmem.tcp.failcnt
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.kmem.tcp.limit_in_bytes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.kmem.tcp.max_usage_in_bytes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-r--r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.kmem.tcp.usage_in_bytes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-r--r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.kmem.usage_in_bytes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.limit_in_bytes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.max_usage_in_bytes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.move_charge_at_immigrate
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-r--r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.numa_stat
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.oom_control
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">---------- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.pressure_level
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.soft_limit_in_bytes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-r--r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.stat
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.swappiness
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-r--r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.usage_in_bytes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 memory.use_hierarchy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> ssut root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 notify_on_release
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rw-r--r-- &lt;span class="m">1&lt;/span> root root &lt;span class="m">0&lt;/span> 8월 &lt;span class="m">8&lt;/span> 23:19 tasks
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;code>/sys/fs/cgroup/*/groupname&lt;/code> 경로에 있는 파일을 통해 그룹의 여러 옵션들을 변경 가능&lt;/p>
&lt;/blockquote>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">// testgrp 최대 메모리 사용량을 2MB로 제한
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ &lt;span class="nb">echo&lt;/span> &lt;span class="m">2000000&lt;/span> &amp;gt; /sys/fs/cgroup/memory/testgrp/memory.kmem.limit_in_bytes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="정리">정리&lt;/h2>
&lt;p>&lt;img src="https://codemario318.github.io/post/docker_2/docker_0.png"
width="1024"
height="768"
srcset="https://codemario318.github.io/post/docker_2/docker_0_hue9d68cbf9111f86fb8f3a72a1a21b38c_93810_480x0_resize_box_3.png 480w, https://codemario318.github.io/post/docker_2/docker_0_hue9d68cbf9111f86fb8f3a72a1a21b38c_93810_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="320px"
>&lt;/p>
&lt;p>&lt;code>LXC&lt;/code>, &lt;code>LibContainer&lt;/code>, &lt;code>runC&lt;/code> 등은 위에서 설명한 &lt;code>cgrups&lt;/code>, &lt;code>namespaces&lt;/code>를 표준으로 정의해둔 &lt;strong>OCI(Open Container Initative&lt;/strong>) 스펙을 구현한 컨테이너 기술의 구현체이다.&lt;/p>
&lt;p>LXC는 캐노니컬(Canonical)이 지원하고 있는 리눅스 컨테이너 프로젝트로 Docker의 경우 1.8 이전 버전까지 LXC를 이용해 구현해서 사용되었다.&lt;/p>
&lt;p>이후 Docker는 libcontainer → runC(libcontainer의 리팩토링 구현체)로 자체 구현체를 갖게 되었다.&lt;/p>
&lt;h2 id="docker">Docker&lt;/h2>
&lt;p>&lt;img src="https://codemario318.github.io/post/docker_2/docker_1.png"
width="639"
height="285"
srcset="https://codemario318.github.io/post/docker_2/docker_1_huc907679b82100c393cee3016a545dcdb_13595_480x0_resize_box_3.png 480w, https://codemario318.github.io/post/docker_2/docker_1_huc907679b82100c393cee3016a545dcdb_13595_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="224"
data-flex-basis="538px"
>&lt;/p>
&lt;p>도커는 1.11버전부터 위와 같은 구조로 작동한다.&lt;/p>
&lt;p>containerd는 OCI 구현체(주로 runC)를 이용해 container를 관리해주는 deamon이다.&lt;/p>
&lt;p>도커 엔진 자체는 이미지, 네트워크, 디스크 등의 리소스 관리 역할을 수행하며, 여기서 도커 엔진과 containerd 각각이 완전히 분리되어 도커 엔진을 재시작 해도, 컨테이너 재시작 없이 사용할 수 있다.&lt;/p>
&lt;p>위와 같이 각각 역할이 분리됨에 따라 도커는 4개의 독립적인 프로세스로 작동하고 있다. (&lt;code>docker&lt;/code>, &lt;code>docker-containerd&lt;/code>, &lt;code>docker-containerd-shim&lt;/code>, &lt;code>docker-runc&lt;/code>)&lt;/p></description></item></channel></rss>