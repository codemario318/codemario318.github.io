<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Distributed System on Mario Blog</title><link>https://codemario318.github.io/tags/distributed-system/</link><description>Recent content in Distributed System on Mario Blog</description><generator>Hugo -- gohugo.io</generator><language>ko</language><lastBuildDate>Sun, 27 Jul 2025 16:38:00 +0900</lastBuildDate><atom:link href="https://codemario318.github.io/tags/distributed-system/index.xml" rel="self" type="application/rss+xml"/><item><title>Raft 합의 알고리즘</title><link>https://codemario318.github.io/post/common/raft/</link><pubDate>Sun, 27 Jul 2025 16:38:00 +0900</pubDate><guid>https://codemario318.github.io/post/common/raft/</guid><description>&lt;img src="https://codemario318.github.io/post/common/raft/cover.png" alt="Featured image of post Raft 합의 알고리즘" />&lt;p>오늘은 저번에 작성한 &lt;a class="link" href="https://codemario318.github.io/common/quorum-based-consensus" >정족수 합의&lt;/a>에 이어서 &lt;strong>Raft 합의 알고리즘&lt;/strong>을 살펴보려고 합니다.&lt;/p>
&lt;p>저번 글에도 설명했지만, 분산 시스템에서 가장 어려운 문제 중 하나는 &lt;strong>모든 노드가 하나의 동일한 결정을 내리는 것, 즉 &lt;em>합의(Consensus)&lt;/em>&lt;/strong> 입니다.&lt;/p>
&lt;p>이런 상황에서도 시스템이 정상적으로 동작하려면 신뢰할 수 있는 합의 메커니즘이 필요하며, 그 중 하나가 Raft입니다.&lt;/p>
&lt;h2 id="raft란">Raft란?&lt;/h2>
&lt;blockquote>
&lt;p>Raft는 Paxos와 동일한 목표를 가지면서도 &lt;strong>이해하기 쉽도록 설계된 합의 알고리즘&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>2014년 Diego Ongaro와 John Ousterhout가 발표한 논문 “In Search of an Understandable Consensus Algorithm” 에서는 Paxos의 복잡성을 극복할 수 있는 대안으로 Raft를 제시했습니다.&lt;/p>
&lt;p>Raft는 다음의 세 가지 주요 목표를 가지고 있습니다.&lt;/p>
&lt;ul>
&lt;li>이해하기 쉬울 것&lt;/li>
&lt;li>정상적인 상태 유지뿐 아니라 복구 절차도 직관적일 것&lt;/li>
&lt;li>실제 구현에 적합할 것&lt;/li>
&lt;/ul>
&lt;p>&lt;del>Paxos는 다음 글에 작성해보겠습니다.&lt;/del>&lt;/p>
&lt;h2 id="raft의-구조-리더-중심">Raft의 구조: 리더 중심&lt;/h2>
&lt;p>Raft는 노드 간의 역할을 명학히 나눕니다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Leader(1개)&lt;/strong>: 모든 클라이언트 요청을 받고, 로그를 다른 노드에 전파&lt;/li>
&lt;li>&lt;strong>Follower(N개)&lt;/strong>: 리더의 명령을 따름&lt;/li>
&lt;li>&lt;strong>Candidate&lt;/strong>: 리더 선출을 위해 임시로 전환되는 상태&lt;/li>
&lt;/ul>
&lt;p>모든 노드는 기본적으로 팔로워로 싲가하며, 주기적으로 리더로부터 heartbeat를 받지 못하면 후보 상태가 되어 선거를 시작합니다.&lt;/p>
&lt;h3 id="리더-선출">리더 선출&lt;/h3>
&lt;p>리더는 항상 하나만 존재해야합니다. 이를 위해 임기(term) 개념이 도입하고 있습니다.&lt;/p>
&lt;ul>
&lt;li>일정 시간 동안 리더의 heartbeat를 받지 못하면, follower는 선거를 시작&lt;/li>
&lt;li>본인을 리더로 추천하고 다른 노드의 투표를 요청&lt;/li>
&lt;li>&lt;strong>과반수의 투표&lt;/strong>를 얻은 후보는 리더로 선출&lt;/li>
&lt;/ul>
&lt;p>이 과정을 토해 네트워크 지연이나 장애 상황에서도 안정적으로 새로운 리더를 선출할 수 있게 해줍니다.&lt;/p>
&lt;h3 id="로그-복제-log-replication">로그 복제 Log Replication&lt;/h3>
&lt;p>리더가 되면 클라이언트 요청을 받아 로그 항목으로 저장하고, 팔로워들에게 &lt;strong>AppendEntries RPC&lt;/strong>를 보냅니다.&lt;/p>
&lt;p>복제는 다음과 같은 방식으로 수행됩니다.&lt;/p>
&lt;ol>
&lt;li>리더는 새로운 로그 항목을 자신의 로그에 추가&lt;/li>
&lt;li>팔로워들에게 &lt;strong>AppendEntries&lt;/strong> 메시지 전송&lt;/li>
&lt;li>팔로워가 수신 성공 시 응답&lt;/li>
&lt;li>&lt;strong>과반수 이상&lt;/strong>의 팔로워가 응답하면 커밋(Commit) 처리&lt;/li>
&lt;li>리더가 커밋을 팔로워들에게 전파&lt;/li>
&lt;/ol>
&lt;p>로그 복제가 실패할 경우에는 이전 로그 상태로 되돌아가며 재동기화하여 결과적 일관성(eventually consistent)를 보장합니다.&lt;/p>
&lt;h3 id="안정성-보장">안정성 보장&lt;/h3>
&lt;p>Raft는 다음과 같은 메커니즘으로 안전성을 보장합니다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Log Matching Property&lt;/strong>
&lt;ul>
&lt;li>동일한 인덱스와 term의 로그 항목은 동일해야함&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Leader Completeness Property&lt;/strong>
&lt;ul>
&lt;li>커밋된 로그는 다음 term의 리더 로그에 반드시 포함됨&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>팔로워는 오직 리더의 로그에 맞춰 자신을 수정함&lt;/li>
&lt;/ul>
&lt;p>이러한 동작으로 시스템은 리더가 바뀌어도 로그가 일관되게 유지될 수 있습니다.&lt;/p>
&lt;h3 id="시나리오-예시-클라이언트-요청-처리">시나리오 예시: 클라이언트 요청 처리&lt;/h3>
&lt;pre class="mermaid" style="text-align: center;">sequenceDiagram
participant Client
participant Leader
participant Follower1
participant Follower2
Client->>Leader: Append(command)
Leader->>Follower1: AppendEntries
Leader->>Follower2: AppendEntries
Follower1-->>Leader: Success
Follower2-->>Leader: Success
Leader->>All: Commit
Leader-->>Client: Success
&lt;/pre>
&lt;p>좋습니다! 아래는 **“Raft 사용 시 주의할 점”**을 블로그 글의 &lt;strong>하위 섹션 형태&lt;/strong>로 정리하고, 각 주제를 쉽게 이해할 수 있도록 &lt;strong>Mermaid 다이어그램&lt;/strong>도 함께 구성해드렸습니다.&lt;/p>
&lt;hr>
&lt;h2 id="raft-사용-시-주의할-점">Raft 사용 시 주의할 점&lt;/h2>
&lt;p>Raft는 이해하기 쉬운 구조 덕분에 많이 사용되지만, &lt;strong>실제 서비스에 적용할 때는 주의해야 할 점&lt;/strong>이 분명 존재합니다. 특히 고가용성과 성능, 데이터 일관성 측면에서 다양한 고려가 필요합니다.&lt;/p>
&lt;h3 id="과반수-장애-시-시스템-정지">과반수 장애 시 시스템 정지&lt;/h3>
&lt;p>Raft는 &lt;strong>과반수 노드의 응답&lt;/strong>이 있어야 리더를 선출하거나 로그를 커밋할 수 있습니다. 따라서 과반수 이상이 장애나 네트워크 격리로 인해 응답하지 못하면, &lt;strong>쓰기 작업이 모두 중단&lt;/strong>됩니다.&lt;/p>
&lt;pre class="mermaid" style="text-align: center;">graph TD
A[5개 노드 구성] --> B{3개 이상 응답 가능?}
B -- Yes --> C[정상적으로 리더 선출 및 커밋 가능]
B -- No --> D[쓰기 불가, 시스템 정지]
&lt;/pre>
&lt;blockquote>
&lt;p>최소 3개, 가능하면 5개 등 &lt;strong>홀수 구성&lt;/strong>을 유지하고, 과반수가 살아있도록 클러스터를 배치해야 합니다.&lt;/p>
&lt;/blockquote>
&lt;h3 id="네트워크-분할partition-상황-대응">네트워크 분할(Partition) 상황 대응&lt;/h3>
&lt;p>리더가 일부 팔로워들과만 연결된 상태에서 나머지 노드와 &lt;strong>분리(network partition)&lt;/strong> 될 경우,
기존 리더는 자신이 리더인 줄 알고 요청을 받을 수 있지만, 로그 커밋은 실패합니다.&lt;/p>
&lt;pre class="mermaid" style="text-align: center;">sequenceDiagram
participant Leader (격리됨)
participant Client
participant Follower1
participant Follower2 (다수)
Client->>Leader (격리됨): 요청
Leader (격리됨)->>Follower1: AppendEntries
Note over Follower1: quorum 부족
Leader (격리됨)-->>Client: 실패 또는 타임아웃
&lt;/pre>
&lt;blockquote>
&lt;p>클라이언트 타임아웃을 짧게 잡고, 요청을 다른 노드로 재시도하게 하는 전략이 필요합니다.&lt;/p>
&lt;/blockquote>
&lt;h3 id="로그-무한-증가와-스냅샷">로그 무한 증가와 스냅샷&lt;/h3>
&lt;p>Raft는 로그를 계속 누적하며 동작합니다. 아무 처리 없이 장시간 운영하면 &lt;strong>디스크 용량이 부족&lt;/strong>해질 수 있습니다.&lt;/p>
&lt;pre class="mermaid" style="text-align: center;">flowchart TD
A[로그 누적]
A --> B[용량 증가]
B --> C[디스크 부족]
C --> D[성능 저하 또는 장애]
A --> E[주기적 스냅샷 생성]
E --> F[이전 로그 압축/삭제]
F --> G[안정적인 운영]
&lt;/pre>
&lt;blockquote>
&lt;p>주기적으로 스냅샷을 생성하고, 오래된 로그를 안전하게 삭제하는 정책이 필요할 수 있습니다.&lt;/p>
&lt;/blockquote>
&lt;h3 id="리더-선거-충돌">리더 선거 충돌&lt;/h3>
&lt;p>여러 노드가 &lt;strong>동시에 candidate가 되면&lt;/strong>, 과반수 득표에 실패하고 리더가 선출되지 않을 수 있습니다 (split vote).&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>election timeout&lt;/code>을 노드마다 &lt;strong>랜덤하게 설정&lt;/strong>하여 충돌 가능성을 줄일 수 있습니다.&lt;/p>
&lt;/blockquote>
&lt;h3 id="느린-팔로워로-인한-성능-저하">느린 팔로워로 인한 성능 저하&lt;/h3>
&lt;p>느린 follower 노드가 로그 복제 타이밍을 따라가지 못하면, 리더의 커밋 속도도 느려질 수 있습니다.&lt;/p>
&lt;blockquote>
&lt;p>비동기 전파로 느린 노드를 따로 관리하거나, 너무 뒤처진 follower는 로그 스냅샷으로 초기화가 도움이 될 수 있습니다.&lt;/p>
&lt;/blockquote>
&lt;h3 id="클라이언트-요청-중복-처리">클라이언트 요청 중복 처리&lt;/h3>
&lt;p>리더 전환 중, 동일한 클라이언트 요청이 중복될 수 있습니다. 이 때 &lt;strong>같은 명령이 여러 번 실행&lt;/strong>될 수 있습니다.&lt;/p>
&lt;blockquote>
&lt;p>클라이언트 요청마다 고유 ID를 부여하고(멱등성), &lt;strong>중복 요청은 캐싱&lt;/strong> 또는 &lt;strong>deduplication 로직&lt;/strong>으로 방지해야 합니다.&lt;/p>
&lt;/blockquote>
&lt;h3 id="읽기-일관성-주의">읽기 일관성 주의&lt;/h3>
&lt;p>Raft 팔로워는 오래된 데이터를 가지고 있을 수 있기 때문에, &lt;strong>정확한 읽기(read-your-write)&lt;/strong> 를 보장하려면 리더에게 요청해야 합니다.&lt;/p>
&lt;blockquote>
&lt;p>Raft에서는 &lt;code>ReadIndex&lt;/code>, &lt;code>LeaseRead&lt;/code> 같은 기법을 사용해 &lt;strong>선거 없는 일관된 읽기&lt;/strong>를 구현할 수 있습니다.&lt;/p>
&lt;/blockquote>
&lt;h2 id="raft-vs-정족수-합의-어떤-합의-방식이-더-적합할까">Raft vs 정족수 합의: 어떤 합의 방식이 더 적합할까?&lt;/h2>
&lt;p>분산 시스템에서 합의는 필수적인 요소입니다. 이전에 학습했던 정족수 합의와는 어떤 차이점이 있을까요?&lt;/p>
&lt;ul>
&lt;li>&lt;strong>정족수 기반 합의 (Quorum-based Consensus)&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Raft 합의 알고리즘 (Log Replication with Leader Election)&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>두 방식은 목적과 구현 방식이 다릅니다. 아래에서 차이점을 비교해보겠습니다.&lt;/p>
&lt;h3 id="기본-개념의-차이">기본 개념의 차이&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>항목&lt;/th>
&lt;th>정족수 합의&lt;/th>
&lt;th>Raft&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>합의 방식&lt;/td>
&lt;td>클라이언트가 직접 다수 노드로 읽기/쓰기 요청 후, 정족수 응답을 수집하여 결정&lt;/td>
&lt;td>리더가 모든 요청을 처리하고, 로그 복제를 통해 과반수에 커밋&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>구조&lt;/td>
&lt;td>리더 없음 (모든 노드가 대등)&lt;/td>
&lt;td>리더 중심 구조&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>일관성 보장&lt;/td>
&lt;td>&lt;code>W + R &amp;gt; N&lt;/code>일 때 eventual consistency 보장&lt;/td>
&lt;td>리더 커밋 이후 응답 → strong consistency 보장&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>장애 허용성&lt;/td>
&lt;td>R 또는 W만 만족하면 일관성 일부 손해를 감수하고 처리 가능&lt;/td>
&lt;td>과반수 미만 장애 시 리더 선출 불가 → 쓰기 중단&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>성능 유연성&lt;/td>
&lt;td>클라이언트가 &lt;code>R&lt;/code>, &lt;code>W&lt;/code> 조정 가능&lt;/td>
&lt;td>구조적 한계로 조정 불가, but 일관성 명확&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="처리-흐름-비교">처리 흐름 비교&lt;/h3>
&lt;p>&lt;strong>정족수 합의 (예: DynamoDB)&lt;/strong>&lt;/p>
&lt;pre class="mermaid" style="text-align: center;">sequenceDiagram
participant Client
participant A
participant B
participant C
Client->>A: Write(X)
Client->>B: Write(X)
Client->>C: Write(X)
A-->>Client: OK
B-->>Client: OK
Note right of Client: W = 2 → Write Success
Client->>A: Read()
Client->>B: Read()
A-->>Client: X
B-->>Client: X
Note right of Client: R = 2 → Read Success
&lt;/pre>
&lt;hr>
&lt;p>&lt;strong>Raft 합의&lt;/strong>&lt;/p>
&lt;pre class="mermaid" style="text-align: center;">sequenceDiagram
participant Client
participant Leader
participant Follower1
participant Follower2
Client->>Leader: Write(X)
Leader->>Follower1: AppendEntries(X)
Leader->>Follower2: AppendEntries(X)
Follower1-->>Leader: OK
Follower2-->>Leader: OK
Leader->>All: Commit(X)
Leader-->>Client: Success
&lt;/pre>
&lt;h3 id="선택-기준-요약">선택 기준 요약&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>상황&lt;/th>
&lt;th>적합한 방식&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>일관성이 가장 중요 (예: 금융, 설정 저장소)&lt;/td>
&lt;td>&lt;strong>Raft&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>빠른 응답과 가용성 중시 (예: 추천 캐시, 로그 수집기)&lt;/td>
&lt;td>&lt;strong>정족수 합의&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>클라이언트가 유연한 조정 가능해야 하는 경우&lt;/td>
&lt;td>&lt;strong>정족수 합의&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>복잡한 충돌 해결 없이 안정된 흐름이 필요한 경우&lt;/td>
&lt;td>&lt;strong>Raft&lt;/strong>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="마무리">마무리&lt;/h3>
&lt;p>기존 Paxos의 복잡함을 단순함으로 대체한 합의 알고리즘이 바로 Raft는 분산 시스템 입니다.&lt;/p>
&lt;p>정족수 합의와 Raft는 서로 다른 철학과 목적을 가진 합의 방식입니다.&lt;/p>
&lt;p>어떤 방법이 더 좋다고 단정하기보다는, 서비스의 특성과 우선순위에 따라 적합한 방식을 선택하는 것이 중요합니다. 일관성이 핵심이라면 Raft, 가용성과 유연성이 필요하다면 정족수 합의가 더 적합할 수 있습니다.&lt;/p></description></item><item><title>정족수 합의</title><link>https://codemario318.github.io/post/common/quorum-based-consensus/</link><pubDate>Sun, 08 Jun 2025 15:42:00 +0900</pubDate><guid>https://codemario318.github.io/post/common/quorum-based-consensus/</guid><description>&lt;img src="https://codemario318.github.io/post/common/quorum-based-consensus/cover.png" alt="Featured image of post 정족수 합의" />&lt;h2 id="분산-시스템에서-합의의-필요성">분산 시스템에서 합의의 필요성&lt;/h2>
&lt;p>분산 시스템에서는 하나의 중앙 서버가 모든 결정을 내리지 않습니다. 대신 여러 노드가 함께 데이터를 저장하고 처리합니다.&lt;/p>
&lt;p>이때 문제가 되는 것은 &amp;ldquo;어느 노드의 응답을 믿을 것인가?&amp;ldquo;입니다.&lt;/p>
&lt;p>일부 노드가 장애를 일으키거나 서로 다른 데이터를 갖고 있을 때, 시스템은 어떤 기준으로 정답을 결정해야 할까요?&lt;/p>
&lt;p>이러한 상황에서 필요한 것이 바로 &amp;ldquo;&lt;strong>합의 알고리즘&lt;/strong>&amp;ldquo;이며, 그중 하나가 정족수 합의(quorum-based consensus)입니다.&lt;/p>
&lt;h2 id="정족수란">정족수란?&lt;/h2>
&lt;p>정족수(quorum)란, 시스템이 올바르게 동작하기 위해 필요한 최소한의 응답 수를 의미합니다. 예를 들어, 세 명이 의사결정을 내리는 회의에서 두 명 이상이 동의해야 결정을 내릴 수 있다면, 그 두 명이 바로 정족수입니다.&lt;/p>
&lt;p>분산 시스템에서도 비슷합니다. 노드가 3개 있을 때 2개 이상의 응답이 있어야 데이터가 유효하다고 판단한다면, 그 2개가 정족수가 되는 것입니다.&lt;/p>
&lt;blockquote>
&lt;p>예시: 노드 A, B, C가 있는 시스템에서 노드 C가 장애를 일으켜도, A와 B가 같은 응답을 보냈다면 이를 정답으로 간주할 수 있습니다. 이처럼 정족수를 만족하는 응답이 있으면 시스템은 계속 동작할 수 있습니다.&lt;/p>
&lt;/blockquote>
&lt;h2 id="정족수-합의의-특징">정족수 합의의 특징&lt;/h2>
&lt;p>&lt;strong>장점&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>구조가 단순하여 구현이 쉬움&lt;/li>
&lt;li>일부 노드 장애를 허용하므로 높은 가용성 확보&lt;/li>
&lt;li>읽기/쓰기 정족수를 조절해 성능을 유연하게 조정 가능&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>&lt;strong>단점&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>네트워크 분할 발생 시 일관성 유지 어려움&lt;/li>
&lt;li>중복 쓰기나 충돌 상황에서는 복잡한 처리 필요&lt;/li>
&lt;/ul>
&lt;h2 id="어떻게-구현되는가">어떻게 구현되는가?&lt;/h2>
&lt;p>정족수 합의에서 중요한 수식은 다음과 같습니다.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>W + R &amp;gt; N&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>N: 전체 노드 수&lt;/li>
&lt;li>W: 쓰기 정족수 (쓰기 시 필요한 응답 수)&lt;/li>
&lt;li>R: 읽기 정족수 (읽기 시 필요한 응답 수)&lt;/li>
&lt;/ul>
&lt;p>이 조건이 만족되면 읽기와 쓰기 연산이 최소 한 노드에서 겹치므로 일관성을 확보할 수 있습니다.&lt;/p>
&lt;blockquote>
&lt;p>예시: N = 3일 때, W = 2, R = 2 라면 항상 최소 한 노드에서 읽기와 쓰기가 교차하므로 최신 데이터를 읽을 수 있습니다.&lt;/p>
&lt;/blockquote>
&lt;h2 id="어떤-시스템에서-사용되나">어떤 시스템에서 사용되나?&lt;/h2>
&lt;p>&lt;strong>DynamoDB&lt;/strong>, &lt;strong>Cassandra&lt;/strong> 같은 NoSQL 시스템은 정족수 기반의 유연한 일관성 모델을 제공합니다. 예를 들어 Cassandra에서는 클라이언트가 &lt;code>QUORUM&lt;/code>, &lt;code>ONE&lt;/code>, &lt;code>ALL&lt;/code> 같은 일관성 수준을 선택할 수 있습니다.&lt;/p>
&lt;p>이러한 시스템은 &lt;strong>쓰기와 읽기 성능&lt;/strong>, &lt;strong>지연 시간&lt;/strong>, &lt;strong>가용성&lt;/strong>을 조절할 수 있어 대규모 서비스에서 유용하게 사용됩니다.&lt;/p>
&lt;h2 id="dynamodb에서의-정족수-합의-과정">DynamoDB에서의 정족수 합의 과정&lt;/h2>
&lt;p>Amazon DynamoDB는 완벽한 일관성을 보장하기보다는 가용성과 확장성을 우선으로 하는 설계 철학을 따릅니다. 이 과정에서 정족수 합의(quorum-based consensus)는 핵심적인 역할을 합니다.&lt;/p>
&lt;pre class="mermaid" style="text-align: center;">sequenceDiagram
participant Client as 클라이언트
participant A as 노드 A
participant B as 노드 B
participant C as 노드 C (지연)
Client->>A: 쓰기 요청 (데이터 X)
Client->>B: 쓰기 요청 (데이터 X)
Client->>C: 쓰기 요청 (데이터 X)
A-->>Client: 쓰기 성공
B-->>Client: 쓰기 성공
Note right of Client: W = 2 → 쓰기 완료
Client->>A: 읽기 요청
Client->>B: 읽기 요청
A-->>Client: 데이터 X
B-->>Client: 데이터 X
Note right of Client: R = 2 → 읽기 완료, 최신 값 확정
&lt;/pre>
&lt;h3 id="배경-데이터-복제">배경: 데이터 복제&lt;/h3>
&lt;p>Dynamo는 데이터를 여러 노드(또는 파티션)에 복제하여 저장합니다. 일반적으로 N개의 복제본을 유지하며, 이들 중 일부는 읽기/쓰기 요청을 처리하는 데 참여합니다.&lt;/p>
&lt;h3 id="쓰기-요청-처리-w">쓰기 요청 처리 (W)&lt;/h3>
&lt;ul>
&lt;li>클라이언트는 데이터를 쓰기 위해 요청을 보냅니다.&lt;/li>
&lt;li>쓰기 정족수 W는 데이터를 성공적으로 저장해야 하는 노드 수입니다.&lt;/li>
&lt;li>Dynamo는 W개의 노드에 데이터가 저장되면 클라이언트에게 성공을 응답합니다.&lt;/li>
&lt;li>이때 W &amp;lt; N이면, 일부 노드에는 최신 데이터가 없을 수 있습니다.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>예: N = 3, W = 2인 경우 → 3개의 복제본 중 2개에만 쓰기가 성공하면 OK로 간주&lt;/p>
&lt;/blockquote>
&lt;h3 id="읽기-요청-처리-r">읽기 요청 처리 (R)&lt;/h3>
&lt;ul>
&lt;li>클라이언트가 데이터를 읽기 위해 요청을 보내면, 시스템은 최대 R개의 노드에서 데이터를 조회합니다.&lt;/li>
&lt;li>응답값들 중 최신 값을 선택하거나 conflict resolution을 수행합니다.&lt;/li>
&lt;li>읽기 정족수 R이 W와 합쳐서 N보다 크다면, 최소 1개 노드는 최신 값을 반드시 갖고 있으므로 일관성을 확보할 수 있습니다.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>W + R &amp;gt; N: 정족수 기반 일관성 보장의 조건&lt;/p>
&lt;/blockquote>
&lt;h3 id="conflict-해결">Conflict 해결&lt;/h3>
&lt;p>Dynamo는 강한 정합성 대신 &amp;ldquo;eventual consistency&amp;quot;를 선택하기 때문에, 충돌이 발생할 수 있습니다. 이 경우 다음과 같은 방식으로 처리됩니다.&lt;/p>
&lt;ul>
&lt;li>각 버전의 데이터를 함께 반환&lt;/li>
&lt;li>클라이언트가 이를 병합&lt;/li>
&lt;li>서버 측에서 vector clock을 사용해 자동 병합&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>DynamoDB 자체는 내부적으로 이런 과정을 완전히 노출하진 않지만, 유사한 원리를 기반으로 하여 요청 처리와 복제, 동기화를 관리하고 있습니다. 개발자는 필요에 따라 &amp;ldquo;강력한 일관성(ReadConsistency = STRONG)&amp;ldquo;을 선택해 안전한 읽기도 요청할 수 있습니다.&lt;/p>
&lt;h2 id="paxos-raft와의-차이점">Paxos, Raft와의 차이점&lt;/h2>
&lt;p>Paxos나 Raft는 분산 시스템에서 강력한 일관성을 보장하기 위한 합의 알고리즘입니다. 반면 정족수 합의는 어느 정도의 일관성을 포기하고 가용성과 응답 속도를 높이는 데 집중합니다.&lt;/p>
&lt;blockquote>
&lt;p>글로벌 캐시 시스템에서는 일관성보다는 응답 속도가 중요하므로 정족수 기반 시스템이 적합한 반면, 금융 거래 시스템에서는 Paxos나 Raft가 더 적합합니다.&lt;/p>
&lt;/blockquote>
&lt;h2 id="마무리">마무리&lt;/h2>
&lt;p>정족수 합의는 모든 시스템에 적합하진 않지만, 다음과 같은 경우에 효과적일 수 있습니다.&lt;/p>
&lt;ul>
&lt;li>약간의 일관성 손해를 감수하더라도 빠른 응답과 높은 가용성이 필요한 경우&lt;/li>
&lt;li>네트워크가 불안정하거나 노드 장애가 자주 발생하는 환경&lt;/li>
&lt;/ul>
&lt;p>정족수 기반 합의는 완벽한 정답을 요구하지 않고, 일부 노드의 응답만으로도 시스템을 안정적으로 운영할 수 있게 해주는 현실적인 접근 방법입니다.&lt;/p></description></item><item><title>논문으로 다시 이해한 CAP 정리</title><link>https://codemario318.github.io/post/common/cap/</link><pubDate>Wed, 21 May 2025 16:10:00 +0900</pubDate><guid>https://codemario318.github.io/post/common/cap/</guid><description>&lt;img src="https://codemario318.github.io/post/common/cap/cover.png" alt="Featured image of post 논문으로 다시 이해한 CAP 정리" />&lt;h2 id="들어가며">들어가며&lt;/h2>
&lt;p>분산 시스템 설계에 대해 학습하다보면, CAP 정리가 항상 언급되는 것 같습니다.&lt;/p>
&lt;p>CAP 정리는 일관성(Consistency), 가용성(Availability), 분할 내성(Partition Tolerance) 세 가지를 동시에 만족할 수 없다는 원칙으로 요약되는데요&lt;/p>
&lt;p>이 정리는 2000년 Eric Brewer가 학회 발표에서 처음 제안한 이후, 분산 시스템 설계의 기본 전제로 널리 인용되어 왔습니다.&lt;/p>
&lt;p>처음 봤을 땐 그렇구나 하고 넘어갔는데, 계속 등장하니 조금 더 깊은 내용이 궁금해졌습니다.&lt;/p>
&lt;p>그래서 이 글에서는 해당 추측을 이론적으로 증명한 논문, &lt;a class="link" href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;amp;type=pdf&amp;amp;doi=24cece61e2128780072bc58f90b8ba47f624bc27" target="_blank" rel="noopener"
>&lt;em>Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services&lt;/em>&lt;/a>를 바탕으로 CAP가 어떤 의미인지 정확히 파악해보려고 합니다.&lt;/p>
&lt;p>논문의 내용을 단락 단위로 해석하고, 그 의미를 함께 짚어보는 방식으로 살펴보겠습니다.&lt;/p>
&lt;h2 id="도입부">도입부&lt;/h2>
&lt;h3 id="brewer의-추측">Brewer의 추측&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>At PODC 2000, Brewer, in an invited talk, made the following conjecture: it is impossible for a web service to provide the following three guarantees: consistency, availability, and partition-tolerance.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>2000년 PODC 학회에서 Eric Brewer는 웹 서비스가 다음의 세 가지 특성을 동시에 만족시킬 수 없다는 추측을 언급했습니다.&lt;/p>
&lt;ul>
&lt;li>일관성 (Consistency)&lt;/li>
&lt;li>가용성 (Availability)&lt;/li>
&lt;li>분할 내성 (Partition tolerance)&lt;/li>
&lt;/ul>
&lt;p>이 세 가지는 각각의 의미도 중요하지만, 무엇보다 &amp;ldquo;동시에 만족시킬 수 없다&amp;quot;는 점이 핵심입니다. 처음엔 추측으로 제안된 이 주장은, 지금은 CAP 정리라는 이름으로 잘 알려져 있죠.&lt;/p>
&lt;p>그러한 이유로 CAP 정리를 브루어의 정리라고도 부르기도 합니다. 논문은 이 세 가지가 왜 동시에 만족될 수 없는지를 구체적인 모델 위에서 이론적으로 증명합니다.&lt;/p>
&lt;h3 id="일관성에-대한-기대">일관성에 대한 기대&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>Most web services today attempt to provide strongly consistent data. &amp;hellip; (중략) &amp;hellip; once a transaction is committed it is permanent (durable).&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>현실의 웹 서비스는 ACID 트랜잭션을 전제로 강한 일관성을 기본 기대값으로 두고 있다&lt;/strong>고 볼 수 있습니다.&lt;/p>
&lt;p>사용자는 데이터가 중간에 잘렸거나, 쓰기 도중 일부만 반영된 상태를 보는 것을 원하지 않습니다. 쓰기가 완료되었다면 어떤 노드에서 읽더라도 동일한 값을 받을 수 있어야 한다는 기대가 전제되어 있습니다.&lt;/p>
&lt;p>이런 이유로 대부분의 시스템은 트랜잭션을 기반으로 한 일관성 보장 메커니즘을 필수 요소로 고려하게 됩니다.&lt;/p>
&lt;h3 id="가용성에-대한-기대">가용성에 대한 기대&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>Web services are similarly expected to be highly available. &amp;hellip; (중략) &amp;hellip; The goal of most web services today is to be as available as the network on which they run: if any service on the network is available, then the web service should be accessible.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>웹 서비스는 언제든지 요청에 응답할 수 있어야 한다는 기대를 받는다&lt;/strong>고 정리할 수 있습니다.&lt;/p>
&lt;p>서비스 중단은 단순한 불편을 넘어서 실제 피해로 이어질 수 있습니다. 논문에서는 증권 거래 사이트(E-Trade)의 사례를 들어, 서비스가 중요한 시점에 응답하지 못할 경우 법적 문제까지 발생할 수 있다고 설명합니다.&lt;/p>
&lt;p>그만큼 가용성은 기술적 요구를 넘어 &lt;strong>서비스의 신뢰성과 직접 연결되는 특성&lt;/strong>입니다.&lt;/p>
&lt;p>이 때문에 대부분의 시스템은 네트워크 어딘가에 살아 있는 노드가 있다면, 전체 시스템 역시 반드시 응답할 수 있어야 한다는 전제를 갖고 설계됩니다.&lt;/p>
&lt;h3 id="분할-내성에-대한-필요">분할 내성에 대한 필요&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>Finally, on a highly distributed network, it is desirable to provide some amount of fault-tolerance. &amp;hellip; (중략) &amp;hellip; In this note we will not consider stopping failures, though in some cases a stopping failure can be modeled as a node existing in its own unique component of a partition.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>분산 시스템에서는 네트워크 분할 상황에서도 동작을 계속할 수 있는 능력, 즉 분할 내성이 요구된다&lt;/strong>고 볼 수 있습니다.&lt;/p>
&lt;p>노드나 통신 경로에 장애가 발생하는 상황은 실제 시스템에서 흔히 일어납니다. 이때 전체 시스템이 멈추지 않고 가능한 범위 내에서 계속 동작해야 한다는 요구가 생깁니다.&lt;/p>
&lt;p>논문에서는 일부 노드나 메시지가 단절된 상황을 &amp;ldquo;분할(partition)&amp;ldquo;로 간주하고, 이러한 환경에서도 서비스가 가능한 상태를 유지할 수 있어야 한다는 점을 분할 내성의 핵심으로 설명합니다.&lt;/p>
&lt;h2 id="논문에서-정의하는-세-가지-특성">논문에서 정의하는 세 가지 특성&lt;/h2>
&lt;p>앞에서 소개한 세 가지 특성은 모두 중요하지만, 그 의미가 모호하거나 해석에 따라 다르게 받아들여질 수 있습니다.&lt;/p>
&lt;p>따라서 논문에서는 본격적인 증명에 앞서, 이 세 가지 특성을 정확히 어떤 의미로 사용할지를 형식적으로 정의하고 있습니다.&lt;/p>
&lt;p>각 정의는 논문 전체에서 사용하는 모델의 핵심 가정이기도 하며, 이후 증명의 기반이 되는 전제 조건으로 작용합니다.&lt;/p>
&lt;h3 id="atomic-data-object">Atomic Data Object&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>The most natural way of formalizing the idea of a consistent service is as an atomic data object. &amp;hellip; (중략) &amp;hellip; See [6] for a more complete definition of atomic consistency.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>Atomic Data Object는 분산된 환경에서도 순차적으로 동작하는 것처럼 보이는 일관성을 모델링한 개념이다&lt;/strong>라고 이해할 수 있습니다.&lt;/p>
&lt;p>논문에서는 일관성을 형식적으로 정의하기 위해 &lt;strong>Atomic Data Object&lt;/strong>를 사용합니다.&lt;/p>
&lt;p>이 객체는 **atomic consistency (또는 linearizability)**를 만족해야 하며, 이는 모든 연산이 일관된 전역 순서를 따르는 것처럼 동작함을 의미합니다.&lt;/p>
&lt;p>예를 들어, 어떤 쓰기 연산이 완료된 이후 시작된 읽기 연산은 반드시 그 값 또는 그 이후의 값을 반환해야 합니다.&lt;/p>
&lt;p>이런 모델은 사용자 입장에서 시스템이 &lt;strong>단일 노드처럼 예측 가능하게&lt;/strong> 동작한다고 느낄 수 있게 해줍니다.&lt;/p>
&lt;h3 id="availability">Availability&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>For a distributed system to be continuously available, every request received by a non-failing node in the system must result in a response. &amp;hellip; (중략) &amp;hellip; even when severe network failures occur, every request must terminate.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>고장나지 않은 노드로 들어온 요청은 반드시 응답을 받아야 한다&lt;/strong>라고 이해할 수 있습니다.&lt;/p>
&lt;p>논문에서는 가용성을 &amp;ldquo;모든 요청이 언젠가는 응답을 받아야 한다&amp;quot;는 조건으로 정의합니다. 특히 이 정의는 &lt;strong>메시지가 유실될 수 있는 환경에서도 요청을 포기하지 않고 처리해야 함&lt;/strong>을 전제로 하고 있습니다.&lt;/p>
&lt;p>이는 단순한 성능 문제가 아니라 &lt;strong>시스템이 지속적으로 응답 가능하다는 보장&lt;/strong>, 즉 실시간으로 동작하는 서비스가 가져야 할 기본 조건으로 제시됩니다.&lt;/p>
&lt;p>또한 부분 실패 상황, 예를 들어 다른 모든 노드가 실패하더라도 하나의 노드만 살아 있다면 그 노드는 여전히 &lt;strong>요청에 응답할 수 있어야 한다&lt;/strong>는 점을 강조합니다.&lt;/p>
&lt;h3 id="partition-tolerance">Partition Tolerance&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>In order to model partition tolerance, the network will be allowed to lose arbitrarily many messages sent from one node to another. &amp;hellip; (중략) &amp;hellip; a valid (atomic) response must be generated.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>분할 내성은 네트워크 일부가 단절되어도 시스템이 동작을 멈추지 않아야 한다는 특성을 의미한다&lt;/strong>고 볼 수 있습니다.&lt;/p>
&lt;p>논문에서는 네트워크 분할을 모델링하기 위해, &lt;strong>일부 노드 간의 메시지가 영구적으로 손실되는 상황&lt;/strong>을 허용합니다. 이 경우 시스템은 전체가 멈추는 것이 아니라, &lt;strong>분리된 각 노드가 독립적으로 응답을 계속할 수 있어야 한다&lt;/strong>는 요구를 받습니다.&lt;/p>
&lt;p>분할 내성을 만족하는 시스템은 하나의 노드만 살아 있고 나머지 노드들과 통신할 수 없는 상황에서도, 해당 노드는 &lt;strong>자체적으로 요청을 처리하고 응답할 수 있어야 합니다.&lt;/strong>&lt;/p>
&lt;p>이 정의는 논문 전반에서 &lt;strong>Partition Tolerance를 &amp;lsquo;메시지 유실이 있는 환경에서의 정상 동작 보장&amp;rsquo;으로 해석한다&lt;/strong>는 점에서 중요합니다.&lt;/p>
&lt;h2 id="비동기-네트워크에서의-불가능성">비동기 네트워크에서의 불가능성&lt;/h2>
&lt;p>논문에서는 본격적인 증명에 앞서 정의한 세 가지 특성이 어떤 조건 하에서 동시에 만족될 수 없는지를 순차적으로 보여줍니다.&lt;/p>
&lt;p>이 섹션에서는 먼저 &lt;strong>비동기 네트워크 모델&lt;/strong>에서의 불가능성을 다루고 있으며, 이를 통해 CAP 정리가 단순한 직관이 아니라 &lt;strong>논리적으로 증명 가능한 제약&lt;/strong>임을 밝히고 있습니다.&lt;/p>
&lt;h3 id="정리-1-비동기-모델에서-cap-불가능성-증명">정리 1: 비동기 모델에서 CAP 불가능성 증명&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>In the asynchronous model, there is no clock, and nodes must make decisions based only on the messages received and local computation. &amp;hellip; (중략) &amp;hellip; Theorem 1: It is impossible in the asynchronous network model to implement a read/write data object that guarantees the following properties: Availability, Atomic consistency, in all fair executions (including those in which messages are lost).&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>비동기 네트워크 모델에서는 가용성과 원자적 일관성을 동시에 만족하는 읽기/쓰기 객체를 구현할 수 없다&lt;/strong>는 것이 이 정리의 핵심입니다.&lt;/p>
&lt;p>논문에서 사용하는 비동기 네트워크 모델은 다음과 같은 특징을 가집니다.&lt;/p>
&lt;ul>
&lt;li>시스템에는 전역 시계가 없음&lt;/li>
&lt;li>노드는 수신한 메시지와 로컬 상태만으로 동작을 결정&lt;/li>
&lt;li>메시지는 유실될 수 있고, 언제 도착할지 보장되지 않음&lt;/li>
&lt;/ul>
&lt;p>이러한 조건 아래에서, 논문은 &lt;strong>쓰기 연산이 완료된 후에도 일부 노드가 이전 값을 반환하는 상황이 발생&lt;/strong>할 수 있음을 보여줍니다.&lt;/p>
&lt;pre class="mermaid" style="text-align: center;">sequenceDiagram
participant Client
participant G1 as 노드 G1
participant G2 as 노드 G2
Note over G1, G2: 초기 상태: v₀ (G1 = v₀, G2 = v₀)
Client->>G1: 쓰기 요청 (v₁)
G1->>G1: v₁ 저장
G1-->>Client: 쓰기 완료 응답
G1-xG2: 메시지 유실 (v₁ 전달 실패)
Client->>G2: 읽기 요청
G2-->>Client: v₀ 반환
&lt;/pre>
&lt;ol>
&lt;li>노드 G1에서 쓰기 연산이 완료&lt;/li>
&lt;li>G1과 G2 사이의 네트워크가 분할되어, G2는 그 값을 전달받지 못함&lt;/li>
&lt;li>이후 클라이언트가 G2에 읽기 요청을 보냄&lt;/li>
&lt;li>G2는 여전히 초기 값을 반환&lt;/li>
&lt;/ol>
&lt;p>이 상황에서 시스템은 가용성을 만족하기 위해 G2의 요청에 응답하지만, 그 결과 일관성이 깨지는 응답을 줄 수밖에 없다는 모순이 발생합니다.&lt;/p>
&lt;p>이를 통해 CAP 정리에서 말하는 세 가지 특성이 &lt;strong>왜 동시에 만족될 수 없는지를, 수학적 모델을 통해 명확히 보여주는 출발점&lt;/strong>이 됩니다.&lt;/p>
&lt;h3 id="따름정리-11-메시지-유실-여부를-판단할-수-없는-상황">따름정리 1.1: 메시지 유실 여부를 판단할 수 없는 상황&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>Corollary 1.1: It is impossible in the asynchronous network model to implement a read/write data object that guarantees the following properties: Availability, in all fair executions, and Atomic consistency, in fair executions in which no messages are lost.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>메시지가 유실되지 않는 경우에도 가용성과 원자적 일관성을 동시에 만족하는 시스템은 구현할 수 없다&lt;/strong>는 것이 이 따름정리의 핵심입니다.&lt;/p>
&lt;p>정리 1에서는 메시지 유실이 가능한 상황을 통해 모순을 구성했지만, Corollary 1.1에서는 &lt;strong>메시지가 유실되지 않는 경우만 고려하더라도 동일한 모순이 발생&lt;/strong>함을 강조합니다.&lt;/p>
&lt;p>그 이유는 &lt;strong>비동기 모델에서는 메시지가 단순히 지연된 것인지, 실제로 유실된 것인지 시스템이 구분할 수 없기 때문&lt;/strong>입니다.&lt;/p>
&lt;pre class="mermaid" style="text-align: center;">sequenceDiagram
participant Client
participant G1 as 노드 G1
participant G2 as 노드 G2
Note over G1, G2: 초기 상태: v₀ (G1 = v₀, G2 = v₀)
Client->>G1: 쓰기 요청 (v₁)
G1->>G1: v₁ 저장
G1-->>Client: 쓰기 완료 응답
Client->>G2: 읽기 요청
G2-->>Client: v₀ 반환
G1->>G2: 메시지 수신
G2->>G2: v₁ 저장
&lt;/pre>
&lt;p>위 상황은 G1의 메시지가 클라이언트가 G2에게 보내는 요청 보다 빨리 도달했을 때를 가정해봤습니다.&lt;/p>
&lt;p>이처럼 메시지가 유실되지 않았더라도, 노드는 언제까지 기다려야 할지 알 수 없고, 결국 같은 방식의 일관성 위반이 발생하게 됩니다.&lt;/p>
&lt;p>이 따름정리는 비동기 모델에서의 &lt;strong>정보 부족의 한계&lt;/strong>를 명확히 드러냅니다. 즉, 유실 여부를 알 수 없다는 사실 하나만으로도, 어떤 실행에서는 일관성을 만족시키는 것이 불가능해질 수 있습니다.&lt;/p>
&lt;p>논문에서는 완전한 비동기 환경이라면 노드가 메시지가 지연되는지, 유실되었는지 판단할 수 없기 때문에 메시지가 유실되지 않는 경우 가용성과 원자적 일관성을 동시에 만족한다는 의미는 결과적으로 &lt;strong>모든 상황(유실, 지연)에 가용성과 원자적 일관성을 동시에 만족&lt;/strong>하는 상황이라고 강조합니다.&lt;/p>
&lt;h2 id="비동기-모델에서의-현실적인-조합">비동기 모델에서의 현실적인 조합&lt;/h2>
&lt;p>앞서 정리 1과 따름정리 1.1을 통해,&lt;br>
비동기 네트워크에서는 &lt;strong>Atomic consistency, Availability, Partition tolerance&lt;/strong>를 동시에 만족하는 것이 불가능하다는 점이 증명되었습니다.&lt;/p>
&lt;p>하지만 세 가지를 모두 만족할 수 없다면, 그중 두 가지 조합은 실현 가능할 수 있음을 언급하며, 이를 다음과 같이 세 가지 조합으로 구분하고, 각각 어떤 상황에서 가능한지를 간단한 모델을 통해 설명합니다.&lt;/p>
&lt;blockquote>
&lt;p>이때 사용되는 &amp;ldquo;Atomic&amp;quot;은 앞서 정의한 atomic consistency, 즉 선형화 가능한 일관성을 의미합니다.&lt;/p>
&lt;/blockquote>
&lt;h3 id="atomic--available">Atomic + Available&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>If partition-tolerance is not required, then atomicity and availability are easy to guarantee. &amp;hellip; (중략) &amp;hellip; This technique only works if the centralized node is always accessible.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>Partition tolerance를 포기하면 atomic consistency와 availability는 함께 보장할 수 있습니다.&lt;/strong>&lt;/p>
&lt;p>이 조합은 Partition tolerance를 전제하지 않는 경우에 가능합니다.&lt;/p>
&lt;p>즉, 네트워크가 항상 안정적이며, 노드 간 통신이 절대 실패하지 않는 환경이라면 atomic consistency와 availability를 동시에 만족하는 시스템을 구현할 수 있습니다.&lt;/p>
&lt;p>논문에서는 이러한 조건 아래에서 중앙 집중형 알고리즘을 통해 모든 요청을 직렬화하고, 실패가 없는 한 모든 요청에 응답할 수 있는 구조를 예시로 제시합니다.&lt;/p>
&lt;p>다만 Partition tolerance가 없는 분산 시스템은 현실적으로 성립하기 어렵기 때문에, 이 조합은 제한된 환경(예: 단일 데이터센터, LAN 등)에서만 현실적으로 가능하다는 점도 강조하고 있습니다.&lt;/p>
&lt;h3 id="available--partition-tolerant">Available + Partition Tolerant&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>It is possible to provide high availability and partition tolerance, if atomic consistency is not required. &amp;hellip; (중략) &amp;hellip; Web caches are one example of a weakly consistent network.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>atomic consistency를 포기하면 availability와 partition tolerance는 동시에 만족할 수 있습니다.&lt;/strong>&lt;/p>
&lt;p>이 조합은 시스템이 항상 응답 가능해야 하면서도, 네트워크 분할 상황에서도 멈추지 않고 동작해야 할 때 선택할 수 있습니다. 대신 모든 노드가 동일한 시점의 값을 보장하는 일관성은 포기해야 합니다.&lt;/p>
&lt;p>논문에서는 이 조합을 만족시키는 극단적인 예로, 모든 읽기 요청에 초기값 &lt;code>v₀&lt;/code>만을 반환하는 시스템을 제시합니다.&lt;/p>
&lt;p>이 경우 일관성은 없지만, 어떤 상황에서도 요청은 처리됩니다.&lt;/p>
&lt;p>실제로는 이보다는 **약한 일관성(weak consistency)**을 부분적으로 유지하면서, 가능한 한 최신 데이터를 반환하려는 구조들이 현실적인 선택지로 사용됩니다.&lt;/p>
&lt;ul>
&lt;li>이러한 접근은 이후 논문에서 다루는 약한 일관성과 지연된 일관성 개념으로 이어집니다.&lt;/li>
&lt;/ul>
&lt;h3 id="atomic--partition-tolerant">Atomic + Partition Tolerant&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>If availability is not required, then it is easy to achieve atomic data and partition tolerance. &amp;hellip; (중략) &amp;hellip; If there are no failures, then liveness is guaranteed.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>가용성을 포기하면 atomic consistency와 partition tolerance는 동시에 만족시킬 수 있습니다.&lt;/strong>&lt;/p>
&lt;p>이 조합은 Availability를 포기한 경우에 가능합니다. 시스템이 일관성과 분할 내성을 만족해야 한다면, 일부 요청에 대해서는 응답하지 않아도 되는 상황을 허용해야 합니다.&lt;/p>
&lt;p>논문에서는 중앙 집중형 알고리즘을 예로 들며 이 조합이 어떻게 성립하는지를 설명합니다.&lt;/p>
&lt;p>요청을 받은 노드는 중앙 노드에 메시지를 전송하고, 중앙 노드로부터 응답을 받은 뒤에야 클라이언트에 응답합니다. 하지만 네트워크 분할로 인해 중앙 노드와 통신이 불가능한 경우, 클라이언트 요청에 대한 응답은 무한히 지연되거나 아예 실패할 수 있습니다.&lt;/p>
&lt;p>이 방식은 모든 메시지가 정상적으로 전달된다면 시스템은 응답 가능하지만, 분할이 발생하는 실행에서는 일관성을 보존하기 위해 가용성을 희생하게 됩니다.&lt;/p>
&lt;h2 id="부분-동기-모델에서의-논의">부분 동기 모델에서의 논의&lt;/h2>
&lt;p>비동기 네트워크에서는 CAP 세 가지를 동시에 만족할 수 없다는 점이 증명되었지만, 현실의 시스템은 대부분 &lt;strong>완전히 비동기적이지는 않습니다&lt;/strong>.&lt;/p>
&lt;p>일정 시간 안에 메시지가 도달하거나, 노드가 타임아웃을 기준으로 동작을 조정하는 식의 &lt;strong>부분적인 동기성&lt;/strong>이 존재합니다.&lt;/p>
&lt;p>논문에서는 이러한 현실적인 조건을 반영한 **부분 동기 네트워크 모델(partially synchronous model)**을 도입하여, 이 환경에서도 여전히 불가능한 것이 무엇인지 분석합니다.&lt;/p>
&lt;h3 id="정리-2-시계가-있어도-해결되지-않는-모순">정리 2: 시계가 있어도 해결되지 않는 모순&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>Theorem 2: It is impossible in the partially synchronous network model to implement a read/write data object that guarantees the following properties: Availability and Atomic consistency.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>부분 동기 모델에서도 Availability와 Atomic consistency는 동시에 만족시킬 수 없습니다.&lt;/strong>&lt;/p>
&lt;p>이 정리는 분산 시스템의 각 노드가 시계를 가지고 있고, 메시지가 일정 시간 내에 도착하거나 유실된다는 조건이 주어진 상황에서도 앞서와 같은 모순이 여전히 발생함을 보여줍니다.&lt;/p>
&lt;p>논문에서는 G1에서 쓰기 연산이 완료된 뒤 충분한 시간이 흐른 후, G2에서 읽기 연산이 수행되는 시나리오를 구성합니다.&lt;/p>
&lt;pre class="mermaid" style="text-align: center;">sequenceDiagram
participant Client
participant G1 as 노드 G1
participant G2 as 노드 G2
Note over G1, G2: 초기 상태: G1 = v₀, G2 = v₀
rect rgba(0, 200, 0, 0.05)
Note over G1: 쓰기 연산 α₁
Client->>G1: 쓰기 요청 (v₁)
G1->>G1: v₁ 저장
G1-->>Client: 쓰기 완료
end
G1-xG2: 메시지 손실 (v₁ 미전달)
rect rgba(255, 255, 0, 0.1)
Note over G1, G2: α₁ 이후 충분한 시간 간격 (α′₂)
end
rect rgba(0, 200, 0, 0.05)
Note over G2: 읽기 연산 α₂
Client->>G2: 읽기 요청
G2-->>Client: v₀ 반환 (최신 값 아님)
end
&lt;/pre>
&lt;ol>
&lt;li>G1에서 쓰기 연산 α₁이 발생하여 v₁을 저장&lt;/li>
&lt;li>G1과 G2 사이의 메시지는 손실됨&lt;/li>
&lt;li>α₁ 이후 충분한 시간 간격(α′₂)이 존재함&lt;/li>
&lt;li>G2에서 읽기 연산 α₂가 발생하지만, 여전히 v₀을 반환 (최신 값을 알 수 없음)&lt;/li>
&lt;li>G2는 가용성을 만족하기 위해 응답하지만, 일관성을 위반&lt;/li>
&lt;/ol>
&lt;p>이때 메시지가 손실된 경우, 읽기는 여전히 이전 값을 반환할 수밖에 없고, 이는 &lt;strong>일관성 위반&lt;/strong>이 됩니다.&lt;/p>
&lt;p>즉, 타임아웃을 통해 메시지 유실 여부를 추론할 수 있다 하더라도 CAP 세 가지를 동시에 만족시키는 것은 여전히 불가능하다는 점을 이 정리를 통해 강조합니다.&lt;/p>
&lt;h3 id="네트워크-안정-시-가능해지는-조건적-보장">네트워크 안정 시 가능해지는 조건적 보장&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>While it is not possible to implement an atomic read/write register in the partially synchronous model, if availability is required, it is still possible to implement such an object if atomicity is required only in executions in which all messages are delivered.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>부분 동기 모델에서도 모든 메시지가 전달되는 실행에 대해서는 atomic consistency를 만족할 수 있습니다.&lt;/strong>&lt;/p>
&lt;p>정리 2에서는 가용성과 일관성을 동시에 보장하는 것이 불가능하다고 했지만, &lt;strong>그 불가능은 모든 실행에 대해 동시에 만족시킬 수 없다는 의미&lt;/strong>입니다.&lt;/p>
&lt;p>만약 시스템이 &lt;strong>일관성을 제공해야 하는 상황을 &amp;ldquo;모든 메시지가 eventually 전달되는 실행&amp;quot;으로 한정&lt;/strong>한다면,그 범위 안에서는 atomicity를 구현하는 것이 가능합니다.&lt;/p>
&lt;p>논문에서는 중앙 집중형 알고리즘을 예로 들어 이 가능성을 설명합니다.&lt;/p>
&lt;pre class="mermaid" style="text-align: center;">flowchart TD
Client[클라이언트] --> A[로컬 노드 A]
A --> C[중앙 노드 C]
C --> A
A --> Client
C --> N1[다른 노드 1]
C --> N2[다른 노드 2]
C --> N3[다른 노드 3]
style C fill:#fdf6b2,stroke:#e0c000,stroke-width:2px
style A fill:#e0f7fa,stroke:#00acc1,stroke-width:1px
style Client fill:#eeeeee,stroke:#aaaaaa
&lt;/pre>
&lt;p>요청은 중앙 노드로 전달되고, 중앙 노드가 이를 직렬화하여 처리하는 구조로, 네트워크가 안정적이라면 모든 요청은 eventually 수락되고, 그 결과는 atomic하게 보장될 수 있습니다.&lt;/p>
&lt;p>이 조건부 보장은 완전한 불가능을 대체하지는 않지만, &lt;strong>현실적인 시스템 설계에서 atomicity를 제한적으로라도 달성할 수 있는 여지를 제공&lt;/strong>한다는 점에서 의미가 있습니다.&lt;/p>
&lt;h2 id="약한-일관성-조건">약한 일관성 조건&lt;/h2>
&lt;p>앞서 살펴본 바와 같이, 네트워크가 비동기적이거나 분할 상황이 존재한다면 원자적 일관성과 가용성, 분할 내성을 동시에 만족하는 것은 불가능합니다.&lt;/p>
&lt;p>이러한 제약을 인정하면서도 시스템이 제공하는 응답의 품질을 통제하기 위해, 논문은 이 섹션에서 &lt;strong>일관성을 약화시키되, 그 조건을 형식적으로 정의하는 모델&lt;/strong>을 제안합니다.&lt;/p>
&lt;p>이 모델은 Delayed-t Consistency라고 불리며, 약한 일관성 조건을 시간 기반으로 규정합니다.&lt;/p>
&lt;h3 id="완전한-일관성을-항상-보장할-수-없는-상황">완전한 일관성을 항상 보장할 수 없는 상황&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>While it is useful to guarantee that atomic data will be returned in executions in which all messages are delivered (within some time bound), it is equally important to specify what happens in executions in which some of the messages are lost.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>모든 메시지가 도달하는 경우만 고려해서는 충분하지 않으며, 메시지가 유실되는 실행에서는 어떤 값이 반환될 수 있는지도 명시해야 합니다.&lt;/strong>&lt;/p>
&lt;p>앞서 정리 1과 정리 2에서는, 비동기 및 부분 동기 네트워크 모델에서 atomic consistency와 availability를 동시에 만족할 수 없음을 보였습니다.&lt;/p>
&lt;p>그렇다고 해서 메시지가 유실되거나 지연되는 상황에서 시스템이 무조건 응답을 중단하거나, 무의미한 값을 반환하도록 내버려 둘 수는 없습니다.&lt;/p>
&lt;p>논문은 이 지점에서 **&amp;ldquo;약한 일관성 조건(weaker consistency conditions)&amp;rdquo;**이라는 방향을 제시합니다. 즉, 메시지 유실이 존재하는 환경에서도 &lt;strong>응답 값이 지켜야 할 최소한의 일관성 조건을 형식적으로 정의하는 방법&lt;/strong>을 제안하는 것입니다.&lt;/p>
&lt;h3 id="delayed-t-consistency란">Delayed-t Consistency란?&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>This consistency model ensures that if messages are delivered, then eventually some notion of atomicity is restored.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>일정 시간 동안 메시지가 안정적으로 전달된다면, 그 이후에는 atomic consistency가 다시 보장되어야 한다&lt;/strong>는 모델입니다.&lt;/p>
&lt;p>이 일관성 모델은 다음과 같은 전제를 가집니다.&lt;/p>
&lt;ul>
&lt;li>네트워크가 일시적으로 분할되거나 메시지가 손실될 수는 있지만,&lt;/li>
&lt;li>&lt;strong>일정 시간 t 이상 메시지 손실이 발생하지 않는 구간이 존재한다면&lt;/strong>,&lt;/li>
&lt;li>그 구간 이후부터는 시스템이 &lt;strong>다시 원자적(atomic)인 응답을 보장&lt;/strong>해야함&lt;/li>
&lt;/ul>
&lt;p>논문에서는 이를 &lt;strong>Delayed-t Consistency&lt;/strong>라고 정의하고, 그 조건을 만족하는 실행에 대해 어떤 순서 보장과 응답 품질이 필요한지를 형식적으로 정의합니다.&lt;/p>
&lt;p>이 모델은 완전한 atomic consistency를 제공하지는 않지만, &lt;strong>네트워크가 회복된 뒤 일정 시간이 지나면 다시 강한 일관성이 회복됨을 보장&lt;/strong>합니다.&lt;/p>
&lt;p>이는 eventually consistent 시스템보다 더 구체적이며, “언젠가 일관성이 회복된다”가 아니라 &amp;ldquo;&lt;strong>정해진 시간 이후에는 반드시 회복된다&lt;/strong>&amp;ldquo;는 점에서 의미 있는 차이가 있습니다.&lt;/p>
&lt;h3 id="definition-3-delayed-t-consistency의-형식적-정의">Definition 3: Delayed-t Consistency의 형식적 정의&lt;/h3>
&lt;blockquote>
&lt;p>*Definition 3: A timed execution, α, of a read-write object is Delayed-t Consistent if:&lt;/p>
&lt;ol>
&lt;li>P is a partial order that orders all write operations, and orders all read operations with respect to the write operations.&lt;/li>
&lt;li>The value returned by every read operation is exactly the one written by the previous write operation in P (or the initial value, if there is no such previous write in P).&lt;/li>
&lt;li>The order in P is consistent with the order of read and write requests submitted at each node.&lt;/li>
&lt;li>(Atomicity) If all messages in the execution are delivered, and an operation θ completes before an operation φ begins, then φ does not precede θ in the partial order P.&lt;/li>
&lt;li>(Weakly Consistent) Assume there exists an interval of time longer than t in which no messages are lost. Further, assume an operation, θ, completes before the interval begins, and another operation, φ, begins after the interval ends. Then φ does not precede θ in the partial order P.*&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>Delayed-t Consistency는 메시지 유실이 없는 일정 시간(t) 이후부터는 연산 순서가 atomic consistency에 부합하도록 제한되는 일관성 모델&lt;/strong>입니다.&lt;/p>
&lt;p>각 조건의 의미는 다음과 같습니다.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>1. P는 모든 쓰기 연산을 순서화하고, 읽기 연산은 그에 대해 순서화되어야 한다.&lt;/strong>&lt;br>
→ 전체 실행을 정렬하는 하나의 순서가 있으며, 읽기/쓰기 간 순서 관계를 명확히 정의합니다.&lt;/p>
&lt;p>&lt;strong>2. 모든 읽기는 순서상 가장 가까운 직전 쓰기 값(또는 초기값)을 반환해야 한다.&lt;/strong>&lt;br>
→ 읽기가 이전 쓰기를 반드시 반영해야 함을 요구합니다.&lt;/p>
&lt;p>&lt;strong>3. 각 노드 내의 연산 순서는 부분 순서 P와 일치해야 한다.&lt;/strong>&lt;br>
→ 단일 노드 기준으로 볼 때, 로컬에서 일어난 순서는 전역 순서와 모순되지 않아야 합니다.&lt;/p>
&lt;p>&lt;strong>4. (Atomicity 조건)&lt;/strong>&lt;br>
→ 메시지가 유실되지 않은 실행에서, 어떤 연산 θ가 φ보다 먼저 끝났다면 φ는 절대 θ보다 앞선 순서에 위치할 수 없습니다. (순서가 뒤집히지 않아야 함)&lt;/p>
&lt;p>&lt;strong>5. (약한 일관성 조건)&lt;/strong>&lt;br>
→ 메시지 유실이 없는 구간이 t 이상 존재하고, 그 구간 이전에 끝난 연산 θ와 구간 이후 시작된 연산 φ가 있다면 φ는 θ보다 먼저 일어난 것처럼 간주되어서는 안 됩니다. 즉, &lt;strong>네트워크가 안정된 이후에는 순서를 지키라는 조건&lt;/strong>입니다.&lt;/p>
&lt;p>이 정의는 단순히 “eventually consistency”처럼 막연하게 일관성이 회복된다고 말하는 것이 아니라, &lt;strong>일관성 회복의 조건(시간, 순서, 메시지 전파 범위)을 명시적으로 제한&lt;/strong>하는 점에서 현실적인 구현 기준이 됩니다.&lt;/p>
&lt;h3 id="설계-가능성-중앙-집중형-알고리즘-설명">설계 가능성: 중앙 집중형 알고리즘 설명&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>A variant of the centralized algorithm described in Section 4.3 is Delayed-t consistent. Assume node C is the centralized node. The algorithm behaves as follows: &amp;hellip;&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>논문은 Definition 3을 만족하는 구조로 중앙 집중형 알고리즘의 변형을 제시합니다.&lt;/strong>&lt;/p>
&lt;p>이 알고리즘은 기존의 중앙 집중형 모델을 기반으로 하되, &lt;strong>시간 기반 메시지 처리&lt;/strong>와 &lt;strong>타임아웃에 따른 보조 흐름&lt;/strong>을 추가한 구조입니다.&lt;/p>
&lt;pre class="mermaid" style="text-align: center;">sequenceDiagram
participant Client
participant A as 로컬 노드 A
participant C as 중앙 노드 C
participant N1 as 다른 노드
%% 쓰기 요청 처리 흐름
Client->>A: 쓰기 요청 (v₁)
A->>C: 중앙 노드에 전달
C->>C: v₁ 저장, 시퀀스 번호 증가
C->>A: 응답
A->>Client: 쓰기 응답
C->>N1: v₁ 전파
%% 메시지 유실 시 타임아웃 처리
Note over C,N1: 메시지 손실 시 C는 일정 시간 내 재전송 시도
&lt;/pre>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>쓰기 요청 처리&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>노드 A가 중앙 노드 C에 새로운 값을 보냄&lt;/li>
&lt;li>C는 시퀀스 번호를 증가시키고 값을 저장&lt;/li>
&lt;li>C는 모든 노드에 값을 전파&lt;/li>
&lt;li>메시지 유실이 의심되면 C는 t−2×timeout 시간 안에 재전송을 수행&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>읽기 요청 처리&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>노드 A는 C에 현재 값을 요청&lt;/li>
&lt;li>응답이 정상적으로 오면 그 값을 클라이언트에 전달&lt;/li>
&lt;li>응답이 없으면, 로컬에 저장된 가장 최신 값(또는 초기값)을 반환&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;p>이 구조는 메시지가 도달하지 않더라도 &lt;strong>가용성(응답)을 보장하면서&lt;/strong>, 네트워크가 안정된 이후에는 &lt;strong>atomic consistency로 회복되는&lt;/strong> 설계입니다.&lt;/p>
&lt;h3 id="theorem-4-이-알고리즘은-정의를-만족함">Theorem 4: 이 알고리즘은 정의를 만족함&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>Theorem 4: The modified centralized algorithm is Delayed-t consistent.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>요약하면, &lt;strong>앞서 설명한 중앙 집중형 알고리즘은 Delayed-t Consistency의 다섯 가지 조건을 모두 만족한다.&lt;/strong> 입니다.&lt;/p>
&lt;p>논문은 Definition 3에서 제시한 일관성 조건들을 하나씩 검토하며, 해당 알고리즘이 그것들을 모두 충족함을 논리적으로 설명합니다.&lt;/p>
&lt;ol>
&lt;li>&lt;strong>부분 순서(P) 정의&lt;/strong>
&lt;ul>
&lt;li>중앙 노드 C가 모든 쓰기 요청에 시퀀스 번호를 부여하므로, 연산 간 전역 순서가 정해짐&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>읽기 연산은 이전 쓰기 값을 반환&lt;/strong>
&lt;ul>
&lt;li>각 읽기는 로컬 또는 중앙 노드로부터 가장 최근의 쓰기 값을 참조&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>로컬 순서와 부분 순서의 일치&lt;/strong>
&lt;ul>
&lt;li>각 노드는 자신의 연산 순서를 전역 순서와 동일하게 유지&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>모든 메시지가 전달되는 실행에서 atomic consistency 보장&lt;/strong>
&lt;ul>
&lt;li>C가 직렬화하고 모든 노드에 전파하므로, 메시지 손실이 없는 경우에는 원자성을 만족&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>메시지 유실 없는 시간 구간 이후 순서 보장&lt;/strong>
&lt;ul>
&lt;li>일정 시간(t) 동안 모든 메시지가 전달되면, 그 이후 연산은 앞선 연산을 순서상 앞서지 않도록 설계&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>이로써 Delayed-t Consistency는 단순한 이론적 모델이 아니라, &lt;strong>구체적인 알고리즘을 통해 구현 가능하고 증명 가능한 약한 일관성 조건&lt;/strong>이라는 점이 논리적으로 뒷받침됩니다.&lt;/p>
&lt;h2 id="마무리">마무리&lt;/h2>
&lt;p>CAP 정리는 세 가지 특성을 모두 만족할 수 없다는 이야기로 자주 소개되지만, 이 논문을 통해 그 제약이 단순한 경험법칙이 아니라 모델과 조건이 주어졌을 때 논리적으로 증명 가능한 것임을 알 수 있었습니다. &lt;del>논문에서 논리로만 설명할 줄은 몰랐네요😂&lt;/del>&lt;/p>
&lt;p>정리에 따르면 비동기나 부분 동기 네트워크에서는 일관성과 가용성을 동시에 만족시키는 것이 원칙적으로 불가능하고, 이런 한계를 어떻게 받아들이고 설계할 것인지는 시스템마다 다를 수밖에 없다는 것을 강조하고 있습니다.&lt;/p>
&lt;p>논문에서 제시한 Delayed-t Consistency는 일관성을 완전히 포기하는 대신, 메시지 손실이 없는 구간 이후에는 다시 원자성을 회복할 수 있도록 제한하는 방식이 인상적이었습니다.&lt;/p>
&lt;p>정리하면서 CAP에 대해 그동안 놓치고 있었던 부분을 정리할 수 있어 좋았네요, 비슷한 고민을 하신 분께도 조금은 도움이 되었으면 합니다.&lt;/p>
&lt;p>끝까지 읽어주셔서 감사합니다.😊&lt;/p></description></item></channel></rss>