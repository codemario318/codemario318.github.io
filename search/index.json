[{"content":"\n프레젠터는 험블 객체(Humble Object) 패턴을 따른 형태로, 아키텍처 경계를 식별하고 보호하는 데 도움이 된다.\n험블 객체 패턴 험블 객체 패턴은 디자인 패턴으로, 테스트하기 어려운 행위와 테스트하기 쉬운 행위를 단위 테스트 작성자가 분리하기 쉽게 하는 방법으로 고안되었다.\n행위들을 두 개의 모듈 또는 클래스로 나눈다. 가장 기본적인 본질은 남기고, 테스트하기 어려운 행우를 모두 험블 객체로 옮긴다. 나머지 모듈에는 험블 객체에 속하지 않은, 테스트하기 쉬운 행위를 모두 옮긴다. GUI는 화면에서 각 요소가 필요한 위치에 적절히 표시되었는지 테스트 해야하므로 작성하기 어려운 반면, GUI에서 수행하는 행위의 대다수는 쉽게 테스트할 수 있다.\n험블 객체 패턴을 사용하면 두 부류의 행위를 분리하여 프레젠터와 뷰라는 서로 다른 클래스로 만들 수 있다.\n프레젠터와 뷰 뷰\n뷰는 데이터를 GUI로 이동시키지만, 데이터를 직접 처리하지는 않는다.\n험블 객체이고 테스트하기 어렵다. 포함된 코드를 가능한 간단하게 유지한다. 프레젠터\n애플리케이션으로 부터 데이터를 받아 화면에 표현할 수 있는 포멧으로 만든다.\n뷰가 데이터를 받아 화면에 전달하는 간단한 일만 처리하도록 만들어준다. 테스트하기 쉬워진다. 뷰는 뷰 모델의 데이터를 화면으로 로드할 뿐이며, 이 외에 뷰가 밭은 역할은 전혀 없기 때문에 뷰는 보잘것없다(Humble)\n테스트와 아키텍처 테스트 용이성은 좋은 아키텍처가 지녀야 할 속성으로 오랫동안 알려저 왔다.\n험블 객체 패턴은 테스트하기 쉬운 부분과 테스트하기 어려운 부분으로 분리하여 아키텍처 경계를 정의하므로 테스트가 용이하다.\n즉, 아키텍처에 험블 객체 패턴을 적절히 활용하면 프레젠터와 뷰와 같이 테스트드가 용이한 구조를 만들 수 있다.\n데이터베이스 게이트웨이 유스케이스 인터랙터와 데이터베이스 사이에는 데이터베이스 게이트웨이(데이터베이스의 쿼리를 처리하는 모든 메서드가 포함)가 위치한다.\n유스케이스 레이어는 SQL을 허용하지 않아야 하므로 필요한 메서드를 제공하는 게이트웨이 인터페이스를 호출하게 되는데, 이 때 데이터베이스 레이어에 존재하는 인터페이스의 구현체가 험블 객체이다.\n이와 달리 인터랙터는 애플리케이션에 특화된 업무 규칙을 캡슐화화기 때문에 험블 객체가 아니므로 게이트웨이는 가짜 데이터(Mock, stub, test-double, fake)를 통해서도 테스트를 할 수 있기 때문에 테스트하기 쉬워진다.\n데이터 매퍼 하이버네이트와 같은 ORM은 어느 계층에 속하는가?\n객체는 데이터 구조가 아니기 때문에, 사실 객체 관계 매퍼(Object Relational Model, ORM)같은 건 사실 존재하지 않는다.\n객체를 사용하는 관점에서 공개된 메서드만 볼 수 있으므로, 단순히 오퍼레이션의 집합이다.\n객체와 달리 데이터 구조는 함축된 행위를 가지지 않는 공개된 변수의 집합이고 사용되는 ORM은 관계형 데이터베이스 테이블로부터 가져온 데이터를 데이터 구조에 맞게 담아주는 역할을 수행하므로 사실 데이터 매퍼이다.\n이러한 ORM 시스템은 데이터베이스 레이어에 위치하며 게이트웨이 인터페이스와 데이터베이스 사이의 일종에 또 다른 험블 객체 경계를 형성한다.\n서비스 리스너 서비스에서도 험블 객체 패턴을 발견할 수 있다.\n애플리케이션은 데이터를 간단한 데이터 구조 형태로 로드한 후, 이 데이터 구조를 경계를 거슬러 특정 모듈로 전달하고, 데이터 구조를 전달받은 모듈이 적절한 포맷으로 만들어 외부 서비스로 전송하게 된다.\n외부로부터 데이터를 수신하는 서비스는 서비스 리스너가 서비스 인터페이스로부터 데이터를 수신하고, 데이터를 애플리케이션에서 사용할 수 있게 간단한 데이터 구조로 포맷을 변경하여 서비스 경계를 가로질러 내부로 전달된다.\n결론 아키텍처 경계마다 험블 객체 패턴을 발견할 수 있다.\n경계를 넘나드는 통신은 거의 모두 간단한 데이터 구조를 수반할 때가 많고, 대개 그 경계는 테스트하기 어려운 무언가와 테스트하기 쉬운 무언가로 분리된다.\n이러한 아키텍처 경계에서 험블 객체 패턴을 사용하면 전체 시스템의 테스트 용이성을 크게 높일 수 있다.\n","date":"2024-04-03T22:31:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/23/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/23/","title":"23. 프레젠터와 험블 객체"},{"content":" 육각형 아키텍처(Hexagonal Architecture) DCI(Data, Context and Interaction) BCE(Boundary-Control-Entity) 위와 같은 아키텍처들의 목적은 관심사의 분리(Separation of concerns)이다.\n소프트웨어를 계층으로 분리함으로써 관심사의 분리라는 목표를 달성할 수 있었다.\n각 아키텍처는 최소한 업무 규칙을 위한 계층 하나와, 사용자와 시스템 인터페이스를 위한 또 다른 계층 하나를 반드시 포함하며, 시스템이 다음과 같은 특징을 가지도록 만든다.\n프레임워크 독립성 테스트 용이성 UI 독립성 데이터베이스 독립성 모든 외부 에이전시에 대한 독립성 의존성 규칙 그림의 각 동원은 소프트웨어의 서로 다른 영역을 표현하는데, 안으로 들어갈수록 고수준의 소프트웨어가 된다.(바깥쪽은 메커니즘, 안쪽은 정책)\n이러한 아키텍처가 동작하도록 하는 가장 중요한 규칙은 소스 코드 의존성은 반드시 안쪽으로, 고수준의 정책을 향해야 한다는 의존성 규칙(Dependency Rule)이다.\n내부의 원에 속한 요소는 외부의 원에 속한 어떤 것도 알지 못한다. 함수, 클래스, 변수, 엔티티 등 외부의 원에 선언된 데이터 형식도 내부의 원에서 절대 사용해서는 안된다. 외부 원에 위치한 어떤 것도 내부 원에 영향을 주지 않아야한다.\n엔티티 엔티티는 전사적인 핵심 업무 규칙을 캡슐화한 것이다.(단순한 단일 애플리케이션이라면 애플리케이션의 업무 객체)\n메서드를 가지는 객체 일련의 데이터 구조와 함수의 집합 기업의 다양한 애플리케이션에서 엔티티를 재사용할 수만 있다면, 그 형태는 그다지 중요하지 않다.\n운영 관점에서 특정 애플리케이션에 무언가 변경이 필요하더라도 엔티티 계층에는 절대로 영향을 주어서는 안 된다.\n유스케이스 유스케이스 계층의 소프트웨어는 애플리케이션에 특화된 업무 규칙을 포함하며, 시스템의 모든 유스케이스를 캡슐화하고 구현한다.\n엔티티로 들어오고 나가는 데이터 흐름을 조정한다. 엔티티가 자신의 핵심 업무 규칙을 사용해서 유스케이스의 목적을 달성하도록 이끈다. 유스케이스 레이어를 잘 격리하여 발생한 변경이 엔티티에 영향을 줘서는 안될 뿐만 아니라, 외부 요소에서 발생한 변경이 이 계층에 영향을 줘서도 안된다.\n유스케이스 레이어는 아래와 같은 상황에서만 영향을 받아야한다.\n운영 관점에서 애플리케이션이 변경되어 유스케이스 자체가 영향을 받은 경우 유스케이스의 세부 사항이 변경된 경우 인터페이스 어댑터 인터페이스 어댑터 레이어는 일련의 어댑터들(Controllers, Gateways, Presenters)로 구성된다.\n어댑터는 데이터를 유스케이스와 엔티티에게 가장 편리한 형식에서 데이터베이스나 웹 같은 외부 에이전시에게 가장 편리한 형식으로 변환한다.\nex) GUI의 MVC 아키텍처 모델은 데이터 구조 정도에 지나지 않으며, 컨트롤러에서 유스케이스로 전달되고, 다시 유스케이스에서 프레젠터와 뷰로 되돌아간다.\n인터페이스 어댑터 레이어는 데이터를 엔티티와 유스케이스에게 가장 편리한 형식에서 영속성용으로 사용 중인 임의의 프레임워크(ex. DB)가 이용하기에 가장 편리한 형식으로 변환한다.\n또한 데이터를 외부 서비스와 같은 외부적인 형식에서 유스케이스나 엔티티에서 사용되는 내부적인 형식으로 변환하는 또 다른 어댑터가 필요하다.\n프레임워크와 드라이버 가장 바깥 레이어인 프레임워크와 드라이버 레이어는 모든 세부사항이 위치하는 곳으로, 일반적으로 데이터베이스나 웹 프레임워크 같은 프레임워크나 도구들로 구성된다.\n이 계층에서는 안쪽 원과 통신하기 위한 접합 코드 외에는 특별히 더 작성해야 할 코드가 그다지 많지 않다.\n웹, 데이터베이스와 같은 세부사항을 모두 외부에 위치시켜서 피해를 최소화한다.\n원은 네 개여야만 하는가? 그림의 원들은 하나의 예시일 뿐이며 더 많은 원이 필요할 수 있다.\n하지만 어떤 경우에도 의존성 규칙은 적용된다.\n소스코드 의존성은 항상 안족을 향한다. 안쪽으로 이동할수록 추상화와 정책의 수준은 높아진다. 가장 바깥쪽 원은 저수준의 구체적인 세부사항으로 구성된다. 안쪽으로 이동할수록 소프트웨어는 점점 추상화되고 더 높은 수준의 정책들을 캡슐화한다. 경계 횡단하기 위 예시에서 컨트롤러와 프레젠터가 다음 계층에 속한 유스케이스와 통신하는 모습을 확인할 수 있다.\n컨트롤에서 시작 유스케이스를 지남 프레젠터에서 실행 유스케이스가 내부 원의 인터페이스를 호출하도록 하고, 외부 원의 프레젠터가 그 인터페이스를 구현하도록 만들어 의존성 흐름을 역전시켰다.\n이처럼 제어흐름과 의존성의 방향이 명백히 반대여야 하는 경우, 의존성 역전 원칙을 사용하여 해결한다.\n인터페이스와 상속 관계 등 아키텍처 경계를 횡단할 때 언제라도 동일한 기법을 사용할 수 있다.\n경계를 횡단하는 데이터는 어떤 모습인가? 경계를 가로지르는 데이터는 흔히 간단한 데이터 구조로 이루어져 있다.\n기본적인 구조체, 간단한 데이터 전송 객체(DTO) 등 함수 호출 시 간단한 인자 해시맵, 객체 중요한 점은 엔티티 객체나 데이터베이스의 행을 전달하는 것이 아닌 격리되어 있는 간단한 데이터 구조가 경계를 가로질러 전달되어야 한다.\n위와 같은 경우 결과적으로 어떠한 형태로든 전달되는 데이터 구조를 통해 내부의 원에서 외부 원의 무언가를 알게 되므로 의존성 규칙을 위배하게된다.\n따라서 경계를 가로질러 데이터를 전달할 때, 데이터는 항상 내부의 원에서 사용하기에 가장 편리한 형태를 가져야한다.\n결론 위와같은 간단한 규칙들을 준수하는 것은 어렵지 않으며, 향후에 겪을 수많은 고통거리를 덜어준다.\n소프트웨어를 계층으로 분리하고 의존성 규칙을 준수한다면 본질적으로 테스트하기 쉬운 시스템을 만들게 될 것이며, 그에 다른 이점을 누릴 수 있다.\n데이터베이스나 프레임워크와 같은 시스템의 외부 요소를 교체하더라도 훨씬 수월해진다. ","date":"2024-04-03T16:22:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/22/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/22/","title":"22. 클린 아키텍처"},{"content":"건물의 청사진을 살펴본다고 가정했을 때, 커다란 정문, 체크인과 체크아웃을 담당할 사서를 위한 공간, 독서 공간, 작은 회의실, 책장을 배치한 진열실이 나타난다면, 이 아키텍처는 \u0026ldquo;도서관\u0026quot;을 위한 아키텍처임을 예상해볼 수 있다.\n이처럼 잘 만들어진 소프트웨어 아키텍처라면 상위 수준의 디렉터리 구조, 최상위 패키지에 담긴 소스 파일등을 살펴보면 어떠한 역할을 수행하는 소프트웨어인지 한눈에 파악할 수 있다.\n아키텍처의 테마 소프트웨어 아키텍처는 시스템의 유스케이스를 지원하는 구조이다.\n- 이바 야콥슨 Ivar Jacobson,\nObject-Oriented Software Engineering: Use Case Driven Approach\n소프트웨어 애플리케이션의 아키텍처도 애플리케이션의 유스케이스에 대해 소리처야한다.\n아키텍처는 프레임워크에 대한 것이 아니며 절대로 그래서도 안된다. 아키텍처를 프레임워크로부터 제공받아서는 절대 안된다. 프레임워크는 사용하는 도구일 뿐, 아키텍처가 준수해야 할 대상이 아니다.\n아키텍처를 프레임워크 중심으로 만들어버리면 유스케이스가 중심이 되는 아키텍처는 절대 나올 수 없다.\n아키텍처의 목적 좋은 아키텍처는 유스케이스를 그 중심에 두기 때문에, 프레임워크나 도구, 환경에 전혀 구애받지 않고 유스케이스를 지원하는 구조를 아무런 문제 없이 기술할 수 있다.\n건물의 청사진의 관심사는 목적에 맞는 공간임을 확실히 하는 것이지, 어떤 기법, 재질로 지어지는지 확인하는 것이 아니다. 좋은 소프트웨어 아키텍처는 유스케이스에 중점을 두며, 지엽적인 관심사에 대한 결합을 분리시켜 개발 환경 문제나 도구에 대해서는 결정을 미루고, 쉽게 번복할 수 있도록 한다.\n하지만 웹은? 웹은 전달 메커니즘(입출력 장치)이며, 애플리케이션 아키텍처에서도 그와 같이 취급해야한다.\n웹을 통해 전달된다는 사실 자체가 세부 사항이므로, 시스템 구조를 지배해서는 안된다. 시스템 아키텍처는 과도한 문제를 일으키거나 근본적인 아키텍처를 뜯어고치지 않더라도 시스템을 콘솔 앱, 웹 앱, 리치 클라이언트, 웹서비스 앱등 다양한 방식으로 전달할 수 있어야 한다.\n프레임워크는 도구일 뿐, 삶의 방식은 아니다 프레임워크는 매우 강력하고 상당히 유용할 수 있지만, 프레임워크가 아키텍처의 기준이 되서는 안된다.\n좋은 아키텍트라면 아키텍처를 유스케이스에 중점을 둔 채 그대로 보존할 수 있을지를 생각해야 하며, 프레임워크가 아키텍처의 중심을 차지하는 일을 막을 수 있는 전략을 개발해야한다.\n테스트하기 쉬운 아키텍처 아키텍처가 유스케이스를 최우선으로 하고, 이로인해 프레임워크와는 적당한 거리를 둔다면, 프레임워크를 전혀 준비하지 않더라도 필요한 유스케이스 전부에 대해 단위 테스트를 할 수 있어야 한다.\n테스트를 돌리는 데 웹 서버가 반드시 필요한 상황이 되어서는 안된다. 데이터베이스가 반드시 연결되어 있어야만 테스트를 돌릴 수 있어서도 안된다. 엔티티 객체는 반드시 오래된 방식의 간단한 객체(Plain Old Object)여야 하며, 여타 복잡한 것들에 의존해서는 안된다.\n유스케이스 객체가 엔티티 객체를 조작하도록 해야하며, 최종적으로 프레임워크로 인한 어려움을 겪지 않고도 이 모두를 있는 그래도 테스트할 수 있어야 한다.\n결론 아키텍처는 유스케이스를 통해 시스템을 이야기해야 하며, 시스템에 적용한 프레임워크에 대해 이야기해서는 안된다.\n시스템이 어떻게 전달될지 알지 못한 상태에서도 시스템의 모든 유스케이스를 이해할 수 있어야한다.\n","date":"2024-04-02T16:19:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/21/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/21/","title":"21. 소리치는 아키텍처"},{"content":"애플리케이션을 업무 규칙과 플러그인으로 구분하려면 업무 규칙이 실제로 무엇인지를 잘 이해해야만 한다.\n핵심 업무 규칙(Critical Business Rule)\n업무 규칙은 사업적으로 수익을 얻거나 비용을 줄일 수 있는 규칙 또는 절차이다. 컴퓨터상으로 구현했는지와 상관없이, 사업적으로 수익을 얻거나 비용을 줄일 수 있어야 한다. 핵심 업무 규칙은 사업 자체에 핵심적이며, 규칙을 자동화하는 시스템이 없더라도 업무 규칙은 그대로 존재한다.\n핵심 업무 데이터\n핵심 업무 규칙이 요구하는 데이터 시스템으로 자동화되지 않은 경우에도 존재하는 데이터이다. 핵심 규칙과 핵심 데이터는 본질적으로 결함되어 있기 때문에 객체로 만들 좋은 후보가 되며 이러한 유형의 객체를 엔티티(Entity)라고 부른다.\n엔티티 엔티티는 컴퓨터 시스템 내부의 객체로서, 핵심 업무 데이터를 기반으로 동작하는 일련의 조그만 핵심 업무 규칙을 구체화한다.\n핵심 업무 데이터를 직접 포함할 수 있다. 핵심 업무 데이터에 매우 쉽게 접근할 수 있다. 엔티티의 인터페이스는 핵심 업무 데이터를 기반으로 동작하는 핵심 업무 규칙을 구현한 함수들로 구성된다.\n엔티티를 생성할 때는\n업무에서 핵심적인 개념을 구현하는 소프트웨어를 한데 모은다. 구축 중인 자동화 시스템의 나머지 모든 고려사항과 분리시킨다. 이 클래스는 업무의 대표자로서 독립적으로 존재한다.\n엔티티는 순전히 업무만을 위한 것이므로 데이터베이스, 사용자 인터페이스, 서드파티 프레임워크에 대한 고려사항들로 인해 오염되어서는 절대 안된다.\n어떤 시스템에서도 업무를 수행할 수 있어야 한다. 시스템의 표현 형식이나 데이터 저장 방식, 시스템에서 컴퓨터가 배치되는 방식과도 무관하다. 엔티티의 유일한 요구 조건은 핵심 업무 데이터와 핵심 업무 규칙을 하나로 묶어 별도의 소프트웨어 모듈로 만들어야 한다는 것이다.\n유스케이스 유스케이스는 자동화된 시스템이 동작하는 방법을 정의하고 제약함으로써 수익을 얻거나 비용을 줄이는 업무 규칙이다.\n자동화된 시스템이 사용되는 방법을 설명한다. 사용자가 제공해야 하는 입력을 기술한다. 사용자에게 제공해야하는 출력을 기술한다. 해당 출력을 생성하기 위한 처리 단계를 기술한다. 엔티티 내의 핵심 업무 규칙과는 반대로, 애플리케이션에 특화된 업무 규칙을 설명한다.\n인터페이스로 들어오는 데이터와 인터페이스에서 도될려주는 데이터를 형식 없이 명시한다는 점만 빼면, 유스케이스는 사용자 인터페이스를 기술하지 않는다.\n유스케이스는 시스템이 사용자에게 어떻게 보이는지를 설명하지 않는다.\n애플리케이션에 특화된 규칙을 설명하며, 이를 통해 사용자와 엔티티 사이의 상호작용을 규정한다.\n애플리케이션에 특화된 업무 규칙을 구현하는 하나 이상의 함수를 제공한다. 입력 데이터를 포함한다. 출력 데이터를 포함한다. 유스케이스가 상호작용하는 엔티티에 대한 참조 데이터를 포함한다. 유스케이스는 단일 애플리케이션에 특화되어 있으며, 따라서 해당 시스템의 입력과 출력에 보다 가깝게 위치하므로, 엔티티와 같은 고수준 개념은 유스케이스와 같은 저수준 개념에 대해 아무것도 알지 못한다.\n즉, 유스케이스는 엔티티에 의존하며, 엔티티는 유스케이스에 의존하지 않는다.\n요청 및 응답 모델 유스케이스는 입력 데이터를 받아서 출력 데이터를 생성한다.\n하지만 제대로 구성된 유스 케이스 객체라면 데이터를 사용자나 또 다른 컴포넌트와 주고 받는 방식에 대해서는 전혀 눈치챌 수 없어야 한다.\n유스케이스는 단순한 요청 데이터 구조를 입력으로 받아들이고 단순한 응답 데이터 구조를 출력으로 반환하는 역할만 수행하며, 이러한 데이터 구조는 어떤것에도 의존하지 않아야 한다.\nHttpRequest, HttpResponse 등 요청 및 응답 모델이 독립적이지 않다면 그 모델에 의존하는 유스케이스도 결국 해당 모델이 수반하는 의존성에 간접적으로 결합이 되므로 의존성을 제거해야 한다.\n엔티티와 요청/응답 모델은 많은 데이터를 공유하므로 엔티티의 참조를 요청/응답 데이터 구조에 포함하려는 유혹을 받을 수 있다.\n하지만 두 객체의 목적은 완전히 다르므로, 시간이 지남에 따라 다른 이유로 변경될 것이다.\n따라서 어떤 식으로든 함께 묶는 행위는 공통 폐쇄 원칙과 단일 책임 원칙을 위배하게 되며, 결국 코드에는 수많은 떠돌이 데이터가 만들어지고, 이로인해 수많은 조건문이 추가되어 버린다.\n결론 업무 규칙은 소프트웨어 시스템이 존재하는 이유, 핵심적인 기능이다.\n업무 규칙은 수익을 내고 비용을 줄이는 코드를 수반하는 매우 중요한 요소이다.\n따라서 사용자 인터페이스나 데이터베이스와 같은 저수준의 관심사로 인해 오염되어서는 안 되며, 원래 그대로의 모습으로 남아 있어야 한다.\n이상적으로는 업무 규칙을 표현하는 코드는 반드시 시스템의 심장부에 위치해야 하며, 덜 중요한 코드는 이 심장부에 플러그인되어야 한다.\n업무 규칙은 시스템에서 가장 독립적이며 가장 많이 재사용할 수 있는 코드여야 한다.\n","date":"2024-03-22T00:23:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/20/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/20/","title":"20. 업무 규칙"},{"content":"소프트웨어 시스템이란 정책을 기술한 것이다.\n컴퓨터 프로그램은 각 입력을 출력으로 변환하는 정책을 상세하게 기술한 설명서이다.\n대다수의 주요 시스템에서 하나의 정책은 이 정책을 서술하는 여러 개의 조그만 정책들로 쪼갤 수 있다.\n소프트웨어 아키텍처를 개발하는 기술에는 정책을 신중하게 분리하고, 정책이 변경되는 양상에 따라 정책을 재편성하는 일도 포함된다.\n동일한 이유로 동일한 시점에 변경되는 정책은 동일한 수준에 위치하며, 동일한 컴포넌트에 속해야 한다. 서로 다른 이유, 다른 시점에 변경되는 정책은 다른 수준에 위치하며, 반드시 컴포넌트로 분리해야 한다. 아키텍처 개발은 재편성된 컴포넌트들을 비순환 방향 그래프(directed acyclic graph)로 구성하는 기술을 포함한다.\n정점(node): 동일한 수준의 정책을 포함하는 컴포넌트 간선(edge): 컴포넌트 사이의 의존성 좋은 아키텍처라면 각 컴포넌트를 연결할 때 의존성의 방향이 컴포넌트의 수준을 기반으로 저수준 컴포넌트가 고수준 컴포넌트에 의존하도록 설계해야 한다.\n수준 수준(level)은 입력과 출력까지의 거리로 정의할 수 있다.\n시스템의 입력과 출력 모두로부터 멀리 위치할수록 정책의 수준은 높아지며, 입력과 출력을 다루는 정책이라면 시스템에서 최하위 수준에 위치한다.\n간단한 암호화 프로그램의 설계 예시 처럼 프로그램을 제대로 설계했다면 소스 코드 의존성은 곧은 점선처럼 표시되어야 한다.\nTranslate 컴포넌트는 입력과 출력에서 가장 멀리 떨어져 있으므로 최고 수준의 컴포넌트이다.\n굽은 실선은 데이터의 흐름을 나타내는데, 보는 것과 같이 데이터 흐름과 소스 코드 의존성이 항상 같은 방향을 가리키지는 않는다.\n소스 코드 의존성은 그 수준에 따라 결합되어야 하며, 데이터 흐름을 기준으로 결합되어서는 안된다는 것이다. 1 2 3 4 function encrypt() { while(true) writeChar(translate(readChar())); } 위와 같은 예시는 고수준인 encrypt 함수가 저수준인 readChar, writeChar 함수에 의존하기 때문에 잘못된 아키텍처이다.\nEncrypt 클래스, CharWriter와 CharReader 인터페이스를 둘러싸고 있는 점선으로 된 경계로 묶인 영역이 이 시스템에서 최고 수준의 구성요소이며, 횡단하는 의존성은 모두 경계 안쪽으로 향한다.\n입력과 출력에 변화가 생기더라도 암호화 정책은 거의 영향을 받지 않기 때문에, 고수준의 암호화 정책을 저수준의 입력/출력 정책으로 부터 분리시켜 암호화 정책을 더 넓은 맥락에서 사용할 수 있다.\n정책을 컴포넌트로 묶는 기준은 정책이 변경되는 방식에 달려있다.\n단일 책임 원칙(SRP)과 공통 폐쇄 원칙(CCP)에 따르면 동일한 이유로 동일한 시점에 변경되는 정책은 함께 묶인다.\n고수준 정책: 입력/출력에서부터 멀리 떨어진 정책 저수준 정책에 비해 덜 빈번하게 변경된다. 보다 중요한 이유로 변경된다. 저수준 정책: 입력과 출력에 가까운 정책 더 빈빈하게 변경된다. 긴급성을 요하며, 덜 중요한 이유로 변경된다. 이처럼 소스 코드 의존성 방향이 고수준 정책을 향할 수 있도록 정책을 분리했다면 변경의 영향도를 줄일 수 있다.\n시스템의 최저 수준에서 중요하지 않지만 긴급한 변경이 발생하더라도, 중요한 수준에 미치는 영향이 거의 없게 된다. 이는 저수준 컴포넌트가 고수준 컴포넌트에 플러그인되어야 한다는 관점으로도 바라볼 수 있다.\n결론 정책을 분리하는 것은 단일 책임 원칙, 개방 폐쇄 원칙, 공통 폐쇄 원칙, 의존성 역전 원칙, 안정된 의존성 원칙, 안정된 추상화 원칙을 모두 포함한다.\n","date":"2024-03-21T23:41:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/19/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/19/","title":"19. 정책과 수준"},{"content":"시스템 아키텍처는 일련의 소프트웨어 퀌포넌트와 그 컴포넌트들을 분리하는 경계에 의해 정의된다.\n이러한 경계는 다양한 형태로 나타난다.\n경계 횡단하기 \u0026lsquo;런타임에 경계를 횡단한다\u0026rsquo;는 의미는 경계 한쪽에 있는 기능에서 반대편 기능을 호출하여 데이터를 전달하는 일에 불과하다.\n적절한 위치에서 경계를 횡단하게 하는 비결은 소스 코드 의존성 관리에 있다.\n소스 코드 모듈 하나의 변경으로 읜존하는 다른 소스 코드 모듈을 변경하거나, 다시 컴파일해서 새로 배포해야 할 지도 모르기 때문 경계는 소스 코드 변경이 전파되는 것을 막는 방화벽을 구축하고 관리하는 수단으로써 존재한다.\n두려운 단일체 아키텍처 경계 중 가장 단순하며 흔한 형태는 물리적으로 엄격하게 구분되지 않는 형태다.\n이는 앞서 언급했던 소스 수준 분리 모드로, 함수와 데이터가 단일 프로세서에서 같은 주소 공간을 공유하며 나름의 규칙에 따라 분리되어있는 상태이다.\n배포 관점에서 이는 단일체(monolith)라고 불리는 단일 실행 파일이므로, 외부에서 볼 때(물리적으로) 경계가 밖으로 드러나지는 않는다.\n가장 단순한 형태의 경계 횡단은 저수준 클라이언트에서 고수준 서비스로 향하는 함수 호출이다.\n이 경우 런타임 의존성과 컴파일타임 의존성은 모두 저수준 컴포넌트에서 고수준 컴포넌트로 향한다.\n고수준 클라이언트가 저수준 서비스를 호출해야 한다면 동적 다형성을 사용하여 제어흐름과는 반대 방향으로 의존성을 역전시킬 수 있다.\n경계를 횡단할 때 의존성은 모두 고수준 컴포넌트를 향하고 있으며, 데이터 구조의 정의가 호출하는 쪽에 위치한다.\n정적 링크된 모노리틱 구조의 실행 파일이라도 규칙적인 방식으로 구조를 분리하면 프로젝트를 개발, 테스트, 배포하는 작업에 큰 도움이 된다.\n단일체에서 컴포넌트간 통신은 전형적인 함수 호출에 지나지 않으므로 매우 빠르고 값싸며, 이러한 이유로 소스 수준에서 결합이 분리되면 경계를 가로지르는 통신은 상당히 빈번할 수 있다.\n배포형 컴포넌트 아키텍처의 경계가 물리적으로 드러날 수도 있는데 그중 가장 단순한 형태는 동적 링크 라이브러리다.\n이는 배포 수준 결합 분리 모드에 해당하며 컴포넌트를 동적 링크 라이브러리 형태로 배포하면 따로 컴파일하지 않고 곧바로 사용할 수 있는 대신 바이너리와 같이 배포 가능한 형태로 전달된다.\n배포 관점에서 이러한 형태는 단순히 배포 가능한 단위를 좀 더 편리한 형태로 묶으므로, 단일체와 동일하다.\n모든 함수가 동일한 프로세서와 주소 공간에 위치하며, 컴포넌트 간 의존성을 관리하는 전략도 동일하다. 단일체와 마찬가지로 경계를 가로지르는 통신은 함수 호출에 지나지 않으므로 값싸기 때문에, 경계를 가로지르는 통신은 대체로 매우 빈번하다.\n스레드 스레드는 아키텍처 경계도 아니며 배포 단위도 아니다.\n단일체와 배포형 컴포넌트 모두 스레드를 활용할 수 있다.\n스레드는 실행 계획과 순서를 체계화 하는 방법에 가까우며, 모든 스레드가 단 하나의 컴포넌트에 포함될 수도 있고, 분산될 수도 있다.\n로컬 프로세스 로컬 프로세스는 이전 언급한 경계보다 훨씬 강한 물리적 형태를 띈다.\n주로 명령행이나 그와 유사한 시스템 호출을 통해 생성되고, 동일한 프로세서 또는 하나의 멀티코어 시스템에 속한 여러 프로세서들에서 실행되지만, 독립된 주소 공간에서 실행된다.\n종종 공유 메모리 파티션을 사용하기도 하지만, 일반적으로는 메모리 보호를 통해 프로세스들이 메모리를 공유하지 못하게 한다.\n대개의 경우 소켓, 메일박스, 메시지 큐와 같이 운영체제에서 제공하는 통신 기능을 이용하여 서로 통신한다.\n각 로컬 프로세스는 정적으로 링크된 단일체 이거나 동적으로 링크된 여러개의 컴포넌트로 구성될 수 있다.\n정적으로 링크된 단일체 여러 모노리틱 프로세스가 같은 컴포넌트들을 가지고 있을 수 있다. 컴파일하고 정적 링크하는 과정에서 각 컴포넌트의 바이너리가 단일체에 물리적으로 복사되기때문 동적으로 링크된 배포형 컴포넌트 동적으로 링크된 배포형 컴포넌트들을 서로 공유할 수 있다. 로컬 프로세스는 컴포넌트 간 의존성을 동적 다형성을 통해 관리하는 저수준 컴포넌트로 구성되므로 일종의 최상위 컴포넌트로 볼 수 있다.\n로컬 프로세스 간 분리 전략은 단일체나 바이너리 컴포넌트의 경우와 동일하다.\n소스 코드 의존성은 고수준 컴포넌트를 향해야하므로 로컬 프로세스에서는 고수준 프로세스의 소스 코드가 저수준 프로세스의 이름, 물리 주소, 레지스트리 조회 키를 절대로 포함해서는 안된다.\n로컬 프로세스 경계를 지나는 통신에는 운영체제 호출, 데이터 마샬링, 언마샬링, 프로세스 간 문맥 교환 등이 있으며, 이들은 제법 비싼 작업이므로 통신이 너무 빈번하게 이뤄지지 않도록 신중하게 제한해야한다.\n서비스 물리적인 형태를 띠는 가장 강력한 경계는 서비스다.\n서비스는 프로세스로, 일반적으로 명령행 또는 그와 동등한 시스템 호출을 통해 구동된다.\n서비스들은 모든 통신이 네트워크를 통해 이뤄진다고 가정하므로 자신의 물리적 위치에 구애받지 않는다.\n서비스 경계를 지나는 통신은 함수 호출에 매우 느리므로 가능하다면 빈번하게 통신하는 일을 피해야 하며, 지연(latency)에 따른 문제를 고수준에서 처리할 수 있어야 한다.\n이를 제외하고는 로컬 프로세스에 적용한 규칙이 서비스에 적용된다.\n저수준 서비스는 반드시 고수준 서비스에 플러그인되어야하며, 고수준 서비스의 소스 코드에는 저수준 서비스를 특정 짓는 어떤 물리적인 정보(URI 등)도 절대로 포함해서는 안된다.\n결론 단일체를 제외한 대다수의 시스템은 한 가지 이상의 경계 전략을 사용한다.\n실제로 서비스는 상호작용하는 일련의 로컬 프로세스 퍼사드에 불과할 때가 많다.\n또한 개별 서비스 또는 로컬 프로세스는 거의 언제나 소스 코드 컴포넌트로 구성된 단일체이거나, 동적으로 링크된 배포형 컴포넌트의 집합이다.\n즉, 한 시스템 안에서도 통신이 빈번한 로컬 경계와 지연을 중요하게 고려해야 하는 경계가 혼합되어 있음을 의미한다.\n","date":"2024-03-21T15:36:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/18/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/18/","title":"18. 경계 해부학"},{"content":"소프트웨어 아키텍처는 선을 긋는 기술이며, 이러한 선을 경계(Boundary)라고 부른다.\n경계는 소프트웨어 요소를 서로 분리하고, 경계 한편에 있는 소소가 반대편에 있는 요소를 알지 못하도록 막는다.\n이 중 초기에 그려지는 선은 가능한 한 오랫동안 결정을 연기시키기고, 결정이 핵심적인 업무 로직을 오염시키지 못하게 만들려는 목적으로 쓰인다.\n아키텍트의 목표는 시스템을 만들고 유지하는 데 드는 자원을 최소화 하는 것인데, 효율을 떨어뜨리는 요인은 결합(Coupling)이며, 너무 일찍 내려진 결정에 따른 결합은 더 큰 영향을 미친다.\n너무 일찍 내려진 결정은 시스템의 업무 요구사항, 즉 유스케이스와 아무런 관련이 없는 결정을 의미한다.\n프레임워크 데이터베이스 웹 서버 유틸리티 라이브러리 의존성 주입에 대한 결정 등 좋은 시스템 아키텍처란 유스케이스와 아무런 관련이 없는 결정에 의존하지 않아 이러한 결정이 부수적이며, 연기할 수 있는 아키텍처다.\n경계선을 긋는 행위는 결정을 늦추고 연기하는 데 도움이되며, 궁극적으로는 시간을 엄청나게 절약해주었으며, 골치를 썩지 않게 해준다.\n어떻게 선을 그을까? 언제 그을까? 관련이 있는 것과 없는 것 사이에 선을 긋는다.\nGUI는 업뮤 규칙과는 관련 없기 때문에, 둘 사이에는 반드시 선이 있어야한다. 데이터베이스는 GUI와는 고나련이 없으므로, 둘 사이에는 반드시 선이 있어야한다. 데이터베이스는 업무 규칙과 관련이 없으므로, 둘 사이에도 선이 있어야한다. 데이터베이스는 업무 규칙과 서로 떼어놓을 수 없는 관계라고 배운 사람이 많으며, 심지어 업무 규칙이 구체화된 것이 데이터베이스라고 확신하는 사람도 더러 있지만 이는 잘못된 생각이다.\n업무 규칙은 데이터를 가져오고 저장할 때 사용할 수 있는 함수 집합이 있다는 사실이 전부여야한다.\n이러한 함수 집합을 통해 데이터베이스를 인터페이스 뒤로 숨길 수 있다.\nDatabaseAccess에서 출발하는 화살표는 클래스로부터 바깥쪽으로 향하는데 이는 DatabaseAccess가 존재하는 클래스는 없다는 의미이다.\nDatabaseInterface 클래스는 BusinessRules 컴포넌트에 속하며, DatabaseAccess 클래스는 Database 컴포넌트에 속하므로, Database는 BusinessRules에 대해 알고있지만, BusinessRules은 Database에 관해 알지 못한다.\n따라서 BusinessRules에게 있어 Database는 문제가 되지 않지만, Database는 BusinessRules 없이는 존재할 수 없다.\nDatabase 컴포넌트는 BusinessRules가 만들어 낸 호출을 데이터베이스의 쿼리 언어로 변환하는 코드를 담고 있으며, 이 변환 코드가 BusinessRules를 알고 있는 것이다.\n두 컴포넌트 사이에 경계선, 화살표의 방향이 BusinessRules를 향하도록 만들었기 때문에 어떤 종류의 데이터베이스도 사용할 수 있게된다.\n따라서 데이터베이스에 대한 결정을 연기할 수 있으며, 데이터베이스를 결정하기에 앞서 업무 규칙을 먼저 작성하고 테스트하는 데 집중할 수 있다.\n입력과 출력은? 입력과 출력은 중요하지않다.\n시스템의 행위를 입출력이 지닌 행위적 측면에서 생각하는 경향이 있는데, 이러한 입출력 뒤에는 모델(데이터 구조와 함수로 구성된 정교한 집합)이 존재한다는 사실을 잊는다.\n이러한 모델은 GUI가 없이도 동작할 수 있으므로 중요하지 않고, 실제로 중요한 것은 업무 규칙이다.\nGUI 컴포넌트는 BusinessRules 컴포넌트에게 의존하기 때문에 경계선으로 분할할 수 있다.\n따라서 GUI는 다른 종류의 인터페이스로 얼마든지 교체할 수 있으며 BusinessRules에 끼치는 영향은 없다.\n플러그인 아키텍처 데이터베이스와 GUI에 대해 내린 두 가지 결정을 하나로 합쳐서 보면 컴포넌트 추가와 관련한 일정의 패턴이 만들어진다.\n소프트웨어 개발 기술의 역사는 프러그인을 손쉽게 생성하여, 확장 가능하며 유지보수가 쉬운 시스템 아키텍처를 확립할 수 있게 만드는 방법에 대한 이야기이다.\n선택적이거나 또는 수많은 다양한 형태로 구현될 수 있는 나머지 컴포넌트로부터 핵심적인 업무 규칙은 분리되어 있고, 또한 독립적이다. 위와 같은 설계에서 사용자 인터페이스는 플러그인 형태로 고려되었기에, 수많은 종류의 사용자 인터페이스를 플러그인 형태로 연결할 수 있게 된다.\n데이터베이스도 동일하게 적용할 수 있다.\nGUI, 데이터베이스 등의 교체 작업이 쉬운 작업은 아니고, 업무 규칙 활용을 위해 추가적인 작업도 필요할 수 있지만, 플러그인 구조를 가정한 채 시작함으로써, 이러한 변경 작업을 현실성 있도록 만든다.\n플러그인에 대한 논의 시스템을 플러그인 아키텍처로 배치함으로써 변경이 전파될 수 없는 방화벽을 생성할 수 있다.\n경계는 변경의 축(Axis of change)이 있는 지점에 그어진다. 그리고 경계의 한쪽에 위치한 컴포넌트는 경계 반대편의 컴포넌트와는 다른 속도로, 다른 이유로 변경된다.\n따라서 다른 시점에 다른 속도로 변경되는 둘 사이에는 반드시 경계가 필요하다.\n이 역시도 단일 책임 원칙에 해당하며, 단일 책임 원칙은 어디에 경계를 그어야 할지 알려준다.\n결론 소프트웨어 아키텍처에서 경계선을 그리려면\n먼저 시스템을 컴포넌트 단위로 분할해야한다.\n일부 컴포넌트는 핵심 업무 규칙에 해당한다. 나머지 컴포넌트는 플러그인으로 핵심 업무와는 직접적인 관련이 없지만 필수 기능을 포함한다. 컴포넌트 사이의 화살표가 핵심 업무를 향하도록 컴포넌트의 소스를 배치한다.\n이는 의존성 역전 원칙과 안정된 추상화 원칙을 응용한 것이며, 의존성의 화살표는 저수준 세부사항에서 고수준의 추상화를 향하도록 배치된다.\n","date":"2024-03-20T21:36:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/17/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/17/","title":"17. 경계: 선 긋기"},{"content":"앞서 진행한 데이터셋 구조 확인을 바탕으로, 간단하게 데이터베이스 설계를 해봤습니다.\nTAGS, RATINGS 테이블은 user_id, movie_id 컬럼을 활용한 관계 테이블인데 USERS 테이블에 관련된 데이터들을 별도로 제공하지 않았기 때문에 있다고 가정하고 진행했습니다.\nerDiagram MOVIES { int movie_id PK string title string genres int rating_count bigint rating_total } RATINGS { int user_id PK,FK int movie_id PK,FK int rating timestamp created_at } REVIEWS { int review_id PK int user_id FK int movie_id FK string content timestamp created_at timestamp updated_at } TAGS { string name PK timestamp created_at } REVIEWS_TAGS { int user_id PK,FK int movie_id PK,FK int tag_name PK,FK } USERS { int id PK } MOVIES ||--o{ RATINGS: \"\" RATINGS }o--|| USERS: \"\" MOVIES ||--o{ REVIEWS: \"\" REVIEWS }o--|| USERS: \"\" REVIEWS ||--o{ REVIEWS_TAGS: \"\" REVIEWS_TAGS }o--|| TAGS: \"\" 기존 데이터를 기반으로 일부 컬럼들과 테이블을 추가하였습니다.\n자세한 내용은 테이블 별로 설명해보겠습니다.\nMOVIES erDiagram MOVIES { int movie_id PK string title string genres int rating_count bigint rating_total } 기존 제공된 movie_id, title, genres는 그대로 활용하고, 평균 평점 계산에 활용될 rating_count, rating_total 컬럼을 추가했습니다.\ngenres 컬럼은 장르들이 | 문자로 구분되어 문자열 형태로 저장되는 형식이었는데, 처음 RDBMS를 이용해 만드는 방식에서는 검색 조건등의 활용을 배제하고, 이후 검색 엔진 같은 다른 방식으로 제공해 볼 예정입니다.\nrating_count, rating_total 컬럼은 조회 후 서버나 클라이언트 측에서 직접 계산하는 방식을 사용할 수 있고, MySQL, PostgreSQL등과 같이 함수 기반 인덱스 기능을 제공하는 RDBMS를 활용한다면 조건을 활용한 조회 성능에도 이슈가 발생하지 않을 것이라 생각되어 추가하였습니다.\nrating_count, rating_total 컬럼은 사용자가 평점을 추가하거나 수정할 때 변경되는 값이므로 평점 등록, 수정, 삭제 처리시 해당 컬럼 처리 로직 구현에 주의가 필요하겠습니다.\nRATINGS erDiagram RATINGS { int user_id PK,FK int movie_id PK,FK int rating timestamp create_ts } 기존 제공된 user_id, movie_id, create_ts는 그대로 활용하고, rating 컬럼의 데이터 타입을 변경하였습니다.\nrating 같은 경우 기존에는 0.5 간격 0~5 까지의 소수를 저장하고 있었지만, MOVIES 테이블의 rating_total 처리와 평균 평점 계산 처리가 필요하므로, 개인 최고점을 10점으로 변경하고 서비스 로직에서 평점을 계산하는 방식을 고려하는게 더 합리적이라 판단하여 정수형 데이터타입을 선택하였습니다.\n이외에도 rating_id와 같은 별도 PK를 만드는 것을 고려해봤지만, rating_id 컬럼이 다른 테이블에서 활용되거나 PK가 변경거나 user_id, movie_id 없이 단독으로 할용되는 경우는 없을 것이라 판단되어 별도로 추가하지는 않았습니다.\nTAGS, REVIEWS, REVIEWS_TAGS 기본 제공된 데이터에서 TAGS는 전체 2,328,315건 중 유니크한 tag는 153,950개 였습니다.\nerDiagram TAGS { int user_id PK,FK int movie_id PK,FK int tag PK,FK timestamp create_ts } 위와 같은 구조를 확인할 수 있었는데, 일반적으로 이러한 형태의 태그는 글과 함께 사용되는 해시태그 같은 기능이어서 어떻게 구조를 가져가야할지 고민이 들었습니다.\n제 추측으로는 이 데이터의 목적은 추천 모델 개발이므로 사용자와 영화의 관계, 사용자가 자주 활용한 태그, 특정 영화에 많이 활용된 태그 등 user_id, movie_id를 기준으로 활용하는 것이 더 적합했기 때문이라고 생각되었습니다.\n이 부분들을 고려하여 구성한 테이블 구조는 아래와 같습니다.\nerDiagram TAGS { string name PK timestamp create_ts } REVIEWS_TAGS { int user_id PK,FK int movie_id PK,FK int tag_name PK,FK } REVIEWS { int review_id PK int user_id FK int movie_id FK string content timestamp create_ts timestamp update_ts } TAGS ||--o{ REVIEWS_TAGS: \"\" REVIEWS_TAGS }o--|| REVIEWS: \"\" 기존 TAGS를 REVIEWS_TAGS로 바꿔 연결 테이블로 구성했습니다.\nTAGS 테이블은 tag만을 가지게 구성하고, 사용자가 영화에 후기를 남기는 기능을 고려하여 REVIEWS 테이블을 추가하였습니다.\nTAGS 테이블은 앞서 언급했던 데이터 중복 뿐만 아니라 특정 태그를 활용한 게시글 검색등 활용될 여지가 많기 때문에 추가하였습니다.\n사용 횟수 같은 컬럼들을 추가하는 것도 고려하였으나, 현 상황에서는 (tag_name) 인덱스 추가 정도로 충분히 빠른 조회를 수행할 수 있기 때문에 게시글 작성, 수정, 삭제 등의 로직 처리에 변수들을 배제하는것이 좋겠다고 판단하였습니다.\nREVIEWS 테이블은 비즈니스 요구사항(베스트 후기 등)에 따라 단독으로 활용될 여지가 많다고 생각되어 RATINGS 테이블과 다르게 review_id를 추가하였습니다.\nREVIEWS_TAGS 테이블은 기존 외래키인 user_id, movie_id를 review_id로 변경하지 않고 사용했는데, 위에서 언급한대로 사용자와 영화의 관계를 쉽게 이용하려면 기존 방식이 더 적합하고, REVIEWS와 관련된 처리가 필요하다면 인덱스를 활용하여 충분히 쉽게 처리할 수 있다고 판단되었습니다.\n끝으로 데이터셋 구조 확인에 이어 간단히 데이터베이스 구조를 구상해봤습니다.\n만들어 놓고 보니 뭔가 찜찜한 기분이 계속 남네요..\n제 역량이 부족해서 어떤 문제들이 발생하게 될지 예상할 수는 없지만 앞으로 발견되는 부족한 부분들은 프로젝트를 진행하면서 조금씩 수정해나가겠습니다.\n끝까지 읽어주셔서 감사합니다 :)\n","date":"2024-03-19T16:00:01+09:00","image":"https://codemario318.github.io/post/movielens/2/cover_hu223669cbbbe5efa1c70286790dba8a54_16582_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/movielens/2/","title":"[MovieLens] 2. DB 설계"},{"content":"안녕하세요 :D 서칭하던 중 재미있어보이는 데이터를 발견했습니다.\n미네소타 대학의 컴퓨터 과학 연구 그룹인 GroupLens에서 공개한 MovieLens 데이터셋으로, GroupLens의 연구 분야 중 하나인 인공지능 추천 시스템을 위해 만들어진 데이터셋입니다.\n86,537편의 영화와 그와 관련된 33,832,162개의 평점 및 2,328,315개의 태그로 구성되어있는데, 볼륨이 꽤 커서 직접 활용해보지 못했던 기술들을 실전에 유사한 환경에서 만들어 볼 수 있을것 같다는 생각이 들었습니다.\n프로젝트는 처음 RDBMS 부터 시작하여, 일부 데이터들을 다른 기술들로 전환하여 점진적으로 개선하는 방식으로 진행해보려고 합니다.\n오늘은 첫 걸음으로 데이터가 어떤 형식으로 구성되어있는지 간략하게 살펴보고, 어떤 형태로 설계할지 고민해보겠습니다.\nPython3와 Pandas를 활용하여 간단히 확인를 했는데, 환경 구성과 같은 내용들은 Movielens 데이터셋 구조 확인 레포지토리를 참고해주시면 되겠습니다.\n데이터셋에 대한 정보는 README.html에 상세히 나와있습니다.\n데이터셋 구성 데이터셋은 userId, movieId를 공통으로 활용하고 있고, 앞서 설명드렸던 영화, 평점, 태그 포함 총 6개의 .csv 파일로 구성되어 있습니다.\nuserId\nMovieLens의 사용자로 무작위 선택 movieId\nMovieLens에 등록된 영화로, 등급이나 태그가 하나 이상 등록된 영화 포함된 데이터 중 영화에 대한 태그 관련성 점수를 포함하는 데이터 셋인 태그 게놈(genome-scores.csv, genome-tags.csv)와 영화 데이터의 다른 소스에 연결하기 위한 식별자 정보가 담겨있는 links.csv 데이터는 활용하지 않겠습니다.\nmovies 영화 데이터는 movieId, title, genres 로 구성되어 있습니다.\n영화의 제목들은 직접 입력되었거나, https://www.themoviedb.org/에서 가져온 데이터로, 괄호 안에 개봉년이 포함되어 있으나 정확하지 않을 수 있다고 합니다.\n영화 장르들을 모두 포함하고있는 genres 컬럼은 아래의 19개 장르중 일부를 | 문자로 합친 형태로 구성된다고 합니다.\nAction Adventure Animation Children\u0026rsquo;s Comedy Crime Documentary Drama Fantasy Film-Noir Horror Musical Mystery Romance Sci-Fi Thriller War Western (no genres listed) 장르가 없는 경우는 (no genres listed)가 입력되는 것으로 보아 NULL을 허용하지는 않는 것으로 보이네요.\n1 2 3 4 5 import pandas as pd df = pd.read_csv(\u0026#34;dataset/movies.csv\u0026#34;) len(df) # 86537 파일을 pandas DataFrame으로 열고 건수를 확인해보니, 처음 언급한대로 86,537건이었습니다.\n1 df.head() 최상단 5건을 확인해보니, README.html에 언급된 형식으로 데이터들이 저장되어 있는 것으로 보입니다.\nmovieId title genres 1 Toy Stroy (1995) Adventure|Animation|Children|Comedy|Fantasy 2 Jumanji (1995) Adventure|Children|Fantasy 3 Grumpier Old Men (1995) Comedy|Romance 4 Waiting to Exhale (1995) Comedy|Drama|Romance 5 Father of the Bride Part II (1995) Comedy genres 컬럼 같은 경우 공식 설명과는 달리 총 20개로 IMAX가 추가되있습니다. 누락되었나보네요.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 genres_list = df[\u0026#34;genres\u0026#34;].map(lambda x: x.split(\u0026#34;|\u0026#34;)) genres = set() for l in genres_list: genres.update(l) print(len(genres)) # 20 print(genres) \u0026#39;\u0026#39;\u0026#39; { \u0026#39;Adventure\u0026#39;, \u0026#39;War\u0026#39;, \u0026#39;(no genres listed)\u0026#39;, \u0026#39;Action\u0026#39;, \u0026#39;Mystery\u0026#39;, \u0026#39;Fantasy\u0026#39;, \u0026#39;Film-Noir\u0026#39;, \u0026#39;Comedy\u0026#39;, \u0026#39;Romance\u0026#39;, \u0026#39;IMAX\u0026#39;, \u0026#39;Crime\u0026#39;, \u0026#39;Musical\u0026#39;, \u0026#39;Animation\u0026#39;, \u0026#39;Drama\u0026#39;, \u0026#39;Horror\u0026#39;, \u0026#39;Children\u0026#39;, \u0026#39;Sci-Fi\u0026#39;, \u0026#39;Thriller\u0026#39;, \u0026#39;Western\u0026#39;, \u0026#39;Documentary\u0026#39; } \u0026#39;\u0026#39;\u0026#39; 1 len(df[df[\u0026#39;genres\u0026#39;].str.contains(\u0026#39;IMAX\u0026#39;)]) # 195 IMAX가 포함된 장르를 가지는 레코드들은 총 195건으로 많지는 않지만 데이터 분포 같은 것들이 중요한 요소는 아니기때문에 그냥 활용해도 괜찮을 것 같습니다.\n눈에 띄는점은 genres 컬럼인데, 각 장르가 | 문자로 구분되어 여러개 항목이 들어있습니다. 장르들이 영어 오름차순으로 정렬되어 저장되어있는 것으로 보이네요.\n이러한 경우 일반적인 RDBMS에서 genres를 조건으로 이용하여 SELECT하게 될 경우 %{keyword}%로 처리해야하므로 인덱스를 활용할 수 없고, 이로 인해 성능에서 문제가 발생할 수 있습니다.\ngenres 컬럼들의 각 장르들을 카테고리 테이블로 분리하고 movieId와 장르 간 1:N 테이블을 추가하는 방식으로 바꾼다면 장르를 이용한 검색 조건으로 인덱스를 사용할 수 없는 문제는 해결할 수 있습니다만\u0026hellip;\ngenres가 여러개의 장르를 포함할 수 있다는 특성으로 인해 쿼리 작성할 때 조인을 사용하면 movies의 컬럼들이 중복됩니다.\n이로 인해 서브 쿼리나 CONCAT과 같은 처리를 필요로 하거나, movieId로 장르를 조회하는 방식으로 처리해야 하므로 조회 성능이 떨어질 수 있음은 물론, 테이블 구조가 불필요하게 복잡해집니다.\ngenres의 데이터 형태와 한번 등록되면 변하지 않는 장르의 특성으로 볼 때, 단순 조회를 위한 컬럼이지 않을까 추측되고, 검색 기능은 검색 엔진을 별도로 구성하는 방식으로 처리되고 있을 것 같다는 예상을 해봅니다.\n일단 RDBMS를 이용한 genres 컬럼 조건 검색은 배제하고 이후 더 좋은 방법을 고려하는 것이 좋겠습니다.\ntags 태그 데이터는 userId, movieId, tag, timestamp로 구성되어 있습니다.\n각 행은 한명의 사용자가 한 영화에 적용한 하나의 태그를 의미합니다.\ntag는 단일 단어나 짧은 문구로 구성되며, 의미, 가치, 목적은 각 사용자의 목적에 의해 결정됩니다.\n데이터는 총 2,328,315건으로 확인됩니다.\n1 2 df = pd.read_csv(\u0026#34;dataset/tags.csv\u0026#34;) df.head() userId movieId tag timestamp 10 260 good vs evil 1430666558 10 260 Harrison Ford 1430666505 10 260 sci-fi 1430666538 14 1221 Al Pacino 1311600756 14 1221 mafia 1311600746 tags.csv 파일을 읽어 최상단 5건을 확인해보면 1명의 사용자가 여러개의 영화에 여러개의 태그를 남길 수 있다는 것을 예상해볼 수 있습니다.\n1 2 unique_tags = df[\u0026#34;tag\u0026#34;].drop_duplicates() len(unique_tags) # 153950 유니크한 tag 값은 153,950건 입니다.\ntags.csv는 사용자가 한 영화에 적용한 태그로 1명의 유저가 여러 영화에 여러 종류의 태그를 적용할 수 있습니다.\n전체 항목은 2,328,315건 이지만, tag 컬럼의 유니크한 값의 개수는 153,950개 인 것을 확인할 수 있습니다.\n이러한 특성을 지닌 tag.csv의 데이터를 RDBMS에 저장하는 것이 좋은 방법인가 의구심이 들긴 하지만, 일반적인 서비스 정책으로 예상해 볼 때(특정 영화에 적용된 태그들 조회, 사용자가 활용한 태그들 조회, 사용자가 특정 영화에 남긴 태그 조회 등) 인덱스 설정만 잘 해준다면 화면에 노출 될 데이터 조회 성능에는 큰 이슈는 없어 보입니다.\n처음엔 RDBMS로 처리하더라도 이후 새로운 요구가 있다면, 다른 방법으로 변경을 고려할 수 있겠습니다.\nratings 1 2 df = pd.read_csv(\u0026#34;dataset/ratings.csv\u0026#34;) df.head() userId movieId rating timestamp 1 1 4.0 1225734739 1 110 4.0 1225865086 1 158 4.0 1225733503 1 260 4.5 1225735204 1 356 5.0 1225735119 평가 데이터는 userId, movieId, rating, timestamp로 구성되어 있습니다.\n각 행은 한 사용자가 한 영화에 남긴 점수를 의미합니다.\n1 2 print(len(df)) # 33832162 print(min(df[\u0026#34;rating\u0026#34;]), max(df[\u0026#34;rating\u0026#34;])) # 0.5 5.0 총 데이터 수는 33,832,162건, 최대, 최소값은 각각 0.5, 5.0 입니다.\nratings.csv는 영화에 대한 사용자의 평점으로 영화 하나에 하나만 만들 수 있습니다.\n영화 목록을 검색할 때 일반적으로 평균 평점이 포함되는데, 현재 상태로 테이블을 생성할 시 조회 처리에서 GROUP BY나 서브 쿼리를 통해 평균을 계산한다던가, 활용된 값들을 조회하는 처리가 필요하게 되고, 이 때문에 조회 성능에 문제가 발생할 수 있습니다.\n이를 방지 위해 movies 테이블에 통계 정보에 활용될 컬럼을 만들어 두는 방식으로 설계하는 것도 고려해볼 수 있겠습니다.\nrating 컬럼도 꼭 소수로 넣을 필요는 없어보이네요.\n끝으로 간단하게 MovieLens 데이터셋의 주요 데이터들을 확인해봤습니다.\n영화들에 대한 메타데이터가 없어 다채로운 기능들은 구현할 수 없을 것 같다는 점이 조금 아쉬운 마음에 찾아보니 MovieLens 데이터를 기반으로 TBMS API이용하여 만든 The Movies Dataset 데이터가 Kaggle에 공개되어 있었습니다.\n하지만 데이터가 최신화가 안되어 사용할 수 없는 데이터가 많아 사용은 보류했습니다.\n일단 생각하고 있는 기능들은 MovieLens 데이터으로 충분해서 필요는 없지만, 여유가 된다면 제가 최신화를 해봐도 괜찮을 것 같네요.\n다음은 각 데이터의 특성을 고려하여 테이블을 설계하고, DB에 적재해볼예정입니다.\n부족한 글 끝까지 읽어주셔서 감사합니다 :D\n","date":"2024-03-18T11:00:01+09:00","image":"https://codemario318.github.io/post/movielens/1/cover_hu223669cbbbe5efa1c70286790dba8a54_16582_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/movielens/1/","title":"[MovieLens] 1. 데이터 살펴보기"},{"content":"좋은 아키텍처는 다음을 지원해야한다.\n시스템의 유스케이스 시스템의 운영 시스템의 개발 시스템의 배포 유스케이스 시스템의 아키텍처는 시스템의 의도를 지원해야 한다는 뜻이다.\n실제로 아키텍트의 최우선 관심사는 유스케이스이며, 아키텍처에서도 유스케이스가 최우선이다. 따라서 아키텍처는 반드시 유스케이스를 지원해야한다.\n아키텍처는 시스템의 행위 차제에는 큰 영향을 주지 않으며, 행위와 관련하여 아키텍처가 열어둘 수 있는 선택사항의 거의 없다.\n하지만, 아키텍처의 시스템의 행위에 대한 직접적인 영향력과 별개로 좋은 아키텍처는 시스템의 행위를 지원할 수 있다.\n행위를 명확히 하고 외부로 드러낸다. 이를 통해 시스템이 지닌 의도를 아키텍처 수준에서 알아볼 수 있게 만든다. 좋은 아키텍처를 가진다면 시스템의 유스케이스는 시스템 구조 자체에서 한눈에 드러난다.\n이러한 시스템의 행위는 일급 요소 first-class element이며 시스템의 최상위 수준에서 알아볼 수 있으므로, 개발자가 일일이 찾아 헤매이지 않아도 된다. 이들 요소는 클래스이거나 함수 또는 모듈러서 아키텍처 내에서 핵심적인 자리를 차지할 뿐만 아니라, 자신의 기능을 분명하게 설명하는 이름을 가져야한다. 운영 시스템의 운영 지원 관점에서 볼 때 아키텍처는 더 실질적이며 덜 피상적인 역할을 맡는다.\n예시\n시스템이 초당 100,000명의 고객을 처리해야 한다면, 아키텍처는 이 요구와 관련된 각 유스케이스에 걸맞은 처리량과 응답시간을 보장해야 한다. 시스템에서 수 밀리초 안에 3차원의 빅데이터 테이블에 질의해야 한다면, 반드시 이러한 운영 작업을 허용할 수 있는 형태로 아키텍처를 구조화해야한다. 예시와 같이 형태를 지원한다는 말은 다양한 의미를 지닌다.\n시스템의 처리 요소를 일련의 작은 서비스들로 배열하여, 서로 다른 많은 서버에서 병렬로 실행할 수 있는 시스템 경량의 수많은 스레드가 단일 프로세서에서 같은 주소 공간을 공유하는 시스템 독립된 주소 공간에서 실행되는 소수의 프로세스만으로도 충분한 시스템 단일 프로세스에서 실행되는 단순한 모노리틱 프로그램 위와 같은 운영 관점의 아키텍처 결정은 뛰어난 이키텍트라면 열어두어야 하는 선택 사항 중의 하나다.\n아키텍처에서 각 컴포넌트를 적절히 격리하여 유지하고 컴포넌트 간 통신 방식을 특정 형태로 제한하지 않는다면, 시간이 지나 운영에 필요한 요구사항이 바뀌더라도 스레드, 프로세스, 서비스로 구성된 기술 스펙트럼 사이를 전환하는 일이 훨씬 쉬워질 것이다.\n개발 아키텍처는 개발환경을 지원하는데 있어 핵심적인 역할을 수행한다.\n시스템을 설계하는 조직이라면 어디든지 그 조직의 의사소통 구조와 동일한 구조의 설계를 만들어 낼 것이다.\n- 콘웨이 법칙 -\n많은 팀으로 구성되며 관심사가 다양한 조직에서 어떤 시스템을 개발해야 한다면, 각 팀이 독립적으로 행동하기 편한 아키텍처를 반드시 확보하여 개발하는 동안 팀들이 서로 방해하지 않도록 해야 한다. 잘 격리되어 독립적으로 개발 가능한 컴포넌트 단위로 시스템을 분할 할 수 있어야 한다. 배포 아키텍처는 배포 용이성(즉각적인 배포 immediate deployment)을 결정하는 데 중요한 역할을 한다.\n좋은 아키텍처라면 시스템이 빌드된 후 즉각 배포할 수 있도록 지원해야 한다.\n수십 개의 작은 설정 스크립트나 속성 파일을 약간씩 수정하는 방식을 사용하지 않는다. 꼭 필요한 디렉터리나 파일을 수작업으로 생성하게 내버려 두지 않는다. 이러한 아케텍처를 만들려면 시스템을 컴포넌트 단위로 적절하게 분할하고 격리시켜야 한다.\n마스터 컴포넌트는 시스템 전체를 하나로 묶고, 각 컴포넌트를 올바르게 구동하고 통합하고 관리해야 한다.\n선택사항 열어놓기 좋은 아키텍처는 컴포넌트 구조와 관련된 이 관심사들 사이에서 균형을 맞추고, 각 관심사 모두를 만족시켜야 하지만, 현실에서는 이러한 균형을 잡기가 매우 어렵다.\n모든 유스케이스를 알 수는 없다. 운영하는 데 따르는 제약사항, 팀 구조, 배포 요구사항을 알지 못한다. 이러한 사항들을 알고 있더라도, 시스템이 생명주기의 단계를 하나씩 거쳐감에 따라 이 사항들도 반드시 변한다. 도달하려는 목표는 뚜렷하지 않을 뿐만 아니라 시시각각 변한다.\n이러한 변화속에서도 몇몇 아키텍처 원칙을 구현하는 비용이 비교적 비싸지 않으며, 이를 통해 관심사들 사이에서 균형을 잡는데 도움이 된다는 사실은 변하지 않는다.\n균형을 맞추려는 목표점을 명확히 그릴 수 없는 경우에도 관심사들 사이에서 균형을 잡는 데 도움이 된다. 시스템을 제대로 격리된 컴포넌트 단위로 분할할 때 도움이 된다. 이를 통해 선택사항을 가능 한 많이, 가능한 오랫동안 열어 둘 수 있게 해준다. 좋은 아키텍처는 선택사항을 열어 둠으로써, 향후 시스템에 변경이 필요할 때 어떤 방향으로든 쉽게 변경할 수 있도록 한다.\n계층 결합 분리 UI, 애플리케이션 자체와 밀접한 업무 규칙과, 도메인에 밀접한 업무 규칙, 기술적인 세부사항 등은 각자 다른 속도로, 다른 이유로 변경되므로 서로 분리하고, 독립적으로 변경할 수 있도록 만들어야만 한다.\n업무 규칙은 그 자체가 애플리케이션과 밀접한 관련이 있거나, 혹은 더 범용적일 수 있는데, 아키텍트는 이들을 시스템의 나머지 부분으로부터 분리하여 독립적으로 변경할 수 있도록 해야한다.\n입력 필드 유효성 검사는 애플리케이션 자체와 밀접하게 관련된 업무 규칙이다. 계좌의 이자 계산이나 재고품 비교는 업무 도메인에 더 밀접하게 연관된 업무 규칙이다. 이러한 계층의 예로는 UI, 애플리케이션에 특화된 업무 규칙, 애플리케이션과는 독립접인 업무 규칙, 데이터베이스와 같은 기술적인 세부사항등이 있다.\n유스케이스 결합 분리 유스케이스 자체도 서로 다른 이유로 변경될 수 있다.\n예) 주문 입력 시스템의 주문 추가 유스케이스와 주문 삭제 유스케이스 유스케이스는 시스템의 수평적인 계층을 가르지르도록 자른 수직으로 좁다란 조각이기도 하다.\n각 유스케이스는 UI의 일부, 애플리케이션 특화 업무 규칙의 일부, 애플리케이션 독립적 업무 규칙의 일부 데이터베이스 기능의 일부 를 사용한다. 따라서 시스템을 수평적 계층으로 분할하면서 동시에 해당 계층을 가로지르는 얇은 수직적인 유스케이스로 시스템을 분할할 수 있다.\n이와 같이 수직, 수평 결합을 동시에 분리하여 시스템의 맨 아래 계층까지 수직으로 내려가며 유스케이스들이 각 계층에서 서로 겹치지 않게 한다.\n서로 다른 이유로 변경되는 요소들의 결합을 분리하면 기존 요소에 지장을 주지 않고도 새로운 유스케이스를 계속해서 추가할 수 있다.\n또한 유스케이스를 뒷받침하는 서로 다른 관점(aspect)을 사용하게 되면, 새로운 유스케이스를 추가하더라도 기존 유스케이스에 영향을 주는 일은 거의 없을 것이다.\n개발 독립성 컴포넌트가 완전히 분리되면 팀 사이의 간섭은 줄어든다.\n기능 팀, 컴포넌트 팀, 계층 팀, 혹은 다른 형태의 팀이라도, 계층과 유스케이스의 결합이 분리되는 한 시스템의 아키텍처는 그 팀 구조를 뒷받침해 줄 것이다.\n배포 톡립성 유스케이스와 계층의 결합이 분리되면 배포 측면에서도 고도의 유연성이 생긴다.\n결합을 제대로 분리했다면 운영 중인 시스템에서도 계층과 유스케이스를 교체할 수 있다.\n가짜 중복 아키텍트는 중복에 대한 공포로부터 발생하는 함정에 빠지곤 한다.\n소프트웨어에서 중복은 일반적으로 나쁜 것이나, 중복에도 여러 종류가 있다.\n진짜 중복 인스턴스가 변경되면, 동일한 변경을 그 인스턴스의 모든 복사본에 반드시 적용해야한다. 가짜 중복: 거짓된 또는 우발적인 중복 중복으로 보이는 두 코드 영역이 각자의 경로로 발전한다면(서로 다른 속도와 다른 이유로 변경된다면) 이 코드는 중복이 아니다. 의도적으로 만든 중복(비슷하지만 다른 방향으로 발전할 것이 예상되는)우발적 중복을 통합한다면 나중에 코드를 다시 분리하느라 큰 수고를 감수해야 한다.\n유스케이스를 수직으로 분리할 때 이러한 문제와 마주치는 경우가 많으므로, 중복이 진짜 중복인지 확인하고 통합해야한다.\n계층을 수평으로 분리하는 경우, 특정 데이터베이스 레코드의 데이터 구조가 특정 화면의 데이터 구조와 상당히 비슷한 경우를 자주 볼 수 있는데, 이때 데이터베이스 레코드와 동일한 형태의 뷰 모델을 만들어서 각 항목을 복사하는 방식을 선택해야한다.\n결합 분리 모드 위와 같이 결합을 분리하면 운영 관점에서 여러가지 장점을 살펴볼 수 있다.\n유스케이스에서 서로 다른 관점이 분리되었다면,\n높은 처리량을 보장해야 하는 유스케이스와 낮은 처리량으로도 충분한 유스케이스는 이미 분리되어 있을 가능성이 높다. UI와 데이터베이스가 업무 규칙과 분리되어 있다면, UI와 데이터베이스는 업무 규칙과는 다른 서버에서 실행될 수 있다. 높은 대역폭을 요구하는 유스케이스는 여러 서버로 복제하여 실행할 수 있다. 유스케이스를 위해 수행하는 그 작업들(분리된 것들)은 운영에도 도움이 될 수 있다.\n운영 측면에서 이점을 살리기 이해선 결합을 분리할 때 적절한 모드를 선택해야 한다.\n분리된 컴포넌트를 서로 다른 서버에서 실행해야 하는 상황: MSA 등 때때로는 컴포넌트를 서비스 수준까지도 분리해야 한다.\n좋은 아키텍처는 선택권을 열어둔다는 말에서 결합 분리 모드는 이러한 선택지 중 하나다.\n계층과 유스케이스의 결합을 분리하는 방법은 다양하다.\n소스 수준 분리 모드 소스 코드 모듈 사이의 의존성을 제어할 수 있다. 하나의 모듈이 변하더라도 다른 모듈을 변경하거나 재컴파일하지 않도록 만들 수 있다. 모노리틱 구조 배포 수준 분리 모드 .jar, DLL, 공유 라이브러리와 같이 배포 가능한 단위들 사이의 의존성을 제어할 수 있다. 소스 코드가 변하더라도 다른 모듈을 재빌드하거나 재배포하지 않도록 만들 수 있다. 많은 컴포넌트가 같은 주소 공간에 상주하며, 단순한 함수 호출을 통해 통신할 수 있다. 어떤 컴포넌트는 동일한 프로세스의 다른 프로세스에 상주하고, 프로세스 간 통신, 소켓, 공유 메모리를 통해 통신할 수 있다. 결합이 분리된 컴포넌트가 독립적으로 배포할 수 있는 단위로 분할되어 있다. 서비스 수준 분리 모드 의존하는 수준을 데이터 구조 단위까지 낮추고, 네트워크 패킷을 통해서만 통신하도록 만들 수 있다. 모든 실행 가능한 단위는 소스와 바이너리 변경에 대해 서로 완전히 독립적이게 된다. 마이크로서비스, 서비스 프로젝트 초기 단계는 어떤 모드가 최선인지 알기 어려우며, 프로젝트가 성숙해갈수록 최적인 모드가 달라질 수 있다.\n좋은 아키텍처는 시스템이 모노리틱 구조로 태어나서 단일 파일로 배포되더라도,이후에는 독립적으로 배포 가능한 단위들의 집합으로 성장하고, 또 독립적인 서비스나 마이크로서비스 수준까지 성장할 수 있도록 만들어져야 한다.(반대도 마찬가지)\n좋은 아키텍처는 결합 분리 모드를 선택사항으로 남겨두어서 배포 규모에 다라 가장 적합한 모드를 선택해 사용할 수 있게 만들어 준다.\n결론 시스템의 결합 분리 모드는 시간이 지나면서 바뀌기 쉬우며, 뛰어난 아키텍트라면 이러한 변경을 예측하여 큰 무리 없이 반영할 수 있도록 만들어야한다.\n","date":"2024-03-13T14:51:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/16/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/16/","title":"16. 독립성"},{"content":"소프트웨어 아키텍트란?\n소프트웨어 아키텍트는 프로그래머이며, 앞으로도 계속 프로그래머로 남아야하므로, 고수준 문제에만 집중하여 코드와 동떨어져서는 안 된다.\n소프트웨어 아키텍트는 개발에 참여하며 나머지 팀원들이 생산성을 극대화할 수 있는 설계를 하도록 방향을 이끌어준다.\n따라서 프로그래밍을 작업에 계속 참여하며 문제를 직접 경험하고 이를 통해 다른 프로그래머를 지원하는 작업을 수행해야한다.\n소프트웨어 시스템의 아키텍처란?\n시스템을 구축했던 사람들이 만들어낸 시스템의 형태로 컴포넌트로 분할하는 방법, 분할된 컴포넌트를 배치하는 방법, 컴포넌트가 서로 의사소통하는 방식에 따라 정해진다.\n이러한 시스템 형태는 아키텍처 안에 담긴 소프트웨어 시스템이 쉽게 개발, 배포, 운영, 유지보수되도록 만들어진다.\n개발, 배포, 운영, 유지보수를 용이하게 만들기 위해서는 가능한 한 많은 선택지를, 가능한 한 오래 남겨두는 전략을 따라야한다.\n시스템 아키텍처는 시스템의 동작 여부와는 거의 관련이 없다.\n형편없는 아키텍처를 갖춤 시스템도 수없이 많지만, 그런데로 잘 동작한다. 이러한 경우 대체로 운영에서는 문제가 없지만 배포, 유지보수, 계속되는 개발 과정에서 어려움을 겪는다. 시스템 아키텍처가 시스템이 제대로 동작하는 것과는 직접적으로 관련이 없지만, 시스템이 제대로 동작하도록 지원한다(간접적).\n시스템을 쉽게 이해할 수 있게 한다. 쉽게 개발할 수 있게 한다. 쉽게 유지보수할 수 있게 한다. 쉽게 배포하게 해준다. 아키텍처의 주된 목적은 시스템의 생명주기를 지원하는 것이다. 궁극적으로 시스템의 수명과 관련된 비용은 최소화하고, 프로그래머의 생산성은 최대화하는 데 있다.\n개발 시스템 아키텍처는 개발팀(들)이 시스템을 쉽게 개발할 수 있도록 뒷받침해야만 한다.\n팀 구조\n팀 구조가 다르다면 아키텍처 관련 결정에서도 차이가 난다.\n작은 규모 팀 규모가 작아 상위 구조로 인한 장애물이 없기를 바라기 때문에 아키텍처 없이 시작하는 경우가 많다. 잘 정의된 컴포넌트나 인터페이스가 없더라도 서로 효율적으로 협력하여 모노리틱 시스템을 개발할 수 있다. 개발 초기에는 아키텍처 관련 제약들이 오히려 방해가 된다고 여길 가능성이 높다. 큰 규모 다른 요소를 고려하지 않는다면 아키텍처는 팀별 단일 컴포넌트(큰 관심사 별 컴포넌트)로 각각 발전될 가능성이 높다. 팀별 단일 컴포넌트 아키텍처는 시스템을 배포, 운영, 유지보수하는 데 최적은 아닐 확률이 높다. 시스템을 신뢰할 수 있고 안정된 인터페이스를 갖춘, 잘 설계된 컴포넌트 단위로 분리하지 않으면 개발이 진척되지 않는다. 배포 배포 비용이 높을수록 시스템의 유용성은 떨어진다.\n따라서 소프트웨어 아키텍처는 시스템을 단 한번에 쉽게 배포할 수 있도록 만드는 데 그 목표를 두어야 한다.\n초기 개발 단계에서는 배포 전략을 거의 고려하지 않기 때문에 개발하기는 쉬워도 배포하기는 어려운 아키텍처가 만들어진다. 마이크로서비스 아키텍처 micro-service architecture\n컴포넌트 경계가 매우 뚜렷해지고, 인터페이스가 대체로 안정화되므로 시스템을 매우 쉽게 개발할 수 있다.\n하지만 늘어난 수많은 마이크로서비스로인해 서로 연결하기 위해 설정하고 작동 순서를 결정하는 과정에서 오작동이 발생할 원천이 스며들어 배포하기 어려워진다.\n아키텍트가 배포 문제를 초기에 고려했다면, 더 적은 서비스를 사용하고, 서비스 컴포넌트와 프로세스 수준의 컴포넌트를 하이브리드 형태로 융합하며, 좀 더 통합된 도구를 사용해 상호 연결을 관리했을 것이다.\n운영 아키텍처가 시스템 운영에 미치는 영향은 개발, 배포, 유지보수에 미치는 영향보다는 덜 극적이다.\n운영에서 겪는 대다수의 어려움은 단순히 하드웨어를 더 투입해서 해결할 수 있다.\n소프트웨어 아키텍처가 비효율적이라면 스토리자와 서버를 추가하는 것만으로 제대로 동작하도록 만들 수 있을 때가 많다. 하드웨어는 값싸고 인력은 비싸다. 운영 관점에서 소프트웨어 아키텍처는 비용 공식 관점에서 보다는 개발, 배포 유지보수 쪽으로 더 기운다는 의미를 가진다.\n운영 방식\n시스템 아키텍처가 개발자에게 시스템의 운영 방식을 잘 드러내 준다.\n좋은 소프트웨어 아키텍처는 시스템을 운영하는 데 필요한 요구도 알려준다.\n유스케이스, 기능, 시스템의 필수 행위를 일급 엔티티로 격상시킨다. 위 요소가 개발자에게 주요 목표로 인식되도록 해야한다. 이를 통해 시스템을 이해하기 쉬워지며, 따라서 개발과 유지보수에 큰 도움이 된다.\n유지보수 유지보수는 모든 측면에서 소프트웨어 시스템에서 가장 많은 비용이 발생한다.\n유지보수의 가장 큰 비용은 탐사 spelunking와 이로 인한 위험부담에 있다.\n탐사란?\n기존 소프트웨어에 새로운 기능을 추가하거나 결함을 수정할 때, 소프트웨어를 파헤쳐서 어디를 고치는 게 최선인지, 어떤 전략을 쓰는 게 최적일지를 결정할 때 드는 비용\n변경사항을 반영할 때 의도치 않은 결함이 발생할 가능성은 항상 존재하며, 이로인한 위험부담 비용이 추가된다.\n주의를 기울여 신중하게 아키텍처를 만들면 이 비용을 크게 줄일 수 있다.\n시스템을 컴포넌트로 분리 안정된 인터페이스를 두어 서로 격리 이를 통해 미래에 추가될 기능에 대한 길을 밝혀 둘 수 있을 뿐만 아니라 의도치 않은 장재가 발생할 위험을 크게 줄일 수 있다.\n선택사항 열어두기 책 초반에 언급했듯, 소프트웨어는 행위적 가치와 구조적 가치를 지니며, 아키텍처 관점에서 구조적 가치를 추구해야한다.\n소프트웨어를 만든 이유는 기계의 행위를 빠르고 쉽게 변경하는 방법이 필요했기 때문인데, 이러한 유연성은 시스템의 형태, 컴포넌트의 배치 방식, 컴포넌트가 상호 연결되는 방식에 상당히 크게 의존한다.\n소프트웨어의 유연성을 유지하는 방법은 선택사항(세부사항)을 가능한 한 많이, 가능한 한 오랫동안 열어두는 것이다.\n정책과 세부사항\n모든 소프트웨어 시스템은 정책과 세부사항으로 분해할 수 있다.\n정책 모든 업무 규칙과 업무 절차를 구체화한다. 시스템의 진정한 가치가 살아있는 곳 세부사항 사람, 외부 시스템, 프로그래머가 정책과 소통할 때 필요한 요소 정책이 가진 행위에는 조금도 영향을 미치지 않는다. 입출력 장치, DB, 웹 시스템, 서버, 프레임워크, 프로토콜 등 아키텍트의 목표는 시스템에서 정책을 가장 핵심적인 요소로 식별하고, 동시에 세부사항은 정책에 무관하게 만들 수 있는 형태의 시스템을 구축하는 데 있다.\n이를 통해 세부사항을 결정하는 일은 미루거나 연기할 수 있게 된다.\n예시\n개발 초기에는 데이터베이스 시스템을 선택할 필요가 없다. 고수준의 정책을 데이터베이스가 관계형인지, 분산형인지, 계층형인지 등에 관계 없도록 만들어야 한다. 개발 초기에는 웹 서버를 선택할 필요가 없다. 고수준의 정책은 자신이 웹을 통해 전달된다는 사실을 알아서는 안 된다. 웹 개발 기술들에 대해 고수준의 정책이 전혀 알지 못하게 만들면, 프로젝트 후반까지는 어떤 종류의 웹 시스템을 사용할지를 결정하지 않아도 된다. 시스템을 웹을 통해 전송할 것인지조차도 결정할 필요가 없다. 개발 초기에는 REST를 적용할 필요가 없다. 고수준의 정책은 외부 세계로의 인터페이스에 대해 독립적이어야 한다. 의존성 주입 프레임워크를 적용할 필요가 없다. 고수준의 정책은 의존성을 해석하는 방식에 대해 신경써서는 안된다. 세부사항에 몰두하지 않은 채 고수준의 정책을 만들 수 있다면, 이러한 세부사항에 대한 결정을 오랫동안 미루거나 연기할 수 있다.\n이러한 결정을 더 오래 참을 수 있다면, 더 많은 정보를 얻을 수 있고, 이를 기초로 제대로 된 결정을 내릴 수 있다. 선택사항을 더 오랫동안 열어 둘 수 있다면 더 많은 실험을 해볼 수 있고 더 많은 것을 시도할 수 있다. 좋은 아키텍트는 결정되지 않은 사항의 수를 최대화한다.\n결론 좋은 아케텍트는 세부사항을 정책으로부터 신중하게 가려내고, 정책이 세부사항과 결합되지 않도록 엄격하게 분리한다.\n이를 통해 정책은 세부사항에 관한 어떠한 지식도 갖지 못하게 되며, 어떤 경우에도 세부사항에 의존하지 않게 된다.\n좋은 아키텍트는 세부사항에 대한 결정을 가능한 한 오랫동안 미룰 수 있는 방향으로 정책을 설계한다.\n","date":"2024-03-12T15:37:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/15/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/15/","title":"15. 아키텍처란?"},{"content":"지금부터 다룰 세 가지 원칙은 컴포넌트 사이의 관계를 설명한다.\n컴포넌트 사이의 관계는 응집도와 마찬가지로 개발 가능성과 논리적 설계 사이의 균형을 다룬다.\n컴포넌트 구조와 관련된 아키텍처를 침범하는 힘은 기술적이며, 정치적이고, 가변적이다.\nADP: 의존성 비순환 원칙 컴포넌트 의존성 그래프에 순환(Cycle)이 있어서는 안된다.\n숙취 증후군 The morning after syndrome\n무언가를 작동하게 만들놓았는데, 무언가를 의존하고 있던 무언가를 다른 사람이 수정하여 동작하지 않는 현상을 저자는 숙취 증후군이라 부른다.\n숙취 증후군은 많은 개발자가 동일한 소스 파일을 수정하는 환경에서 발생한다.\n소수의 개발자로 구성된 작은 프로젝트에서는 큰 문제가 되지 않지만, 프로젝트와 개발팀 규모가 커지면 큰 문제로 다가온다.\n지난 수십 년 동안 이 문제의 해결책으로 두 가지 방법이 발전되어 왔다.\n주 단위 빌드(Weekly build) 의존성 비순환 원칙(Acyclic Dependencies Principle) 주 단위 빌드 중간 규모의 프로젝트에서는 흔하게 사용되는 방법으로 일주일의 첫 4일 동안은 각자 작업한 후 금요일이 되면 변경된 코드를 모두 통합하여 시스템을 빌드한다.\n이 접근법은 5일 중 4일 동안 개발자가 걱정없이 개발할 수 있다는 장점이 있지만, 금요일에 통합과 관련된 막대한 업보를 치러야한다는 단점을 가진다.\n프로젝트가 커질수록 통합은 하루만에 끝마치는게 불가능해진다. 통합 일정이 길어진다. 개발보다 통합에 드는 시간이 늘어나면서 팀의 효율성도 서서히 나빠진다. 빌드 주기가 길어진다. 이러한 과정이 반복되며 효율성을 유지하기 위해 빌드 일정을 계속 늘려야 하고, 빌드 주기가 늦어질수록 프로젝트가 감수할 위험은 더 커지게된다.\n통합과 테스트를 수행하기가 점점 어려워진다. 빠른 피드백이 주는 장점을 잃는다. 순환 의존성 제거하기 이 문제의 해결책은 개발 환경을 릴리스 가능한 컴포넌트 단위로 분리하는 것이다.\n이를 통해 컴포넌트는 개별 개발자 또는 단일 개발팀이 책임질 수 있는 작업 단위가 된다.\n개발자가 해당 컴호넌트가 동작하도록 만든 후, 해당 컴포넌트를 릴리스하여 다른 개발자가 사용할 수 있도록 만든다. 컴포넌트에 릴리스 번호를 부여하고, 다른팀에서 사용할 수 있는 디렉터리로 이동시킨다. 개발자는 자신만의 공간에서 해당 컴포넌트를 지속적으로 수정한다. 나머지 개발자는 릴리스된 버전을 사용한다. 컴포넌트가 새로 릴리스되어 사용할 수 있게 되면, 다른팀에서는 새 릴리스를 적용할 지 결정한다. 특정 컴포넌트가 변경 되더라도 다른 팀에 영향을 즉각 영향을 주지는 않으므로 어떤 팀도 다른 팀에 의해 좌우되지 않는다.\n각 팀은 특정 컴포넌트가 새로 릴리스되면 자신의 컴포넌트를 해당 컴포넌트에 맞게 수정할 시기를 스스로 결정할 수 있다. 통합은 작고 점진적으로 이뤄진다. 이 같은 작업 절차는 단순하며 합리적이여서 널리 사용되는 방식이나, 성공적으로 동작하려면 컴포넌트 사이의 의존성 구조를 반드시 관리해야한다.\n컴포넌트 다이어그램에서는 컴포넌트를 조립하여 애플리케이션을 만드는 다소 전형적인 구조로, 중요한 점은 컴포넌트 간의 의존성 구조이다.\n의존성 관계는 일방향이다. 어느 컴포넌트에서 시작하더라도, 의존성 관계를 따라가면서 최초의 컴포넌트로 되돌아갈 수 없다. 어떠한 컴포넌트의 새로운 릴리스를 만들게 되면 영향을 받는 컴포넌트는 의존성 화살표를 거꾸로 따라가면 알 수 있다. Presenters \u0026gt; View, Main Main은 새로 릴리스되더라도 영향을 받는 컴포넌트가 전혀 없다. Presenters 컴포넌트를 만드는 개발자가 이 컴포넌트를 테스트하려고 한다면, 현재 사용중인 버전의 Interactors와 Entities를 이용해서 Presenters 자체 버전을 빌드하면 그만이다.\n테스트를 구성할 때 대체로 적은 노력이 든다. 고려해야 할 변수도 상대적으로 적다. 시스템 전체를 릴리스해야 할 때가 오면 릴리스 절차는 상향식으로 진행된다.\nEntities 컴포넌트를 컴파일하고, 테스트하고, 릴리스한다. Database와 Interactors에서도 동일한 과정을 거친다. Presenters, Views, Controllers, Authorizer 순으로 진행한다. 마지막으로 Main을 처리한다. 구성요소 간 의존성을 파악하고 있으면 시스템을 빌드하는 방법을 알 수 있다.\n순환이 컴포넌트 의존성 그래프에 미치는 영향 요구사항 발생으로 Entities의 User 클래스가 Authorizer의 Permissions 클래스를 사용한다고 가정하면, 아래와 같이 순환 의존성이 발생한다.\nDatabase 컴포넌트를 릴리스하려면 Entities와의 호환되어야 하므로 Authorizer과 확인도 필요하며, Authorizer는 Interactors와도 의존하므로 세 개의 컴포넌트가 사실상 하나의 거대한 컴포넌트가되어 릴리스 하기 어려워진다.\n해당 컴포넌트의 개발자들은 항상 정확하게 동일한 릴리스를 사용해야하므로 모두 서로에게 얽매이게 된다.\nEntites 컴포넌트를 테스트할 때도 Authorizer와 Interactors 까지도 반드시 빌드하고 통합해야한다.\n이처럼 순환이 생기면 컴포넌트를 분리하기가 상당히 어려워진다.\n단위테스트를 하고 릴리스를 하는 일도 굉장히 어려워진다. 에러가 쉽게 발생한다. 모듈의 개수가 많아짐에 따라 빌드 관련 이슈는 기하급수적으로 증가한다. 컴포넌트를 어떤 순서로 빌드해야 올바를지 파악하기가 상당히 힘들어진다. 순환이 생기면 올바른 순서라는 것 자체가 없을 수 있다. 순환 끊기 컴포넌트 사이의 순환을 끊고 의존성을 다시 DAG로 원상복구하는 일은 언제라도 가능하다.\n의존성 역전 원칙 적용\nUser가 필요로하는 메서드를 제공하는 인터페이스를 생성한다. 이 인터페이스를 Entities에 위치시키고, Authorizer에서 이 인터페이스를 상속받는다. 이를 통해 Entities와 Authorizer 사이의 의존성을 역전시킬 수 있고, 이를 통해 순환을 끊을 수 있다.\n새로운 컴포넌트\nEntities와 Authorizer가 모두 의존하는 새로운 컴포넌트를 만든다. 두 컴포넌트가 모두 의존하는 클래스들을 새로운 컴포넌트로 이동시킨다. 흐트러짐(Jitters) 새로운 컴포넌트를 생성하는 두 번째 해결책에서 시사하는 바는 요구사항이 변경되면 컴포넌트 구조도 변경될 수 있다는 사실이다.\n애플리케이션이 성장함에 따라 컴포넌트 의존성 구조는 서서히 흐트러지며 또 성장하므로, 의존성 구조에 순환이 발생하는지를 항상 관찰하여 순환이 발생하면 어떤 식으로든 끊어야 한다.\n이로인해 새로운 컴포넌트를 생성하거나 의존성 구조가 더 커질수도 있다.\n하향식(Top-down) 설계 지금까지 논의로 컴포넌트 구조는 하향식으로 설계될 수 없다는 결론에 다다른다.\n컴포넌트는 시스템에서 가장 먼저 설계할 수 있는 대상이 아니며, 오히려 시스템이 성장하고 변경될 때 함께 진화한다.\n컴포넌트와 같이 큰 단위로 분해된 구조는 고수준의 기능적인 구조로 다시 분해할 수 있다고 기대하기 때문에 동의하지 않을 수 있지만, 컴포넌트 의존성 다이어그램은 애플리케이션의 기능을 기술하는 일과는 거의 관련이 없다.\n컴포넌트 의존성 다이어그램은 애플리케이션의 빌드 가능성과 유지보수성을 보여주는 지도이므로 빌드하거나 유지보수할 소프트웨어가 없다면 빌드와 유지보수에 관한 지도 도한 필요 없기 때문에 컴포넌트 구조는 프로젝트 초기에 설계할 수 없다.\n하지만 프로젝트가 진행되고 발전함에 따라 의존성관리, 영향범위 최소화, 안정적인 컴포넌트, 재사용성 등의 대한 요구로 점점 아키텍처를 새로 만들고 가다듬게 되어 컴포넌트 의존성 그래프는 조금씩 흐트러지고 성장한다.\n이처럼 아직 아무런 클래스도 설계하지 않은 상태에서 컴포넌트 의존성 구조를 설계하려고 시도한다면 큰 실패를 맛볼 수 있다.\n공통 폐쇄에 대해 파악할 수 없다. 재사용 가능한 요소도 알 수 없다. 컴포넌트 생성 시 대부분 순환 의존성이 발생하게된다. 따라서 컴포넌트 의존성 구조는 시스템의 논리적인 설계에 발맞춰 성장하며 또 진화해야한다.\nSDP: 안정된 의존성 원칙 안정성의 방향으로(더 안정된 쪽에) 의존하라.\n설계를 유지하다 보면 변경은 불가피하므로 결코 정적일 수 없다.\n공통 폐쇄 원칙을 준수함으로써 컴포넌트가 다른 유형의 변경에는 영향받지 않으면서도 특정 유형의 변경에만 민감하게 만들 수 있으며, 이에 맞추어 특정 컴포넌트는 변동성을 지니도록 설계한다.\n변동성이 높은(변동이 예상되는) 컴포넌트에 한번 의존하게 되면 변동성이 큰 컴포넌트도 결국 변경이 어려워지므로 변경이 쉽지 않은 컴포넌트가 변동이 예상되는 컴포넌트에 의존하게 만들어서는 절대 안된다.\n모듈을 만들 때는 변경하기 쉽도록 설계했지만, 그 모듈에 의존성을 매달아 버리면 해당 모듈도 변경하기 어려워진다. 만든 모듈에서는 한 줄의 코드도 변경되지 않았더라도 변경하는 일이 상당히 도전적인 일이 되어버린다. 안정된 의존성 원칙을 준수하면 변경하기 어려운 모듈이 변경하기 쉽게 만들어진 모듈에 의존하지 않도록 만들 수 있다.\n안정성 동전을 세로로 세우면 안정성이 떨어지지만 건드리지 않으면 넘어지지 않는 것 처럼, 안정성은 변화가 발생하는 빈도와는 직접적인 관련이 없다.\n웹스터 사전에서는 안정성을 \u0026lsquo;쉽게 움직이지 않는\u0026rsquo;이라고 정의하는데 이는 변경을 만들기 위해 필요한 작업량과 관련된다.\n변경을 위해 필요한 작업량이 많으면 안정적이다. 소프트웨어 컴포넌트를 변경하기 어렵게 만드는 데는 크기, 복잡도, 간결함 등 많은 요인이 있지만, 변경하기 어렵게 만드는 확실한 방법 중 하나는 수 많은 컴포넌트가 해당 컴포넌트에 의존하게 만드는 것이다.\n컴포넌트 안쪽으로 들어오는 의존성이 많아지면, 사소한 변경이라도 의존하는 모든 컴포넌트를 만족시키면서 변경하기 위해 많은 노력이 들기 때문에 상당히 안정적이다. 안정된 컴포넌트\nX는 세 컴포넌트를 책임 responsible진다 X에 3개 컴포넌트가 의존하기 때문에 X를 변경하지 말아야할 이유는 3가지이다. X는 독립적이다. 어디에도 의존하지 않으므로 변경되도록 만들 수 있는 외적인 영향이 전혀 없다. 불안정한 컴포넌트\n책임성이 없다. 어떤 컴포넌트도 Y에 의존하지 않는다. 의존적이다. 3개 컴포넌트에 의존하므로 변경이 발생할 수 있는 외부 요인이 3가지이다. 안정성 지표 컴포넌트로 들어오고 나가는 의존성의 개수를 세어 컴포넌트가 위치상(Positional) 어느 정도의 안정성을 가지는지 계산할 수 있다.\nFan-in: 안으로 들어오는 의존성 컴포넌트 내부의 클래스에 의존하는 컴포넌트 외부의 클래수 개수 Fan-out: 바깥으로 나가는 의존성 컴포넌트 외부의 크래스에 의존하는 컴포넌트 내부의 클래수 개수 I(불안정성): I = Fan-out / (Fan-in + Fan-out) 0 ~ 1 0이면 최고로 안정된 컴포넌트 1이면 최고로 불안정한 컴포넌트 Fan-in과 Fan-out 지표는 특정 컴포넌트 내부의 클래스에 의존하는, 컴포넌트 외부에 위치한 클래스의 개수를 세어서 계산할 수 있다.\nI 값이 1이면 어떤 컴포넌트도 해당 컴포넌트에 의존하지 않지만, 해당 컴포넌트는 다른 컴포넌트에 의존한다는 뜻이다. 최고로 불안정한 상태, 책임이 없으며 의존적이다. 자신에게 의존하는 컴포넌트가 없으므로 변경하지 말아야 할 이유가 없다. 다른 컴포넌트에 의존하기 때문에 언젠가는 해당 컴포넌트를 변경해야 할 이유가 있다. I 값이 0이면 해당 컴포넌트에 의존하는 다른 컴포넌트가 있지만, 해당 컴포넌트 자체는 다른 컴포넌트에 의존하지 않는다는 뜻이다. 최고로 안정된 상태, 다른 컴포넌트를 책임지면서 독립적이다. 자신에게 의존하는 컴포넌트가 있으므로 변경하기가 어렵다. 해당 컴포넌트를 변경하도록 강제하는 의존성은 같지 않는다. SDP에서 컴포넌트 I 지표는 그 컴포넌느가 의존하는 다른 컴포넌트들의 I보다 커야한다고 말한다.\n즉, 의존성 방향으로 갈수록 I 지표 값이 감소해야한다.\n모든 컴포넌트가 안정적이어야 하는 것은 아니다. 모든 컴포넌트가 최고로 안정적인 시스템이라면 변경이 불가능하다.\n이는 바람직한 상황은 아니며, 컴포넌트 구조를 설계할 때 기대하는 것은 불안정한 컴포넌트와 안정된 컴포넌트가 공존하는 상태다.\n위 다이어그램은 세 컴포넌트로 구성된 이상적인 구조이다.\n추상 컴포넌트\n인터페이스만을 포함하는 추상 컴포넌트는 상당히 안정적이므로, 덜 안정적인 컴포넌트가 의존할 수 있는 이상적인 대상이다.\n안정적인 Stable의 내부 클래스 U가 불안정한 Flexible의 내부 클래스 C를 사용해야한다면, DIP를 도입해 문제를 해결할 수 있다.\nUS라는 인터페이스를 생성한 수 UServer 컴포넌트에 넣고, 해당 인터페이스를 구현하도록 하면 Stable의 Flexible에 대한 의존성을 끊을 수 있고, UServer 자체는 어디에도 의존하지 않으므로 안정된 상태이므로, 모든 의존성이 감소하는 방향으로 개선할 수 있다.\nSAP: 안정된 추상화 원칙 컴포넌트는 안정된 정도만큼만 추상화되어야 한다.\n고수준 정책을 어디에 위치시켜야 하는가? 고수준 아키텍처가 정책 결정과 관련된 소프트웨어는 자주 변경되서는 절대로 안되는 소프트웨어이다.\n따라서 시스템에서 고수준 정책을 캡슐화하는 소프트웨어는 반드시 안정된 컴포넌트(I = 0)에 위치해야 한다.\n불안정한 컴포넌트(I = 1)는 반드시 변동성이 큰 소프트웨어, 즉 쉽고 바르게 변경할 수 있는 소프트웨어만을 포함해야 한다.\n하지만 고수준 정책을 안정된 컴포넌트에 위치시키면, 그 정책을 포함하는 소스코드는 수정하기 어려워져 시스템 전체 아키텍쳐가 유연성을 잃게 된다.\n개방 폐쇄 원칙의 추상화를 통해 컴포넌트가 최고로 안정된 상태이면서도 동시에 변경에 충분히 대응할 수 있을 정도로 유연하게 만들 수 있다.\n안정된 추상화 원칙 안정된 추상화 원칙은 안정성과 추상화 정도 사이의 관계를 정의한다.\n안정된 컴포넌트는 추상 컴포넌트여야한다. 이를 통해 안정성이 컴포넌트를 확장하는 일을 방해해서는 안된다. 불안정한 컴포넌트는 반드시 구체 컴포넌트여야한다. 컴포넌트 내부의 구체적인 코드를 쉽게 변경할 수 있어야 하므로 따라서 안정적인 컴포넌트라면 반드시 인터페이스와 추상 클래스로 구성되어 쉽게 확장할 수 있어야 한다.\n안정된 컴포넌트가 확장 가능해지면 유연성을 얻게 되고 아키텍처를 과도하게 제약하지 않게 된다.\nSDP에서는 의존성이 반드시 안정성의 방향으로 향해야 한다고 말하며, SAP에서는 안정성이 결국 추상화를 의미한다고 말하기 때문에 SAP와 SDP를 결합하면 컴포넌트에 대한 DIP와 같다.\n따라서 의존성은 추상화의 방향으로 향하게 된다. 추상화 정도 측정하기 SDP + SPA 조합이 컴포넌트에 대한 DIP라고 언급했었지만, DIP는 클래스에 대한 원칙이므로 추상적이거나 그렇지 않거나 둘중 하나이다.\n하지만 SDP + SAP 조합은 컴포넌트에 대한 원칙이므로 컴포넌트의 어떤 부분은 추상적이면서 다른 부분은 안정적일 수 있다.\nA 지표는 컴포넌트의 클래스 총 수 대비 인터페이스와 추상 클래스의 개수를 단순히 계산하여 컴포넌트의 추상화 정도를 측정한 값이다.\nNc: 컴포넌트의 클래스 개수 Na: 컴포넌트의 추상 클래스와 인터페이스 개수 A: 추상화 정도 A = Na / Nc A지표는 0과 1 사이의 값을 가지며, 0이면 추상 클래스가 하나도 없다는 뜻, 1이면 추상클래스만 포함한다는 뜻이다.\n주계열: Main Sequence 최고로 안정적이며 추상화된 컴포넌트는 (0, 1) 최고로 불안정하며 구체화된 컴포넌트는 (1, 0) 컴포넌트는 추상화와 안정화 정도가 다양하므로 두 지점에만 분포할 수는 없다.\n따라서 A/I 그래프 상에서 컴포넌트가 위치할 수 있는 합리적인 지점을 정의하는 궤적이 있을 것이라 가정해 볼 수 있으며, 이 궤적은 컴포넌트가 절대로 위치해서는 안 되는 영역(배제할 구역)을 찾는 방식으로 추론할 수 있다.\n고통의 구역: Zone of Pain (0, 0) 주변 구역에 위치한 컴포넌트는 매우 안정적이며 구체적이므로 굉장히 뻣뻣한 상태로 바람직한 상태는 아니다.\n추상적이지 않으므로 확장할 수 없다. 안정적이므로 변경하기 어렵다. 제대로 설계된 컴포넌트라면 (0, 0) 근처에는 위치하지 않을 것이라 보는게 일반적이다.\n일부 소프트웨어 엔티티는 고통의 구역에 위치한다.\n데이터베이스 스키마 변동성이 매우 높다. 극단적으로 구체적이다. 많은 컴포넌트가 의존한다. OO 애플리케이션과 데이터베이스 사이에 위치한 인터페이스는 관리하기 굉장히 어렵다. 스키마가 변경되면 대체로 고통을 수반한다. 유틸리티 라이브러리(String 컴포넌트) I가 1일지라도, 실제로는 변동성이 거의 없다. 속한 클래스가 모두 구체 클래스이다. 광범위하게 사용되어 수정하면 혼란을 초래할 수 있다. 변동될 가능성이 거의 없는 컴포넌트는 (0, 0)에 위치했더라도 해롭지 않다.\n고통의 구역에서 문제가 되는 경우는 변동성이 있는 소프트웨어 컴포넌트이다.\n쓸모없는 구역: Zone of Uselessness (1, 1)에 위치한 컴포넌트는 최고로 추상적이지만 누구도 그 컴포넌트에 의존하지 않기 때문에 쓸모없는 컴포넌트이며 이로인해 쓸모없는 구역이라 부른다.\n이 영역에 존재하는 소프트웨어 엔티티는 폐기물과도 같다.\n대부분 누구도 구현하지 않은 채 남겨진 추상 클래스 쓸모없는 구역 내부 깊숙이 자리잡은 컴포넌트는 이러한 엔티티의 상당 부분을 포함할 가능성이 높다.\n배제 구역 벗어나기 변동성이 큰 컴포넌트 대부분은 두 배제 구역으로부터 가능한 멀리 떨어뜨려야 한다.\n주계열에 위치한 컴포넌트는 자신의 안정성에 비해 너무 추상적이지도 않고, 추상화 정도에 비해 너무 불안정하지도 않다.\n쓸모없지 않으면서 고통을 안겨 주지도 않는다. 추상화된 수준에 어울릴 정도로만 다른 컴포넌트가 의존한다. 구체화된 수준에 어울릴 정로로만 다른 컴포넌트에 의존한다. 컴포넌트가 위치할 수 있는 가장 바람직한 지점은 주계열의 두 종점이다.\n하지만 대규모 시스템에서 일부 컴포넌트는 완벽히 추상적이거나 완전하게 안정적일 수 없으므로, 이러한 컴포넌트는 주 계열 바로 위에 또는 가깝게 위치하게 설계하는 것이 이상적이다.\n주계열과의 거리 추상화 정도를 측정하는 A/I 그래프에서 (1, 0), (0, 1)를 연결하는 직선인 주계열을 기준으로 해당 컴포넌트가 얼마나 떨어져있는지 거리를 측정하여 지표으로 사용할 수 있다.\nD: 거리, D = |A + I - 1| 유효범위: [0, 1] 0이면 주계열 바로 위에 위치, 1이면 주계열로부터 가정 멀리 위치 이 지표를 통해 컴포넌트가 주계열에 대체로 일치하도록 설계되었는지(추상화, 구체화 수준에 어울리는 의존성을 갖는지) 분석하고, 거리가 먼 컴포넌트들을 개선할 수 있다.\n통계적 활용\n값으로 계산할 수 있으므로 통계적으로 분석 가능해진다.\n설계에 포함된 컴포넌트들의 D를 계산하고 이들의 평균과 분산을 측정하였을때, 주계열에 일치하도록 설계되었다면 평균과 분산은 0에 가까워진다.\n분산을 통해 다른 컴포넌트에 비해 극히 예외적인 컴포넌트르 식별할 수 있으므로 관리 한계를 결정하는데 유용할 수 있다. 시간에 따른 변화 확인\n각 컴포넌트의 D 값을 시간의 흐름에 따라 확인해보면, 해당 컴포넌트가 시간이 지남에 따라 의존성이 어떻게 변화하는지 확인할 수 있다.\n이를 통해 주계열에서 멀리 벗어난 컴포넌트들을 분석하여 인사이트를 얻는 것도 가능하다.\n결론 의존성 관리 지표는 설계의 의존성과 추상화 정도가 휼륭한 패턴이라고 생각하는 수준에 얼마나 잘 부합하는지를 측정한다.\n하지만 지표는 임의로 결정된 표준을 기초로 한 측정값이므로 진리가 아니며, 의존성의 좋고 나쁨을 지표로만 판단해서는 안된다.\n그러나 이러한 지표를 통해 이상을 확인할 여지가 충분하고, 이를 상세하게 분석하여 아키텍처 개선에 유용하게 사용될 수 있다.\n","date":"2024-03-07T15:31:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/14/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/14/","title":"14. 컴포넌트 결합"},{"content":"이 장에서는 컴포넌트 응집도와 관련된 세 가지 원칙을 논의한다.\nREP: 재사용/릴리스 등가 원칙 재사용 단위는 릴리스 단위와 같다.\n재사용 가능한 컴포넌트나 컴포넌트 라이브러리가 매우 많이 만들어짐에 따라 모듈 관리 도구가 점점 중요해졌다.\n소프트웨어 컴포넌트가 릴리스 절차를 통해 추적 관리되지 않거나, 릴리스 번호가 부여되지 않는다면 해당 컴포넌트를 재사용하고 싶어도 할 수도 없고, 하지도 않을 것이다.\n릴리스 번호가 없다면 재사용 컴포넌트들이 서로 호환되는지 보증할 방법이 전혀 없다. 릴리스 번호를 통해 새로운 버전이 언제 출시되고 무엇이 변했는지를 소프트웨어 개발자들이 알아야 한다. 따라서 릴리스 절차에는 적절한 공지와 함께 릴리스 문서 작성도 포함되어야 한다. 이 원칙을 소프트웨어 설계와 아키텍처 관점에서 보면 단일 컴포넌트는 응집성 높은 클래스와 모듈들로 구성되어야 함을 뜻한다.\n다른 시각으로 바라보면 하나의 컴포넌트로 묶인 클래스와 모듈은 반드시 함께 릴리스할 수 있어야 한다.\n하나의 컴포넌트로 묶인 클래스와 모듈은 버전 번호가 같아야 한다. 동일한 릴리스로 추적 관리되어야 한다. 동일한 릴리스 문서에 포함되어야 한다. REP의 이러한 내용은 개발자들 간의 관습이라는 점이 약점이다. 하지만 그렇기 때문에 컴포넌트가 REP를 위배하면 컴포넌트 사용자가 알게 되고, 아키텍트로서 능력을 높게 평가하지 않을 것이다.\n이 원칙의 약점은 다른 두 원칙이 지닌 강점을 통해 충분히 보완할 수 있다.\nCCP: 공통 폐쇄 원칙 동일한 시점에 변경되는 클래스를 같은 컴포넌트로 묶어라.\n서로 다른 시점에 다른 이유로 변경되는 클래스는 다른 컴포넌트로 분리하라.\nSRP와 유사성 이 원칙은 단일 책임 원칙(SRP)을 컴포넌트 관점에서 다시 쓴 원칙이다.\n동일한 시점에 동일한 이유로 변경되는 것들을 한데 묶어라. 서로 다른 시점에 다른 이유로 변경되는 것들은 서로 분리하라.\nSRP에서 클래스의 변경 이유는 하나여야 한다고 말하듯이, 공통 폐쇄 원칙에서도 마찬가지로 단일 컴포넌트는 변경의 이유가 여러개여서는 안된다고 말한다.\n대다수의 애플리케이션에서 유지보수성(Maintainability)은 재사용성보다 훨씬 중요하다.\n애플리케이션의 코드가 반드시 변경되어야 한다면, 여러 컴포넌트 도처에서 분산되어 발생하기 보다는, 변경 모두가 단일 컴포넌트에서 발생하는 편이 낫다.\n변경을 단일 컴포넌트로 제한할 수 있다면, 해당 컴포넌트만 재배포하면 되므로 의존하지 않는 다른 컴포넌트는 다시 검증하거나 배포할 필요가 없어진다.\nCCP는 같은 이유로 변경될 가능성이 있는 클래스는 모두 한곳으로 묶을 것을 권한다.\n물리적 또는 개념적으로 강하게 결합되어 항상 함께 변경되는 클래스들은 하나의 컴포넌트에 속해야 한다. 이를 통해 소프트웨어를 릴리스, 재검증, 배포하는 일과 관련된 작업량을 최소화 할 수 있다.\nOCP와의 유사성 이 원칙은 개방 폐쇄 원칙과도 밀접하게 관련되어 있다.\nOCP에서는 클래스가 변경에는 닫혀있고 확장에는 열려있어야 한다고 말하는데, 100% 완전한 폐쇄란 불가능하므로 전략적으로 폐쇄해야 하며, 이에 따라 발생할 가능성이 있거나 과거에 발생했던 대다수의 공통적인 변경에 대해서 클래스가 닫혀 있도록 설계한다.\nCCP에서는 동일한 유형의 변경에 대해 닫혀있는 클래스들을 하나의 컴포넌트로 묶음으로써 OCP에서 얻은 교훈을 확대 적용한다.\n따라서 변경이 필요한 요구사항이 발생했을 때, 그 변경이 영향을 주는 컴포넌트들이 최소한으로 한정될 가능성이 확실히 높아진다.\nCRP: 공통 재사용 원칙 컴포넌트 사용자들을 필요하지 않는 것에 의존하게 강요하지 말라.\n공통 재사용 원칙도 클래스와 모듈을 어느 컴포넌트에 위치시킬 지 결정할 때 도움되는 원칙이다.\nCRP에서는 같이 재사용되는 경향이 있는 클래스와 모듈들은 같은 컴포넌트에 포함해야 한다고 말한다.\n재사용 가능한 클래스는 재사용 모듈의 일부로써 해당 모듈의 다른 클래스와 상호작용하는 경우가 많은데, CRP에서는 이런 클래스들이 동일한 컴포넌트에 포함되어야 한다고 말한다.\n예를 들어 컨테이너 클래스와 해당 클래스의 이터레이터 클래스는 서로 강하게 결합되어 있기 때문에 함께 재사용되므로 이들 클래스는 반드시 동일한 컴포넌트에 위치해야 한다. 이처럼 CRP는 각 컴포넌트에 어떤 클래스들을 포함시켜야 하는지를 설명해준다.\nISP와의 관계 CRP는 인터페이스 분리 원칙의 포괄적인 버전이다.\n필요하지 않은 것에 의존하지 말라.\nISP는 사용하지 않은 메서드가 있는 클래스에 의존하지 말라고 조언하고, CRP는 사용하지 않는 클래스를 가진 컴포넌트에 의존하지 말라고 조언한다.\n이를 통해 CRP는 동일한 컴포넌트로 묶어서는 안되는 클래스가 무엇인지도 말해준다.\n어떤 컴포넌트가 다른 컴포넌트를 사용하면, 두 컴포넌트 사이에는 의존성이 생겨난다.\n사용하는 컴포넌트가 사용되는 컴포넌트의 극히 일부만을 사용한다고 하더라도 의존성은 조금도 약해지지 않는다.\n사용되는 컴포넌트가 변경될 때마다 사용하는 컴포넌트도 변경해야 할 가능성이 높다. 사용하는 컴포넌트를 변경하지 않더라도, 재컴파일, 재검증, 재배포를 해야 하는 가능성은 여전히 남아있다. 따라서 의존하는 컴포넌트가 있다면 해당 컴포넌트의 모든 클래스에 대해 의존함을 확실히 인지해야한다.\n한 컴포넌트에 속한 클래스들은 더 작게 그룹지을 수 없다. 그 중 일부 클래스에만 의존하고 다른 클래스와는 독립적일 수 없음을 확실히 인지해야한다. CRP는 어떤 클래스를 한데 묶어도 되는지보다는, 어떤 클래스를 한데 묶어서는 안 되는지에 대해 훨씬 더 많은 것을 이야기한다.\n강하게 결합되지 않은 클래스들을 동일한 컴포넌트에 위치시켜서는 안된다고 말한다. 컴포넌트 응집도에 대한 균형 다이어그램 응집도 관점으로 세 원칙이 상충된다.\n포함 원칙: REP, CCP 컴포넌트의 크기를 더욱 크게 만든다. 배제 원칙: CRP 컴포넌트를 더욱 작게 만든다. 뛰어난 아키텍트라면 이 원칙들이 균형을 이루는 방법을 찾아야한다.\nREP와 CRP에만 중점을 두면, 사소한 변경이 생겼을 때 너무 많은 컴포넌트에 영향을 미친다. CCP와 REP에만 과도하게 집중하면 불필요한 릴리스가 너무 빈번해진다. 이 균형 삼각형에서 개발팀이 현재 관심을 기울이는 부분을 충족시키는 위치를 찾아야 하며, 시간이 흐르면서 개발팀이 주의를 기울이는 부분 역시 변한다는 사실도 이해하고 있어야 한다.\nex) 프로젝트 초기에는 개발 가능성이 더욱 중요하므로 CCP가 REP보다 훨씬 더 중요하다. 일반적으로 프로젝트는 삼각형의 오른쪽에서 시작하여 재사용성을 희생한다.\n프로젝트가 성숙하고, 파생된 또 다른 프로젝트가 시작되면, 점차 왼쪽으로 이동해간다.\n즉, 프로젝트의 컴포넌트 구조는 시간과 성숙도에 따라 변하며, 프로젝트가 실제로 수행하는 일 자체보다는 프로젝트가 발전되고 사용되는 방법과 더 관련이 깊다.\n결론 과거에는 응집도를 \u0026lsquo;모듈은 단 하나의 기능만 수행해야한다.\u0026lsquo;는 속성 정도로 훌씬 단순했다.\n하지만 컴포넌트 응집도에 관한 세 가지 원칙은 응집도가 가질 수 있는 훨씬 복잡한 다양성을 설명해 준다.\n어느 클래스들을 묶어서 컴포넌트로 만들지 결정할 때, 재사용성과 개발 가능성이라는 상충하는 힘을 고려하여 애플리케이션의 요구에 맞게 균형을 잡는 일은 중요하다.\n이 균형점은 거의 항상 유동적이므로 컴포넌트를 구성하는 방식도 조금씩 흐트러지고 또 진화해야한다.\n","date":"2024-03-07T15:09:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/13/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/13/","title":"13. 컴포넌트 응집도"},{"content":" SOLID 원칙이 벽과 방에 벽돌을 배치하는 방법을 알려준다면, 컴포넌트 원칙은 빌딩에 방을 배치하는 방법을 설명한다.\n큰 빌딩과 마찬가지로 대규모 소프트웨어 시스템은 작은 컴포넌트들로 만들어지므로, 컴포넌트가 무엇인지, 구성하는 요소는 무엇인지 알아보고, 컴포넌트를 결합하여 시스탬을 구성하는 방법에 대해 논의한다.\n컴포넌트는 시스템의 구성 요소로 배포할 수 있는 가장 작은 단위다.\njava: .jar, ruby: gem, .net: DLL 컴파일 언어에서는 바이너리 파일의 결합체 인터프리터 언어의 경우 소스 파일의 결합체 여러 컴포넌트를 서로 링크하여 실행 가능한 단일 파일로 생성할 수 있다.\n단일 아카이브: .war 플러그인: .jar, dll 실행 파일: .exe 컴포넌트가 마지막에 어떤 형태로 배포되든, 잘 설계된 컴포넌트라면 반드시 독립적으로 배포 가능한, 따라서 독립적으로 개발 가능한 능력을 갖춰야한다.\n컴포넌트의 간략한 역사 소프트웨어 개발 초창기에는 메모리에서의 프로그램 위치와 레이아웃을 프로그래머가 직접 제어하여, 프로그램 위치가 결정되면, 재배치가 불가능했다.\n라이브러리 함수의 소스 코드를 애플리케이션 코드에 직접 포함시켜 단일 프로그램으로 컴파일했다. 라이브러리는 바이너리가 아니라 소스 코드 형태로 유지되었다. 자원이 한정적이었기 때문에 소스코드 전체를 여러번에 걸쳐서 읽어야하지만, 코드 전체를 메모리에 상주시킬 수가 없어 여러차례 읽어야했다. 컴파일 시간을 단축시키기위해 함수 라이브러리의 소스코드를 애플리케이션 코드로부터 분리했다. 개별적으로 컴파일하고, 컴파일된 바이너리를 메모리의 특정 위치에 로드했다. 하지만 애플리케이션은 점점 커졌고, 할당된 공간을 넘어서게 되어 두 개의 주소 세그먼트로 분리하고 함수 라이브라리 공간을 사이에 두고 오가며 동작하게 배치해야했다. 사용하는 메모리가 늘어날스록 이러한 단편화는 계속될 수밖에 없었기 때문에 지속가능하지 않았다. 재배치성 이러한 문제의 해결책은 재배치가 가능한 바이너리(Relocatable binary)였다.\n지능적인 로더를 사용해서 메모리에 재배치할 수 있는 형태의 바이너리를 생성하도록 컴파일러를 수정 함수 라이브러리를 로드할 위치와 애필리케이션을 로드할 위치를 로더에게 지시할 수 있게 되었다.\n이를 통해 오직 필요한 함수만을 로드할 수 있게 되었다.\n컴파일러는 재배치 가능한 바이너리 안의 함수 이름을 메타데이터 형태로 생성하도록 수정되었다.\n프로그램이 라이브러리 함수를 호출한다면 컴파일러는 라이브러리 함수 이름을 외부 참조(External reference) 라이브러리 함수를 정의하는 프로그램이라면 외부 정의(External definition) 이렇게 함으로써 외부 정의를 로드할 위치가 정해지만 하면 로더가 외부 참조를 외부 정의에 링크시킬 수 있게 되었고, 이렇게 링킹 로더(Linking Loader)가 탄생했다.\n링커 링킹 로더의 등장으로 프로그래머는 프로그램을 개별적으로 컴파일하고 로드할 수 있는 단위로 분할할 수 있게 되었다.\n하지만 프로그램도 훨씬 커지게 되며 링킹 로더의 성능으로 감당할 수 없는 수준이 되었다.\n이에 따라 로드와 링크가 두 단계로 분리되었다.\n링커라는 별도의 애플리케이션으로 링크가 완료된 재배치 코드를 만들어주어 로딩 과정이 아주 빨라졌다. 한번 만들어둔 실행 파일은 언제라도 빠르게 로드할 수 있게 되었다. 이후 C와 같은 고수준 언어를 사용하기 시작하며 소스 모듈은 .c파일에서 .o 파일로 컴파일된 후, 링커로 전달되어 바르게 로드될 수 있는 형태의 실행 파일로 만들어졌다.\n하지만 프로그램의 규모가 커지며 전체 모듈을 컴파일 하는데 꽤 시간이 걸렸고, 이후에 링크에서는 더 많은 시간이 소요되었다.\n로드 시간은 여전히 빨랐지만, 컴파일-링크 시간이 병목 구간이었다.\n하지만 자원의 성능이 비약적으로 향상되며 프로그램을 성장시키는 속도보다 링크 시간이 줄어드는 속고가 더 빨라지기 시작했고, 소규모 작업이라면 링킹 로더마저도 다시금 사용할 만하게 되었다.\n이렇게 액티브 X와 공유 라이브러리 시대가 열렸고 .jar 파일도 등장하기 시작했다.\n다수의 .jar 파일 또는 다수의 공유 라이브러리를 순식간에 서로 링크한 후, 링크가 끝난 프로그램을 실행할 수 있게 되었고, 이렇게 **컴포넌트 플러그인 아키텍쳐(Component Plugin Architecture)**가 탄생했다.\n결론 런타임에 플러그인 형태로 결합할 수 있는 동적 링크 파일이 이 책에서 말하는 소프트웨어 컴포넌트에 해당한다.\n과거에는 초인적인 노력을 들여아만 컴포넌트 플러그인 아키텍처를 적용할 수 있었지만, 이제는 기본으로 쉽게 사용할 수 있는 지점까지 다다랐다.\n","date":"2024-03-07T14:21:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/12/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/12/","title":"12. 컴포넌트"},{"content":"의존성 역전 원칙에서 말하는 \u0026lsquo;유연성이 극대화된 시스템\u0026rsquo;이란 소스 코드 의존성이 추상(Abstraction)에 의존하며 구체(Concretion)에는 의존하지 않는 시스템이다.\n자바와 같은 정적 타입 언어에서 이 말은 use, import, include 구분은 오직 인터페이스나 추상 클래스 같은 추상적인 선언만을 참조해야 한다는 뜻이다.\n마찬가지로 동적 타입 언어에서도 소스 코드 의존 관계에서 구체 모듈은 참조해서는 안된다.\n하지만 구체 모듈이 무엇인지 정의하기 다소 어렵고, 호출할 함수가 구현된 모듈이라면 참조하지 않기가 특히 어렵다.\n소프트웨어 시스템이라면 구체적인 많은 장치에 반드시 의존하기 때문에 규칙으로 보기에 비현실적이다.\nex) 자바 String은 구체 클래스 애써 추상 클래스로 만들려는 시도는 현실성이 없다. java.lang.String 구체 클래스에 대한 소스 코드 의존성은 벗어날 수 없고, 벗어나서도 안된다. String 클래스는 변경되는 일이 거의 없고, 있더라도 엄격하게 통제되므로 매우 안정적이다.\n개발자와 아키텍트는 String 클래스에서 변경이 자주 발생할 것이라고 염려할 필요가 없다. 이러한 이유로 DIP를 논할 때 운영체제나 플랫폼 같이 안정성이 보장된 환경에 대해서는 무시하는 편이다.\n환경에 대한 의존성을 용납하는 이유는 변경되지 않기 때문이다. 즉, 변동성이 큰(Volatile) 구체적인 요소가 의존하지 않도록 만들어야 하는 목표이며, 구체적인 요소는 지속적인 개발을 통해 계속 변경될 수 밖에 없는 모듈들이다.\n안정된 추상화 안정된 소프트웨어 아키텍처란 변동성이 큰 구현제에 의존하는 일은 지양하고, 안정된 추상 인터페이스를 선호하는 아키텍처이다.\n인터페이스는 구현체보다 변동성이 낮다.\n따라서 인터페이스를 변경하지 않고도 구현체에 기능을 추가할 수 있는 방법을 통해 인터페이스의 변동성을 낮춰야한다.\n변동성이 큰 구체 클래스를 참조하지 말라. 추상 인터페이스를 참조하라 객체 생성 방식을 강하게 제약하며, 일반적으로 추상 팩토리(Abstract Factory)를 사용하도록 강제한다. 변동성이 큰 구체 클래스로부터 파생하지 말라. 상속은 소스 코드에 존재하는 모든 관계 중에서 가장 강력한 동시에 뻣뻣해서 변경하기 어렵다. 상속을 아주 신중하게 사용해야한다. 구체 함수를 오버라이드 하지 말라. 구체 함수는 대체로 소스 코드 의존성을 필요로한다. 구체 함수를 오버라이드 하면 이러한 의존성을 제거할 수 없게되며, 그 의존성을 상속하게 된다. 추상 함수로 선언하고 구현체들에서 각자의 용도에 맞게 구현한다. 구체적이며 변동성이 크다면 절대로 그 이름을 언급하지 말라. 팩토리 사실상 모든 언어에서 객체를 생성하려면 해당 객체를 구체적으로 정의한 코드에 대해 소스 코드 의존성이 발생하므로, 변동성이 큰 구체적인 객체는 특별히 주의해서 생성해야한다.\n대다수의 객체 지향 언어에서 바람직하지 못한 의존성을 처리할 때 추상 팩토리를 사용할 수 있다.\n소스 코드 의존성 곡선은 구체적인 것들로부터 추상적인 것들은 분리하고 있다. 소스 코드 의존성은 해당 곡선과 교차할 때 모두 추상적인 쪽으로 향한다. 컴포넌트 시스템이 곡선을 기준으로 추상 컴포넌트와 구체 컴포넌트로 분리된다. 제어흐름은 소스 코드 의존성과는 정반대 방향으로 곡선을 가로지르고 있는데, 소스 코드 의존성은 제어흐름과는 반대 방향으로 역전된다.\n이러한 이유로 이 원칙을 의존성 역전이라고 부른다.\n구체 컴포넌트 위 예제의 경우 ServiceFactoryImpl 구체 클래스가 ConcreteImpl 구체 클래스에 의존하고 있으므로 DIP를 위배한다고 볼 수 있다.\nDIP 위배를 모두 없앨수는 없지만, DIP를 위배하는 클래스들은 적은 수의 구체 컴포넌트 내부로 모을 수 있고, 이를 통해 시스템의 나머지 부분과는 분리할 수 있다.\n결론 DIP는 고수준의 아키텍처 원칙에서 지속적으로 언급되는 규칙이다.\nDIP는 아키텍처 다이어그램에서 가장 눈에 드라나는 원칙이 될 것이다.\n곡선은 아키텍처의 경계가 되고, 의존성은 곡선을 경계로 더 추상적인 엔티티가 있는 쪽으로만 향한다.\n","date":"2024-03-03T10:17:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/11/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/11/","title":"11. DIP: 의존성 역전 원칙"},{"content":"classDiagram direction BT class User1 { } class User2 { } class User3 { } class OPS { op1() op2() op3() } OPS \u003c-- User1 OPS \u003c-- User2 OPS \u003c-- User3 다수의 사용자가 OPS 클래스의 오퍼레이션을 사용함 UserN은 opN OPS는 정적 타입 언어로 작성된 클래스 위와 같은 경우 User1이 op2, op3를 전혀 사용하지 않음에도 이 두 메서드에 의존하게되고, 사용하지 않는 소스 코드가 변경되면 다시 컴파일한 후 새로 배포해야한다.\n인터페이스 분리 원칙은 오퍼레이션을 인터페이스 단위로 분리하여 이러한 의존성을 제거한다.\nclassDiagram class User1 { } class User2 { } class User3 { } class U1Ops { \u003c\u003c interface \u003e\u003e op1() } class U2Ops { \u003c\u003c interface \u003e\u003e op2() } class U3Ops { \u003c\u003c interface \u003e\u003e op3() } class OPS { op1() op2() op3() } OPS --|\u003e U1Ops OPS --|\u003e U2Ops OPS --|\u003e U3Ops U1Ops \u003c-- User1 U2Ops \u003c-- User2 U3Ops \u003c-- User3 User1의 소스 코드는 U1Ops와 op1에는 의존하지만 OPS에는 의존하지 않게 되어 OPS 에서 발생한 변경이 User1과 전혀 관계없는 변경이라면, User1을 다시 컴파일하고 새로 배포하는 상황은 초래되지 않는다.\nISP와 언어 정적 타입 언어는 사용자가 import, use, include와 같은 타입 선언문을 사용하도록 강제한다.\n이러한 \u0026lsquo;포함된(included)\u0026rsquo; 선언문으로 인해 소스 코드 의존성이 발생하고, 이로 인해 재컴파일 도는 재배포가 강제되는 상황이 무조건 초래된다.\n동적 타입 언어네서는 소스 코드에 이러한 선언문이 존재하지 않고 런타임에 추론이 발생한다. 소스 코드 의존성이 아예 없으며, 재컴파일과 재배포가 필요없다. 이러한 이유로 동적 타입 언어를 사용하면 정적 타입 언어를 사용할 때보다 유연하며 결합도가 낮은 시스템을 만들 수 있다. ISP를 아키텍처가 아니라, 언어와 관련된 문제라고 결론내릴 여지가 있다.\nISP와 아키텍처 ISP를 사용하는 근본적인 동기를 살펴보면, 잠재되어 있는 더 깊은 우려사항을 볼 수 있다.\n일반적으로 필요 이상으로 많을 걸 포함하는 모듈에 의존하는 것은 해로운 일이다. 소스 코드 의존성의 경우 불필요한 재컴파일과 재배포를 강제하기 때문이다. 위와 같은 문제는 물론 아키텍처 수준에서도 마찬가지의 상황이 발생한다.\nflowchart LR a[System S] b[Framework F] c[Database D] a --\u003e b --\u003e c S 시스템을 구축하며 F라는 프레임워크를 도입하려고한다. F 프레임워크는 D라는 특정 데이터베이스를 반드시 사용하도록 만들었다. S는 F에 의존하고, F는 D에 의존하게 된다. 위와 같은 상황에서 F에서는 불필요한 기능, 즉 S와는 전혀 관계없는 기능이 D에 포함되어있다고 가정하면,\n불필요한 기능으로 인해 D 내부가 변경되면, F를 재배포 해야할 수도 있다. D 내부의 기능 중 F와 S에서 불필요한 기능에 문제가 발생해도 F와 S에 영향을 준다. 결론 불필요한 짐을 실은 무언가에 의존하면 예상치도 못한 문제에 빠진다.\n","date":"2024-03-03T09:49:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/10/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/10/","title":"10. ISP: 인터페이스 분리 원칙"},{"content":" 하위 타입(Subtype)\nS타입의 객체 o1 각각에 대응하는 T타입 객체 o2가 있고, T타입을 이용해서 정의한 모든 프로그램 P에서 o2의 자리에 o1을 치환하더라도 P의 행위가 변하지 않는다면, S는 T의 하위 타입이다.\n상속을 사용하도록 가이드하기 classDiagram class Billing { } class License { \u003c\u003c interface \u003e\u003e calcFee() } class PersonalLicense { } class BusinessLicense { users } Billing --\u003e License License \u003c|-- PersonalLicense License \u003c|-- BusinessLicense Billing 애플리케이션의 행위가 License타입 중 무엇을 사용하는지에 전혀 의존하지 않이 때문에, 이들 하위 타입은 모두 License 타입을 치환할 수 있으므로 위 설계는 리스코프 치환 원칙을 준수한다.\n정사각형/직사각형 문제 리스코프 치환 원칙을 위반하는 전형적인 문제로 유명한 정사각형/직사각형 문제가 있다.\nclassDiagram direction LR class User { } class Rectangle { H, W setH() setW() } class Square { setSide() } User --\u003e Rectangle Rectangle \u003c|-- Square 위 예제에서 Rectangle의 높이와 너비는 서로 독립적으로 변경될 수 있는 반면, Square의 높이와 너비는 반드시 함께 변경되므로 Square는 Rectangle의 하위 타입으로는 부적합하다.\n이러한 경우 User는 대화하고 있는 상대가 Rectangle 이라고 생각하므로 혼동이 생길 수 있다.\n1 2 3 4 Rectangle r = ... r.setW(5); r.setH(2); assert(r.area() == 10); 위와 같은 경우 ...에 Square를 생성한다면(치환한다면), assert문은 실패하게된다.\n이러한 형태의 리스코프 치환 원칙 위반을 막기 위한 유일한 방법은 검사하는 메커니즘을 User에 추가하는 것 인데, User의 행위가 사용하는 타입에 의존하게 되므로, 결국 타입을 서로 치환할 수 없게 된다.\nLSP와 아키텍처 LSP는 상속을 사용하도록 가이드하는 방법 정도로 간주 되었으나, 시간이 지나면서 LSP는 인터페이스와 구현체에도 적용되는 더 광범위한 소프트웨어 설계 원칙으로 변모해왔다.\n인터페이스\n위에서 말하는 인터페이스는 여러 의미로 해석 가능하다.\n인터페이스 하나와 이를 구현하는 여러 개의 클래스 동일한 메서드 시그니처를 공유하는 여러 개의 클래스 동일한 REST 인터페이스에 응답하는 서비스 집단 잘 정의된 인터페이스와 그 인터페이스의 구현체끼리의 상호 치환 가능성에 기대는 사용자들이 존재하기 때문에 대부분의 상황에서 LSP를 적용할 수 있다.\nLSP 위배 사례: 택시 파견 서비스 요구사항\n고객이 어느 택시업체인지는 신경쓰지 않고 자신의 상황에 가장 적합한 택시를 찾는다. 택시를 결정하면, 시스템은 REST 서비스를 통해 선택된 택시를 고객 위치로 파견한다. URI가 운전기사 데이터베이스에 저장되어 있다. URI 정보를 이용하여 해당 기사를 고객 위치로 파견한다. ex) Bob의 URI: purplecab.com/driver/Bob 요청 예시 1 2 3 4 purplecab.com/driver/Bob /picupAddress/24 Maple St. /pickupTime/153 /destination/ORD 이러한 서비스를 만들 때 다양한 택시업체에서 동일한 REST 인터페이스를 반드시 준수하도록 만들어야한다.\n만약 택시업체 ACME에서 destination 필드를 dest로 축약해서 사용했다고 가정하면, 해당 예외 사항을 처리하는 로직을 추가해야만 한다.\n1 if (driver.getDispatchUri().startWith(\u0026#34;acme.com\u0026#34;))... \u0026ldquo;acme\u0026quot;라는 단어를 코드 자체에 추가하면, 끔찍할 뿐만 아니라 이해할 수도 없는 온갖 종류의 에러가 발생할 여지를 만들게 된다. 새로운 택시업체 추가시 또 다른 if문이 필요할 수 있다. 위와 같은 버그를 방지하기 위해 설정용 데이터베이스를 이용하는 파견 명령 생성 모듈을 만들어야 할 수도 있다. URI Dispatch Format Acme.com /pickupAddress/%s/pickupTime/%s/dest/%s *.* /pickupAddress/%s/pickupTime/%s/destination/%s REST 서비스들의 인터페이스가 서로 치환 가능하지 않다는 사실을 처리하는 중요하고 복잡한 매커니즘을 추가해야 한다. 결론 LSP는 아키텍처 수준까지 확장할 수 있고, 반드시 확장해야만 한다.\n치환 가능성을 조금이라도 위배하면 시스템 아키텍처가 오염되어 상당량의 별도 메커니즘을 추가해야 할 수 있기 때문이다.\n","date":"2024-03-02T16:20:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/9/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/9/","title":"9. LSP: 리스코프 치환 원칙"},{"content":" 소프트웨어 개체(Artifact)는 확장에는 열려 있어야 하고, 변경에는 닫혀 있어야 한다.\n개방 폐쇄 원칙은 소프트웨어 개체의 행위는 확장할 수 있어야 하지만, 이때 개체를 변경해서는 안된다는 원칙이다.\n요구사항을 살짝 확장하는 데 소프트웨어를 많이 수정해야 한다면, 그 소프트웨어 시스템을 설계한 아키텍트는 엄청난 실패에 맞닥뜨린 것이다.\nOCP는 클래스와 모듈을 설계할 때 도움이 되는 원칙으로 알려져 있지만, 아키텍처 컴포넌트 수준에서 고려할 때 훨씬 중요한 의미를 가진다.\n사고 실험: 재무제표 시스템 요구사항\n웹 페이지에 표시되는 데이터는 스크롤할 수 있음 음수는 빨간색으로 출력 추가 요구 사항: 동일한 정보를 보고서 형태로 변환해서 흑백 프린터로 출력 가능 페이지 번호가 제대로 메겨져 있어야 함 페이지마다 적절한 머리글과 바닥글 있어야 함 표의 각 열에는 레이블이 있어야 함 음수는 괄호로 감싸야함 소프트웨어 아키텍처가 훌륭하다면 추가 요구사항에 따라 변경되는 코드의 양이 가능한 최소화 될 것이다.(이상적인 변경량은 0)\n이러한 경우 서로 다른 목적으로 변경되는 요소를 적절하게 분리하고(단일 책임 원칙), 이들 요소 사이의 의존성을 체계화함으로(의존성 역전 원칙) 변경량을 최소화 할 수 있다.\nflowchart LR a[재무 데이터] b((재무 분석기)) c[보고서용 재무 데이터] d((보고서를 웹에 표시)) e((보고서를 프린터로 출력)) a --\u003e b b --\u003e c c --\u003e d c --\u003e e 보고서 생성이 두 개의 책임으로 분리 \u0026gt; 웹, 프린터 책임을 분리했다면, 책임 중 하나에서 변경이 발생하더라도 다른 하나는 변경되지 않도록 의존성도 확실히 조직화 해야한다. 행위가 확장될 때 변경이 발생하지 않음을 보장해야한다. 이러한 목적을 달성하려면 처리과정을 클래스 단위로 분할, 컴포넌트 단위로 구분해야 한다.\n모든 의존성이 소스 코드 의존성을 나타낸다. FinancialDataMapper는 구현 관계를 통해 FinancialDataGateway를 알고 있지만, FinancialDataGateway는 FinancialDataMapper에 대해 알지 못한다. 이중선은 화살표와 오직 한 방향으로만 교차한다. 모든 컴포넌트 관계는 단 방향으로 이루어진다. 화살표는 변경으로부터 보호하려는 컴포넌트를 향하도록 그려진다. A 컴포넌트에서 발생한 변경으로부터 B 컴포넌트를 보호하려면 반드시 A 컴포넌트가 B 컴포넌트에 의존해야한다.\nPresenter에서 발생한 변경으로부터 Controller를 보호 View에서 발생한 변경으로부터 Presenter를 보호 Interactor는 다른 모든 것에서 발생한 변경으로부터 보호 Interactor는 변경 폐쇄 원칙을 가장 잘 준수하는 곳에 위치하고 있는데, 이는 Interactor가 애플리케이션에서 가장 높은 수준의 정책, 가장 중요한 문제를 담당하고 있기 때문이다.\n보호의 계층구조는 수준 level이라는 개념을 바탕으로 구성되어야하며, 높은 수준일수록(세부 구현과 멀 수록) 변경에 대해 강력한 보호가 필요하다.\n아키텍처 수준에서 개방 폐쇄 원칙은 아키텍트는 기능이 어떻게, 왜, 언제 발생하는지에 따라서 기능을 분리하고, 분리한 기능을 컴포넌트의 계층구조로 조직화한다.\n컴포넌트 계층구조를 이와 같이 조족화하면 저수준 컴포넌트에서 발생한 변경으로부터 고수준 컴포넌트를 보호할 수 있다.\n방향성 제어 FinancialDataGateway 인터페이스는 Interactor 컴포넌트에서 Database 컴포넌트의 의존성을 역전시키기 위해 FinancialReportGenerator와 FinancialDataMapper 사이에 위치한다.\nFinancialDataGateway 인터페이스가 없었다면 의존성이 Interactor 컴포넌트에서 Database 컴포넌트로 바로 향하게된다.\n정보 은닉 FinancialReportRequester 인터페이스는 FinancialReportController가 Interactor 내부에 대해 너무 많이 알지 못하도록 막기 위해서 존재한다.\n인터페이스가 없었다면 Controller는 FinancialEntities에 대해 추이 종속성(Transitive dependency)을 가지게 된다. 추이 종속성을 가지게 되면, 소프트웨어 엔티티는 **\u0026lsquo;자신이 직접 사용하지 않는 요소에는 절대로 의존해서는 안 된다.\u0026rsquo;**는 소프트웨어 원칙을 위반하게 된다. Controller에서 발생한 변경으로부터 Interactor를 보호하는 일의 우선순위가 가장 높지만, 반대로 Interactor에서 발생한 변경으로 부터 Controller도 보호되길 바라기 때문에 Interactor 내부를 은닉한다.\n추이 종속성(Transitive dependency)\n클래스 A가 클래스 B에 의존하고, 클래스 B가 클래스 C에 의존한다면, 클래스 A는 클래스 C에 의존하게 된다.\n클래스 이외의 소프트웨어의 모든 엔티티에도 동일하게 적용된다. 클래스 의존성이 순환적이라면, 모든 클래스가 서로 의존하게 되는 문제가 있다. 결론 개방 폐쇄 원칙은 아키텍처를 떠받치는 원동력 중 하나다. OCP의 목표는 시스템을 확장하기 쉬운 동시에 변경으로 인해 시스템이 너무 많은 영향을 받지 않도록 하는데 있다.\n시스템을 컴포넌트 단위로 분리한다. 저수준 컴포넌트에서 발생한 변경으로부터 고수준 컴포넌트를 보호할 수 있는 형태의 의존성 계층구조가 만들어지도록 한다. ","date":"2024-03-01T09:06:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/8/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/8/","title":"8. 개방 폐쇄 원칙"},{"content":"SOLID SOLID 원칙\n함수와 데이터 구조를 클래스로 배치하는 방법 클래스를 서로 결합하는 방법 좋은 소프트웨어 시스템은 깔끔한 코드(Clean Code)를 전제한다.\n하지만 깔끔한 코드를 사용한다고 하더라도 아키텍처가 좋지 못하다면 좋은 소프트웨어 시스템을 만들 수 없기 때문에 깔끔한 코드로 좋은 아키텍처를 정의하는 원칙이 필요하다.\nSOLID 원칙의 목적은 중간 수준의 소프트웨어 구조가 아래와 같도록 만드는 것이다.\n변경에 유연 이해하기 쉬움 많은 소프트웨어 시스템에 사용될 수 있는 컴포넌트의 기반 중간 수준의미는?\n프로그래머가 이들 원칙을 모듈 수준에서 작업할 때 적용할 수 있다.\n코드 수준보다는 조금 더 상위에서 적용되며 모듈과 컴포넌트 내부에서 사용되는 소프트웨어 구조를 정의하는 데 도움을 준다.\nSRP: Single Responsibility Principle, 단일 책임 원칙 콘웨이(Conway) 법칙 조직이 설계한 시스템은 해당 조직의 커뮤니케이션 구조를 반영한다. 조직의 내부 구조와 상호 작용 방식은 그 조직이 개발하는 소프트웨어의 구조와 유사해진다. 조직 내에 분산된 팀이 서로 간의 교류가 원할하지 않을 경우 개발하는 소프트웨어도 모듈화 인터페이스 설계에 문제가 발생한다. 따름 정리: 특정한 제한을 둔 조직 구조가 특정한 형태의 소프트웨어 아키텍처를 유도한다. 특정한 형테의 조직 구조가 특정한 형태의 소프트웨어 아키텍처를 촉진하거나 제한할 수 있다. 따라서 각 소프트웨어의 모듈은 변경의 이유가 하나여야만 한다. OCP: Open-Closed Principle, 개방 폐쇠 원칙 기존 코드를 수정하기보다는 반드시 새로운 코드를 추가하는 방식으로 시스템의 행위를 변경할 수 있도록 설계해야만 시스템을 쉽게 변경할 수 있다. LSP: Liskov Substitution Principle, 리스코프 치환 원칙 상호 대체 가능한 구성요소를 이용해 소프트웨어 시스템을 만들 수 있으려면, 구성요소는 반드시 서로 치환 가능해야 한다. ISP: Interface Segregation Principle, 인터페이스 분리 원칙 사용하지 않은 것에 의존하지 않아야 한다. DIP: Dependency Inversion Principle, 의존성 역전 원칙 고수준 정책을 구현하는 코드는 저수준 세보사항을 구현하는 코드에 절대로 의존해서는 안된다. 세부사항이 정책에 의존해야 한다. SRP: 단일 책임 원칙 단일 모듈은 변경의 이유가 하나, 오직 하나뿐이어야 한다.\n소프트웨어 시스템은 사용자와 이해관계자를 만족시키기 위해 변경된다.\n따라서 SRP가 말하는 변경의 이유는 **사용자와 이해관계자 집단(액터, Actor)**을 의미하며, 이러한 관점에서 단일 책임 원칙은 다음과 같이 말할 수 있다.\n하나의 모듈은 한의 액터에 대해서만 책임져야 한다\n모듈이란?\n소스 파일 단순히 함수와 데이터 구조로 음집된 집합 단일 액터를 책임지는 코드르 함께 묶어주는 힘이 바로 응집성 Cohesion이다.\n징후 1: 우발적 중복 classDiagram class Employee { calculatePay() reportHours() save() } calculatePay() 회계팀에서 기능을 정의 CFO 보고를 위해 사용 reportHours() 인사팀에서 기능을 정의하고 사용 COO 보고를 위해 사용 save() DBA가 기능을 정의 CTO 보고를 위해 사용 Employee 클래스는 서로 매우 다른 액터를 책임지기 때문에 SRP를 위반하게 된다.\nEmployee는 단일 클래스 내의 각각 다른 액터를 책임지는 메서드들로 인해 액터간의 결합이 발생하였고, 이러한 결합으로 인해 의존하는 무언가에 영향을 줄 수 있다.\nflowchart TD a[calculatePay] b[reportHours] c[regularHours] a --\u003e c b --\u003e c 위 처럼 calculatePay와 reportHours 메소드의 시간 계산 코드 중복을 피하기 위해 regularHours 메소드를 추가했다고 가정하고, calculatePay의 정책 변경으로 인해 reqularHours 메소드를 일부 변경했다.\n이러한 경우 해당 메소드와 연관된 액터가 CFO, COO 둘이지만, CFO의 요구사항으로 변경되었기 때문에 변경을 원하지 않는 COO의 기능에도 영향을 미치게 되는데, 관심사가 달라 의존 관계를 확인하기 어려워 확인이 누락이 될 수 있다.\n(이로인해 reportHours 메소드에서는 이러한 변경으로 인해 잘못된 결과를 얻을 수 있다.)\n결과적으로 이러한 문제는 서로 다른 액터가 의존하는 코드를 너무 가까이 배치했기 때문에 발생한 문제이며, SRP는 이러한 문제를 서로 다른 액터가 의존하는 코드를 서로 분리하여 예방하라고 말하고 있다.\n징후 2: 병합 메서드가 서로 다른 액터를 책임진다면 병합이 발생할 가능성이 확실히 더 높아진다.\n많은 사람이 서로 다른 목적으로 동일한 소스 파일을 변경하는 경우에 발생한다.\n다른 목적으로 인해 같은 코드를 변경할 가능성이 높아지고 이로인해 변경사항이 충돌할 여지가 많다. 이러한 문제는 서로 다른 액터를 뒷받침하는 코드를 서로 분리하는 것으로 이러한 문제를 벗어날 수 있다.\n해결책 이 문제의 해결책은 다양하지만, 모두 메서드를 각기 다른 클래스로 이동시키는 것은 공통적으로 포함한다.\n가장 확실한 해결책은 데이터와 메서드를 분리하는 방식으로, 아무런 메서드가 없는 데이터 구조인 EmployeeData 클래스를 만들어 세 개의 클래스가 공유하도록 만든다.\nclassDiagram direction LR class EmployeeData { datas... } class PayCalculator { calculatePay() } class HourReporter { reportHours() } class EmployeeSaver { saveEmployee() } PayCalculator --\u003e EmployeeData HourReporter --\u003e EmployeeData EmployeeSaver --\u003e EmployeeData 세 클래스는 서로의 존재를 모르기 때문에 우연한 중복을 피할 수 있다.\n위 방식은 개발자가 세 가지 클래스를 인스턴스화하고 추적해야 한다는 단점이 있는데, 퍼사드 패턴을 활용하여 개선이 가능하다.\nclassDiagram direction LR class EmployeeFacade { calculatePay() reportHours() save() } class EmployeeData { datas... } class PayCalculator { calculatePay() } class HourReporter { reportHours() } class EmployeeSaver { saveEmployee() } EmployeeFacade --\u003e PayCalculator EmployeeFacade --\u003e HourReporter EmployeeFacade --\u003e EmployeeSaver PayCalculator --\u003e EmployeeData HourReporter --\u003e EmployeeData EmployeeSaver --\u003e EmployeeData 중요한 업무 규칙을 데이터와 가깝게 배치하는 방식을 원한다면 아래와 같이 구성할 수도 있다.\nclassDiagram direction LR class Employee { employeeData calculatePay() reportHours() save() } class HourReporter { reportHours() } class EmployeeSaver { saveEmployee() } Employee --\u003e HourReporter Employee --\u003e EmployeeSaver 결론 단일 책임 원칙은 메서드와 클래스 수준의 원칙이다.\n하지만 이보다 상위 두 수준에서도 다른 형태로 다시 등장한다.\n컴포넌트 수준: 공통 폐쇄 원칙(Common Closure Principle) 아키텍처 수준: 아키텍쳐 경계(Architectural Boundaray)의 생성을 책임지는 변경의 축(Axis of Change) ","date":"2024-02-29T16:27:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/7/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/7/","title":"7. 단일 책임 원칙"},{"content":"함수형 프로그래밍이라는 개념은 프로그래밍 그 자체보다 앞서 등장했다.\n이 패러다임에서 핵심이 되는 기반은 람다(Lambda) 계산법으로 안론조 처치가 1930년대에 발명했다.\n불변성과 아키텍처 일반적인 언어들과 달리 함수형 언어에서는 변수가 한번 초기화되면 절대로 변경되지 않는다.\n가변 변수로 인해 경합 조건, 교착상태 조건, 동시 업데이트 문제가 발생하는데, 만약 어떠한 변수도 갱신되지 않는다면 경합 조건이나 동시 업데이트 문제가 발생하지 않는다.\n락(lock)이 가변적이지 않다면 교착상태도 일어나지 않는다.\n동시성 애플리케이션에서 마주치는 모든 문제, 즉 다수의 스레드와 프로세스를 사용하는 애플리케이션에서 마주치는 모든 문제는 가변 변수가 없다면 절대로 생기지 않는다.\n불변성은 저장 공간이 무한하고 프로세서의 속도가 무한히 빠르다면 보장 가능하지만 그럴 수 없으므로 어느정도 타협이 필요하다.\n가변셩의 분리 불변성과 관련하여 가장 주요한 타협 중 하나는 애플리케이션 또는 애플리케이션 내부의 서비스를 가변 컴포넌트와 불변 컴포넌트로 분리하는 일이다.\n불변 컴포넌트에서는 순수하게 함수형 방식으로만 작업이 처리되며, 어떤 가변 변수도 사용되지 않는다.\n불변 컴포넌트는 변수의 상태를 변경할 수 있는, 죽 순수 함수형 컴포넌트가 아닌 하나 이상의 다른 컴포넌트와 서로 통신한다.\n상태 변경은 컴포넌트를 갖가지 동시성 문제에 노출하는 꼴이므로, 흔히 트랜잭션 메모리와 같은 실천법을 사용하여 동시 업데이트와 경합 조건 문제로부터 가변 변수를 보호한다.\n트랜잭션 메모리는 트랜잭션을 사용하거나 도는 재시도 기법을 통해 변수를 보호한다.\n애플리케이션을 제대로 구조화하려면 변수를 변경하는 컴포넌트와 변경하지 않는 컴포넌트를 분리해야한다.\n그리고 이렇게 분리하려면 가변 변수들을 보호하는 적절한 수단을 동원해 뒷받침해야한다.\n현명한 아키텍트라면 가능한 한 많은 처리를 불변 컴포넌트로 옮겨야 하고, 가변 컴포넌트에서는 가능한 한 많은 코드를 빼내야 한다.\n이벤트 소싱 이벤트 소싱은 상태가 아닌 트랜잭션을 저장하자는 전략이다.\n상태가 필요해지면 단순히 상태의 시작점부터 모든 트랜잭션을 처리한다.\n이 전략은 많은 저장 공간을 필요로 하지만 현재는 저장 공간을 충분히 확보할 수 있다.\n데이터 저장소에서 삭제되거나 변경되는 것이 하나도 없으므로, 결과적으로 애플리케이션은 CRUD가 아닌 그저 CR만 수행한다. 데이터 저장소에서 변경과 삭제가 전혀 발생하지 않으므로 동시 없데이트 문제 또한 일어나지 않는다. 저장 공간과 처리 능력이 충분하면 완전한 불변성을 갖도록 만들 수 있고, 따라서 완전한 함수형으로 만들 수 있다.\n버전 관리 시스템이 정확히 이방식으로 동작한다. 결론 구조적 프로그래밍은 제어흐름의 직접적인 전환에 부과되는 규율이다. 객체 지향 프로그래밍은 제어흐름의 간접적인 전환에 부과되는 규율이다. 함수형 프로그래밍은 변수 할당에 부과되는 규율이다. 세 패러다임 모두 무언가를 하지 못하게 제한하고 있으며, 코드를 작성하는 방식의 형태를 한정시킨다.\n지난 반세기 동안 우리가 배운 것은 해서는 안되는 것에 대해서이다.\n도구는 달라졌고 소프트웨어도 변했지만, 소프트웨어는 순차, 분기, 반복 참조로 구성된다는 것은 변하지 않았다.\n","date":"2024-02-25T20:36:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/6/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/6/","title":"6. 함수형 프로그래밍"},{"content":"좋은 아키텍처를 만드는 일은 객체 지향 설계 원칙을 이해하고 응용하는 데서 출발한다.\nObject Oriented란 무엇인가? 데이터와 함수의 조합 대체로 이런 방식으로 많이 설명되지만 만족스러운 대답은 아니다. o.f()가 f(o)와 다르다는 의미를 내포한다. 실제 세계를 모델링하는 새로운 방법 얼버무리는 수준에 지나지 않는다. 의도가 불분명하며, 정의가 너무 모호하다. 캡슐화, 상속, 다형성 세가지 개념을 적절하게 조합한 것 캡슐화(Encapsulation)? OO를 정의하는 요소 중 하나로 캡슐화를 언급하는 이유는 데이터와 함수를 쉽고 효과적으로 캡슐화하는 방법을 OO 언어가 제공하기 때문이다.\n이를 통해 데이터와 함수가 응집력 있게 구성된 집단을 서로 구분할 수 있다.\nprivate, public 하지만 이러한 개념은 OO에만 국한된 것은 아니다.\npoint.h\n1 2 3 struct Point; struct Point* makePoint(double x, double y); double distance (struct Point *p1, struct Point *p2); point.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #include \u0026#34;point.h\u0026#34; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;math.h\u0026gt; struct Point { double x, y; } struct Point* makepoint(double x, double y) { struct Point* p = malloc(sizeof(struct Point)); p-\u0026gt;x = x; p-\u0026gt;y = y; return p; } double distance(struct Point* p1, struct Point* p2) { double dx = p1-\u0026gt;x - p2-\u0026gt;x; double dy = p1-\u0026gt;y - p2-\u0026gt;y; return sqrt(dx * dx + dy * dy); } 위 예시에서 point.h를 사용하는 측에서 struct Point의 멤버에 접근할 방법이 전혀 없다.\n사용자는 makePoint() 함수와 distance() 함수를 호출할 수는 있지만, Point 구조체의 데이터 구조와 함수가 어떻게 구현되었는지에 대해서는 조금도 알지 못한다. 이것이 완벽한 캡슐화이며, OO가 아닌 언어에서도 충분히 가능하다.\n오히려 OO를 지원하는 언어들이 캡슐화를 훼손하고 있는 경우가 많아 언어에 public, private, protected 키워드를 도입하여 불완전한 캡슐화를 보완한다.\nC++는 컴파일러가 클래스의 인스턴스 크기를 알아야하는 기술적인 이유로 클래스의 멤버변수를 해당 클래스의 헤더파일에 선언해야하고, 이로인해 사용측에서 멤버변수의 존재를 알게된다. 자바, C#은 헤더와 구현체를 분리하는 방식을 버렸다. 이로인해 클래스 선언과 정의를 구분하는게 불가능하다. 이 때문에 OO가 강력한 캡슐화에 의존한다는 정의는 받아들이기 힘들며, 대부분의 OO를 제공하는 언어들이 실제로는 C언어에서 누렸던 완벽한 캡슐화를 약화시켰다.\n많은 언어가 캡슐화를 거의 강제하지 않는다. OO 프로그래밍은 프로그래머가 캡슐화를 우회하지 않을 것 이라는 믿음을 기반으로 한다. 상속? OO 언어가 더 나은 캡슐화를 제공하지는 못했지만, 상속만큼은 확실히 제공했다.\n하지만 상속이란 단순히 어떤 변수와 함수를 하나의 유효 범위로 묶어 재정의하는 일에 불과하다.\nnamedPoint.h\n1 2 3 4 5 struct NamedPoint; struct NamedPoint* makeNamedPoint(double x, double y, char* name); void setName(struct NamedPoint* np, char* name); char* getName(struct NamedPoint* np); namedPoint.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #include \u0026#34;NamedPoint.h\u0026#34; #include \u0026lt;stdlib.h\u0026gt; struct NamedPoint { double x, y; char* name; } struct NamedPoint* makeNmaedPoint(double x, double y, char* name) { struct NamedPoint* p = malloc(sizeof(struct NamedPoint)); p-\u0026gt;x = x; p-\u0026gt;y = y; p-\u0026gt;name = name; return p; } void setName(struct NamedPoint* np, char* name) { np-\u0026gt;name = name; } void getName(struct NmaedPoint* np) { return np-\u0026gt;name; } main.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #include \u0026#34;point.h\u0026#34; #include \u0026#34;namedPoint.h\u0026#34; #include \u0026lt;stdlib.h\u0026gt; int main(int ac, char** av) { struct NamedPoint* origin = makeNamedPoint(0.0, 0.0, \u0026#34;origin\u0026#34;); struct NamedPoint* upperRight = makeNamedPoint(1.0, 1.0, \u0026#34;upperRight\u0026#34;); printf( \u0026#34;distance=%f\\n\u0026#34;, distance( (struct Point*) origin, // namedPoint를 point로 쓰고있음 (struct Point*) upperRight // namedPoint를 point로 쓰고있음 ) ); } main 프로그램을 살펴보면 NamedPoint 데이터 구조가 Point 데이터 구조로부터 파생된 구조인 것 처럼 동작한다는 사실을 볼 수 있다.\n이처럼 눈속임처럼 보이는 방식은 OO가 출현하기 이전부터 프로그래머가 흔히 사용했던 기법이다. 실제로 C++은 이 방법을 이용해 단일 상속을 구현하였다.\nOO 언어가 고안되기 훨씬 이전에도 상속과 비슷한 기법이 사용되었지만, 상속만큼 편리한 방식은 절대 아니다.\n따라서 OO 언어가 완전히 새로운 개념을 만들지는 못했지만, 상당히 편리한 방식으로 제공했다고 볼 수 있다.\n다형성? OO 언어가 있기 이전에도 다형성을 표현할 수 있었다.\n1 2 3 4 5 6 #include \u0026lt;stdio.h\u0026gt; void copy() { int c; while ((c = getchar()) != EOF) putchar(c); getchar()는 STDIN에서 문자를 읽고, putchar()는 STDOUT으로 문자를 쓴다.\nSTDIN과 STDOUT은 다양한 장치가 될 수 있기 때문에 이러한 함수는 다형적(Polymorphic)이다.\n유닉스 운영체제의 경우 모든 입출력 장치 드라이버가 다섯 가지 표준 함수를 제공할 것을 요구한다\nopen, close, read, write, seek FILE 데이터 구조는 이들 다섯 함수를 가리키는 포인터들을 포함한다.\n1 2 3 4 5 6 7 struct FILE { void (*open)(char* name, int mode); void (*close)(); int (*read)(); void (*write)(char); void (*seek)(long index, int mode); }; 입출력 드라이버에서는 이들 함수를 아래와 같이 전의하며, FILE 데이터 구조를 함수에 대한 주소와 함께 로드한다.\n1 2 3 4 5 6 7 8 9 #include \u0026#34;file.h\u0026#34; void open(char* name) {/* ... */} void close() {/* ... */} int read() {int c; /* ... */ return c;} void write(char c) {/* ... */} void seek(long index, int mode) {/* ... */} struct FILE console = {open, close, read, write, seek}; STDIN을 FILE*로 선언하면 콘솔 데이터 구조를 가리키므로, getchar()는 아래와 같은 방식으로 구현할 수 있다.\n1 2 3 4 5 extern struct FILE* STDIN; int getchar() { return STDIN-\u0026gt;read(); } 다시말해 getchar()는 STDIN으로 참초되는 FILE 데이터 구조의 read 포인터가 가르키는 함수를 단순히 호출할 뿐이다.\n이처럼 단순한 기법이 모든 OO가 지는 다형성이 근간이 되며, 즉 함수를 가리키는 포인터를 응용한 것이 다형성이고 OO가 새롭게 만든 개념이 아니다.\n하지만, OO 언어는 좀 더 안전하고 편리하게 사용할 수 있게 해준다.\n함수 포인터를 사용하기 위한 관례들을 없애 실수할 위험이 없다. OO는 제어흐름을 간접적으로 전환하는 규칙을 부과한다고 결론 지을 수 있다. (??)\n다형성이 가진 힘 복사 프로그램 예제에서 새로운 입출력 장치가 생겨도 프로그램의 아무런 변경이 필요하지 않다.\n복사 프로그램의 소스 코드는 입출력 드라이버의 소스 코드에 의존하지 않기 때문이다. 입출력 드라이버가 FILE에 정의된 다섯가지 표준 함수를 구현한다면, 복사 프로그램에서는 어떠한 입출력 드라이버도 사용할 수 있다. 플러그인 아키텍처(Plugin architecture)\n플러그인 아키텍처는 이처럼 입출력 장치 독립성을 지원하기 위해 만들어졌고, 등장 이후 거의 모든 운영체제에서 구현되었다.\n그런데도 대다수의 프로그래머는 함수 포인터의 위험함 때문에 이러한 개념을 확장하여 적용하지 않았는데, OO의 등장으로 언제 어디서든 플러그인 아키텍처를 적용할 수 있게 되었다.\n의존성 역전 다형성을 안전하고 편리하게 적용할 수 있기 전에는 main 함수가 고수준 함수를 호출하고, 고수준 함수는 다시 중간 수준 함수를, 중간 수준 함수는 저수준 함수를 호출한다.\n이로인해 소스 코드 의존성의 방향은 반드시 제어흐름을 따르게 된다.\n하지만 다형성을 활용하면 의존성의 방향이 바뀐다.\nHL1 모듈은 ML1 모듈의 F() 함수를 호출한다. 하지만 ML1과 I 인터페이스 사이의 소스 코드 의존성(상속 관계)이 제어흐름과는 반대이다.\n이는 의존성 역전(Dependency inversion)이라고 부르며, 소프트웨어 아키텍처 관점에서 이러한 현상은 소스 코드 의존성을 어디에서든 역전시킬 수 있다는 뜻이기도 하다.\n이러한 접근법을 사용한다면, OO 언어로 개발된 시스템을 다루는 소프트웨어 아키텍트는 시스템의 소스 코드 의존성 전부에 대한 방향을 결정할 수 있는 절대적인 권한을 가지게 된다.\n소스 코드 의존성이 제어흐름의 방향과 일치되도록 제한되지 않는다. 호출하는 모듈이든 호출 받는 모듈이든 관계없이 소스 코드 의존성을 원하는 방향으로 설정할 수 있다. flowchart LR a[UI] b[Business Rules] c[Database] a--\u003eb b-.-\u003ea c--\u003eb b-.-\u003ec 위 와 같은 경우 업무 규칙이 데이터베이스와 UI에 의존하는 대신에, 시스템의 소스코드 의존성을 반대로 배치하여 데이터베이스와 UI가 업무 규칙에 의존하게 만들 수 있다.\nUI와 데이터베이스가 업무 규칙의 플러그인이 된다는 뜻이다. 업무 규칙의 소스 코드에서는 UI나 데이터베이스를 호출하지 않는다. 결과적으로 업무 규칙, UI, 데이터베이스는 세 가지로 분리된 컴포넌트 또는 배포 가능한 단위로 컴파일할 수 있고, 의존하지 않기 때문에 각 컴포넌트들을 독립적으로 배포할 수 있다.\n특정 컴포넌트의 소스 코드가 변경되면, 해당 코드가 포함된 컴포넌트만 다시 배포한다. 이를 배포 독립성(Independent deployability)이라고 한다. 시스템의 모듈을 독립적으로 배포할 수 있게 되면, 다른 팀에서 각 모듈을 독립적으로 개발할 수 있고, 이것을 개발 독립성 이라 한다. 결론 소프트웨어 아키텍트 관점에서 OO란 다형성을 이용하여 전체 시스템의 모든 소스코드 의존성에 대한 절대적인 제어 권한을 획득할 수 있는 능력이다.\nOO를 사용하면 아키텍트는 플러그인 아키텍처를 구성할 수 있고, 이를 통해 고수준의 전책을 포함하는 모듈은 저수순의 세부사항을 포함하는 모듈에 대해 독립성을 보장할 수 있다.\n저수준의 세부사항은 중요도가 낮은 플러그인 모듈로 만들 수 있고, 고수준의 정책을 포함하는 모듈과는 독립적으로 개발하고 배포할 수 있다.\n","date":"2024-02-25T17:46:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/5/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/5/","title":"5. 객체 지향 프로그래밍"},{"content":"구조적 프로그래밍(Structured Programming)은 프로그래밍의 한 접근 방법으로, 프로그램을 작성할 때 제어 흐름을 구조화하여 코드를 보다 이해하기 쉽고 유지 보수하기 쉽게 만들려는 것을 중점으로한다.\n구조적 프로그래밍은 프로그래밍 분야에서 중요한 발전 중 하나로, 초기 프로그래밍 방법론이 가진 한계(복잡성, 프로그래밍 실수, 소프트웨어 신뢰성 등)를 극복하고자 탄생하였다.\ngoto문의 해로움 구조적 프로그래밍을 발견한 데이크스트라는 프로그래밍은 어렵고, 프로그래머는 프로그래밍을 잘하지 못한다고 생각하였는제 아래와 같은 이유가 원인이라고 생각했다.\n모든 프로그램은 단순할지라도 너무 많은 세부사항을 담고 있었다. 아주 작은 세부사항이라도 간과하면 예상 외의 방식으로 실패하곤 했다. 데이크스트라는 증명이라는 수학적인 원리를 적용하여 이러한 문제를 해결하려고자 했고, 수학자가 유클리드 계층구조를 사용하여 증명하는 방식을 프로그래머도 사용할 수 있다고 믿었다.\n유클리드 계층구조\n공리(axiom)는 증명 없이 참으로 받아들이는 명제를 뜻한다.\n정리(theorem)는 증명이라는 과정을 통해 참이라는 것이 밝혀진 명제다.\n이러한 정리를 증명하는 데 필요한 정리를 보조정리(lemma)로 부르며, 정리를 통해 자연스럽게 도출되는 정리를 따름정리(corollary)라고 부른다.\n이를 위한 연구를 진행하면서 합리적인 증명을 위한 분할 정복 접근법을 적용하려 했는데 goto 문장을 이용한 제어 흐름 전환이 모듈을 더 작은 단위로 재귀적으로 분해하는 과정에 방해가 된다는 것을 발견하였다.\n반면 goto 문장을 사용하더라도 모듈을 분해할 때 문제가 되지 않는 경우는 if/then/else와 do/while 같은 분기와 반복이라는 단순한 제어 구조에 해당한다는 사실을 발견했다.\n다익스트라는 제어 구조가 순차 실행(Sequential execution)과 결합해야 한다고 깨닳았다.\nStructured program theorem Böhm–Jacopini 정리라고도 불리는 구조적 프로그래밍 정리는 프로그램을 작성하는 데 사용되는 모든 제어 구조를 표한할 수 있는 최소한의 구조가 있음을 보여준다.\n순차(Sequence) 프로그램의 각 문장들은 순차적으로 실행 어떤 작업을 차례대로 수행하는 것을 의미함 분기(Selection) 조건문을 사용하여 프로그램의 흐름을 분기 조건에 따라 다른 명령문 또는 블록을 실행 일반적으로 if-else문이나 switch-case문을 사용 반복(Iteration) 루프를 사용하여 특정 조건이 충족될 때까지 특정한 명령문 또는 블록을 반복하여 실행 대표적으로는 for루프나 while루프 이러한 제어 구조들이 모든 프로그램을 작성하는 데 필요한 최소한의 구조라는 것을 보여주며, 즉 어떤 프로그램이라도 세 가지 구조로 구현할 수 있다는 것을 의미한다.\n기능적 분해 구조적 프로그래밍을 통해 모듈을 증명 가능한 더 작은 단위로 분해할 수 있게 되었다.\n이는 결국 모듈을 기능적으로 분해할 수 있음을 뜻하며, 거대한 문제 기술서를 받더라도 문제를 고수준의 기능들로 분해할 수 있다. 분해한 기능들은 구조적 프로그래밍의 제한된 제어구조를 이용하여 표현할 수 있다. 이를 토대로 구조적 분석이나 구조적 설계와 같은 기법이 인기를 끌었으며, 개선되어 널리 알려졌다.\n대규모 시스템을 모듈과 컴포넌트로 나눌 수 있다. 모듈과 컴포넌트는 입증할 수 있는 작은 기능들로 세분화할 수 있다. 과학적 방법(Scientific Method) 테스트는 버그가 있음을 보여줄 뿐, 버그가 없음을 보여줄수는 없다.\n소프트웨어 개발이 수학적인 구조를 다루는 듯 보이더라도, 소프트웨어 개발은 수학적인 시도가 아니다.\n오히려 올바르지 않음을 증명하는 데 실패함으로 올바름을 보여주는 과학과 같다.\n이러한 부정확함에 대한 증명은 입증 가능한 프로그램에만 적용할 수 있으며, 구조적 프로그래밍은 프로그램을 증명 가능한 세부 기능 집합으로 재귀적으로 분해할 것을 강요한다.\n그러고 나서 테스트를 통해 증명 가능한 세부 기능들이 거짓인지를 증명하려고 시도하며, 거짓임을 증명하는 테스트가 실패한다면, 충분히 참이라고 여기게 된다.\n현대적 가치 구조적 프로그래밍에 대한 논의는 많은 새로운 언어를 낳았으며, 기존의 언어에 구조적인 면이 추가되는 등 언어의 발전에 도움이 되었다. 그리고 이후에 나온 프로그래밍 패러다임들에도 영향을 끼쳤다.\n구조적 프로그래밍은 프로그래머의 습관을 바꾸었다.\n프로그램의 정확성을 증명하는 문제를 떠나서 데이크스트라가 그의 논문에서 말한 대로 시간에 따라 변하는 동적인 과정을 시각화하는 것은 인간에게 매우 어려운 일이다.\n꼭 GOTO문만의 문제가 아니라 구조화된 흐름 제어문을 사용한다고 할지라도 너무 복잡하게 중첩되어 있거나 스코프의 길이가 너무 긴 코드를 작성한다거나 너무 긴 길이의 하위프로그램을 작성하는 일을 가급적 피하게 경향이 생겼다.\n그리고 이런 습관은 다른 사람이 작성한 프로그래밍 코드를 쉽게 이해하는 데 도움을 준다.\n결론 구조적 프로그래밍이 가치있는 이유는 프로그래밍에서 반증 가능한 단위를 만들어 낼 수 있는 능력 때문이다. 가작 작은 기능에서 부터 가장 큰 컴포넌트에 이르기까지 모든 수준에서 소프트웨어는 과학과 같고, 반증 가능성에 의해 주도된다. 스프트웨어 아키텍트는 모듈, 컴포넌트, 서비스가 쉽게 반증 가능하도록(테스트하기 쉽도록) 만들기위해 노력해야 한다. 구조적 프로그래밍과 유사한 제한적인 규칙들을 통해 쉽게 반증 가능한 구조를 만들 수 있다. ","date":"2024-02-20T20:33:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/4/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/4/","title":"4. 구조적 프로그래밍"},{"content":"구조적 프로그래밍 구조적 프로그래밍은 제어흐름의 직접적인 전환에 대해 규칙을 부과한다.\n최초로 적용된 패러다임(최초로 만들어진 패러다임은 아님)으로 1968년 에츠허르 비버 데이크스트라가 발견했다.\n무분별한 점프(goto 문장)는 프로그램 구조에 해롭다는 사실을 제시함 이러한 점프들을 if/then/else, do/while/until과 같이 더 익숙한 구조로 대체함 객체 지향 프로그래밍 객체 지향 프로그래밍은 제어흐름의 간접적인 전환에 대해 규칙을 부과한다.\n두 번째로 도입된 패러다음으로 구조적 프로그래밍보다 2년 앞선 1966년 올레 요한 달과 크리스텐 니가드에 의해 등장했다.\n알골(ALGOL) 언어의 함수 호출 스택 프레임을 힙으로 옮기면, 함수 호출이 반환된 이후에도 함수에서 선언된 지역 변수가 오랫동안 유지될 수 있음을 발견했다.\n이러한 함수가 클래스의 생성자가 되었고, 지역 변수는 인스턴스 변수, 중첩 함수는 메서드가 되었다. 함수 포인터를 특정 규칙에 따라 사용하는 과정을 통해 필연적으로 다형성이 등장하게 되었다. 함수형 프로그래밍 함수형 프로그래밍은 할당문에 대해 규칙을 부과한다.\n최근에 들어서야 겨우 도입되기 시작했지만 가장 먼저 만들어졌다.\n알론조 처치는 앨런 튜링도 똑같이 흥미를 느꼈던 수학적 문제를 해결하는 과정에서 람다(Lambda) 계산법을 발명했는데, 함수형 프로그래밍은 이러한 연구 결과에 직접적인 영향을 받아 만들어졌다.\n1958년에 존 매카시가 만든 LISP 언어에 근간이 되는 개념이 바로 이 람다 계산법이다.\n람다 계산법의 기초가 되는 개념은 불변성으로, 심볼의 값이 변경되지 않는다는 개념이다. 이는 함수형 언어에는 할당문이 전혀 없다는 뜻이기도 하다. 대다수의 함수형 언어가 변수값을 변경할 수 있는 방법을 제공하기는 하지만, 굉장히 까다로운 조건 아래서만 가능하다. 생각할 거리 각 패러다임은 프로그래머에게서 권한을 박탈한다. 어느 패러다임도 새로운 권한을 부여하지 않는다.\n각 패러다임은 추가적인 규칙을 통해 특정 행동들을 하지 못하게 제약한다. 패러다임은 무엇을 해야 할지를 말하기보다는 무엇을 해서는 안되는지를 말해준다. 각 패러다임이 우리에게서 무언가를 빼앗는다는 사실을 인지하는 것이다.\n구조적 프로그래밍: goto문 객체 지향 프로그래밍: 함수 포인터 함수형 프로그래밍: 할당문 이 외에 개발자에게 가져갈 수 있는 것이 남아있지 않으므로, 프로그래밍 패러다임은 앞으로도 세 가지 밖에 없을 것이다.\n패러다임이 10년 동안 모두 만들어진 이후 수십 년이 지났지만 새롭게 등장한 패러다임은 전혀 없다. 결론 패러다임의 역사로부터 얻을 수 있는 이러한 교훈은 아키텍처와 큰 관계가 있다.\n아키텍처 경계를 넘나들기 위한 메커니즘으로 다형성을 이용한다. 함수형 프로그래밍을 이용하여 데이터의 위치와 접근 방법에 대해 규칙을 부과한다. 모듈 기반 알고리즘으로 구조적 프로그래밍을 사용한다. 세 가지 패러다임과 아키텍처의 세 가지 큰 관심사(함수, 컴포넌트 분리, 데이터 관리)가 어떻게 서로 연관되는지에 주목하자.\n","date":"2024-02-18T19:54:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/3/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/3/","title":"3. 패러다임 개요"},{"content":"모든 소프트웨어 시스템은 이해관계자에게 행위(Behavior)와 구조(Structure)라는 두가지 가치를 제공한다.\n따라서 소프트웨어 개발자는 두 가치를 모두 반드시 높게 유지해야 하는 책임을 진다.\n하지만 한 가지 가치에만 집중하고 나머지 가치는 배제하곤 하며, 대체로 덜 중요한 가치에 집중하여 결국에는 소프트웨어 시스템이 쓸모 없게 만들어버린다.\n행위(기능) 프로그래머를 고용하는 이유는 이해관계자를 위해 기계가 수익을 창출하거나 비용을 절약하도록 만들기 위해서이다.\n이해관계자가 기능 명세서나 요구사항 문서를 구체화할 수 있도록 돕는다. 이해관계자의 기계가 이러한 요구사항을 만족하도록 코드를 작성한다. 요구사항을 위반하면 문제를 고친다. 행위는 개발자가 구현해야하는 기능을 의미하며, 기능을 구현하고 만들어진 기능을 운영하는 것만을 개발자의 역할이라고 생각한다.\n아키텍처 소프트웨어라는 단어는 부드러운(soft)과 제품(ware)의 합성어이다.\n소프트웨어는 부드러움을 지니도록 만들어졌으며, 소프트웨어를 만든 이유는 기계의 행위를 쉽게 변경할 수 있도록 하기 위해서이다.\n따라서 소프트웨어가 가진 본연의 목적을 추구하려면 소프트웨어는 반드시 부드러움, 즉 변경이 쉬워야하며 이해관계자가 기능에 대한 생각을 바꾸면 이러한 변경사항을 간단하고 쉽게 적용할 수 있어야 한다.\n변경사항을 적용하는 데 드는 어려움은 변경되는 범위에 비례해야하며, 변경사항의 형태와는 관련이 없어야 한다.\n소프트웨어 개발 비용의 증가를 결정짓는 주된 요인은 바로 변경사항의 범위와 형태의 차이에 있다.\n이해관계자는 범위가 비슷한 일련의 변경사항을 제시할 뿐이지만, 개발자 입장에서는 복잡도가 지속적으로 증가하는 퍼즐 판 위에서 이해관계자가 계속해서 퍼즐 조각을 맞추라는 지시를 하는 것처럼 느낀다.\n이는 시스템의 형태와 요구사항의 형태가 서로 맞지 않기 때문인데, 원인은 소프트웨어 아키텍처다.\n아키텍처가 특정 형태를 다른 형태보다 선호하면 할수록, 새로운 기능을 이 구조에 맞추는 게 더 힘들어진다.\n따라서 아키텍처는 항상 형태에 독립적이어야하고, 그럴수록 더 실용적이다.\n더 높은 가치 기능과 아키텍처 둘 중 어느 것의 가치가 더 높은지 업무 관리자에게 묻는다면, 대다수가 소프트웨어 시스템이 동작하는 것이 더 중요하다고 대답하지만, 개발자는 아키텍처에 더 가치를 둬야한다.\n양 극단 사례 검토\n완벽하게 동작하지만 수정이 불가능한 프로그램은 요구사항이 변경될 때 동작하지 않게 되어 쓸모가 없다. 동작은 하지 않니만 변경이 쉬운 프로그램은 개발자가 돌아가도록 만들 수 있고, 변경사항이 발생하더라도 여전히 동작하여 유용한채로 남는다. 변경에 드는 비용이 변경으로 창출되는 수익을 초과하는 경우 수정이 현실적으로 불가능하며, 이로 인해 기능 또는 설정 측면에서 만은 시스템이 현실적으로 수정할 수 없는 상황에 빠진다.\n현재의 기능 동작을 위해 미래의 유연성을 희생한다면, 변경에 드는 비용이 높아지게되어 현실적으로 수정할 수 없는 상황에 빠지게되고, 결과적으로 책임은 개발자에게 돌아간다.\n아이젠하워 매트릭스 긴급한 문제가 아주 중요한 문제일 경우는 드물고, 중요한 문제가 몹시 긴급한 경우는 거의 없다.\n긴급 O, 중요 O 긴급 X, 중요 O 긴급 O, 중요 X 긴급 X, 중요 X 아이젠하워 매트릭스에서는 위와 같은 우선순위로 문제를 해결할 것을 제안하고 있다.\n첫 번째 가치인 행위는 대부분 긴급하지만 매번 높은 중요도를 가지는 것은 아니며, 두 번째 가치인 아키텍처는 중요하지만 즉각적인 긴급성을 필요로 하는 경우는 절대 없다.\n아키텍처는 1, 2를 차지하는 반변, 행위는 1, 3에 위치한다.\n많은 업무 관리자와 개발자가 3번에 위치한 항목을 1번으로 격상시키는 실수를 많이 한다.\n긴급하지만 중요하지 않은 기능과 진짜로 긴급하면서 주용한 기능을 구분하지 못한다. 이러한 실패로 중요도가 높은 아키텍처를 무시한 채 중요도가 떨어지는 기능을 선택하게 된다.\n아키텍처를 위해 투쟁하라 더 중요한 가치인 아키텍처가 더 낮은 우선순위를 가지게 되는 이유는 대부분의 업무 관리자가 아키텍처의 중요성을 평가하지 못하기 때문이다.\n따라서 개발자, 개발팀은 다른 이해관계자들을 설득해야 할 의무가 있다.\n기능의 긴급성이 아닌 아키텍처의 중요성을 설득하는 일은 소프트웨어 개발팀이 책임을 져야한다. 소프트웨어 개발자도 이해관계자이며, 소프트웨어를 안전하게 보호해야 할 책임이 있다. 소프트웨어 아키텍트는 시스템이 제공하는 특성이나 기능보다는 시스템의 구조에 더 중점을 둔다. 아키텍트는 이러한 특성과 기능을 개발하기 쉽고, 간편하게 수정할 수 있으며, 확장하기 쉬운 아키텍처를 만들어야 한다.\n개발자가 아키텍처에 더 높은 우선순위를 둘 수 있도록 이해관계자들과 투쟁하는 것은 장기적인 관점에서 더 나은 소프트웨어를 만들 수 있는 가능성을 높힌다.\n아키텍처가 후순위가 되면 시스템을 개발하는 비용이 점점 더 많이 들게되고, 결국 일부 또는 전체 시스템에 변경을 가하는 일이 현실적으로 힘들어진다.\n이러한 상황이 발생하도록 용납했다면, 이는 결국 소프트웨어 개발팀이 스스로 옳다고 믿는 가치를 위해 충분히 투쟁하지 않았다는 뜻이다.\n","date":"2024-02-18T19:03:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/2/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/2/","title":"2. 두 가지 가치에 대한 이야기"},{"content":"설계와 아키텍처의 차이 설계(Design)와 아키텍처(Architecture)의 정의가 모호하여 오랫동안 많은 혼란이 있었지만 실제로는 둘의 차이는 없다.\n아키텍처는 저수준 세부사항과는 분리된 고수준의 무언가를 가릴킬 때 흔히 사용된다. 설계는 저수준의 구조 또는 결정사항 등을 의미할 때가 많다. 하지만 아키텍트가 실제로 하는 일을 살펴보면 이러한 구분은 무의미하다.\n새로운 집 새로운 집을 설계하는 아키텍트가 있다면 이 집의 아키텍처는 형태, 외관, 입면도, 공간이나 방의 배치등이 포함된다.\n하지만 아키텍트가 만든 도면을 살펴보면 콘센트, 전등 스위치, 전등이 모두 어디에 위치하는 지 등 세부사항도 모두 확인할 수 있으며, 벽, 지붕 기초 공사등이 어떻게 진행될지도 상세히 확인할 수 있다.\n이처럼 모든 고수준의 결정사항을 지탱하는 모든 세부사항과 고수준의 결정사항은 집의 전체 설계의 구성요소가 된다.\n소프트웨어 설계도 마찬가지로, 저수준의 세부사항과 고수준의 구조는 모두 소프트웨어 전체 설계의 구성요소다.\n저수준 세부사항과 고수준 세부사항은 단절 없이 이어진 직물과 같으며, 이를 통해 대상 시스템의 구조를 정의한다.\n이 둘은 개별로 존재할 수 없으며, 경계 또한 뚜렸하지 않고 고수준에서 저수준으로 향하는 의사결정의 연속성만이 있을 뿐이다.\n목표는? 이러한 의사결정, 좋은 소프트웨어 설계 즉 소프트웨어 아키텍처의 목표는 필요한 시스템을 만드록 유지보수하는데 투입되는 인력을 최소화하는 데 있다.\n설계 품질의 척도는 고객의 요구를 만족시키는 데 드는 비용 척도와 다름 없다.\n비용이 낮을 뿐만 아니라 시스템의 수명이 다할 때까지 낮게 유지할 수 있다면 좋은 설계라고 말할 수 있다. 새로운 기능을 출시할 때 마다 비용이 증가한다면 나쁜 설계다. 좋은 설계가 필요한 이유 소프트웨어 아키텍처가 나쁘다면 소프트웨어가 진화함에 따라 점점 비용이 증가한다. (생산성이 떨어진다.)\n이러한 비용의 상승은 사업 모델의 수익을 고갈시킨다. 회사의 성장을 멈추게 하거나 심지어는 완전히 망하게 만든다. 시스템을 급하게 만들거나, 결과물의 총량을 순전히 프로그래머 수만으로 결정하거나, 코드와 설계와 구조를 깔끔하게 만들려는 생각을 전혀 하지 않는다면, 시간이 지남에 따라 비용이 급격히 상승하게되고, 이를 통해 생산성이 바닥을 치게 된다.\n이러한 현상이 발생하게 되면 개발자가 기능 개발보다는 엉망이 된 상황에 대처하는 데 소모되기 시작하며, 개발자들이 쏟은 노력의 가치를 보잘것없게 만든다.\n무엇이 잘못 되었나? 생산성을 유지할 수 있다는 착각\n일부 개발자들은 생산성을 유지할 수 있다고 자신의 능력을 과신한다.(언제든지 돌아가 생산성을 회복시킬 수 있다고 생각한다.)\n현대의 개발자들은 빠른 시장 출시가 경쟁자보다 앞서 가는 것이라 생각하며 \u0026ldquo;코드는 나중에 정리하면 돼. 당장은 시장에 출시하는 게 먼저야!\u0026ldquo;라고 스스로를 속인다.\n하지만 시장의 압박은 절대 수그러들지 않기 때문에 태세를 전환하지 않고 정리하는 일은 매우 드물게 되며, 이로 인해 휼륭하고 깔끔하게 잘 설계된 코드와 점점 더 거리가 멀어지게 된다.\n이러한 상황에서 계속해서 새로운 기능들이 추가가 되어 결국 엉망진창이 되고, 생산성이 0으로 수렴하기 시작한다.\n지저분한 코드를 작성하면 단기간에 빠르게 갈 수 있다는 착각\n지저분한 코드를 작성하면 단 기간에는 빠르게 갈 수 있고, 장기적으로 볼 때만 생산성이 낮아진다는 견해는 엉망으로 코드를 짜기위한 자기합리화이며, 진실은 엉망으로 만들면 깔끔하게 유지할 때보다 항상 더 느리다.\n제이슨 고먼은 코드를 깔끔하게 유지하는 잘 알려진 방법 중 하나인 TDD를 사용 여부로 생산성을 측정했다.\nTDD를 적용했을때가 훨씬 더 빨랐으며, 심지어 TDD를 적용한 가장 느렸던 날이 적용하지 않은 가장 빨리 작업한 날보다 더 빨랐다.\n빨리 가는 유일한 방법은 제대로 가는 것이다.\n생산성이 감소되고 비용이 증가하는 현상을 되돌릴 수 있는 유일한 방법은 없다.\n이러한 문제를 해결하기 위해 처음부터 다시 시작하더라도 한번 문제를 발생시킨 개발자(팀)는 똑같은 문제를 반복하는 경우가 많으며, 이 때문에 항상 코드와 설계를 깔끔하게 만들려는 노력을 지속해야한다.\n결론 어떤 경우라도 개발 조직이 할 수 있는 최고의 선택지는 조직에 스며든 과신을 인지하여 방지하고, 소프트웨어 아키텍처의 품질을 심각하게 고민하기 시작하는 것이다.\n소프트웨어 아키텍처를 심각하게 고려할 수 있으려면 좋은 소프트웨어 아키텍처가 무엇인지 이해해야 한다.\n비용은 최소화하고 생산성은 최대화 할 수 있는 설계와 아키텍처를 가진 시스템을 만드려면, 시스템 아키텍처가 지닌 속성을 알고 있어야 한다. 이 책은 훌륭하고 깔끔한 아키텍처와 설계가 무엇인지 설명하고, 이를 통해 소프트웨어 개발자가 장시간에 걸쳐 수익을 창출하는 시스템을 만들 수 있게 하고자 한다.\n","date":"2024-02-18T18:09:13+09:00","image":"https://codemario318.github.io/post/clean-architecture/1/cover_hu01783d23203014d58ee585db2e3121c4_1011761_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/clean-architecture/1/","title":"1. 설계와 아키텍처란?"},{"content":"비지터(Visitor) 패턴 다양한 객체에 새로운 기능을 추가해야 하는데 캡슐화가 별로 중요하지 않다면 비지터 패턴을 쓴다.\n비지터 패턴은 객체 구조를 정의하고, 이 구조를 기반으로 알고리즘을 캡슐화하여 새로운 알고리즘을 추가할 때 기존 코드를 수정하지 않고 확장하는 디자인 패턴 중 하나이다.\n주로 데이터 구조와 처리를 분리하여 새로운 처리를 쉽게 추가하고 확장할 수 있도록 한다.\n특징 알고리즘 분리 알고리즘을 객체 구조에서 분리하여 캡슐화하고, 각 알고리즘을 방문자(Visitor)로 정의한다. 확장성 향상 새로운 알고리즘을 추가할 때 기존 코드를 수정하지 않고 확장할 수 있다. 구조 Visitor(방문자) 객체 구조를 방문하여 각 구조에 대한 알고리즘을 수행하는 인터페이스를 정의한다. ConcreteVisitor Visitor 인터페이스를 구현하고, 실제 알고리즘을 구현한다. Element(요소) 방문자가 방문하는 객체의 언터페이스를 정의한다. ConcreteElement Element 인터페이스를 구현하고, 방문자의 방문을 받아들이는 메서드를 구현한다. ObjectStructure(객체 구조) 여러 요소를 포함하는 객체 구조를 정의하고, 방문자를 받아들이는 메서드를 제공한다. 예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 // Visitor public interface Visitor { void visit(ConcreteElementA element); void visit(ConcreteElementB element); } // ConcreteVisitor public class ConcreteVisitor implements Visitor { @Override public void visit(ConcreteElementA element) { System.out.println(\u0026#34;Visit ConcreteElementA\u0026#34;); } @Override public void visit(ConcreteElementB element) { System.out.println(\u0026#34;Visit ConcreteElementB\u0026#34;); } } // Element public interface Element { void accept(Visitor visitor); } // ConcreteElement public class ConcreteElementA implements Element { @Override public void accept(Visitor visitor) { visitor.visit(this); } } // ConcreteElement public class ConcreteElementB implements Element { @Override public void accept(Visitor visitor) { visitor.visit(this); } } // ObjectStructure public class ObjectStructure { private List\u0026lt;Element\u0026gt; elements = new ArrayList\u0026lt;\u0026gt;(); public void addElement(Element element) { elements.add(element); } public void accept(Visitor visitor) { for (Element element : elements) { element.accept(visitor); } } } // Client public class Client { public static void main(String[] args) { ObjectStructure objectStructure = new ObjectStructure(); objectStructure.addElement(new ConcreteElementA()); objectStructure.addElement(new ConcreteElementB()); Visitor visitor = new ConcreteVisitor(); objectStructure.accept(visitor); } } 장단점 장점\n알고리즘 추가의 용이성 새로운 알고리즘을 추가하거나 기존 알고리즘을 수정할 때, 기존 코드를 건드리지 않고 확장할 수 있다. 비교적 손쉽게 새로운 기능을 추가할 수 있다. 구조와 알고리즘의 분리 비지터가 수행하는 기능과 관련된 코드를 한곳에 모아 둘 수 있다. 객체 구조와 알고리즘을 분리하여 코드를 보다 모듈화하고 유지보수성을 높일 수 있다. 단점\n캡슐화 불가능 비지터를 사용하면 복합 클래스의 캡슐화가 깨진다. 구조 변경 어려움 컬렉션 내의 모든 항목에 접근하는 트래버서(클라이언트)가 있으므로 복합 구조를 변경하기가 더 어려워진다. 클래스 증가 새로운 요소나 알고리즘을 추가할 때마다 클래스의 수가 늘어난다. 이로 인해 클래스의 수가 많아지면 유지보수가 어려워질 수 있다. ","date":"2024-01-27T09:36:08+09:00","image":"https://codemario318.github.io/post/gof/15/visitor/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/15/visitor/","title":"14. 다양한 패턴 빠르게 알아보기 - 비지터 패턴"},{"content":"프로토타입(Prototype) 패턴 어떤 클래스의 인스턴스를 만들 때 자원과 시간이 많이 들거나 복잡하다면 프로토타입 패턴을 사용한다.\n프로토타입 패턴은 객체를 생성하는 비용이 큰 경우, 기존 객체를 복사하여 새로운 객체를 생성하는 디자인 패턴 중 하나이다.\n이는 객체를 생성하는 비용을 줄이고, 복잡한 초기화 과정을 반복하지 않고도 객체를 생성할 수 있게한다.\n특징 시스템에서 복잡한 클래스 계층구조에 파묻혀 있는 다양한 형식의 객체 인스턴스를 새로 만들어야 할 때 유용하게 써먹을 수 있다.\n복사를 통한 객체 생성 기존 객체를 복사하여 새로운 객체를 생성한다. 생성 비용 감소 비용이 큰 객체 생성을 피하고, 복사를 통해 새로운 객체를 빠르게 생성한다. 구조 Prototype(프로토타입) 복사될 객체에 대한 인터페이스를 정의한다. ConcretePrototype(구체적인 프로토타입) Prototype 인터페이스를 구현하고, 자신을 복사하여 새로운 객체를 생성하는 메서드를 구현한다. Client 프로토타입 객체를 복사하여 새로운 객체를 생성한다. 예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // Prototype public interface Prototype { Prototype clone(); } // ConcretePrototype public class ConcretePrototype implements Prototype { private String data; public ConcretePrototype(String data) { this.data = data; } public void setData(String data) { this.data = data; } public String getData() { return data; } @Override public Prototype clone() { return new ConcretePrototype(this.data); } } // Client public class Client { public static void main(String[] args) { ConcretePrototype prototype = new ConcretePrototype(\u0026#34;Original Data\u0026#34;); // 복사하여 새로운 객체 생성 ConcretePrototype clonedObject = (ConcretePrototype) prototype.clone(); // 원본과 복제본의 데이터 확인 System.out.println(\u0026#34;Original Data: \u0026#34; + prototype.getData()); System.out.println(\u0026#34;Cloned Data: \u0026#34; + clonedObject.getData()); // 복제본의 데이터 변경 clonedObject.setData(\u0026#34;Modified Data\u0026#34;); // 원본과 복제본의 데이터 확인 System.out.println(\u0026#34;Original Data: \u0026#34; + prototype.getData()); System.out.println(\u0026#34;Cloned Data: \u0026#34; + clonedObject.getData()); } } 장단점 장점\n생성 비용 감소 비용이 큰 객체 생성을 피하고, 복사를 통해 빠르게 새로운 객체를 생성할 수 있다. 유연성 증가 객체의 생성 및 초기화 과정을 반복하지 않고, 복사를 통해 새로운 객체를 생성할 수 있어 유연성이 증가한다. 결합도 감소 클라이언트가 새로운 인스턴스를 만드는 과정을 몰라도 된다. 클라이언트는 구체적인 형식을 몰라도 객체를 생성할 수 있다. 단점\n때로는 객체의 복사본을 만드는 일이 매우 복잡할 수도 있다. 깊은 복사 어려움 객체에 참조 타입 멤버 변수가 있는 경우, 이를 얕은 복사로 처리할 경우 문제가 발생할 수 있다. clone 메서드의 구현 어려움 clone 메서드를 구현하는 것이 쉽지 않을 수 있으며, 실수할 여지가 많다. ","date":"2024-01-27T09:36:08+09:00","image":"https://codemario318.github.io/post/gof/15/prototype/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/15/prototype/","title":"14. 다양한 패턴 빠르게 알아보기 - 프로토타입 패턴"},{"content":"메멘토(Memento) 패턴 객체를 이전 상태로 복구해야 한다면 메멘토(Memento) 패턴을 쓴다. (작업 취소)\n메멘토 패턴은 객체의 상태를 저장하고 이를 나중에 복원할 수 있도록 하는 행위 디자인 패턴 중 하나이다.\n객체의 상태를 외부하시켜, 생태의 변경을 캡슐화하고 객체의 불변성을 보존한다.\n특징 메멘토 객체를 써서 상태를 저장하며, 시스템의 상태를 저장할 때 직렬화를 사용하는 것이 좋다.\n상태 저장 및 복원 객체의 내부 상태를 외부로 저장하고, 나중에 이 상태를 사용하여 객체를 복원한다. 캡슐화 객체의 상태를 외부화시켜, 상태 변경을 캡슐화하여 객체의 불변성을 보존한다. 구조 Originator(원조자) 현재 상태를 저장하고, 상태를 복원하는 객체를 생성한다. Memento(메멘토) 원조자의 내부 상태를 표현하는 객체이다. 원조자의 상태를 저장하고 복원하기 위해 사용된다. Caretaker(관리자) 메멘토를 관리하고, 필요에 따라 메멘토를 저장하거나 복원한다. 예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 // Memento public class Memento { private String state; public Memento(String state) { this.state = state; } public String getState() { return state; } } // Originator public class Originator { private String state; public void setState(String state) { this.state = state; } public String getState() { return state; } public Memento saveStateToMemento() { return new Memento(state); } public void restoreStateFromMemento(Memento memento) { state = memento.getState(); } } // Caretaker public class Caretaker { private List\u0026lt;Memento\u0026gt; mementoList = new ArrayList\u0026lt;\u0026gt;(); public void addMemento(Memento memento) { mementoList.add(memento); } public Memento getMemento(int index) { return mementoList.get(index); } } // Client public class Client { public static void main(String[] args) { Originator originator = new Originator(); Caretaker caretaker = new Caretaker(); originator.setState(\u0026#34;State 1\u0026#34;); originator.setState(\u0026#34;State 2\u0026#34;); caretaker.addMemento(originator.saveStateToMemento()); originator.setState(\u0026#34;State 3\u0026#34;); caretaker.addMemento(originator.saveStateToMemento()); System.out.println(\u0026#34;Current State: \u0026#34; + originator.getState()); originator.restoreStateFromMemento(caretaker.getMemento(1)); System.out.println(\u0026#34;Restored State: \u0026#34; + originator.getState()); } } 장단점 장점\n캡슐화된 상태 객체의 상태가 외부에 의해 캡슐화되어 저장되므로, 객체의 내부 상태를 외부로부터 은닉할 수 있다. 저장된 상태를 핵심 객체와는 다른 별도의 객체에 보관할 수 있어 안전하다. 복원 가능성 객체의 상태를 나중에 복원할 수 있으므로, 객체의 상태를 이전 상태로 쉽게 되돌릴 수 있다. 단점\n상태를 저장하고 복구하는 데 시간이 오래 걸릴 수도 있다. 메모리 사용 상태를 저장하기 위한 메멘토 객체를 생성하고관리하는 데 일정한 메모리 비용이 들 수 있다. 복잡성 큰 객체의 상태를 저장하고 복원하는 과정이 복잡해질 수 있다. ","date":"2024-01-27T09:26:08+09:00","image":"https://codemario318.github.io/post/gof/15/memento/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/15/memento/","title":"14. 다양한 패턴 빠르게 알아보기 - 메멘토 패턴"},{"content":"중재자(Mediator) 패턴 서로 관련된 객체 사이의 복잡한 통신과 제어를 한곳으로 집중화고 싶다면 중재자(Mediator) 패턴을 사용한다.\n중재자 패턴은 객체 간의 상호 작용을 캡슐화하여, 객체 간의 직접적인 통신을 방지하고 중재자를 통해 상호 작용을 조정하는 행위 패턴 중 하나이다.\n이를 통해 객체 간의 결합도를 낮추고, 유연성을 높이며, 유지 보수성을 개선하는 데 사용된다.\n특징 서로 연관된 GUI 구성 요소를 관리하는 용도로 많이 쓰인다.\n상호 작용의 중앙화 객체 간의 상호 작용을 중앙화하여 중재자를 통해 조정한다. 결합도 감소 객체 간의 직접적인 통신을 피하고, 중재자를 통해 간접적으로 통신하여 결합도를 낮춘다. 유연성 향상 객체 간의 결합도가 낮아지므로, 변화에 대한 유연성이 향상된다. 구조 Mediator(중재자) 객체 간의 상호 작용을 조정하는 인터페이스를 정의한다. ConcreteMediator(구체적인 중재자) Mediator를 구현하고, 객체 간의 상호 작용을 조정한다. Colleague(협력자) 중재자와 통신하는 객체로, 상호 작용을 중재자를 통해 수행한다. 예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 // Mediator 인터페이스 public interface Mediator { void sendMessage(String message, Colleague colleague); } // ConcreteMediator 구현 public class ConcreteMediator implements Mediator { private Colleague colleague1; private Colleague colleague2; public void setColleague1(Colleague colleague1) { this.colleague1 = colleague1; } public void setColleague2(Colleague colleague2) { this.colleague2 = colleague2; } @Override public void sendMessage(String message, Colleague colleague) { if (colleague == colleague1) { colleague2.receiveMessage(message); } else { colleague1.receiveMessage(message); } } } // Colleague 인터페이스 public interface Colleague { void sendMessage(String message); void receiveMessage(String message); } // ConcreteColleague 구현 public class ConcreteColleague1 implements Colleague { private Mediator mediator; public ConcreteColleague1(Mediator mediator) { this.mediator = mediator; } @Override public void sendMessage(String message) { mediator.sendMessage(message, this); } @Override public void receiveMessage(String message) { System.out.println(\u0026#34;ConcreteColleague1 received message: \u0026#34; + message); } } // ConcreteColleague 구현 public class ConcreteColleague2 implements Colleague { private Mediator mediator; public ConcreteColleague2(Mediator mediator) { this.mediator = mediator; } @Override public void sendMessage(String message) { mediator.sendMessage(message, this); } @Override public void receiveMessage(String message) { System.out.println(\u0026#34;ConcreteColleague2 received message: \u0026#34; + message); } } // Client public class Client { public static void main(String[] args) { ConcreteMediator mediator = new ConcreteMediator(); ConcreteColleague1 colleague1 = new ConcreteColleague1(mediator); ConcreteColleague2 colleague2 = new ConcreteColleague2(mediator); mediator.setColleague1(colleague1); mediator.setColleague2(colleague2); colleague1.sendMessage(\u0026#34;Hello from Colleague1!\u0026#34;); colleague2.sendMessage(\u0026#34;Hi from Colleague2!\u0026#34;); } } 장단점 장점\n결합도 감소 객체 간의 직접적은 통신을 피하고 중재자를 통해 간접적으로 통신함으로써 결합도를 감소기킨다. 재사용성 증가 시스템과 객체를 분리함으로써 재사용성을 획기적으로 향상시킬수 있다. 제어로직을 한 군데 모아놨으므로 관리하기 수월하다. 시스템에 들어있는 객체 사이에서 오가는 메시지를 대폭 줄이고 단순화 할 수 있다. 단점\n복잡성 증가 디자인을 잘 하지 못하면 중재자 객체가 너무 복잡해질 수 있다. 모든 통신이 중재라를 통해 이루어져야 함 모든 객체 간의 통신이 중재자를 통해 이루어져야 하므로, 중재자에게 모든 통신을 전달하는 부하가 발생 가능 ","date":"2024-01-27T09:14:08+09:00","image":"https://codemario318.github.io/post/gof/15/mediator/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/15/mediator/","title":"14. 다양한 패턴 빠르게 알아보기 - 중재자 패턴"},{"content":"인터프리터(Interpreter) 패턴 어떤 언어의 인터프리터를 만들 때는 인터프리터 패턴을 사용한다.\n인터프리터 패턴은 언어나 문법 규칙을 해석하거나 해석해기 위한 디자인 패턴으로, 주로 언어 해석기를 구현하는 데 사용된다.\n주어진 언어의 문법 규칙을 클래스로 표현하고, 이를 해석하여 실행하는 구조를 제공한다.\n특징 간단한 언어를 구현할 때 인터프리터 패턴이 유용하게 쓰인다.\n효율보다는 단순하고 간단하게 문법을 만드는 것이 더 중요한 경우에 유용하다.\n언어 해석 특정 언어의 문법을 객체로 표현하고 이를 해석하여 실행한다. 문법 구조 분리 문법의 구조를 표현식 클래스에 캡슐화하여 각 표현식을 독립적으로 변경 가능하도록 한다. 구성 요소 AbstractExpression(추상 표현식) 인터프리터 패턴의 인터페이스를 정의 문법 규칙에 해당하는 각 표현식이 해당 인터페이스를 구현한다. TerminalExpression(종단 표현식) 구체적인 표현식으로, 더 이상 나뉠 수 없는 최소 단위의 표현식을 나타낸다. NonterminalExpression(비종단 표현식) 여러 종단 표현식을 조합하여 복잡한 표현식을 나타낸다. Context 해석기가 해석할 문맥 정보 제공 Client 구체적인 표현식을 조합하여 컨텍스트에 표현식을 해석하도록 요청하는 역할 수행 예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 // AbstractExpression public interface Expression { int interpret(Context context); } // TerminalExpression public class NumberExpression implements Expression { private int number; public NumberExpression(int number) { this.number = number; } @Override public int interpret(Context context) { return number; } } // NonterminalExpression public class AddExpression implements Expression { private Expression left; private Expression right; public AddExpression(Expression left, Expression right) { this.left = left; this.right = right; } @Override public int interpret(Context context) { return left.interpret(context) + right.interpret(context); } } // Context public class Context { private String input; private int output; public Context(String input) { this.input = input; } public String getInput() { return input; } public void setInput(String input) { this.input = input; } public int getOutput() { return output; } public void setOutput(int output) { this.output = output; } } // Client public class Client { public static void main(String[] args) { // 예시: 1 + 2 Expression expression = new AddExpression(new NumberExpression(1), new NumberExpression(2)); Context context = new Context(\u0026#34;1 + 2\u0026#34;); int result = expression.interpret(context); System.out.println(context.getInput() + \u0026#34; = \u0026#34; + result); } } 장단점 장점\n유연한 확장성 문법을 클래스로 표현해서 쉽게 언어를 구현할 수 있다. 문법이 클래스로 표현되므로 언어를 쉽게 변경하거나 확장할 수 있다. 클래스 구조에 메소드만 추가하면 프로그램을 해석하는 기본 기능 외에 예쁘게 출력하는 기능이나 더 나은 프로그램 확인 기능 같은 새로운 기능을 추가할 수 있다. 문법 구조 분리 문법의 구조를 표현식 클래스에 캡슐화하여 각 표현식을 독립적으로 변경할 수 있다. 단점\n복잡한 문법 구조 복잡한 문법에 대해 클래스 수가 증가할 수 있다. 이러한 경우 파서나 컴파일러 생성기를 쓰는 편이 낫다. 성능 큰 문장을 해석할 경우 성능에 영향을 미칠 수 있다. ","date":"2024-01-27T09:01:08+09:00","image":"https://codemario318.github.io/post/gof/15/interpreter/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/15/interpreter/","title":"14. 다양한 패턴 빠르게 알아보기 - 인터프리터 패턴"},{"content":"플라이웨이트(Flyweight) 패턴 어떤 클래스의 인스턴스 하나로 여러 개의 가상 인스턴스를 제공하고 싶다면 플라이웨이트 패턴을 사용한다.\n플라이 웨이트 패턴은 여러 객체가 공유되어야 하는 대상을 효과적으로 처리하는 구조적 디자인 패턴이다.\n이 패턴은 객체의 중복된 인스턴스를 피하고 메모리를 절약하면서 객체의 공유 가능성을 지원한다.\n어떤 클래스의 인스턴스가 아주 많이 필요하지만 모두 똑같은 방식으로 제어해야 할 때 유용하게 쓰인다.\n구성 요소 Flyweight 인스턴스를 공유할 수 있도록 만드는 인터페이스 또는 추상 클래스 ConcreteFlyweight Flyweight 인터페이스를 구현하여 공유 가능한 객체 UnsharedConcreteFlyweight Flyweight를 구현하지만, 공유되지 않는 경우의 객체 FlyweightFactory: Flyweight 객체를 생성하고 관리하는 역할 수행 객체를 공유하고, 이미 생성된 객체를 반환 Client Flyweight 객체를 사용하는 클라이언트 코드 팩토리를 통애 이미 생성된 객체를 얻어옴 예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 // Flyweight 인터페이스 public interface Flyweight { void operation(); } // ConcreteFlyweight 구현 public class ConcreteFlyweight implements Flyweight { private String sharedState; public ConcreteFlyweight(String sharedState) { this.sharedState = sharedState; } @Override public void operation() { System.out.println(\u0026#34;ConcreteFlyweight: \u0026#34; + sharedState); } } // FlyweightFactory public class FlyweightFactory { private Map\u0026lt;String, Flyweight\u0026gt; flyweights = new HashMap\u0026lt;\u0026gt;(); public Flyweight getFlyweight(String key) { if (!flyweights.containsKey(key)) { flyweights.put(key, new ConcreteFlyweight(key)); } return flyweights.get(key); } } // Client public class Client { public static void main(String[] args) { FlyweightFactory factory = new FlyweightFactory(); Flyweight flyweight1 = factory.getFlyweight(\u0026#34;A\u0026#34;); Flyweight flyweight2 = factory.getFlyweight(\u0026#34;B\u0026#34;); Flyweight flyweight3 = factory.getFlyweight(\u0026#34;A\u0026#34;); flyweight1.operation(); // ConcreteFlyweight: A flyweight2.operation(); // ConcreteFlyweight: B flyweight3.operation(); // ConcreteFlyweight: A (이미 생성된 객체 공유) } } 특징 장점\n실행 시에 객체 인스턴스의 개수를 줄여서 메모리를 절약할 수 있다. 여러 가상 객체의 상태를 한곳에 모아 둘 수 있다. 단점\n특정 인스턴스만 다른 인스턴스와 다르게 행동하게 할 수 없다. 플라이 웨이트 객체가 공유되므로, 한 객체의 상태를 변경하면 다른 객체에도 영향을 미칠 수 있다. ","date":"2024-01-25T19:39:08+09:00","image":"https://codemario318.github.io/post/gof/15/flyweight/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/15/flyweight/","title":"14. 다양한 패턴 빠르게 알아보기 - 플라이웨이트 패턴"},{"content":"책임 연쇄(Chain of Responsibility) 패턴 1개 요청을 2개 이상의 객체에서 처리해야 한다면 책임 연쇄 패턴을 사용한다.\n책임 연쇄 패턴은 요청을 보내는 객체와 그 요청을 처리하는 객체 간의 결합을 느슨하게 만들기 위한 패턴이다.\n요청이 여러 객체 사이에서 전달되는 과정에서 요청을 처리하는 객체를 동적으로 결정할 수 있게 하는 패턴이다.\n객체들의 연결된 체인을 통해 요청이 전달되고, 각 객체는 자신이 처리할 수 있는지 여부를 판단하여 요청을 처리하거나 다음 객체로 전달할 수 있다.\n이를 통해 요청을 보내는 객체는 어떤 객체가 실제로 요청을 처리하는 지 알 필요가 없게 되며, 처리할 객체들은 유연하게 추가되거나 제거될 수 있다.\n책임 연쇄 패턴은 요청이 처리될 수 있는 적절한 처리자를 찾을 때까지 각 처리자에게 순차적으로 요청을 전달한다. 각 처리자는 요청을 처리하거나, 처리할 수 없으면 다음 처리자에게 전달하고, 이런 식으로 처인 상의 다양한 처리자가 처리할 수 있도록 구성된다.\n구성 요소 Handler(처리자) 인터페이스 요청을 처리하기 위한 메서드를 선언하는 인터페이스나 추상 클래스 ConcreteHandler 실제 요청을 처리하는 구체적인 클래스로 Handler 인터페이스를 구현한다. 자신이 처리할 수 없는 요청에 대해서는 다음 처리자로 요청을 전달하는 링크(체인)을 유지한다. Client(클라이언트) 요청을 시작하는 객체로, 첫 번째 처리자에게 요청을 보낸다. 예시: 상품 할인 DiscountHandler 인터페이스는 할인을 처리하는 메서드를 선언하고, TenPercentDiscountHandler 및 TwentyPercentDiscountHandler는 실제로 할인을 처리하는 구체적인 클래스이다.\n클라이언트는 먼저 TenPercentDiscountHandler에게 할인을 적용하고, TwentyPercentDiscountHandler로 전달되어 체인을 통해 적절한 할인이 적용된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 // Handler 인터페이스 public interface DiscountHandler { void applyDiscount(double amount); void setNextHandler(DiscountHandler nextHandler); } // ConcreteHandler 클래스 public class TenPercentDiscountHandler implements DiscountHandler { private DiscountHandler nextHandler; @Override public void applyDiscount(double amount) { if (amount \u0026gt; 100) { System.out.println(\u0026#34;Applying 10% discount.\u0026#34;); amount *= 0.9; System.out.println(\u0026#34;Discounted amount: \u0026#34; + amount); } else if (nextHandler != null) { nextHandler.applyDiscount(amount); } } @Override public void setNextHandler(DiscountHandler nextHandler) { this.nextHandler = nextHandler; } } public class TwentyPercentDiscountHandler implements DiscountHandler { private DiscountHandler nextHandler; @Override public void applyDiscount(double amount) { if (amount \u0026gt; 200) { System.out.println(\u0026#34;Applying 20% discount.\u0026#34;); amount *= 0.8; System.out.println(\u0026#34;Discounted amount: \u0026#34; + amount); } else if (nextHandler != null) { nextHandler.applyDiscount(amount); } } @Override public void setNextHandler(DiscountHandler nextHandler) { this.nextHandler = nextHandler; } } // Client 클래스 public class Client { public static void main(String[] args) { // 처리자 생성 DiscountHandler handler1 = new TenPercentDiscountHandler(); DiscountHandler handler2 = new TwentyPercentDiscountHandler(); // 처리자 체인 구성 handler1.setNextHandler(handler2); // 클라이언트가 요청 handler1.applyDiscount(150); // 예제에서는 20% 할인이 적용됨 } } 특징 장점\n요청을 보낸 쪽과 받는 쪽을 분리할 수 있다. 객체는 체인의 구조를 몰라도 되고, 그 체인에 들어있는 다른 객체들의 직접적인 레퍼런스를 가질 필요도 없으므로 객체를 단순하게 만들 수 있다. 사슬에 들어가는 객체를 바꾸거나 순서를 바꿈으로써 역할을 동적으로 추가하거나 제거할 수 있다. 단점\n요청이 반드시 수행된다는 보장이 없다. 체인 끝에서도 처리되지 않을 수 있지만, 상황에 따라 장점이 될 수 있다. 실행 시에 과정을 살펴보거나 디버깅하기 힘들다. ","date":"2024-01-25T19:20:08+09:00","image":"https://codemario318.github.io/post/gof/15/chain-of-responsibility/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/15/chain-of-responsibility/","title":"14. 다양한 패턴 빠르게 알아보기 - 책임 연쇄 패턴"},{"content":"빌더(Builder) 패턴 제품을 여러 단계로 나눠서 만들도록 제품 생상 단계를 캡슐화하고 싶다면 빌더 패턴이 적합하다.\n빌더 패턴은 객체 생성을 더욱 유연하게, 가독성있고, 그리고 복잡성을 낮추기 위한 디자인 패턴 중 하나이다.\n이 패턴은 복잡한 객체의 생성 과정을 추상화하고, 객체의 표현과 생성 프로세스를 분리하여 클라이언트가 서로 다른 표현 결과물을 생성할 수 있도록 한다.\n복잡한 객체 생성 객체가 복잡하고 여러 단계에 걸쳐 생성되어야 할 때 빌더 패턴이 유용할 수 있다. 다양한 표현 결과물 생성 동일한 생성 프로세스에서 여러 다른 종류의 객체를 생성하고자 할 때 사용할 수 있다. 가독성 및 유지보수성 향상 생성자의 매개변수가 많고 복잡한 경우, 빌더 패턴을 사용하여 가독성을 향상시키도 코드를 더욱 유지보수하기 쉽게 만든다. 구성 요소 Product: 생성될 객체 빌더 패턴을 통해 성성될 최종 객체 Builder: 빌더 객체를 생성하는 추상 인터페이스 객체의 각 부분을 생성하기 위한 메서드들을 선언 ConcreteBuilder Builder 인터페이스를 구현하여 객체의 각 부분을 실제로 생성하는 클래스 Director: 지시자 클라이언트와 함께 작동하여 실제로 객체를 생성하는 클래스 Builder를 사용하여 객체의 생성 프로세스를 조절한다. Client 객체를 생성하고자 하는 클라이언트 코드 일반적으로 Director를 사용하여 객체 생성 프로세스를 시작한다. 예시: HTML 문서 생성 HTMLDocument가 생성될 최종 객체를 나타내고, HTMLBuilder는 객체 생성을 위한 추상 인터페이스를 정의한다.\nSimpleHTMLBuilder는 HTMLBuilder 인터페이스를 구현하여 객체의 각 부분을 생성하고, HTMLDirector는 실제로 객체의 생성 프로세스를 조절한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 // Product: 생성될 HTML 문서 public class HTMLDocument { private String content; public void setContent(String content) { this.content = content; } public void display() { System.out.println(content); } } // Builder: HTML 문서 생성을 위한 인터페이스 public interface HTMLBuilder { void buildHeader(); void buildBody(); void buildFooter(); HTMLDocument getResult(); } // ConcreteBuilder: HTML 문서를 실제로 생성하는 클래스 public class SimpleHTMLBuilder implements HTMLBuilder { private HTMLDocument document; public SimpleHTMLBuilder() { this.document = new HTMLDocument(); } @Override public void buildHeader() { document.setContent(\u0026#34;\u0026lt;header\u0026gt;Simple HTML Header\u0026lt;/header\u0026gt;\u0026#34;); } @Override public void buildBody() { document.setContent(document.content + \u0026#34;\u0026lt;body\u0026gt;Simple HTML Body\u0026lt;/body\u0026gt;\u0026#34;); } @Override public void buildFooter() { document.setContent(document.content + \u0026#34;\u0026lt;footer\u0026gt;Simple HTML Footer\u0026lt;/footer\u0026gt;\u0026#34;); } @Override public HTMLDocument getResult() { return document; } } // Director: HTML 문서 생성 프로세스를 조절하는 클래스 public class HTMLDirector { private HTMLBuilder builder; public HTMLDirector(HTMLBuilder builder) { this.builder = builder; } public void construct() { builder.buildHeader(); builder.buildBody(); builder.buildFooter(); } } // Client: 빌더 패턴을 사용하는 클라이언트 코드 public class Client { public static void main(String[] args) { HTMLBuilder builder = new SimpleHTMLBuilder(); HTMLDirector director = new HTMLDirector(builder); director.construct(); HTMLDocument document = builder.getResult(); document.display(); } } 특징 빌더 패턴은 객체 생성을 유연하게 다루기 위한 디자인 패턴으로 이로인한 장단점이 존재한다.\n장점\n유연한 객체 생성 객체 생성 과정을 세분화하고 각 부분을 개별적으로 생성함으로써 객체 생성을 유연하게 다룰 수 있다. 다양한 조합으로 객체를 생성할 수 있다. 가독성 향상 생성자의 매개변수가 많은 경우, 빌더 패턴을 사용하여 가독성을 향상시킬 수 있다. 메개변수의 순서를 기억하지 않고도 명시적인 메서드 호출을 통해 객체를 초기화할 수 있다. 객체 표현의 분리 객체 생성과표현을 분리함으로써, 동일한 생성 로직을 사용하면서도 여러 다른 표현이 가능하다. 즉, 동일한 빌더로 다른 결과물을 만들 수 있다. 확장 용이성 새로운 부분 객체나 다른 종류의 객체를 추가하고자 할 때, 새로운 빌더를 구현하여 쉽게 확장할 수 있다. 단점\n코드 복잡성 빌더 패턴을 도입하면 클래스의 수가 증가하고, 빌더 클래스와 디렉터 클래스가 추가로 필요하므로 코드의 복잡성이 증가할 수 있다. 성능 손실 객체 생성 시 매번 빌더 패턴을 사용하면 성능에 손실이 발생할 수 있다. 불필요한 빌더 클래스 생성 간단한 객체의 경우 빌더 패턴을 도입하면 오히려 불필요한 클래스들이 생성될 수 있다. 빌더 패턴은 객체 생성의 복잡성과 유연성을 고려할 때 선택해야 하는 패턴이다.\n작은 규모의 객체나 단순한 객체 생성에는 불필요한 복잡성을 초래할 수 있으므로, 상황에 맞게 적절히 선택하는 것이 중요하다.\n","date":"2024-01-24T16:22:08+09:00","image":"https://codemario318.github.io/post/gof/14/builder/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/14/builder/","title":"14. 다양한 패턴 빠르게 알아보기 - 빌더 패턴"},{"content":"브리지(Bridge) 패턴 구현과 더불어 추상화 부분까지 변경해야 한다면 브리지 패턴을 사용한다.\n브리지 패턴은 객체 지향 프로그래밍에서 사용되는 디자인 패턴 중 하나로, 추상화와 구현을 분리하여 두 부분이 독립적으로 변화할 수 있도록 하는 구조를 제공한다.\n이를 통해 시스템의 확정성을 높이고, 추상화와 구현의 결합도를 낮춘다.\n구성 요소 추상화(Abstraction) 클라이언트가 사용하는 인터페이스를 정의한다. 추상화는 구현에 대한 참조를 가지며, 클라이언트는 이 추상화를 통해 구현에 접근한다. 구현자(Implementor) 추상화의 구현을 위한 인터페이스를 정의한다. 구현을 위한 메소드들을 선언하는데, 이들 메소드는 구체적인 구현 클래스에서 구현된다. 정제된 추상화(Refined Abstraction) 추상화의 하위 클래스로, 추가적인 기능을 제공하거나 확장할 수 있다. 구현자 구현(Concrete Implementor) Implementor의 구현 클래스로, 실제로 구현이 이루어진다. 예시: 간단한 도형 그리기 이 구현에서는 도형과 그리기 도구가 각각 추상화와 구현을 나타내며, 브리지 패턴을 통해 두 부분을 분리한다.\n클라이언트는 도형을 생성하고 원하는 그리기 도구를 지정할 수 있어, 새로운 도형이나 그리기 도구를 추가하더라도 기존 코드에 영향을 덜 주고 시스템을 확장할 수 있다.\nAbstraction: 도형\n도형을 나타내는 추상 클래스로서, 그리기 도구와 연결됨\n1 2 3 4 5 6 7 8 9 public abstract class Shape { protected DrawingAPI drawingAPI; protected Shape(DrawingAPI drawingAPI) { this.drawingAPI = drawingAPI; } public abstract void draw(); } Refined Abstraction: Circle 및 Square\n도형의 구체적인 형태를 나타내는 클래스로서, 추상화된 도형을 확장한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 public class Circle extends Shape { private int x, y, radius; public Circle(int x, int y, int radius, DrawingAPI drawingAPI) { super(drawingAPI); this.x = x; this.y = y; this.radius = radius; } @Override public void draw() { drawingAPI.drawCircle(x, y, radius); } } public class Square extends Shape { private int x, y, side; public Square(int x, int y, int side, DrawingAPI drawingAPI) { super(drawingAPI); this.x = x; this.y = y; this.side = side; } @Override public void draw() { drawingAPI.drawSquare(x, y, side); } } Implementor: DrawingAPI\n그리기 도구를 나타내는 인터페이스로서, 실제 구현은 구체적인 구현 클래스에서 이루어진다.\n1 2 3 4 public interface DrawingAPI { void drawCircle(int x, int y, int radius); void drawSquare(int x, int y, int side); } Concrete Implementor: DrawingAPI1 및 DrawingAPI2\nDrawingAPI를 구현한 구체적인 클래스로서, 실제로 그리기 동작을 수행\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class DrawingAPI1 implements DrawingAPI { @Override public void drawCircle(int x, int y, int radius) { System.out.println(\u0026#34;API1 - Drawing Circle at (\u0026#34; + x + \u0026#34;, \u0026#34; + y + \u0026#34;) with radius \u0026#34; + radius); } @Override public void drawSquare(int x, int y, int side) { System.out.println(\u0026#34;API1 - Drawing Square at (\u0026#34; + x + \u0026#34;, \u0026#34; + y + \u0026#34;) with side \u0026#34; + side); } } public class DrawingAPI2 implements DrawingAPI { @Override public void drawCircle(int x, int y, int radius) { System.out.println(\u0026#34;API2 - Drawing Circle at (\u0026#34; + x + \u0026#34;, \u0026#34; + y + \u0026#34;) with radius \u0026#34; + radius); } @Override public void drawSquare(int x, int y, int side) { System.out.println(\u0026#34;API2 - Drawing Square at (\u0026#34; + x + \u0026#34;, \u0026#34; + y + \u0026#34;) with side \u0026#34; + side); } } 특징 장점\n분리된 추상화와 구현 추상화와 구현을 분리하여 두 부분이 독립적으로 변화할 수 있다록 한다. 이를 통해 코드의 확장성과 유지보수성을 향상시킬 수 있다. 유연한 확장 새로운 추상화나 구현을 추가하는 데 있어 기존 코드에 영향을 최소화 할 수 있다. 새로운 기능이나 클래스를 도입할 때 유연하게 대응할 수 있다. 복잡성 감소 각각의 변경이 다른 부분에 미치는 영향을 최소화하므로 코드를 이해하고 유지보수하는 데 도움을 줄 수 있다. 다형성 강화 추상화와 구현 사이의 느슨한 결합을 제공하므로, 다양한 형태의 추상화와 구현을 조합하여 사용할 수 있다. 단점\n복잡성 증가 추가적인 클래스와 인터페이스를 도입하여 시스템을 구현하므로, 간단한 시스템에서는 과도한 복잡성을 초래할 수 있다. 설계 과정에서 추가 비용 초기 설계 단계에서 브리지 패턴을 도입하는 데 추가적인 비용이 발생할 수 있다. 특히 간단한 시스템에서는 이를 도입하는 것이 과도할 수 있다. 인터페이스 수 증가 구현자와 추상화에 대한 인터페이스가 각각 존재하므로, 클래스 수가 증가할 수 있다. 이로 인해 코드 베이스의 관리를 어렵게 할 수 있다. 정리 브리지 패턴은 시스템이 확장이나 변경이 빈번하게 발생하는 경우에 유용하지만 간단한 시스템에서는 추가적인 복잡성이나 비용이 필요할 수 있으므로 상황에 맞게 적용해야한다.\n","date":"2024-01-24T15:48:08+09:00","image":"https://codemario318.github.io/post/gof/14/bridge/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/14/bridge/","title":"14. 다양한 패턴 빠르게 알아보기 - 브리지 패턴"},{"content":"디자인 패턴의 정의 패턴(Pattern)이란?\n특정 컨텍스트 내에서 주어진 문제의 해결책이다.\n컨텍스트(Context) 패턴이 적용되는 상황을 의미 반복적으로 일어날 수 있는 상황 문제(Problem) 컨텍스트 내에서 이뤄야 하는 목표 컨텍스트 내의 제약조건도 포함 해결책(Solution) 찾아내야 하는 것 제약조건 속에서 누가 적용해도 목표를 이룰 수 있는 일반적인 디자인을 뜻함 어떤 컨텍스트 내에서 일련의 제약조건에 의해 영향을 받는 문제가 발생했다면, 그 제약조건 내에서 목적 달성을 위한 해결책이 되는 디자인을 적용한다.\n패턴은 반복적으로 등장하는 문제에 적용할 수 있어야 한다. 반복적으로 적용할 수 있는 해결책이 아니라면 패턴이라고 할 수 없다. 해결책을 다른 사람에게 알려줘서 그 사람이 처함 문제의 해결책으로 적용할 수 있어야 한다. 디자인 패턴은 일상적이고 반복적으로 등장하는 디자인 문제의 해결책이다. 패턴을 정의할 수 있어야 다양한 장점을 제공하는 패턴 카탈로그를 만들 수 있다.\n패턴 카탈로그 소프트웨어 디자인에서 자주 발생하는 문제들에 대한 해결책을 제시하는 인련의 디자인 패턴을 모아둔 문서 또는 자료이다.\n디자인 패턴 카탈로그는 다양한 디자인 패턴을 설명하고 각 패턴의 사용 시나리오, 구현 방법, 이점 등을 자세히 설명하여 개발자들이 필요할 때 참고할 수 있도록 도와준다.\n이름 패턴에서 중요한 요소로 제대로 된 이름이 없다면 패턴의 정보를 다른 개발자들과 공유하기 힘들어진다. 패턴의 종류 또는 범주 용도(Intent) 패턴의 역할을 간단하게 기술 패턴을 정의할 때 썻던 내용들이 들어가게됨 동기(Motivation) 문제를 기술하고 주어진 해결책이 어떤 시긍로 그 문제를 해결하는지 보여주는 구체적인 시나리오 적용 대성(Applicability) 패턴을 적용할 수 있는 상황을 기술 구조(Structure) 패턴에서 쓰이는 클래스들의 관계를 보여주는 다이어그램 수록 구성 요소(Participant) 클래스와 객체들의 설명 패턴 내에서 각 클래스(객체)가 맡는 임무와 역할을 설명 협동(Collaborations) 각 구성 요소가 패턴 내에서 어떤 식으로 서로 도움을 주는지 설명 결과(Consequences) 이 패턴을 사용했을 때의 효과(장점 및 단점) 수록 구현(Implementation) 패턴을 구현할 때 필요한 기술과 주의사항 샘플 코드(Sample Code) 구현하는 데 도움이 될만한 코드 사용 예(Known uses) 실제 시스템에서 이 패턴을 사용하는 예시 설명 연관 패턴(Related Patterns) 해당 패턴과 다른 패턴 사이의 관계를 설명하는 내용 새로운 디자인 패턴 발견하기 패턴은 만들어지는 것이 아니라 발견되는 것이다.\n특화된 분야에서 일을 하다 보면 유용한 패턴을 새로 발견하는 경우도 있고, 자주 발생하는 문제의 일반적인 해결책을 찾아내는 경우도 있다.\n기존 패턴을 확실하게 파악한다. 새로워 보이는 패턴도 기존 패턴을 변형한 경우가 많다. 패턴을 알아보는 눈이 길러지고 다른 패턴과 연관 짓는 능력도 발달한다. 패턴에 관한 아이디어는 경험(문제와 사용했던 해결책)에서 나온다. 지금까지의 경험을 토대로 곰곰히 생각해 보고, 반복적으로 발생하는 문제를 해결할 수 있는 새로운 디자인이 되도록 다듬는다. 문서화 해본다. 다른 사람들이 직접 적용해 보고 피드백을 제공할 수 있도록 문서로 만든다. 새로운 패턴을 사람들이 사용하게 해서 계속 다듬는다. 패턴을 한 번에 완성하긴 어려우므로, 패턴을 시험해 볼 기회를 제공하고 피드백을 얻는다. 3의 규칙 패턴이 실전 문제 해결에 3번 이상 적용되어야 패턴 자격을 얻을 수 있다. 디자인 패턴 분류하기 점점 더 많은 디자인 패턴이 발견됨에 따라 디자인 패턴을 찾거나 같은 그룹에 속하는 패턴끼리 비교하기 좋게 분류할 필요성이 생겼다.\n대부분 카탈로그에서는 몇 가지 범주에 맞춰 디자인 패턴을 분류하고 있으며, 제일 유명한 분류 방법은 용도에 따라 나누는 방법이다.\n생성 패턴(Creational Pattern) 객체 인스턴스를 생성하는 패턴으로, 클라이언트와 그 클라이언트가 생성해야 하는 객체 인스턴스 사이의 연결을 끊어주는 패턴 행동 패턴(Behavioral Pattern) 클래스와 객체들이 상호작용하는 방법과 역할을 분담하는 방법을 다루는 패턴 구조 패턴(Structural Pattern) 클래스와 객체를 더 큰 구조로 만들 수 있게 구성을 사용하는 패턴 그 외에도 클래스 또는 객체를 다루는 패턴이라던가 하위 범주를 다시 나누는 등 여러 방식으로 분류하고 있다.\n패턴으로 생각하기 패턴으로 생각한다는 것은 어떤 디자인을 봤을 때 패턴 적용 여부를 결정할 수 있는 안목을 가진다는 의미이다.\n최대한 단순하게\n디자인을 할 때 가장 중요한 원칙은 **최대한 단순한 방법(KISS, Keep it Simple)**으로 문제를 해결하기다.\n\u0026ldquo;문제에 어떤 패턴을 적용할 수 있을까?\u0026rdquo; 라는 접근이 아닌 \u0026ldquo;어떻게 하면 단순하게 해결할 수 있을까?\u0026ldquo;에 초점을 맞춘다. 패턴 없이도 정말 단순하고 잘 만들 수 있다면 적용하면 되고, 가장 단순하고 유연한 디자인을 만들 때 패턴이 필요하다면 그때 적용한다.\n디자인 패턴은 만병 통치약이 아니다.\n패턴은 반복적으로 발생하는 문제의 일반적인 해결책이므로, 패턴을 사용할 때 설계한 디자인에 미칠 영향과 결과를 주의 깊게 생각해봐야한다.\n패턴이 필요할 때\n현재 디자인상의 문제에 적합다는 확신이 드는 경우 패턴을 도입해야한다.\n더 간단한 해결책이 있다면 패턴을 적용하기 전 그 해결책 사용을 고려한다. 간단한 해결책만으로는 부족하다고 확신을 가지면 해결해야 할 문제와 제약 조건을 종합적으로 고려하여 적합한 패턴을 적용한다. 간단한 해결책으로 문제가 해결되는 데도 시스템의 어떤 부분이 변경될 것이라 예측되는 상황에는 디자인 패턴을 적용한다. 리펙터링과 패턴\n패턴을 통해 구조가 개선될 수 있을지 검토해 볼 수 있다.\n디자인 패턴 제거\n패턴보다 간단한 해결책이 더 나을 것 같다면 패턴을 제거한다.\n시스템이 복잡해지며 기대했던 유연성이 발휘되지 못하는 경우 등 꼭 필요하지 않은 패턴을 미리 적용하지 않는다.\n지금 당장 변화에 대처하는 디자인을 만꼭 필요하지 않는 데도 패턴을 추가하면 시스템만 복잡해지고, 나중에 그 패턴을 사용하지 않을 수도 있다.\n안티 패턴 안티 패턴(Anti-Pattern)은 어떤 문제의 나쁜 해결책에 이르는 길을 알려준다.\n나쁜 해결책에 유혹되는지 알려준다. 나쁜 해결책이 어떤 식으로 사람들을 꼬시는지 설명함으로써 쓰지 않도록 경고한다. 장기적인 관점에서 나쁜 이유를 알려준다. 어떠한 부정적인 효과와 문제가 발생하는지 알린다. 좋은 해결책을 만들 때 적용할 수 있는 다른 패턴을 제안해준다. 구성\n이름 문제 컨텍스트 원인 잘못된 해결책 바람직한 해결책 예시 핵심 정리 디자인 패턴은 자연스럽게 적용한다. 억지로 적용하면 안된다. 디자인 패턴은 돌에 새겨진 글씨가 아니므로 필요에 따라 적당히 변형한다. 주어진 조건을 만족하는 가잔 간단한 해결책을 사용한다. 디자인 패턴을 무조건 쓸 필요는 없다. 디자인 패턴 카탈로그를 읽어 보고 패턴을 숙지한다. 패턴 사이의 관계도 확실히 이해해야 한다. 패턴을 분류해서 패턴 그룹을 만들 수 있다. 그룹으로 나누는 것이 패턴 이해에 도움이 된다면 적극적으로 나눠본다. 패턴 작가가 되려면 대단한 노력이 필요하다. 오랜 시간 동안 패턴을 다음어야 한다. 이후 접할 패턴은 대부분 기존 패턴을 응용한 것이다. 주변 사람들과 전문 용어를 공유하자. 공통의 언어로 얘기할 수 있다는 점이 패턴의 가장 큰 장점이다. ","date":"2024-01-23T16:23:08+09:00","image":"https://codemario318.github.io/post/gof/13/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/13/","title":"13. 패턴과 행복하게 살아가기 - 실전 디자인 패턴"},{"content":" 복합 패턴은 두 개 이상의 디자인 패턴이 특정 상황이나 문제 해결에 협력하여 사용될 때를 나타낸다. 여러 디자인 패턴을 조합하여 더 복잡하고 효과적인 구조를 만들 수 있다.\nMVC(Model-View-Controller) 패턴 MVC 패턴은 독립된 모델, 뷰, 컨트롤러 컴포넌트들의 상호 작용으로 의존도를 줄이는 것이 목표인 아키텍처이다.\nMVC 패턴은 모델-뷰-컨트롤러의 세 가지 주요 컴포넌트로 소프트웨어를 구성하는 패턴이다.\n사용자 인터페이스를 효과적으로 설계하고 관리하기 위해 고안되었다.\n각 컴포넌트는 특정한 역할을 수행하며, 이들 간의 역할 분리를 통해 유지보수성과 확장성을 향상시킨다.\nModel 애플리케이션의 데이터와 비즈니스 로직을 나타낸다. 데이터의 상태를 저장하고, 데이터에 대한 조작 및 처리를 담당한다. 뷰나 컨트롤러와 직접적으로 상호 작용하지 않으며, 변경이 발생하면 등록된 옵저버들에게 상태 변경을 통지한다. View 사용자에게 정보를 표시하는 역할을 수행한다. 모델의 데이터를 시각적으로 표현하며, 사용자의 입력을 받아 컨트롤러에 전달한다. 모델과 직접적인 관련이 없으며, 모델의 상태 변화를 감지하여 업데이트된다. Controller 사용자의 입력을 받아 모델을 업데이트하거나 뷰를 업데이트하는 역할을 수행한다. 사용자의 액션에 응답하여 모델의 데이터를 갱신하고, 변경된 데이터에 대한 뷰의 업데이트를 처리한다. 모델과 뷰 간의 통신 다리 역할을 하며, 강한 결합을 방지하여 유연성을 제공한다. MVC 패턴의 핵심 아이디어는 각 컴포넌트가 서로 독립적으로 동작하도록 하는 것이다.\n이를 통해 코드의 재사용성이 증가하고, 유지보수 및 확정이 용이해진다. 또한 사용자 인터페이스의 변화가 다른 부분에 미치는 영향을 최소화하여 유연하게 대처할 수 있도록 한다.\nMVC 패턴은 주로 GUI 프레임워크나 웹 애플리케이션 등 사용자 인터페이스를 개발하는 데 적용되는데, 이를 통해 각 부분을 독립적으로 개발하고 테스트 할 수 있다는 장점을 제공한다.\n모델-뷰-컨트롤러에 사용되는 패턴 알아보기 MVC는 여러 개의 패턴이 함께 적용되어 완성된 하나의 디자인이다.\n옵저버 패턴\nMVC 패턴에서 모델과 뷰 간의 통신은 주로 옵저버 패턴을 기반으로 한다.\nflowchart LR view1[/뷰 1/] model1[[\"모델(옵저버블)\"]] subgraph 옵저버 controller{{컨트롤러}} view2[/뷰 1/] view3[/뷰 2/] end view1--\"옵저버 등록\"--\u003emodel1 model1--\"상태 변경\"--\u003econtroller model1--\"상태 변경\"--\u003eview2 model1--\"상태 변경\"--\u003eview3 모델의 상태 변경 시, 등록된 뷰들에게 자동으로 알림을 보내어 뷰를 업데이트한다. 뷰와 모델간의 강한 결합을 피하면서 상태 변화를 효율적으로 전파할 수 있다. 모델은 옵저버 패턴을 써서 상태가 변경되었을 때 그 모델과 연관된 객체들에게 연락한다.\n옵저버 패턴을 사용하여 모델을 뷰와 컨트롤러로부터 완전히 독립 시킬 수 있으며, 이를 통해 한 모델에서 서로 다른 뷰를 사용할 수도 있고, 여러 개의 뷰를 동시에 사용하는 것도 가능하다.\n전략 패턴\n컨트롤러는 사용자의 입력을 받아 처리하는 부분이며, 뷰와 컨트롤러는 고전적인 전략 패턴으로 구현되어 있다.\nflowchart LR v[/뷰/] c1{{컨트롤러 1}} c2{{컨트롤러 2}} v--\u003ec1 v--\u003ec2 뷰 객체를 여러 전략을 써서 설정할 수 있으며, 컨트롤러가 전략을 제공한다. 여러 입력 전략을 정의하고, 컨트롤러가 이 중 하나를 선택하여 사용자 입력에 대응하게 된다. 컨트롤러를 바꾸면 뷰의 행동도 바꿀 수 있다. 뷰는 애플리케이션의 겉모습에만 신경 쓰고, 인터페이스의 행동을 결정하는 일은 모두 컨트롤러에게 맡긴다. 사용자가 요청한 내역을 처리하기 위해 모델에 요청하는 부분을 컨트롤러가 담당하게 되어, 뷰를 모델로부터 분리하는 데에도 도움이 된다.\n컴포지트 패턴\nflowchart LR v[/뷰/] r(((GUI))) v--paint--\u003er 뷰를 구성하는 요소들을 효과적으로 처리하기 위해 컴포지트 패턴을 활용한다. 복합 뷰는 여러 단을 뷰와 하위 복합 뷰로 구성될 수 있다. 컨트롤러가 뷰에게 화면을 갱신해 달라고 요청했을 때, 최상위 뷰 구성 요소에게만 화면을 갱신하라고 전달하면 컴포지트 패턴이 알아서 처리하게 된다.\n그 외 적용된 패턴\n커맨드 패턴 사용자의 액션에 따라 컨트롤러는 모델의 데이터를 변경하거나 뷰를 업데이트 해야한다. 커맨드 패턴은 액션을 캡슐화하여 객체로 나타내고, 컨트롤러가 이 객체를 통해 액션을 수행할 수 있도록 한다. 팩토리 메소드 패턴 뷰나 컨트롤러를 생성하는 데 팩토리 메소드 패턴을 사용할 수 있다. 특정 뷰 또는 컨트롤러의 생성을 서브 클래스에서 결정하도록 하는 패턴으로, 유연성을 제공한다. 데코레이터 패턴 뷰의 추가적인 기능을 동적으로 확장하고 싶을 때 데코레이터 패턴을 사용할 수 있다. 뷰를 감싸는 데코레이터를 사용하여 런타임에 새로운 기능을 추가하거나 변경할 수 있다. 싱글톤 패턴 MVC 패턴에서 모델이나 컨트롤러는 주로 싱글톤 패턴을 사용하여 단일 인스턴스로 유지된다. 애플리케이션 전역에서 해당 객체에 접근이 필요한 경우 사용된다. 핵심 정리 복합 패턴 2개 이상의 패턴을 결합해서 일반적으로 자주 등장하는 문제들의 해법을 제공한다. 모델-뷰-컨트롤러(MVC)는 옵저버, 전략, 컴포지트 패턴으로 이루어진 복합 패턴이다. 모델은 옵저버 패턴을 사용해서 의존성을 없애면서도 옵저버들에게 자신의 상태가 변경되었음을 알릴 수 있다. 컨트롤러는 뷰의 전략 객체이다. 뷰는 컨트롤러를 바꿔서 또 다른 행동을 할 수 있다. 뷰는 컴포지트 패턴을 사용해서 사용자 인터페이스를 구현한다. 보통 패널이나 프레임, 버튼과 같은 중첩된 구성 요소로 이루어진다. 모델, 뷰, 컨트롤러는 3가지 패턴으로 서로 느슨하게 결합되므로 깔끔하면서도 유연한 구현이 가능해진다. 새로운 모델을 기존의 뷰와 컨트롤러에 연결해서 쓸 때는 어댑터 패턴을 활용하면 된다. MVC는 웹에도 적용된다. 클라이언트-서버 애플리케이션 구조에 MVC를 적응시켜 주는 다양한 웹 MVC 프레임워크가 있다. ","date":"2024-01-17T14:20:08+09:00","image":"https://codemario318.github.io/post/gof/12/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/12/","title":"12. 패턴을 모아 패턴 만들기 - 복합 패턴"},{"content":" 프록시 패턴은 특정 객체로의 접근을 제어하는 대리인(특정 객체를 대변하는 객체)를 제공한다.\n프록시 패턴이란? 프록시 패턴은 한 객체가 다른 객체에 대한 인터페이스 역할을 수행하여 접근을 제어하거나 대리 역할을 수행하는 디자인 패턴이다.\n접근을 제어하는 방법을 달리하여 여러 변형이 존재한다.\n원격 프록시(Remote Proxy) 원격 객체에 대한 로컬 표현을 제공하여, 원격 서버에 있는 객체에 쉽게 접근할 수 있도록 한다. 가상 프록시(Virtual Proxy) 객체의 생성 및 초기화에 비용이 많이 들 때, 객체가 실제로 필요한 시점까지 생성을 지연시키는 가상 객체를 제공한다. 보호 프록시(Protection Proxy) 객체에 대한 접근을 제어하여, 특정 사용자나 클라이언트에 대한 권한 검사를 수행한다. 캐싱 프록시(Caching Proxy) 이전에 수행한 연산 결과를 캐시하여 동일한 요청에 대한 성능을 향상시킨다. 프록시 패턴을 사용하면 원격 객체, 생성하기 힘든 객체, 보안이 중요한 객체와 같은 다른 객체로의 접근을 제어하는 대리인 객체를 만들 수 있다.\n이를 통해 복잡한 시스템에서 객체 간의 통신과 상호작용을 조절하고 향상시키는 데 유용하게 사용될 수 있다.\n구조 classDiagram class Subject { \u003c\u003c interface \u003e\u003e request()* } class RealSubject { request() } class Proxy { request() } Subject \u003c|.. RealSubject Subject \u003c|.. Proxy Proxy --\u003e RealSubject : subject Subject(주체) 실제 서비스를 제공하는 객체를 나타내는 인터페이스를 정의한다. RealSubject(실제 주체) Subject의 실제 구현을 제공하는 클래스 이 객체에 대한 접근을 제어하거나 비용이 많이 드는 작업을 수행한다. Proxy(프록시) Subject와 동일한 인터페이스를 구현하며, RealSubject에 대한 참조를 가지고 있다. 실제 서비스 객체에 대한 접근을 제어하거나 추가적인 기능을 제공한다. RealSubject와 Proxy는 Subject 인터페이스를 구현하므로 RealSubject가 들어가야 할 자리에 Proxy를 대신 넣을 수 있다.\n진짜 작업은 RealSubject 객체가 처리하며, Proxy는 이 객체의 대변인 역할을 하면서 RealSubject로의 접근을 제어하게된다.\nProxy는 RealSubject의 레퍼런스를 포함하며(직법 생성하거나 제거하기도 함) 클라이언트는 항상 Proxy를 통해 데이터를 주고받게 된다.\n원격 프록시 원격 프록시는 대리인의 어떤 메소드를 호출하면 원격 객체에게 해당 메소드 호출을 전달하는 원격 객체의 로컬 대리인 역할을 하게된다.\n클라이언트 객체는 원격 객체의 메소드 호출을 하는 것처럼 행동하지만 실제로는 로컬 힙에 들어있는 프록시 객체의 메소드를 호출하며, 프록시에서 네트워크 통신과 관련된 저수준 작업을 처래히준다.\n원격 프록시(Remote Proxy)는 클라이언트와 서버 사이에서 객체에 대한 원격 액세스를 제공하며, 객체의 메서드 호출을 원격으로 전달하여 실행하는 데 중접을 둔다.\nflowchart LR subgraph 원격 a((클라이언트)) -- \"request()\" --\u003e b((프록시)) end subgraph 실제 c((RealSubject)) end b -. \"request()\" .-\u003e c 원격 객체 엑세스 클라이언트가 로컬이 아닌 다른 컴퓨터에 위치한 객체에 액세스해야 할 때, 원격 프록시를 사용하여 해당 객체에 접근한다. 보안 및 권한 제어 원격 프록시를 사용하여 원격 객체에 대한 접근을 제어하고, 보안 및 권한 검사를 수행할 수 있다. 프록시 패턴의 응용 일반적인 프록시 패턴에서 원격 프록시를 사용하여 클라이언트와 실제 객체 사이의 중각 매개체로서 동작하게 할 수 있다. Java RMI(Remote Method Invocation) 원격 프록시는 Java의 RMI와 같은 기술을 사용하여 구현될 수 있다.\n클라이언트는 로컬에서 마치 객체를 다루는 것처럼 원격 프록시를 사용하여 원격 객체의 메서드를 호출할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 // 예시: 원격 프록시 인터페이스 import java.rmi.Remote; import java.rmi.RemoteException; // 표식용(Maker) Remote 인터페이스를 확장하여 인터페이스가 원격 호출을 지원한다는 사실을 알린다. public interface MyRemoteInterface extends Remote { // 모든 원격 메소드 호출은 위험이 따르며 모든 메소드에 RemoteException을 선언하여 클라이언트에서 예외를 처리할 수 있도록 한다. void myRemoteMethod() throws RemoteException; // 원격 메소드의 인자와 리턴값은 반드시 원시 형식(primitive) 또는 Serializable 형식으로 선언해야한다. String sayHello() throws RemoteException; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 예시: 원격 프록시 구현 import java.rmi.RemoteException; import java.rmi.server.UnicastRemoteObject; public class MyRemoteObject extends UnicastRemoteObject implements MyRemoteInterface { protected MyRemoteObject() throws RemoteException { super(); } @Override public void myRemoteMethod() throws RemoteException { // 실제 원격 메서드 구현 System.out.println(\u0026#34;Executing remote method\u0026#34;); } } 가상 프록시 가상 프록시는 프록시 패턴의 한 종류로, 객체의 생성 및 초기화에 비용이 많이 들 때, 실제로 객체가 필요한 시점까지 객체를 생성하지 않고 대리 객체를 사용하는 패턴이다.\n이를 통해 성능 향상 및 자원 절약이 가능하며, 사용자는 가상 프록시를 통해 실제 객체에 접근할 수 있다.\nflowchart LR a((클라이언트)) -- \"request()\" --\u003e b((프록시)) --\u003e c((RealSubject)) 비용이 많이 드는 객체 생성 객체의 생성이나 초기화에 많은 비용이 들 때, 해당 객체를 실제로 사용하기 직전까지 객체를 생성하지 않고 대리 객체를 사용하여 필요한 경우에만 생성한다. 느린 초기화 작업 객체의 초기화 작업이 느리게 수행되는 경우, 초기화 작업이 완료된 후에만 실제 객체를 생성하도록 하는 가상 프록시를 사용하여 지연 로딩을 구현할 수 있다. 자원 소모 최적화 자원을 효율적으로 관리하기 위해 필요할 때만 실제 객체를 생성하고 사용하는 경우, 가상 프록시를 적용하여 자원 소모를 최적화 할 수 있다. 가상 프록시는 실제 객체를 나타내는 인터페이스를 정의하고, 이 인터페이스를 구현하는 가상 프록시 클래스를 만들어 사용하게 된다.\n이를 통해 실제 객체의 생성 및 초기화를 지연시킨다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 예시: 가상 프록시 인터페이스 public interface RealObject { void performOperation(); } // 예시: 실제 객체 구현 public class RealObjectImpl implements RealObject { public RealObjectImpl() { // 객체 생성 및 초기화 작업 } @Override public void performOperation() { // 실제 객체의 동작 System.out.println(\u0026#34;Performing operation\u0026#34;); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 예시: 가상 프록시 구현 public class VirtualProxy implements RealObject { private RealObjectImpl realObject; @Override public void performOperation() { // 필요할 때만 실제 객체 생성 if (realObject == null) { realObject = new RealObjectImpl(); } // 가상 프록시를 통해 실제 객체의 동작 호출 realObject.performOperation(); } } 원격 프록시와 가상 프록시의 차이 원격 프록시와 가상 프록시는 둘 다 프록시 패턴의 변형으로, 객체 간의 통신을 지원하거나 객체의 생성 및 초기화를 최적화하기 위해 상용된다.\n그러나 각각의 주된 목적과 사용 시나리오가 다르기 때문에 차이가 있다.\n목적 원격 프록시: 분산 시스템에서 객체 간의 통신을 가능케 하는 것이 목적이다. 클라이언트와 서버 간의 통신을 원활하게 하기 위해 객체를 원격으로 호출하고 사용한다. 가상 프록시: 객체의 생성 및 초기화에 대한 비용을 최적화한다. 필요한 시점까지 실제 객체를 생성하지 않고 가상적인 대리 객체를 사용함으로 성능을 향상시키는 것이 목적이다. 활용 원격 프록시 객체가 서로 다른 주소 공간에 위치해 있을 때 사용된다. 객체의 메서드를 원격으로 호출하여 분산 시스템에서 투명한 원격 접근을 제공한다. 가상 프록시 객체의 생성이나 초기화에 비용이 많이 들거나, 초기화를 지연시켜야 할 때 사용된다. 필요한 시점에만 실제 객체를 상성하여 자원을 효율적으로 활용할 수 있다. 구현 방식 원격 프록시 원격 객체 간의 통신을 위해 원격 메서드 호출을 지원하는 기술을 활용한다. ex) Java RMI 가상 프록시 객체의 생성 및 초기화를 지연시키는 방식으로 구현된다. 필요할 때만 실제 객체를 생성하고 사용한다. 주요 특징 원격 프록시 분산 환경에서의 통신에 중점을 두고, 보안, 효율성 투명성 등을 고려해야한다. 가상 프록시 객체의 생성 및 초기화 비용을 최적화한다. 필요한 시점까지 실제 객체를 생성하지 않고 가상적인 대리 객체를 사용하여 성능을 개선한다. 원격 프록시는 주로 분산 시스템에서의 객체 간 통신을 위해 사용되고, 가상 프록시는 객체의 생성과 초기화에 대한 비용을 최적화하기 위해 사용한다.\n보호 프록시 보호 프록시는 주로 객체에 대한 접근을 제어하고 보호하는 데 사용된다.\n이 패턴은 객체에 대한 클라이언트의 엑세스를 제한하거나, 특정한 권한 검사를 수행하여 보안을 강화할 수 있다.\n접근 제어 특정 객체에 대한 접근을 제한하거나 허용하기 위해 클라이언트의 요청을 검사하고 필요한 권한 검사를 수행한다. ex) 파일, 데이터베이스 등 보안 강화 객체에 직접적으로 접근하는 것을 방지하여 보안을 강화한다. 실제 객체에 직접 접근하는 대신, 보호 프록시를 통해 간접적으로만 객체에 접근할 수 있다. 캐싱 및 성능 최적화 객체에 대한 엑세스를 제어하는 동시에, 캐싱과 같은 성능 최적화 기법을 적용하여 불필요한 작업을 줄일 수 있다. classDiagram class Subject { \u003c\u003c interface \u003e\u003e *request() } class RealSubject { request() } class Proxy { request() } class InvocationHandler { \u003c\u003c interface \u003e\u003e *invoke() } class ConcreteInvocationHandler { invoke() } Subject \u003c|-- RealSubject Subject \u003c|-- Proxy InvocationHandler \u003c|-- ConcreteInvocationHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 // 보호 프록시 인터페이스 public interface Subject { void request(); } // 실제 객체 public class RealSubject implements Subject { @Override public void request() { System.out.println(\u0026#34;RealSubject: Handling request.\u0026#34;); } } // 보호 프록시 public class Proxy implements Subject { private RealSubject realSubject; private String accessCode; // 예시로 권한 코드를 사용 public Proxy(String accessCode) { this.accessCode = accessCode; } @Override public void request() { if (authenticate()) { if (realSubject == null) { realSubject = new RealSubject(); } realSubject.request(); } else { System.out.println(\u0026#34;Access denied. Authentication failed.\u0026#34;); } } private boolean authenticate() { // 권한 검사 로직 return \u0026#34;1234\u0026#34;.equals(accessCode); } } 정리 프록시 패턴 특정 객체로의 접근을 제어하는 대리인을 제공한다. 프록시 패턴을 사용하면 어떤 객체의 대리인을 내세워서 클라이언트의 접근을 제어할 수 있다. 원격 프록시는 클라이언트와 원격 객체 사이의 데이터 전달을 관리해준다. 가상 프록시는 인스턴스를 만드는 데 많은 비용이 드는 객체로의 접근을 제어한다. 보호 프록시는 호출하는 족의 권한에 따라서 객체에 있는 메소드로의 접근을 제어한다. 그 외에도 캐싱 프록시, 동기화 프록시, 방화벽 프록시, 지연 복사 프록시와 같이 다양한 변형된 프록시 패턴이 있다. 프록시 패턴의 구조는 데코레이터 패턴의 구조와 비슷하지만 그 용도는 다르다. 데코레이터 패턴은 객체에 행동을 추가하지만 프록시 패턴은 접근을 제어한다. 자바에 내장된 프록시 지원 기능을 사용하면 동적 프록시 클래스를 만들어서 원하는 핸들러에서 호출을 처리하도록 할 수 있다. 다른 래퍼를 쓸 때와 마찬가지로 프록시를 쓰면 디자인에 포함되는 클래스와 객체의 수가 늘어난다. ","date":"2023-12-29T10:22:08+09:00","image":"https://codemario318.github.io/post/gof/11/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/11/","title":"11. 객체 접근 제어하기 - 프록시 패턴"},{"content":" 전략 패턴과 상태 패턴은 쌍둥이이다.\n전략 패턴은 알고리즘을 바꾸어 사용하는 반면 상태 패턴은 내부 상태를 바꿈으로써 객체가 행동을 바꿀 수 있도록 도와준다.\n상태 기계 상태 기계(State Machine)는 객체나 시스템이 다양한 상태를 가지고 있고, 특정 이벤트에 의해 이 상태가 변하는 시스템을 나타내는 모델이다.\n상태 기계는 상태, 이벤트, 전이(Transition), 동작(Action) 등의 개념을 포함하며, 객체나 시스템이 특정 상태에서 다른 상태로 전환되는 것을 관리한다.\n상태(State): 시스템이나 객체가 가질 수 있는 다양한 상태를 나타낸다. 각 상태는 특정 시점에서 시스템이 어떤 동작을 수행하고 있는지를 표현한다. 이벤트(Event): 상태 전이를 유발하는 외부나 내부에서 발생하는 사건이나 신호를 나타낸다. 이벤트는 특정 상태에서만 발생하거나 처리될 수 있다. 전이(Transition): 상태 간 전환을 정의한다. 특정 상태에서 특정 이벤트가 발생하면 어떤 다음 상태로 전환되어야 하는지를 정의한다. 동작(Action): 상태 전이가 발생할 때 수행되는 특정 동작이나 처리를 나타낸다. 각 전이에는 연관된 동작이 정의될 수 있다. 상태 기계는 주로 시스템의 복잡한 동작을 모델링하고 이해하기 쉽게 만들기 위해 사용된다.\n소프트웨어 개발에서는 상태 기계를 사용하여 객체의 동작을 상태에 따라 효과적으로 제어할 수 있다.\n상태 기계는 **유한 상태 기계(Finite State Machine, FSM)**로 불리기도 하며, 이는 상태의 수가 유한하다는 특성을 강조한 용어이다.\n뽑기 기계 상태 다이어그램 flowchart LR s([시작]) a((동전 있음)) b((동전 없음)) c((알맹이 판매)) d((알맹이 매진)) e{알맹이 개수 \u003e 0} s--\u003ea a-- 동전 반환 --\u003eb b-- 동전 투입 --\u003ea a-- 손잡이 돌림 --\u003e c c-- 알맹이 내보냄 --\u003ee e-- Y --\u003eb e-- N --\u003e d 동전 있음, 없음, 알맹이 판매, 매진이 상태 손잡이 돌임, 동전 투입, 알맹이 내보냄, 동전 반환 행동을 인터페이스라고 할 수 있음 행동들을 실행할 때 상태가 변경됨 알맹이를 꺼내는 행동은 기계 내에서 자체적으로 진행하는 행동에 가까움 상태 패턴 상태 패턴을 사용하면 객체의 내부 상태가 바뀜에 따라 객체의 행동을 바꿀 수 있다.\n\u0026ldquo;마치 객체의 클래스가 바뀌는 것과 같이\u0026rdquo;\n상태 패턴은 상태를 별도의 클래스로 캡슐화한 다음 현재 상태를 나타내는 객체에게 행동을 위임하므로 내부 상태가 바뀔 때 행동이 달라진다.\n또한 클래스가 구성으로 여러 상태 객체를 바꿔가며 사용하게되므로, 클라이언트 관점에서 현재 사용하는 객체의 행동이 완전히 달라져 마치 객체의 클래스가 바뀌는 것과 같은 결과를 얻을 수 있다.\nclassDiagram class Context { request() } class State { \u003c\u003c interface \u003e\u003e handle()* } class ConcreteStateA { handle() } class ConcreteStateB { handle() } Context --\u003e State State \u003c|-- ConcreteStateA State \u003c|-- ConcreteStateB Context: request()를 통해 상태의 handle()을 호출한다. State: 모든 구상 상태 클래스의 공통 인터페이스를 정의한다. 모든 상태 클래스에서 같은 인터페이스를 구현하므로 바뀌 가면서 사용할 수 있다. ConcreteState: Context로 부터 전달된 요청을 자기 나름의 방식으로 구현하여 처리한다. 이를 통해 Context에서 상태를 바꿀 때마다 행동도 바뀌게 된다. 상태 패턴은 객체의 내부 상태에 따라 수행되는 객체의 행위를 상태 객체로 캡슐화하여 객체의 상태를 표현하고 있는 클래스를 정의하고 상태에 따라 객체의 행동을 변경할 수 있도록 한다.\n이를 통해 객체의 상태를 변경할 때마다 직접 조건문이다 switch문을 사용하는 대신, 상태에게 해당 행동을 위임하여 코드의 유지보수성과 확장성을 향상시킨다.\n특징 장점\n새로운 상태를 추가하거나 상태를 변경할 때 기존 코드를 건드리지 않고 확장이 가능하다. 상태와 관련된 코드가 상태 객체에 캡슐화되어 있어 코드가 더 간결하고 읽기 쉬워진다. 상태 전환 로직이 각 상태에 캡슐화 되어있어 유지보수가 용이하다. 단점\n상태가 많을 경우 클래스의 수가 급격히 증가할 수 있고, 상태 간의 전이 로직이 복잡해질 수 있다. 전략 패턴과의 차이점 전략 패턴과 상태 패턴 모두 객체 간의 알고리즘을 정의하고, 이를 동적으로 교환할 수 있게 하는 구조적 디자인 패턴이지만, 목적과 사용 시나리오에서 차이가 있다.\n목적 전략 패턴 알고리즘의 변형이나 여러 알고리즘 중 하나를 선택해야할 때 사용 주로 알고리즘이나 전략 간의 상호 교환이 필요한 경우 적합 상태 패턴 객체의 내부 상태에 따라 행동이 달라져야 할 때 사용 객체가 상태에 따라 직접적으로 행동을 변경해야 하는 경우 유용함 관리 대상 전략 패턴 알고리즘, 전략, 행동을 캡슐화 객체가 특정 행동을 수행하는 데 사용되는 알고리즘을 변경하고 싶을 때 적용 상태 패턴 객체의 내부 상태를 캡슐화 객체가 내부 상태에 따라 행동을 변경하고 싶을 때 적용 구조적 차이 전략 패턴 Context는 Strategy 인터페이스를 가지며, 여러 ConreteStrategy를 가진다. Context는 전략을 동적으로 교환할 수 있다. 상태 패턴 Context는 State 인터페이스를 가지며, 여러 ConcreteState를 가진다. Context는 전략을 동적으로 교환할 수 있다. 전이 전략 패턴 전략이 변경되면 Context는 완전히 다른 알고리즘으로 전환된다. 상태 패턴 상태가 변경되면 Context의 행동이 변경되지만, 전체 시스템이 완전히 다른 상태로 이동하는 것은 아니다. 예시 전략 패턴 정렬 알고리즘을 선택하는 경우 결제 시스템에서 여러 결제 전략 중 하나를 선택하는 경우 상태 패턴 자동판매기의 동작을 모델링하는 경우 객체가 다양한 상태에 있을 때 각 상태에 따라 다른 행동을 하는 경우 정리 상태 패턴: 클래스에서 상태를 관리해야 한다면 상태 패턴을 써서 상태를 캡슐화 할 수 있다. 내부 상태가 바뀜에 따라 객체의 행동이 바뀔 수 있도록 해준다 마치 객체의 클래스가 바뀌는 것 같은 결과를 얻을 수 있다. 상태 패턴을 사용하면 내부 상태를 바탕으로 여러 가지 서로 다른 행동을 사용할 수 있다. 상태 패턴을 사용하면 프로시저형 상태 기계를 쓸 때와는 달리 각 상태를 클래스로 표현한다. Context 객체는 형재 상태에게 행동을 위임한다. 각 상태를 클래스로 캡슐화해서 나중에 변경해야 하는내용을 국지화할 수 있다. 상태 패턴과 전략 패턴의 클래스 다이어그램은 똑같지만 용도는 다르다. 전략 패턴에서 Context의 내부 상태가 바뀜에 따라 객체가 알아서 행동을 바꿀 수 있도록 할 수 있다. 상태 전환은 State 클래스로 제어할 수도 있고, Context 클래스로 제어할 수도 있다. State 클래스를 여러 Context 객체의 인스턴스에서 공유하도록 디자인할 수도 있다. ","date":"2023-12-05T15:48:08+09:00","image":"https://codemario318.github.io/post/gof/10/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/10/","title":"10. 객체의 상태 바꾸기 - 상태 패턴"},{"content":" 객체를 컬렉션에 추가하는 방법은 정말 다양하고, 클라이언트가 컬렉션에 들어있는 모든 객체에 일일이 접근하고 싶어하는 날이 올 것이다.\n그런 날이 오더라도 클라이언트에게 전부 보여 줄 필요는 없으며, 객체 저장 방식을 보여 주지 않으면서도 클라이언트가 객체에 일일이 접근할 수 있게 해줄 수 있다.\n그리고 한 방에 멋진 자료 구조를 만들 수 있는, 객체들로 구성된 슈퍼 컬렉션을 제공할 수 있다.\n반복자 패턴 반복자(iterator) 패턴은 컬렉션의 요소를 순차적으로 접근할 수 있는 방법을 제공하여 반복을 캡슐화한다.\n이 패턴을 통해 컬렉션의 내부 표현 방식에 독립적으로 요소에 접근할 수 있으며, 클라이언트 코드는 컬렉션 내부 구조의 세부 사항을 알 필요가 없게된다.\n효과\n컬렉션 객체의 모든 항목에 접근하는 방식이 통일되므로 종류에 관계 없이 모든 집합체에 사용할 수 있는 다형적인 코드를 만들 수 있다. 모든 항목에 일일이 접글ㄴ하는 작업을 컬렉션 객체가 아닌 반복자 객체가 맡게된다. 집합체의 인터페이스와 구현이 간단해지고 집합체는 객체 컬렉션 관리에만 전념할 수 있다. 구조 알아보기 classDiagram direction LR class Aggregate { \u003c\u003c interface \u003e\u003e createIterator()* Iterator } class ConcreateAggregate { createIterator() } class Iterator { \u003c\u003c interface \u003e\u003e hasNext()* Boolean next()* Object remove()* void } class ConcreateIterator { hasNext() next() remove() } class Client { } Aggregate \u003c|.. ConcreateAggregate Iterator \u003c|.. ConcreateIterator Aggregate \u003c|-- Client Client --|\u003e Iterator ConcreateAggregate --|\u003e ConcreateIterator Aggregate 인터페이스를 통해 클라이언와 객체 컬렉션의 구현을 분리할 수 있다. Iterator 모든 반복자가 구현해야 하는 인터페이스를 제공한다. 컬렉션에 들어있는 원소에 돌아가면서 접근할 수 있게 해 주는 메소드를 제공한다. ConcreteAggregate 객체 컬렉션이 들어있다. 그 안에 들어있는 컬렉션을 Iterator로 리턴하는 메소드를 구현한다. 모든 ConcreteAggregate는 그 안에 있는 객체 컬렉션을 대상으로 돌아가며 반복 작업을 처리할 수 있게 해주는 ConcreteIterator의 인스턴스를 만들 수 있어야 한다. ConcreteIterator 반복 작업 중에 현재 위치를 관리를 담당한다. 단일 역할 원칙 어떤 클래스에서 맡고 있는 모든 역할은 나중에 코드 변화를 불러올 수 있다.\n즉 역할이 2개 이상 있으면 바뀔 수 있는 부분이 2개 이상이 된다는 의미이다.\n집합체 내부 컬렉션 관련 기능과 반복자용 메소드 관련 기능을 전부 구현한다면 2가지 이유로 클래스가 바뀔 수 있다.\n컬렉션이 어떤 이유로 변경 반복자 관련 기능이 변경 이러한 이유로 어떤 클래스가 바뀌는 이유는 하나 뿐이어야 한다.\n응집도(cohesion)\n클래스 또는 모듈이 특정 목적이나 역할을 얼마나 일관되게 지원하는지를 나타내는 척도이다.\n응집도가 높다는 것은 서로 연관된 기능이 묶여있다는 것을 의미 응집도가 낮다는 것은 서로 상관없는 기능들이 묶여있다는 것을 의미 Java Iterable 인터페이스 알아보기 자바의 모든 컬렉션 유형에서 Iterable 인터페이스르 구현한다.\nclassDiagram class Iterable { \u003c\u003c interface \u003e\u003e iterator()* Iterator +forEach()* +spliterator()* } class Iterator { \u003c\u003c interface \u003e\u003e hasNext()* Boolean next()* Object +remove()* void } class Collection { \u003c\u003c interface \u003e\u003e add()* addAll()* clear()* contains()* containsAll()* equals()* hashCode()* isEmpty()* iterator()* remove()* removeAll()* retainAll()* size()* toArray()* } Iterable \u003c|-- Collection 어떤 클래스에서 Iterable을 구현한다면 그 클래스는 iterator() 메소드르 구현한다. 메소드는 Iterator 인터페이스를 구현하는 반복자를 반환한다. 이 인터페이스는 컬렉션에 있는 항목을 대상으로 반복 작업을 수행하는 방법을 제공하는 forEach() 메소드가 기본으로 포함된다. 컴포지트 패턴 컴포지트 패턴(Composite Pattern)은 객체들을 트리 구조로 구성하여 개별 객체와 복합 객체(그룹화된 객체)를 동일하게 다룰 수 있도록 하는 구조적인 디자인 패턴 중 하나이다.\n이 패턴을 사용하면 클라이언트 코드가 단일 객체와 복합 객체를 구별하지 않고 일관된 방식으로 다룰 수 있다.\n객체의 구성과 개별 객체를 노드로 가지는 트리 형태의 객체 구조를 만들 수 있다. 이런 복합 구조를 사용하면 복합 객체와 개별 객체를 대상으로 똑같은 작업을 적용할 수 있다. 복합 객체와 개별 객체를 구분할 필요가 거의 없어진다. classDiagram class Client { } class Component { \u003c\u003c abstract \u003e\u003e operation()* add(Component)* remove(Component)* getChild(int)* } class Leaf { operation() } class Composite { operation() add(Component) remove(Component) getChild(int) } Client --\u003e Component Component \u003c|-- Leaf Component \u003c|-- Composite Component \u003c-- Composite 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 // 1. Component 인터페이스 interface Component { void operation(); } // 2. Leaf 클래스 (단일 객체) class Leaf implements Component { private String name; public Leaf(String name) { this.name = name; } @Override public void operation() { System.out.println(\u0026#34;Leaf \u0026#34; + name + \u0026#34; operation\u0026#34;); } } // 3. Composite 클래스 (복합 객체) class Composite implements Component { private String name; private List\u0026lt;Component\u0026gt; children = new ArrayList\u0026lt;\u0026gt;(); public Composite(String name) { this.name = name; } public void add(Component component) { children.add(component); } public void remove(Component component) { children.remove(component); } @Override public void operation() { System.out.println(\u0026#34;Composite \u0026#34; + name + \u0026#34; operation\u0026#34;); for (Component child : children) { child.operation(); } } } // 클라이언트 코드 public class Client { public static void main(String[] args) { // Leaf 객체 생성 Leaf leaf1 = new Leaf(\u0026#34;Leaf 1\u0026#34;); Leaf leaf2 = new Leaf(\u0026#34;Leaf 2\u0026#34;); // Composite 객체 생성 및 Leaf 객체 추가 Composite composite = new Composite(\u0026#34;Composite 1\u0026#34;); composite.add(leaf1); composite.add(leaf2); // 두 개의 Leaf와 Composite를 모두 동일한 방식으로 다룸 composite.operation(); } } 컴포지트 패턴은 한 클래스에서 계층구조를 관리하는 일과 관련 작업을 처리하는 일 2가지 역할을 수행한다.\n컴포지트 패턴은 단일 역할 원칙을 깨는 대신 투명성을 확보하는 패턴이라고 할 수 있다.\n투명성(transparency)\nComponent 인터페이스에 자식들을 관리하는 기능과 잎으로써의 기능을 전부 넣어서 클라이언트가 복합 객체와 잎을 똑같은 방식으로 처리할 수 있도록 만들 수 있다.\n이를 통해 어떤 원소가 복합 객체인지 잎인지가 클라이언트에게 투명하게 보인다.\nComponent 클래스에는 두 종류의 기능이 모두 들어있다 보니 안전성은 약간 떨어진다.\n이런 문제는 디자인상의 결정 사항에 속하며, 다른 방향으로 디자인해서 여러 역할을 서로 다른 인터페이스로 분리할 수도 있다.\n어떤 원소에 부적절한 메소드를 호출하는 일이 일어나지 않을 것이고, 컴파일 중 혹은 실행 중 문제가 생기는 일을 예방할 수 있다. 그 대신 투명성이 떨어지게 되고, 코드에서 조건문이라든가 instanceof 연산자 같은 걸 써야한다. 정리 반복자 패턴 컬렉션의 구현 방법을 노출하지 않으면서 집합체 내의 모든 항복에 접근하는 방법을 제공한다. 컴포지트 패턴 객체를 트리 구조로 구성해서 부분-전체 계층 구조를 구현한다. 클라이언트에서 개별 객체와 복합 객체를 똑같은 방법으로 다룰 수 있다. 반복자를 사용하면 내부 구조를 드러내지 않으면서도 클라이언트가 컬렉션 안에 들어있는 모든 원소에 접근하도록 할 수 있다. 반복자 패턴을 사용하면 집합체를 대상으로 하는 반복 작업을 별도의 객체로 캡슐화할 수 있다. 반복자 패턴을 사용하면 컬렉션에 있는 모든 데이터를 대상으로 반복 작업을 하는 역할을 컬렉션에서 분리할 수 있다. 반복자 패턴을 쓰면 반복 작업에 똑같은 인터페이스를 적용할 수 있으므로 집합체에 있는 객체를 활용하는 코드를 만들 때 다형성을 활용할 수 있다. 한 클래스에는 될 수 있으면 한가지 역할만 부여하는 것이 좋다. 컴포지트 패턴은 개별 객체와 복합 객체를 모두 담아 둘 수 있는 구조를 제공한다. 컴포지트 패턴을 사용하면 클라이언트가 개별 객체와 복합 객체를 똑같은 방법으로 다룰 수 있다. 복합 구조에 들어있는 것을 구성 요소라고 부른다. 구성 요소에는 복합 객체와 잎 객체가 있다. 컴포지트 패턴을 적용할 때는 여러 장단점을 고려해야 한다. 상황에 따라 투명성과 안정성 사이에서 적절한 균형을 찾아야한다. ","date":"2023-12-01T11:42:08+09:00","image":"https://codemario318.github.io/post/gof/9/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/9/","title":"9. 컬렉션 잘 관리하기 - 반복자 패턴과 컴포지트 패턴"},{"content":" 탬플랫 메소드 패턴(Template Method Pattern)은 알고리즘의 골격을 정의한다.\n템플릿 메소드를 사용하면 알고리즘의 일부 단계를 서브클래스에서 구현할 수 있으며, 알고리즘의 구조는 그대로 유지하면서 알고리즘의 특정 단계를 서브클래스에서 재정의할 수도 있다.\n템플릿 메소드 패턴은 알고리즘의 템플릿을 만든다.\n템플릿이란?\n일련의 단계로 알고리즘을 정의한 메소드이다.\n여러 단계 가운데 하나 이상의 단계가 추상 메소드로 정의되며, 그 추상 메소드는 서브클래스에서 구현된다.\n이러한 방식을 통해 서브클래스가 일부분의 구현을 처리하면서도 알고리즘의 구조는 바뀌지 않는다. 구성 요소 classDiagram class AbstractClass { \u003c\u003c abstract \u003e\u003e templeteMethod() primitiveOpertion1()* primitiveOpertion2()* } class ConcreteClass { primitiveOperation1() primitiveOperation2() } class templateMethod { primtiveOperation1() primitiveOpertion2() } AbstractClass \u003c|-- ConcreteClass AbstractClass .. templateMethod AbstractClass: 알고리즘의 구조를 정의하며, 일부 단계는 추상 메소드로 선언하여 하위 클래스에게 위임한다. ConcreteClass: 추상 클래스에서 정의된 추상 메소드르 구현하여 알고리즘의 일부를 구체화한다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public abstract class CaffeineBeverage { final void prepareRecipe() { boilWater(); brew(); pourInCup(); addCondiments(); } abstract void brew(); abstract void addCondiments(); void boilWater() { // 메소드 구현 } void pourInCup() { // 메소드 구현 } } 템플릿 메소드는 알고리즘의 각 단계를 정의하며, 서브클래스에서 일부 단계를 구현할 수 있도록 유도하는데, 위 예시에서 카페인 음료를 만드는 알고리즘 템플릿 메소드는 prepareRecipe()이다.\n탬플릿 내에서 알고리즘의 각 단계가 메소드로 표현된다. 어떤 메소드는 CaffeineBerverage에서 처리된다. 어떠한 메소드는 서브클래스에서 처리된다. 서브클래스에서 구현해야 하는 메소드는 abstract로 선언해야한다. 템플릿 메소드 후크 후크(hook)는 추상 클래스에서 선언되지만 기본적인 내용만 구현되어 있거나 아무 코드도 들어있지 않은 메소드이다.\n이러한 방식으로 서브클래스는 다양한 위치에서 알고리즘에 끼어들 수 있다.(무시하고 넘어갈 수도 있다.)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public abstract class CaffeineBeverageWithHook { final void prepareRecipe() { boilWater(); brew(); pourInCup(); if (customerWantsCondiments()) { addCondiments(); } } abstract void brew(); abstract void addCondiments(); void boilWater() { // 메소드 구현 } void pourInCup() { // 메소드 구현 } boolean customerWantsCondiments() { return true; } } 후크 활용하기 후크를 사용하려면 서브클래스에서 후크를 오버라이드해야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 public class CoffeeWithHook extends CaffeineBeverageWithHook { public void brew() { System.out.println(\u0026#39;커피 우리기\u0026#39;); } public void addCondiments() { System.out.println(\u0026#34;이것 저것 추가\u0026#34;); } public boolean customerWantsCondiments() { String answer = getUserInput(); if (answer.toLowerCase().startsWith(\u0026#34;y\u0026#34;)) { return true; } else { return false; } } private String getUserInput() { String answer = null; System.out.println(\u0026#39;이것 저것 추가? (y/n)\u0026#39;); BufferedReader in = new BufferedReader(new InputStreamReader(System.in)); try { answer = in.readLine(); } catch (IOException ioe) { System.out.println(\u0026#34;IO Error!\u0026#34;); } if (answer == null) { answer = \u0026#34;no\u0026#34;; } return answer; } } 템플릿을 만들 때 추상 메소드를 써야할 때와 후크를 써야할 때를 어떻게 구분할 수 있는가? 서브클래스가 알고리즘이 특정 단계를 제공해야 한다면 추상 메소드를 써야한다. 알고리즘의 특정 단계가 선택적으로 적용된다면 후크를 쓴다. 후크를 쓰면 서브클래스에서 필요할 때 후크를 구현할 수도 있지만, 꼭 구현하지 않아도 된다. 후크의 용도 알고리즘에서 필수적이지 않은 부분을 서브클래스에서 구현하도록 만들고 싶을 때 템플릿 메소드에서 앞으로 일어날 일이나 막 일어난 일에 서브클래스가 반응할 수 있도록 기회를 제공하는 용도로 내부적으로 특정 목록을 재정렬한 후에 서브 클래스에서 특정 작업을 수행하도록 하고 싶은 때 등 서브클래스가 추상 클래스에서 진행되는 작업을 처리할 지 말지 결정하게 하는 기능을 부여하는 용도 등 모든 서브클래스에서 모든 추상 메소드를 정의해야한다. 템플릿 메소드에 있는 알고리즘의 단계 중에서 정의되지 않은 부분을 모두 채워줘야함 추상 메소드가 너무 많아지면 서브클래스에서 일일이 추상 메소드를 구현해야 하므로 좋지 않을 수 있다. 알고리즘의 단계를 너무 잘게 쪼개지 않는 것도 한 가지 방법이 도리 수 있다. 하지만 알고리즘을 큼직한 몇 가지 단계로만 나눠 놓으면 유연성이 떨어질 수 있다. 모든 단계가 필수는 아니므로 필수가 아닌 부분을 후크로 구현하면 추상 클래스의 서브클래스를 만들 때 부담이 줄 수 있다. 할리우드 원칙 할리우드 원칙(Hollywood Principle)\n먼저 연락하지 마세요. 저희가 연락 드리겠습니다.\n할리우드 원칙을 활용하면 의존성 부패(Dependency rot)를 방지할 수 있다.\n의존성 부패?\n어떤 고수준 구성 요소가 저수준 구성 요소에 의존하고, 그 저수준 구성 요소는 다시 고수준 구성 요소에 의존하게 되어 의존성의 복잡하게 꼬여있는 상황\n의존성이 부패하면 시스템 디자인이 어떤 식으로 되어있는지 알아보기 힘드므로, 저수준 구성 요소가 시스템에 접속할 수는 있지만 언제, 어떻게 구성 요소를 사용할지는 고수준 구성 요소가 결정해야한다.\n할리우드 원칙과 템플릿 메소드 패턴\n템플릿 메소드 패턴을 써서 디자인하면 고수준 요소에서 저수준 요소를 필요할 때만 호출하는 형식으로 구현되기 때문에 할리우드 원칙을 적용하게 된다.\n할리우드 원칙과 의존성 뒤집기 원칙\n의존성 뒤집기 원칙은 될 수 있으면 구상 클래스 사용을 줄이고 추상화된 것을 사용해야 한다는 원칙이라면, 할리우드 원칙은 저수준 구성 요소가 컴퓨테이션에 참여하면서도 저수준 구성 요소와 고수준 계층 간 의존을 없애도록 프레임워크나 구성 요소를 구축하는 기법이다.\n따라서 객체를 분리한다는 하나의 목표를 공유하지만, 의존성을 피하는 방법에 있어서 의존성 뒤집기 윈칙이 훨씬 더 강하고 일반적인 내용을 담고 있다.\n할리우드 원칙은 저수준 구성 요소를 다양하게 사용할 수 있으면서도 다른 클래스가 구성 요소에 너무 의존하지 않게 만들어 주는 디자인 구현 기법을 제공한다.\n템플릿 메소드 패턴과 전략 패턴의 차이점 템플릿 메소드 패턴과 전략 패턴 모두 알고리즘을 대상으로 하고있지만 다른 접근 방식을 취하고있다.\n템플릿 메소드 패턴은 알고리즘의 구조를 정의하고 일부 단계를 하위 클래스에 위임하여 확장성을 제공하며, 전략 패턴은 알고리즘을 캡슐화하고 동적으로 변경할 수 있도록 하는 데 중점을 둔다.\n목적 템플릿 메소드 패턴 주로 알고리즘의 구조를 정의하고 일부 단계를 하위 클래스에게 위임하여 확정성을 제공한다. 상위 클래스에서 알고리즘의 틀(템플릿)을 제공하고 하위 클래스에서 구체적인 단계를 구현한다. 전략 패턴 알고리즘을 캡슐화하고 해당 알고리즘을 동적으로 변경할 수 있도록 하는 데 사용된다. 알고리즘을 각각의 전략으로 정의하고 이를 동적으로 교체하여 사용한다. 구현 방식 템플릿 메소드 패턴 상위 클래스에는 알고리즘의 템플릿이 존재하며, 일부 단계는 추상 메소드로 선언되어 하위 클래스에서 구현된다. 하위 클래스에서는 알고리즘의 일부를 결정할 수 있다. 전략 패턴 알고리즘은 전략 인터페이스를 통해 정의도고, 각 전략은 해당 인터페이스를 구현한다. 인터페이스를 사용하므로 실행 시에 원하는 전략으로 교체할 수 있다. 확장성 템플릿 메소드 패턴 알고리즘의 구조를 확장하는 데 적합하며, 새로운 알고리즘을 추가하거나 기존 알고리즘을 변경할 때 유용하다. 전략 패턴 새로운 전략을 추가하거나 기존 전략을 변경할 때 확장성이 뛰어나다. 런타인에 동적으로 전략을 변경할 수 있다. 의존성 템플릿 메소드 패턴 상위 클래스와 하위 클래스 간에 강한 의존성이 있다. 하위 클래스에서는 상위 클래스의 구조를 따라야한다. 전략 패턴 컨텍스트와 전략 간에는 느슨한 결합이 있으며, 각 전략은 독립적으로 교체될 수 있다. Java API 속 템플릿 메소드 패턴 템플릿 메소드 패턴은 정말 많이 쓰이는 패턴이므로 쉽게 발견할 수 있다.\n하지만 교과서적인 구현과는 다른 템플릿 메소드도 많이 있어 주의 깊게 살펴보지 않으면 템플릿 메소드 패턴이 적용되어 있다는 사실도 모르고 넘어가기 쉽다.\nArrays 정렬 자바의 Arrays 클래스에는 정렬할 때 쓸 수 있는 편리한 템플릿 메소드가 포함되어 있다.\nArrays에 있는 정렬용 템플릿 메소드에서 알고리즘을 제공하지만, 특정 요소 비교 방법은 compareTo() 메소드로 구현해야한다.\ncompareTo() 메소드를 구현하기만 하면 템플릿 메소드 패턴으로 구현 된 정렬 알고리즘을 이용하여 배열의 항목들을 정렬해준다.\nJFrame JFrame은 가장 기본적인 스윙 컨테이너로 paint() 메소드를 상속받는 컨테이너이다.\npaint() 메소드는 후크 메소드라서 아무 일도 하지 않지만, 오버라이드하여 특정 화면 영역에 특정 내용을 표시하는 JFrame의 알고리즘에 사용자가 원하는 그래픽을 추가할 수 있다.\nAbstractList ArrayList, LinkedList 같은 자바의 리스트 컬렉션은 리스트에서 필요한 기능을 구현해 주는 AbstractList 클래스를 확장한다.\nAbstarctList에는 get(), size() 추상 메소드에 의존하는 subList() 템플릿 메소드가 있어, AbstarctList를 확장해서 나만의 리스트를 만들 때는 이 메소드를 구현해야 한다.\n정리 템플릿 메소드 패턴 알고리즘의 골격을 정의한다. 알고리즘의 일부 단계를 서브클래스에서 구현할 수 있다. 알고리즘의 구조는 그대로 유지하면서 알고리즘의 특정 단계를 서브클래스에서 재정의할 수도 있다. 템플릿 메소드 패턴은 코드 재사용에 큰 도움이 된다. 템플릿 메소드가 들어있는 추상 클래스는 구상 메소드, 추상 메소드, 후크를 정의할 수 있다. 추상 메소드는 서브클래스에서 구현한다. 후크는 추상 클래스에 들어있는 메소드로 아무 일도 하지 않거나 기본 행동만을 정의한다. 서브 클래스에서 후크를 오버라이드 할 수 있다. 할리우드 원칙에 의하면, 저수준 모듈을 언제 어떻게 호출할지는 고수준 모듈에서 결정하는 것이 좋다. 템플릿 메소드 패턴은 실전에서 자수 쓰이지만 교과서적인 방식으로 적용되진 않는다. 전략 패턴과 템플릿 메소드 패턴은 모두 알고리즘을 캡슐화하지만, 전략 패턴은 구성을, 템플릿 메소드 패턴은 상속을 사용한다. 팩토리 메소드 패턴은 특화된 템플릿 메소드 패턴이다. ","date":"2023-11-21T06:37:08+09:00","image":"https://codemario318.github.io/post/gof/8/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/8/","title":"8. 알고리즘 캡슐화하기 - 탬플릿 메소드 패턴"},{"content":" 특정 인터페이스가 필요한 디자인을 다른 인터페이스를 구현하는 클래스에서 필요할 때(호환되지 않는 인터페이스를 사용해야 할 때), 어댑터 패턴을 이용하면 실제와 다른 인터페이스를 가진 것 처럼 보이게 만들 수 있다.\n여기에 더해 퍼사드 패턴을 이용하면 객체를 감싸서 인터페이스를 단순화할 수 있다.\n어댑터 패턴 왜 어댑터 패턴일까? 어댑터 패턴에서 의미하는 어댑터(Adaptor)는 흔히 볼 수 있는 AC 전원 어댑터에 사용되는 의미와 같다.\nAC 전원 어댑터는 다른 규격의 플러그를 필요로하는 소켓에 사용할 수 있게 해주는 역할을 하는데, 객체지향 어댑터는 클라이언트에서 사용해야하는 인터페이스를(플러그)를 사용할 수 있는 인터페이스(소켓)에 맞는 형태로 적응시키는 역할을 수행한다.\n어댑터 패턴의 정의 **어댑터 패턴(Adapter Pattern)**은 인터페이스가 호환되지 않아 같이 쓸 수 없었던 클래스를 특정 클래스 인터페이스를 클라이언트에서 요구하는 다른 인터페이스로 변환하여 사용할 수 있게 도와준다.\n어댑터 패턴을 사용하면 호환되지 않는 인터페이스를 사용하는 클라이언트를 그대로 활용할 수 있다.\n클라이언트와 구현된 인터페이스를 분리할 수 있다. 변경 내역이 어댑터에 캡슐화되므로 나중에 인터페이스가 바뀌더라도 클라이언트를 바꿀 필요가 없다. classDiagram direction LR class Client { } class Target { \u003c\u003c interface \u003e\u003e request()* } class Adapter { request() } class Adaptee { specificRequest() } Client--\u003eTarget Target\u003c..Adapter Adapter--\u003eAdaptee 클라이언트는 타깃 인터페이스만 볼 수 있다. 어댑터에서 타깃 인터페이스를 구현한다. 어댑터는 어댑티로 구성되어있다. 모든 요청은 어댑티에 위임된다. 어댑터는 여러 객체지향 원칙을 반영하고 있다.\n어댑티를 새로 바뀐 인터페이스로 감쌀 때는 객체 구성(composition)을 사용한다. 이러한 접근으로 어댑티의 모든 서브클래스에 어댑터를 쓸 수 있다. 클라이언트를 특정 구현이 아닌 인터페이스에 연결한다. 서로 다른 백엔드 클래스로 변환시키는 여러 어댑터를 사용할 수도 있다. 인터페이스를 기준으로 구현하므로 타깃 인터페이스만 제대로 유지한다면 나중에 다른 구현을 추가하는 것도 가능하다. 클래스 어댑터 어댑터 패턴은 객체 어댑터와 클래스 어댑터로 두 종류가 있다.\n클래스 어댑터는 다중 상속이 가능해야 구현할 수 있다. 클래스 어댑터는 타깃과 어댑티 모두 서브클래스로 만들어서 사용하고, 객체 어댑터는 구성으로 어댑티에 요청을 전달한다.\nclassDiagram direction LR class Client { } class Target { request() } class Adapter { request() } class Adaptee { specificRequest() } Client--\u003eTarget Adapter--|\u003eTarget Adapter--|\u003eAdaptee 객체 어댑터\n구성을 사용하므로 어댑티 클래스와 그 서브클래스에 대해서도 어댑터 역할을 할 수 있다. 구성을 사용하므로 어댑티한테 필요한 일을 시키는 코드만 만들면 되므로 코드를 많이 쓸 필요가 없다. 구성을 사용하므로 유연성을 최대한 확보할 수 있다. 어댑터 코드에 어떤 행동을 추가하면 그 어댑터 코드는 어댑티 클래스와 더불어 모든 서브클래스에 그대로 적용된다. 클래스 어댑터\n특정 어댑티 클래스에만 적용할 수 있지만, 어댑티 전체를 다시 구현하지 않아도 된다는 장점이 있다. 서브클래스라서 어댑티의 행동을 오버라이드할 수 있으므로 코드 분량을 줄일 수 있다. 메소드를 빠르게 오버라이드 할 수 있다. Enumeration을 Iterator에 적응시키기 Enumeration\nclassDiagram class Enumeration { \u003c\u003c interface \u003e\u003e hasMoreElements()* nextElement()* } 자바의 초기 컬렉션 형식(Vector, Stack, Hashtable)은 Enumeration을 반환하는 elements() 메소드가 구현되어 있었는데, Enumeration 인터페이스를 사용하면 컬렉션의 각 항목이 어떻게 관리되는지 신경 쓸 필요 없이 컬렉션의 모든 항목에 접근할 수 있다.\nIterator\nclassDiagram class Iterator { \u003c\u003c interface \u003e\u003e hasNext()* next()* remove()* } 최근에는 컬렉션에 있는 일련의 항목에 접근하고, 그 항목을 제거할 수 있게 해주는 Iterator 인터페이스를 사용하기 시작했다.\nEnumeration 인터페이스를 사용하는 구형 코드를 다뤄야 할 때도 있지만 새로운 코드를 만들 때는 Iterator만 사용하는 게 좋다. 이럴 때 어댑터 패턴을 사용할 수 있다.\n어댑터 디자인하기\n타깃 인터페이스를 구현하고, 어댑티 객체로 구성된 어댑터를 구현해야한다.\nclassDiagram direction TB class Enumeration { \u003c\u003c interface \u003e\u003e hasMoreElements()* nextElement()* } class EnumerationIterator { hasNext() next() remove() } class Iterator { \u003c\u003c interface \u003e\u003e hasNext()* next()* remove()* } Iterator \u003c|.. EnumerationIterator Enumeration \u003c|-- EnumerationIterator hasNext()와 next() 메소드는 타깃에서 어댑티로 바로 연결되지만 remove() 메소드는 제공하지 않으므로 별도 구현이 필요하다.\nremove() 메소드 처리하기\n결론적으로 어댑터 차원에서 완벽하게 작동하는 remove() 메소드 구현 방법은 없다.\n그나마 가장 좋은 방법은 런타임 예외를 던지는 것 이다.\n이러한 상황을 대비하여 Iterator 인터페이스를 디자인 한 사람들은 UnsupportedOperationExcetpion을 지원하도록 구현하였다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public class EnumerationIterator implements Iterator\u0026lt;Object\u0026gt; { EnumerationIterator\u0026lt;?\u0026gt; enumeration; public EnumerationIterator(Enumeration\u0026lt;?\u0026gt; enumeration) { this.enumeration = enumeration; } public boolean hasNext() { return enumeration.hasMoreElements(); } public Object next() { return enumeration.nextElement(); } public void remove() { throw new UnsupportedOperationException(); } } 데코레이터 패턴과 어댑터 패턴 데코레이터 패턴과 어댑터 패턴은 모두 구조적 패턴(Structural pattern)에 속하는데, 기존 클래스들을 조합하여 새로운 기능을 제공하거나 인터페이스를 맞추는 데 사용할 수 있지만, 목적과 사용되는 상황에서 차이가 있다.\n데코레이터 패턴\n데코레이터 패턴은 객체에 동적으로 새로운 책임을 추가하거나 객체를 감싸서 행동을 확장하는 패턴이다.\n상속을 통해 기능을 확장하는 대신 객체를 감싸는 방식으로 기능을 추가하게된다. 클라이언트에게는 원래의 객체와 데코레이터로 감싼 객체를 동일하게 다루도록 한다. 어댑터 패턴\n어댑터 패턴은 서로 다른 인터페이스를 가진 두 클래스를 함께 동작하도록 만들어주는 패턴이다.\n기존의 코드를 수정하지 않고 새로운 인터페이스를 제공한다. 클라이언트 코드가 새로운 인터페이스를 사용할 수 있게 한다. 퍼사드 패턴 퍼사드 패턴은 서브시스템에 있는 일련의 인터페이스를 통합 인터페이스로 묶어준다.\n또한 고수준 인터페이스도 정의하므로 서브시스템을 더 편리하게 사용할 수 있다.\n퍼사드 패턴은 하나 이상의 클래스 인터페이스를 깔끔하면서도 효과적인 퍼사드(facade)로 덮어 인터페이스를 단순하게 바꾸기 위해 인터페이스를 변경한다.\n쓰기 쉬운 인터페이스를 제공하는 퍼사드 클래스를 구현함으로써 복잡한 시스템을 훨씬 편리하게 사용할 수 있다. 퍼사드는 인터페이스를 단순하게 만들고 클라이언트와 구성 요소로 이루어진 서브시스템을 분리하는 역할도 수행할 수 있다. 퍼사드와 어댑터는 모두 여러 개의 클래스를 감쌀 수 있지만 퍼사드는 인터페이스를 단순하게 만드는 용도로 쓰이는 반면, 어댑터는 인터페이스를 다른 인터페이스로 변환하는 용도로 쓰인다.\n활용 퍼사드 패턴을 사용하려면 어떤 서브시스템에 속한 일련의 복잡한 클래스를 단순하게 바꿔서 통합한 클래스를 만들어야 한다.\n다른 패턴과 달리 퍼사드 패턴은 복잡한 추상화가 필요 없으므로 상당히 단순한 편이다. 퍼사드 패턴을 사용하면 클라이언트와 서브시스템이 서로 긴밀하게 연결되지 않아도 되고, 다음 최소 지식 원칙을 준수하는데도 도움을 준다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 // 서브시스템 클래스 class CPU { public void processData() { System.out.println(\u0026#34;Processing data by CPU\u0026#34;); } } class Memory { public void load() { System.out.println(\u0026#34;Loading data into memory\u0026#34;); } } class HardDrive { public void readData() { System.out.println(\u0026#34;Reading data from hard drive\u0026#34;); } } // 퍼사드 클래스 class ComputerFacade { private CPU cpu; private Memory memory; private HardDrive hardDrive; public ComputerFacade() { this.cpu = new CPU(); this.memory = new Memory(); this.hardDrive = new HardDrive(); } public void start() { System.out.println(\u0026#34;Starting computer\u0026#34;); cpu.processData(); memory.load(); hardDrive.readData(); System.out.println(\u0026#34;Computer started\u0026#34;); } public void shutDown() { System.out.println(\u0026#34;Shutting down computer\u0026#34;); // 여러 서브시스템을 종료하는 로직 System.out.println(\u0026#34;Computer shut down\u0026#34;); } } // 클라이언트 코드 public class Main { public static void main(String[] args) { ComputerFacade computer = new ComputerFacade(); computer.start(); // 클라이언트는 퍼사드를 통해서만 컴퓨터를 다룰 수 있음 computer.shutDown(); } } 최소 지식 원칙 객체 사이의 상호작용은 될 수 있으면 아주 가까운 친구사이에만 허용하는 편이 좋다.\n시스템을 디자인할 때 어떤 객체든 그 객체와 상호작용을 하는 클래스의 개수와 상호작용 방식에 주의를 기울여야 한다.\n이 원칙을 잘 따르면 여러 클래스가 복잡하게 얽혀 있어 시스템의 한 부분을 변경했을 때 다른 부분까지 줄줄이 고쳐야 하는 상황을 미리 방지할 수 있다. 여러 클래스가 서로 복잡하게 의존하고 있다면 관리하기도 힘들고, 이해하기 어려운 불안정한 시스템이 만들어진다. 친구를 만들지 않고 다른 객체에 영향력 행사하기 최소 지식 원칙은 친구를 만들지 않는 4개의 가이드라인을 제시한다.\n객체 자체 메소드에 매개변수로 전달된 객체 메소드를 생성하거나 인스턴스를 만든 객체 객체에 속하는 구성 요소 메소드를 호출한 결과로 리턴받은 객체에 들어있는 메소드를 호출하면 다른 객체의 일부분에 요청하게되고, 직접적으로 알고 지내는 객체의 수가 늘어난다.\n이 상황에서 최소 지식 원칙을 따르려면 객체가 대신 요청하도록 만드러야하며, 이를 통해 객체의 한 구성 요소를 알고 지낼 필요가 없어진다.\n원칙을 따르지 않는 경우\n1 2 3 4 public float getTemp() { Thermometer thermometer = station.getThermometer(); return thermometer.getTemperature(); } 원칙을 따르는 경우\n1 2 3 public float getTemp() { return station.getTemperature(); } 최소 지식 원칙을 적용해서 thermometer에게 요청을 전달하는 메소드를 station 클래스에 추가하여 의존해야하는 클래스의 개수를 줄일 수 있다.\n절친에게만 메소드 호출하기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 public class Car { // 구성 요소의 메소드는 호출해도 좋음 Engine engine; // ... public Car() { // ... } public void start(Key key) { // 새로운 객체를 생성 // 이 객체의 메소드는 호출해도 좋음 Doors doors = new Doors(); // 매개변수로 전달된 객체의 메소드는 호출해도 좋음 boolean authorized = key.turns(); if (authorized) { // 구성 요소의 메소드를 호출해도 좋음 engine.start(); // 객체 내의 있는 메소드는 호출해도 좋음 updateDashboardDisplay(); // 직접 생성하거나 인스턴스를 만든 객체의 메소드는 호출해도 좋음 doors.lock(); } } public updateDashboardDisplay() { // 디스플레이 갱신 } } 최소 지식 원칙을 잘 따르면 객체 사이의 의존성을 줄일 수 있으며 소프트웨어 관리가 더 편해진다.\n하지만 적용하다 보면 메소드 호출을 처리하는 래퍼 클래스를 더 만들어야 할 수도 있으며 이에 따라 시스템이 복잡해지고, 개발 시간도 늘어나고, 성능도 떨어질 수 있다.\n퍼사드 패턴과 최소 지식 원칙 퍼사드 패턴에서 클라이언트의 친구는 퍼사드 클래스 하나 뿐이다. 정리 어댑터 패턴\n특정 클래스 인터페이스를 클라이언트에서 요구하는 다른 인터페이스로 변환한다. 인터페이스가 호환되지 않아 같이 쓸 수 없었던 클래스를 사용할 수 있게 도와준다. 퍼사드 패턴\n서브시스템에 있는 일련의 인터페이스를 통합 인터페이스로 묶어준다. 고수준 인터페이스도 정의하므로 서브시스템을 더 편리하게 사용할 수 있다. 기존 클래스를 사용하려고 하는데 인터페이스가 맞지 않으면 어댑터를 쓴다. 큰 인터페이스와 여러 인터페이스를 단순하게 바꾸거나 통합해야 하면 퍼사드를 쓴다. 어댑터는 인터페이스를 클라이언트에서 원하는 인터페이스로 바꾸는 역할을 한다. 퍼사드는 클라이언트를 복잡한 서브시스템과 분리하는 역할을 한다. 어댑터를 구현할 때는 타깃 인터페이스의 크기와 구조에 따라 코딩해야 할 분량이 결정된다. 퍼사드 패턴에서는 서브시스템으로 퍼사드를 만들고 진짜 작업은 서브클래스에 맡긴다. 어댑터 패턴에는 객체 어댑터 패턴과 클래스 어댑터 패턴이 있으며, 클래스 어댑터를 쓰러면 다중 상속이 가능해야 한다. 한 서브시스템에 퍼사드를 여러개 만들어도 된다. 어댑터는 객체를 감싸서 인터페이스를 바꾸는 용도로, 데코레이터는 객체를 감싸서 새로운 행동을 추가하는 용도로, 퍼사드는 일련의 객체를 감싸서 단순하게 만드는 용도로 쓰인다. ","date":"2023-11-16T09:55:08+09:00","image":"https://codemario318.github.io/post/gof/7/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/7/","title":"7. 어댑터 패턴과 퍼사드 패턴"},{"content":" 메소드호출을 캡슐화하면 계산 과정의 각 부분을 결정화할 수 있기에 계산하는 코드를 호출한 객체는 그 일이 어떤 식으로 처리되는지 전혀 신경 쓸 필요가 없어진다.\n커맨드 패턴 커맨드 패턴을 사용하면 요청 내역을 객체로 캡슐화해서 객체를 서로 다른 요청 내역에 따라 매개변수화할 수 있다. 이러면 요청을 큐에 저장하거나 로그로 기록하거나 작업 취소 기능을 사용할 수 있다.\n커맨드 객체는 일련의 행동을 특정 리시버와 연결함으로써 요청을 캡슐화한것이다.\n이를 위해 행동과 리시버를 한 객체에 넣고, excute()라는 메소드 하나만 외부에 공개하는 방법을 써야 한다. 이 메소드 호출에 따라 리시버에서 일련의 작업을 처리하며, 밖에서 볼 때는 어떤 객체가 리시버 역할을 하는지, 그 리시버가 어떤 일을 하는지 알 수 없다. 명령으로 객체를 매개변수화 할 수 있다.\n특정 인터페이스만 구현되어 있다면 그 커맨드 객체에서 실제로 어떤 일을 하는지 신경 쓸 필요가 없다. 기본적인 커맨드 패턴을 조금만 확장하면 큐와 로그를 구현하거나 작업 취소 하는 방법으로 활용될 수 있다.\n기본적인 커맨드 패턴을 제대로 사용할 수 있다면 메타 커맨드 패턴(Meta Command Pattern)도 어렵지 않게 구현할 수 있다.\n메타 커맨드 패턴을 사용하면 여러 개의 명령을 매크로로 한번에 실행할 수 있다. 커맨드 패턴은 어떤 것을 요구하는 객체와 그 요구를 받아들이고 처리하는 객체를 분리하는 객체지향 디자인 패턴의 한 모델이라고 볼 수 있다.\n어떤 작업을 요청하는 쪽과 그 작업을 처리하는 쪽을 분리할 수 있다. 커맨드 객체는 특정 객체에 관한 특정 작업 요청을 캡슐화해준다. 그렇게 객체를 분리하면 패턴이 실제로 어떻게 돌아가는지 파악하기가 조금 어려울 수 있다. flowchart LR a((\"클라이언트createCommandObject()\")) b((\"커맨드excute()\")) c((\"인보커setCommand()\")) d((\"커맨드execute()\")) e((\"리시버action1()action2()\")) a-- 1. createCommandObject() --\u003e b b-- 2. setCommand() --\u003e c a-. 3 .-\u003e c c-- excute() --\u003e d d-- action1(), action2() --\u003e e 클라이언트는 커맨드 객체를 생성해야 한다. 커맨드 객체는 리시버에 전달할 일련의 행동으로 구성된다. 커맨드 객체에는 행동과 리시버(Receiver)의 정보가 같이 들어있다. 커맨드 객체에서 제공하는 메소드는 excute() 하나 뿐이다. 이 메소드는 행동을 캡슐화하며, 리시버에 있는 특정 행동을 처리한다. 클라이언트는 인보커(Invoker) 객체의 setCommand() 메소드를 호출하는데, 이때 커맨드 객체를 넘겨준다. 그 커맨드 객체는 나중에 쓰이기 전까지 인보커 객체에 보관된다. 인보커에서 커맨드 객체의 excute() 메소드를 호출하면 리시버에 있는 행동 메소드가 호출된다. 인보커 로딩 클라이언트에서 커맨드 객체 생성 setCommand()를 호출해서 인보커에 커맨드 객체를 저장 나중에 클라이언트에서 인보커에게 그 명령을 실행하라고 요청 일단 어떤 명령을 인보커에 로딩한 다음 한번만 작업을 처리하고 커맨드 객체를 지우도록 할 수도 있고, 저장해 둔 명령을 여러 번 수행하게 할 수도 있다.\n기본 구조 classDiagram direction LR class Client class Invoker { setCommand() } class Receiver { action() } class Command { \u003c\u003c interface \u003e\u003e excute()* undo()* } class ConcreteCommand { execute() undo() } Client --\u003e Invoker Client --\u003e Receiver Client --\u003e ConcreteCommand Invoker --\u003e Command Receiver \u003c-- ConcreteCommand Command \u003c.. ConcreteCommand 커맨드 패턴 활용하기 커맨드로 컴퓨테이션(computation)의 한 부분(리시버와 일련의 행동)을 패키지로 묶어서 일급 객체 형태로 전달할 수 있다.\n클라이언트 애플리케이션에서 커맨드 객체를 생성한 뒤 오랜 시간이 지나도 그 컴퓨테이션을 호출할 수 있게 된다. 다른 스레드에서 호출할 수도 있다. 이러한 특성을 활용하여 커맨드 패턴을 수케줄러나 스레드 풀, 작업 큐와 같은 다양한 작업에 적용할 수 있다.\n작업 큐 큐 한 쪽 끝은 커맨드를 추가할 수 있도록 되어있고, 다른 쪽 끝에는 커맨드를 처리하는 스레드들이 대기하고 있다.\n각 스레드는 우선 execute() 메소드를 호출하고 호출이 완료되면 커맨드 객체를 버리고 새로운 커맨드 객체를 가져온다.\n작업 큐 클래스는 계산 작업을 하는 객체들과 완전히 분리되어 있고, 한 스레드가 처리를 하다 네트워크로 뭔가를 내려받을 수도 있다고 하더라도, 큐에 커맨드 패턴을 구현하는 객체를 넣으면 그 객체를 처리하는 스레드가 생기고 자동으로 excute()가 호출되므로 작업 큐 객체는 전혀 신경쓸 필요가 없다.\n커맨드 패턴 더 활용하기 어떤 어플래케이션은 모든 행동을 기록해 두었다가 어플리케이션이 다운되었을 때 그 행동을 다시 호출해서 복구할 수 있어야 한다.\n스프레드시트 등 커맨드 패턴을 사용하면 store()와 load() 메소드를 추가해서 이런 기능을 구현할 수 있다.\n자바에서는 직렬화로 구현할 수도 있지만 관련된 제약 조건으로 인해 쉽지 않다. classDiagram class Command { \u003c\u003c interface \u003e\u003e execute()* undo()* store()* load()* } flowchart LR a((Invoker)) b((Command1)) c((Command2)) d((Command3)) e[(storage)] a-- 1. execute() --\u003eb a-- 2. execute() --\u003ec a-- 3. execute() --\u003ed b-- store --\u003ee c-- store --\u003ee d-- store --\u003ee flowchart LR e[(storage)] b((Command1)) c((Command2)) d((Command3)) a((Invoker)) e-- load --\u003eb e-- load --\u003ec e-- load --\u003ed b-. \"1. execute()\" .-a c-. \"2. execute()\" .-a d-. \"3. execute()\" .-a 만능 IOT 리모컨 커맨드 인터페이스 구현 커맨드 객체는 모두 같은 인터페이스를 구현해야 한다.\n그 인터페이스에는 메소드가 하나 뿐이며 일반적으로 excute()라는 이름을 쓴다.\n1 2 3 public interface Command { public void excute(); } 커맨드 클래스 구현 classDiagram class Light { on() off() } 1 2 3 4 5 6 7 8 9 10 11 public class LightOnCommand implements Command { Light light; public LightOnCommand(Light light) { this.light = light; } public void excute() { light.on(); } } 커맨드 객체 사용하기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class SimpleRemoteControl { /** * 커맨드를 저장할 슬롯이 1개 있다. * 이 슬롯으로 1개의 기기를 제어한다. */ Command slot; public SimpleRemoteControl() {}; /** * 슬롯을 가지고 제어할 명령을 설정한다. * 리모컨 버튼의 기능을 바꾸고 싶다면 해당 메소드를 사용해서 얼마든지 바꿀 수 있다. */ public void setCommand(Command command) { slot = command; } /** * 버튼을 누르면 메소드가 호출되며, 슬롯에 연결된 커맨드 객체의 excute() 메소드만 호출하면 된다. */ public void buttonWasPressed() { slot.execute(); } } 간단한 테스트 클래스 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 /** * 커맨드 패턴에서 클라이언트에 해당하는 부분 */ public class RemoteControlTest { public static void main(String[] args) { /** * remote 변수가 인보커 역할을 한다. * 필요한 작업을 요청할 때 사용할 커맨드 객체를 인자로 전달받는다. */ SimpleRemoteControl remote = new SimpleRemoteControl(); /** * 요청을 받아서 처리할 리시버인 Light 객체를 생성한다. */ Light light = new Light(); /** * 커맨드 객체를 생서한다. 이때 리시버를 전달해준다. */ LightOnCommand lightOn = new LigthOnCommand(light); /** * 커맨드 객체를 인보커에게 전달해준다. */ remote.setCommand(lightOn); remote.buttonWasPressed(); } } 리모컨 코드 만들기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 public class RemoteControl { Command[] onCommands; Command[] offCommands; public RemoteControl() { onCommands = new Command[7]; offCommands = new Command[7]; Command noCommand = new NoCommand(); for (int i = 0; i \u0026lt; 7; i++) { onCommands[i] = noCommand; offCommands[i] = noCommand; } } public void setCommand(int slot, Command onCommand, Command offCommand) { onCommands[slot] = onCommand; offCommands[slot] = offCommand; } public void onButtonWasPushed(int slot) { onCommands[slot].execute(); } public void offButtonWasPushed(int slot) { offCommands[slot].execute(); } public String toString() { StringBuffer stringBuff = new StringBuffer(); stringBuff.append(\u0026#34;\\n----- 리모컨 -----\\n\u0026#34;); for (int i = 0; i \u0026lt; onCommands.length; i++) { stringBuff.append( \u0026#34;[slot \u0026#34; + i + \u0026#34;] \u0026#34; + onCommands[i].getClass().getName() + \u0026#34; \u0026#34; + offCommands[i].getClass().getName() + \u0026#34;\\n\u0026#34; ); } return stringBuff.toString(); } } NoCommand 객체 NoCommand 객체는 일종의 널 객체이다.\n널 객체는 딱히 리턴할 객체도 없고 클라이언트가 null을 처리하지 않게 하고 싶을때 활용할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class NoCommand implements Command { public void execute() {} } public class RemoteControl { /** * NoCommand 객체가 없는 경우 처리 예시 */ public void onButtonWasPushed(int slot) { if (onCommands[slot] != null) { onCommands[slot].execute(); } } } 만능 IOT 리모컨의 경우 명령이 아직 할당되지 않은 부분에 NoCommand 객체를 넣어 execute() 메소드가 호출되어도 문제가 생기지 않도록 했다.\n작업 취소 기능 추가하기 Command 클래스에 상태를 추가하여 작업을 취소 기능을 쉽게 추가할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 public interface Command { public void execute(); public void undo(); // 새로 추가 } public class LightOnCommand implements Command { // ... public void undo() { light.off(); } } public class LightOffCommand implements Command { // ... public void undo() { light.on(); } } public class RemoteControlWithUndo { // ... Command undoCommand; public RemoteControlWithUndo() { // ... undoCommand = noCommand; } // ... public void onButtonWasPushed(int slot) { onCommands[slot].execute(); undoCommand = onCommands[slot]; } public void offButtonWasPushed(int slot) { offCommands[slot].execute(); undoCommand = offCommands[slot]; } public void undoButtonWasPushed() { undoCommand.undo(); } } 상태를 활용한 작업 취소 기능 classDiagram class CeilingFan { high() medium() low() off() getSpeed() } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 public class CeilingFan { public static final int HIGH = 3; public static final int MEDIUM = 2; public static final int LOW = 1; public static final int OFF = 0; String location; int speed; public CeilingFan(String location) { this.location = location; speed = OFF; } public void high() { speed = HIGH; } public void medium() { speed = MEDIUM; } public void low() { speed = LOW; } public void off() { speed = OFF; } public int getSpeed() { return speed; } } public class CeilingFanHighCommand implements Command { CeilingFan ceilingFan; int prevSpeed; public CeilingFanHighCommand(CeilingFan ceilingFan) { this.ceilingFan; } public void execute() { prevSpeed = ceilingFan.getSpeed(); ceilingFan.high(); } public void undo() { if (prevSpeed == CeilingFan.HIGH) { ceilingFan.high(); } else if (prevSpeed == CeilingFan.MEDIUM) { ceilingFan.medium(); } else if (prevSpeed == CeilingFan.LOW) { ceilingFan.low(); } else if (prevSpeed == CeilingFan.OFF) { ceilingFan.off(); } } } 매크로 커맨드로 여러 동작 한번에 처리하기 1 2 3 4 5 6 7 8 9 10 11 12 13 public class MacroCommand implements Command { Command[] commands; public MacroCommand(Command[] commands) { this.commands = commands; } public void execute() { for (int i = 0; i \u0026lt; commands.length; i++) { commands[i].execute(); } } } 정리 커맨드패턴 요청 내역을 객체로 캡슐화해서 객체를 서로 다른 요청 내역에 따라 매개변수화 할 수 있다. 이러면 요청을 큐에 저장하거나 로그로 기록하거나 작업 취소 기능을 사용할 수 있다. 커맨드 패턴을 사용하면 요청하는 객체와 요청을 수행하는 객체를 분리할 수 있다. 분리하는 과정의 중심에는 커맨드 객체가 있으며, 객체가 행동이 들어있는 리시버를 캡슐화한다. 인보커는 무언가 요청할 때 커맨드 객체의 execute() 메소드를 호출하면된다. execute() 메소드는 리시버에 있는 행동을 호출한다. 커맨드는 인보커를 매개변수화 할 수 있다. 실행 중 동적으로 매개변수화를 설정할 수도 있다. execute() 메소드가 마지막으로 호출되기 전의 상태로 되돌리는 작업 취소 메소드를 구현하면 커맨드 패턴으로 작업 취소 기능을 구현할 수도 있다. 매크로 커맨드는 커맨드를 확장해서 여러 개의 커맨드를 한 번에 호출할 수 있게 해주는 가장 간편한 방법이다. 매크로 커맨드로도 어렵지 않게 작업 취소 기능을 구현할 수 있다. 프로그래밍을 하다 보면 요청을 스스로 처리하는 \u0026lsquo;스마트\u0026rsquo; 커맨드 객체를 사용하는 경우도 있다. 커맨드 패턴을 활용해서 로그 및 트랜잭션 시스템을 구현할 수 있다. ","date":"2023-11-12T14:01:08+09:00","image":"https://codemario318.github.io/post/gof/6/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/6/","title":"6. 커맨드 패턴 - 호출 캡슐화하기"},{"content":"싱글턴 패턴은 특정 클래스에 객체 인스턴스가 하나만 만들어지도록 해 주는 패턴이다.\n객체를 쓸 때 인스턴스가 2개 이상이면 프로그램이 이상하게 돌아가는 경우 자원을 불필요하게 사용하는 경우 결과에 일관성이 없어지는 경우 특정 상황에서는 객체가 1개만 있어도, 혹은 1개만 있어야 문제없이 수행되는 경우 활용하는 패턴이다.\n스레드 풀 캐시 대화상자 사용자 설정 레지스트리 설정을 처리하는 객체 로그 기록용 객체 디바이스 드라이버 전역 변수에 객체를 대입하면 애플리케이션이 시작될 때 객체가 생성되는데 그 객체가 자원을 많이 차지하면서 사용하지 않는 경우에도 제어할 수 없다.\n하지만 싱글턴 패턴을 활용하면 필요할 때만 객체를 만들 수 있다.\n고전적인 싱글턴 패턴 구현법 생성자를 private로 만들고 static 메소드를 통해 인스턴스를 얻도록 구현하기 때문에 생성자를 통해서 새로운 인스턴스를 얻을 수 있는 방법이 없다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class Singleton { private static Singleton uniqueInstance; private Singleton() {} public static Singleton getInstance() { if (uniqueInstance == null) { uniqueInstance; } return uniqueInstance; } // ... } 아직 인스턴스가 만들어지지 않았다면 private으로 선언된 생성자를 사용하여 Singleton 객체를 만든 다음 uniqueInstance에 그 객체를 대입한다.\n이렇게 처리한다면 인스턴스가 필요한 상황이 닥치기 전까지 아예 인스턴스를 생성하지 않게 된다.\n이러한 방법을 **게으른 인스턴스 생성(lazyinstantation)**이라고 부른다.\n싱글턴 패턴의 정의 **싱글턴 패턴(Singleton Pattern)**은 클래스 인스턴스를 하나만 만드록, 그 인스턴스로의 전역 접근을 제공한다.\n싱글턴 패턴을 실제로 정용할 때는 클래스에서 하나뿐인 인스턴스를 관리하도록 만들면 된다.\n그리고 다른 어떤 클래스에서도 자신의 인스턴스를 추가로 만들지 못하게 해야한다. 어디서든 해당 인스턴스에 접근할 수 있도록 전역 접근 지점을 제공한다.\n언제든 해당 인스턴스가 필요하면 클래스에 요청할 수 있게 만들어 놓고, 요청이 들어오면 하나뿐인 인스턴스를 건네주도록 한다. 자원을 많이 잡아먹는 인스턴스가 있다면 고전적인 싱글턴 처럼 게으른 방식으로 생성되도록 구현할 경우 유용할 수 있다. classDiagram class Singleton { static uniqueInstance: Singleton static getInstance() Singleton } 싱글턴 패턴을 사용할 때는 일반적인 클래스를 만들 때와 마찬가지로 다양한 데이터와 메소드를 사용할 수 있다.\n멀티 스레딩 문제 싱글턴 패턴을 사용할 때 인스턴스를 동시에 요청하는 상황에서 독립적인 인스턴스를 제공하지 못해 문제가 발생한다.\n이러한 문제는 자바는 getInstance에 synchronized 연산자를 추가하면, 메소드 사용 완료 후 요청하기 때문에 쉽게 해결이 가능하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 public class Singleton { private static Singleton uniqueInstance; private Singleton() {} public static synchronized Singleton getInstance() { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } return uniqueInstance; } } 더 효율적으로 멀티스레딩 문제 해결하기 synchronized 연산자를 getInstance에 적용하는 방법은 동기화로 인해 성능 저하가 발생할 수 있고, 싱글턴 패턴에서 멀티스레딩 문제가 발생하는 시점은 메소드가 시작되는 때 뿐이다.\nuniqueInstance 변수에 할당이 완료되면, 항상 새로운 인스턴스를 생성하지 않기 때문에 동기화 처리는 불필요한 오버헤드를 증가시킨다. getInstance 메소드의 성능이 중요하지 않다면 큰 문제가 이닐 수 있지만 메소드를 동기화하면 성능이 100배 정도 저하된다. 따라서 병목이 된다면 별도의 처리가 필요하다.\n인스턴스 처음부터 만들기 애플리케이션에서 Singleton의 인스턴스를 생성하고 계속 사용하거나 인스턴스를 실행 중에 수시로 만들고 관리하기가 번거롭다면 아래와 같은 방식으로 처음부터 인스턴스를 만들 수 있다.\n1 2 3 4 5 6 7 8 9 public class Singleton { private static Singleton uniqueInstance = new Singleton(); private Singleton() {} public static synchronized Singleton getInstance() { return uniqueInstance; } } 이 방법은 클래스가 로딩될 때 JVM에서 Singleton의 하나뿐인 인스턴스를 생성해주며, JVM에서 하나뿐인 인스턴스를 생성하기 전까지 그 어떤 스레드도 uniqueInstance 정적 변수에 접근할 수 없다.\nDCL을 통한 동기화 줄이기 **DCL(Double-Checked Locking)**을 사용하면 인스턴스가 생서오디어 있는지 확인한 다음 생성되어 있지 않았을 때만 동기화 할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public class Singleton { private volatile static Singleton uniqueInstance; private Singleton() {} public static Singleton getInstance() { if (uniqueInstance == null) { synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } volatile 키워드를 사용하면 멀티 스레딩을 쓰더라도 초기화되는 과정이 올바르게 진행된다. DCL은 자바 5보다 낮은 버전을 사용한다면 동기화가 제대로 안될 수 있다. 싱글턴 패턴의 문제 모든 메소드와 변수가 static으로 선언된 클래스를 만들어도 결과적으로 같지만 자바의 정적 초기화를 처리하는 방법으로 인해 디버깅이 어려울 수 있다. 다른 이름의 클래스 로더가 2개 이상이라면 같은 클래스르 ㄹ여러 번 로딩하게되어 싱글턴에 적용 시 인스턴스가 여러개 만들어 질 수 있다. 리플렉션, 직렬화, 역직렬화도 싱글턴에서 문제가 될 수 있다. Singleton에 의존하는 객체는 전부 하나의 객체에 단단하게 결합되므로 느슨한 결합 원칙에 위배된다. enum 활용하기 언급된 동기화 문데, 클래스 로딩 문제, 리플렉션, 직렬화와 역질렬화 문제 등은 enum으로 싱글턴을 생성해서 해결할 수 있다.\n1 2 3 4 5 6 7 8 9 public enum Singleton { UNIQUE_INSTANCE; } public class SingletonClient { public static void main(String[] args) { Singleton singleton = Singleton.UNIQUE_INSTANCE; } } 정리 싱글턴 패턴은 클래스 인스턴스를 하나만 만들고 그 인스턴스로의 전역 접근을 제공하는 방법. 어떤 클래스에 싱글턴 패턴을 적용하면 그 클래스의 인스턴스가 1개만 있도록 할 수 있다. 싱글턴 패턴을 사용하면 하나뿐인 인스턴스를 어디서든지 접근할 수 있도록 할 수 있다. 자바에서 싱글턴 패턴을 구현할 때는 private 생성자와 정적 메소드, 정적 변수를 사용한다. 멀티 스레드를 사용하는 애플리케이션에서는 속도와 자원 문제를 파악해보고 적절한 구현법을 사용한다. 사실 모든 애플리케이션에서 멀티스레딩을 쓸 수 있다고 생각해야한다. DCL을 써서 구현하면 자바 5 이전에 나온 버전에서는 스레드 관련 문제가 생길 수 있다. 클래스 로더가 여러 개 있으면 싱글턴이 제대로 작동하지 않고, 여러개의 인스턴스가 생길 수 있다. 자바의 enum을 쓰면 간단하게 싱글턴을 구현할 수 있다. ","date":"2023-11-05T18:01:08+09:00","image":"https://codemario318.github.io/post/gof/5/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/5/","title":"5. 싱글턴 패턴"},{"content":" 느슨한 결합으로 객체지향 디자인을 만들어봅시다.\n팩토리 패턴은 객체를 생성하는 부분을 분리하여 캡슐화 하는 방식으로 유연성과 확장성이 뛰어난 구조를 제공한다.\n팩토리 객체의 인스턴스를 만드는 작업이 항상 공개되어야 하는 것은 아니며, 오히려 모든 것을 공개했을때 결합 문제가 발생할 수 있다.\n팩토리 패턴은 불필요한 의존성을 없애서 결합 문제를 해결하는데 도움을 줄 수 있다.\nnew를 사용하면 구상 클래스의 인스턴스가 만들어진다.\n인터페이스나 추상클래스 같은 상위 개념으로 타입을 선언한다고 하더라도, 결과적으로 구상클래스(하위 개념)을 할당하기 때문에 특정 구현에 의존된다고 볼 수 있다.\n구상 클래스를 바탕으로 코딩하면 나중에 코드를 수정해야 할 가능성이 커지고, 유연성이 떨어진다. 여러 구상 클래스가 있고 특정 상황에서 선택하여 만들어야 할 상황이라면 아래와 같은 코드를 만들어야한다.\n1 2 3 4 5 6 7 8 9 10 /* 1장 예시 SimDuck 참조 */ Duck duck; if (picnic) { duck = new MallardDuck(); } else if (hunting){ duck = new DecoyDuck(); } else if (inBathTub) { duck = new RubberDuck(); } 이런 코드를 변경하거나 확장해야 할 때는 코드를 다시 확인하고 새로운 코드를 추가하거나 기존 코드를 제거해야 한다.\n따라서 이런한 방식으로 만들면 관리와 갱신이 어려워지고 오류가 생길 가능성도 커진다.\n근본 원인 new 연산자로 오브젝트를 만들때 문제가 발생하는 근본적인 원인은 변화 때문이다.\n인터페이스에 맞춰 코딩하면 시스템에서 일어날 수 있는 여러 변화에 대응할 수 있는 이유는 어떤 클래스든 특정 인터페이스만 구현하면 사용할 수 있는 다형성 덕뿐이다.\n이와 반대로 구상 클래스를 많이 사용하면 변경에 닫혀있기 때문에 새로운 구상 클래스가 추가될 때마다 즉, 변화할 때마다 코드를 고쳐야 하므로 많은 문제가 생길 수 있다.\n이 때문에 새로운 구상 형식을 써서 확장해야 할 때는 어떻게 해서든 다시 열 수 있게 만들어야 하며, 결과적으로 구상 클래스의 인스턴스 생성 부분을 분리해야 한다.\n이렇게 분리되어 객체 생성을 전담하는 영역을 팩토리라고 한다.\n의존성 뒤집기 원칙 디자인 원칙\n추상화된 것에 의존하게 만들고 구상 클래스에 의존하지 않게 만든다.\n의존성 뒤집기 원칙은 고수준 구성 요소가 저수준 구성 요소에 의존하면 안되며, 항상 추상화에 의존하게 만들어야 한다는 것을 의미한다.\n구현보다는 인터페이스에 맞춰 프로그래밍한다라는 원칙과 유사한 점이 많지만 의존성 뒤집기 원칙에서는 추상화를 더 많이 강조한다. 개발에서 고수준과 저수준의 기준은 기계가 이해할 수 있는가를 기준으로 설명하며, 기계쪽에 가까울수록 저수준을 의미한다.\n즉, 추상 클래스와 인터페이스 등을 이용한 추상화된 개념일수록 고수준이고, 컴퓨터가 해석해서 실행하게되는 코드 부분(구상 클래스)에 가까울수록 저수준이다. flowchart a([특정 기능을 선택해야하는 요소]) a --\u003e b([기능 A]) a --\u003e c([기능 B]) a --\u003e d([기능 C]) a --\u003e e([기능 D]) 팩토리 형식의 구현에서 위와 같이 고수준 구성 요소에서 분기를 통해 특정 구상 클래스를 직접 선택하는 형태는 저수준 요소에 의존하고 있기 때문에 구상 클래스의 종류가 추가되거나, 구상 클래스의 구현이 바뀐다면 의존하고있는 고수준 요소의 수정이 필요하다.\nflowchart BT a([특정 기능을 선택해야하는 요소]) f([기능 추상화]) b([기능 A]) --\u003e f c([기능 B]) --\u003e f d([기능 C]) --\u003e f e([기능 D]) --\u003e f f \u003c--\u003e a 따라서 분기를 통해 특정 구상 클래스를 선택해야 하는 구현을 피할 수 없다면 의존성 뒤집기를 통해 구상 클래스들의 공통되는 부분들을 모아 상위 요소(인터페이스, 추상 클래스)를 만들어 의존하게 팩토리를 구성하고, 해당 기능이 필요한 요소에서 만들어진 팩토리를 의존하도록 하는 방식을 제안하고 있다.\n의존성 뒤집기 원칙을 지키는 방법 다음 가이드라인을 따르면 의존성 뒤집기 원칙에 위배되는 객체지향 디자인을 피하는데 도움이된다.\n변수에 구상 클래스의 레퍼런스를 저장하지 않는다. new 연산자를 사용하면 구상 클래스의 레퍼런스를 사용하게된다. 구상 클래스에서 유도된 클래스를 만들지 않는다. 구상클래스에서 유도된 클래스를 만들면 특정 구상 클래스에 의존하게된다. 베이스 클래스에 이미 구현되어 있는 메소드를 오버라이드하지 않는다. 베이스 클래스가 제대로 추상화되지 않는다. 모든 서브클래스에서 공유할 수 있는 것만 정리가 필요하다. 간단한 팩토리 간단한 팩토리(Simple Factory)는 디자인 패턴이라기 보다는 프로그래밍에서 자주 쓰이는 관용구에 가깝다.\n간단한 팩토리는 단순히 객체 생성 부분을 전담한다.\n객체 생성 부분은 공통으로 사용될 수 있기 때문에 팩토리로 캡슐화해 놓으면 구현을 변경할 때 팩토리 하나만 고치면 된다. classDiagram class Client { useProduct() } class SimpleProductFactory { createProduct() } class Product { \u003c\u003c interface \u003e\u003e action()* } class ConcreteProductA { action() } class ConcreteProductB { action() } class ConcreteProductC { action() } Client ..|\u003e SimpleProductFactory SimpleProductFactory ..\u003e Product Product \u003c|-- ConcreteProductA Product \u003c|-- ConcreteProductB Product \u003c|-- ConcreteProductC Client: 팩토리를 사용하는 클라이언트 팩토리를 통해 Product의 구상 클래스 오브젝트를 받음 SimplePizzaFactory: Product 객체를 참조하는 팩토리 유일하게 구상 Product 클래스를 직접 참조함 Product: 팩토리에서 생산하는 구상 클래스들의 인터페이스 ConcreteProduct: 팩토리에서 생산하는 제품에 해당하는 구상 클래스 Product 인터페이스를 구현해야 한다. 구상 클래스여야 한다. 최첨단 피자 가게의 문제 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Pizza orderPizza(String type) { Pizza pizza; /** * 인스턴스를 만드는 구상 클래스를 선택 */ if (type.equals(\u0026#34;cheese\u0026#34;)) { pizza = new CheesePizza(); } else if (type.equals(\u0026#34;greek\u0026#34;)) { pizza = new GreekPizza(); } else if (type.equals(\u0026#34;pepperoni\u0026#34;)) { pizza = new PepperoniPizza(); } pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); return pizza; } orderPizza()에서 가장 문제가 되는 부분은 인스턴스를 만드는 구상 클래스를 선택하는 부분이다.\n위 코드에서 피자 가게의 메뉴를 변경하려면 직접 코드를 수정해야한다. 즉 변경에 닫혀있지 않다. 변경되는 부분인 객체 생성 영역을 별도로 분리할 수 있다.\n최첨단 피자 가게에 적용 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public class SimplePizzaFactory { public Pizza createPizza(String type) { if (type.equals(\u0026#34;cheese\u0026#34;)) { return new CheesePizza(); } else if (type.equals(\u0026#34;greek\u0026#34;)) { return new GreekPizza(); } else if (type.equals(\u0026#34;pepperoni\u0026#34;)) { return new PepperoniPizza(); } throw Exeption(); } } public class PizzaStore { SimplePizzaFactory factory; public Pizza orderPizza(String type) { Pizza pizza; pizza = factory.createPizza(type); pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); return pizza; } // ... } 팩토리 메소드 패턴 팩토리 메소드 패턴(Factory Method Pattern)에서는 객체를 생성할 때 필요한 인터페이스를 만든다.\n어떤 클래스의 인스턴스를 만들지는 서브 클래스에서 결정하기 때문에 클래스 인스턴스 만드는 일을 서브 클래스에게 맡기게 된다.\n모든 팩토리 패턴은 객체 생성을 캡슐화한다.\n팩토리 메소드 패턴은 팩토리를 별도 클래스로 분리하지 않고 생산 클래스 내부에 인터페이스로 팩토리 형식을 구현하도록 하여 객체 생성을 전담하는 방식으로 캡슐화를 수행한다.\n팩토리가 필요한 요소에서 자신의 메소드를 이용하여 팩토리를 구성하는 방식 classDiagram class Creator { \u003c\u003c abstract \u003e\u003e factoryMethod()* anOperation() } class ConcreteCreator { factoryMethod() } class Product { \u003c\u003c abstract \u003e\u003e } class ConcreteProduct Product \u003c|-- ConcreteProduct Creator \u003c|-- ConcreteCreator ConcreteCreator ..|\u003e ConcreteProduct Creator: 추상 클래스로 제품으로 원하는 일을 할 때 필요한 모든 메소드가 구현됨 하지만 제품을 만들어 주는 팩토리 메소드는 추상 메소드로 정의되어 있을 뿐 구현되어 있지 않음. 추상 메소드인 factoryMethod()을 서브클래스에서 직접 구현해야함. ConcreteCreator: 실제로 제품을 생산하는 factoryMetFhod()를 구현 구상 클래스 인스턴스를 만드는 일을 책임진다. 실제 제품을 만드는 방법을 알고 있는 클래스는 이 클래스 뿐이다. Product: 제품 클래스로 모두 똑같은 인터페이스를 구현해야 함 그 제품을 사용할 클래스에서 구상 클래스가 아닌 인터페이스의 레퍼런스로 객체를 참조하게 만들 수 있다. 1 2 3 4 5 6 7 8 9 10 11 12 13 public abstract class Creator { public Product anOperation(String type) { Product product; product = this.factoryMethod(type); /* somethimg */ return product; } protected abstract Product factoryMethod(String type); } Creator 추상 클래스에서 추상 메소드로 팩토리 메소드용 인터페이스(factoryMethod())를 제공한다.\nCreator 추상 클래스에 구현되어 있는 다른 메소드는 팩토리 메소드에 의해 생산된 제품으로 필요한 작업을 처리한다. 팩토리 메소드가 추상 메소드로 선언되었기 때문에 실제 팩토리 메소드를 구현하고 객체 인스턴스를 만드는 일은 서브클래스에서만 할 수 있다.\n팩토리 메소드를 추상 메소드로 선언해서 서브클래스가 객체 생성을 책임지도록 한다. 팩토리 메소드는 클라이언트에서 실제로 생성되는 구상 객체가 무엇인지 알 수 없게 만드는 역할도 겸하게된다. 팩토리 메소드는 특정 객체를 리턴하며, 그 객체는 보통 슈퍼클래스가 정의한 메소드 내에서 쓰인다. 매개변수로 만들 객체의 종류를 선택하게 할 수 있다. 생산자 추상 클래스가 실제 생산될 인스턴스를 전혀 알 수 없고, 사용하는 서브클래스에 따라 생산되는 객체 인스턴스가 정해지기 때문에, 어떤 클래스의 인스턴스를 만들지를 서브클래스에서 결정한다고 표현한다.\n병렬 클래스 계층구조 classDiagram direction TD namespace 생산자 클래스 { class Creator { \u003c\u003c abstract \u003e\u003e create()* Product useProduct() } class ConcreteCreatorA { create() Product } class ConcreteCreatorB { create() Product } } namespace 제품 클래스 { class Product { \u003c\u003c abstract \u003e\u003e } class ConcreteProductA1 class ConcreteProductA2 class ConcreteProductB1 class ConcreteProductB2 } Creator \u003c|-- ConcreteCreatorA Creator \u003c|-- ConcreteCreatorB Product \u003c|-- ConcreteProductA1 Product \u003c|-- ConcreteProductB1 ConcreteProductA1 .. ConcreteProductA2 ConcreteProductB1 .. ConcreteProductB2 ConcreteCreatorA \u003c--\u003e ConcreteProductA1 ConcreteCreatorB \u003c--\u003e ConcreteProductB1 ConcreteCreatorA \u003c--\u003e ConcreteProductA2 ConcreteCreatorB \u003c--\u003e ConcreteProductB2 Creator: 서브클래스에서 객체를 생산하려고 구현하는 팩토리 메소드의 추상 클래스 제품 클래스의 객체는 클래스의 서브클래스로 만들어지므로 생산자 자체는 어떤 구상 제품 클래스가 만들어질지 미리 알 수 없다. 생산자 클래스에 추상 제품 크래스에 의존하는 코드가 들어있을 때도 있다. ConcreteCreator: 팩토리 메소드로 해당 메소드에서 객체를 생산하는 구상 생산자. Product: 팩토리가 생산하게 될 제품의 인터페이스 ConcreteProduct: 팩토리가 생산하게 될 제품 구상 제품은 구상 생산자가 만들어야 할 많은 제품들이고, 구상 생산자 팩토리 메소드를 이용해 많은 구상 제품들 중 필요한 제품을 선택하게 된다.\n이처럼 특정 구상 생산자가 팩토리 메소드로 특정 제품군을 만드는 모든 방법을 캡슐화 되어있게되는 병렬 클래스 계층 구조를 가지게 된다.\n간단한 팩토리와 차이점 간단한 팩토리는 특정 생산자에서만 사용할 수 있는 반면, 팩토리 메소드 패턴은 더 유연하고 재사용 가능한 설계를 할 수 있다.\n생산자 클래스의 서브 클래스로 어떤 구상 제품을 만들지 결정하기 때문에 생성하는 제품을 마음대로 변경하기 쉽다. 추상 클래스의 내부 메소드 구현을 통해 동작을 일반화하여 여러 번 재사용이 가능한 프레임워크를 만들 수 있다. 피자 가게 프레임워크 만들기 팩토리 메소드 패턴을 통해 다양한 팩토리를 구성할 수 있고, 이를 통해 특색있는 피자 가게 지점을 만들 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 public abstract class Pizza { String name; String dough; String sauce; List\u0026lt;String\u0026gt; toppings = new ArrayList\u0026lt;String\u0026gt;(); void prepare() { System.out.println(\u0026#34;준비중: \u0026#34; + name); System.out.println(\u0026#34;도우 만들기...\u0026#34;); System.out.println(\u0026#34;소스 뿌리기...\u0026#34;); System.out.println(\u0026#34;토핑 올리기:\u0026#34;); for (String topping : toppings) { System.out.println(\u0026#34; \u0026#34; + topping); } } void bake() { System.out.println(\u0026#34;굽기\u0026#34;); } void cut() { System.out.println(\u0026#34;자르기\u0026#34;); } void box() { System.out.println(\u0026#34;담기\u0026#34;); } public String getName() { return name; } } public class NYStyleCheesePizza extends Pizza { public NYStyleCheesePizza() { name = \u0026#34;뉴욕 스타일 치즈 피자\u0026#34;; dough = \u0026#34;씬 크러스트\u0026#34;; sauce = \u0026#34;마리나라\u0026#34;; toppings.add(\u0026#34;레지아노 치즈\u0026#34;); } } public class ChicagoStyleCheesePizza extends Pizza { public NYStyleCheesePizza() { name = \u0026#34;시카고 스타일 치즈 피자\u0026#34;; dough = \u0026#34;두꺼운 크러스트\u0026#34;; sauce = \u0026#34;플럼 토마토\u0026#34;; toppings.add(\u0026#34;모짜렐라 치즈\u0026#34;); } @java.lang.Override void cut() { System.out.println(\u0026#34;네모난 모양으로 자르기\u0026#34;); } } public abstract class PizzaStore { public Pizza orderPizza(String type) { Pizza pizza; pizza = cratePizza(type); pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); return pizza; } protected abstract Pizza createPizza(String type); } public class NYPizzaStore extends PizzaStore { Pizza createPizza(String type) { if (item.equals(\u0026#34;cheese\u0026#34;)) { return new NYStyleCheesePizza(\u0026#34;cheese\u0026#34;); } else null; } } public class ChicagoPizzaStore extends PizzaStore { Pizza createPizza(String type) { if (item.equals(\u0026#34;cheese\u0026#34;)) { return new ChicagoStyleCheesePizza(\u0026#34;cheese\u0026#34;); } else null; } } 추상 팩토리 패턴 구상 클래스에 의존하지 않고도 서로 연관되거나 의존적인 객체로 이루어진 제품군을 생산하는 인터페이스를 제공한다.\n구상 클래스는 서브클래스에서 만들게된다.\n추상 팩토리 패턴을 사용하면 클라이언트에서 추상 인터페이스로 일련의 제품을 공급받을수 있다.\n이때, 실제로 어떤 제품이 생산되는지는 전혀 알 필요가 없다. 따라서 클라이언트와 팩토리에서 생산되는 제품을 분리할 수 있게된다.\nclassDiagram direction TD class Client { } class AbstractFactory { \u003c\u003c interface \u003e\u003e CreateProductA()* CreateProductB()* } class ConcreteFactory1 { CreateProductA() CreateProductB() } class ConcreteFactory2 { CreateProductA() CreateProductB() } class AbstractProductA { \u003c\u003c interface \u003e\u003e } class ProductA1 { } class ProductA2 { } class AbstractProductB { \u003c\u003c interface \u003e\u003e } class ProductB1 { } class ProductB2 { } Client --\u003e AbstractFactory Client --\u003e AbstractProductA Client --\u003e AbstractProductB AbstractFactory \u003c|-- ConcreteFactory1 AbstractFactory \u003c|-- ConcreteFactory2 AbstractProductA \u003c|-- ProductA1 AbstractProductA \u003c|-- ProductA2 AbstractProductB \u003c|-- ProductB1 AbstractProductB \u003c|-- ProductB2 ConcreteFactory1 --\u003e ProductA1 ConcreteFactory1 --\u003e ProductB1 ConcreteFactory2 --\u003e ProductA2 ConcreteFactory2 --\u003e ProductB2 추상 팩토리가 일련의 제품을 만드는 데 쓰이는 인터페이스를 정의하려고 만들어졌기 때문에 추상 팩토리 패턴의 메소드가 팩토리 메소드로 구현되는 경우도 종종 있다.\n해당 인터페이스에 있는 각 메소드는 구상 제품을 생산하는 일을 맡고, 추상 팩토리의 서브클래스를 만들어서 각 메소드의 구현을 제공하게된다.\n팩토리 패턴 활용법 어떤 패턴을 쓰든 객체 생성을 캡슐화해서 애플리케이션의 결함을 느슨하게 만드록, 특정 구현에 덜 의존하도록 만들 수 있다.\n팩토리 메소드 패턴, 추상 팩토리 패턴 모두 애플리케이션을 특정 구현으로부터 분리하는 역할을 하지만 방법이 다르기때문에 용도에 맞게 활용한다.\n팩토리 메소드 패턴: 클라이언트 코드와 인스턴스를 만들어야 할 구상 클래스를 분리시켜야 할 때 유용하다. 클래스를 써서 제품을 만든다. 상속을 이용하여 객체를 만든다. 서브클래스로 객체를 만들기 위해 클래스를 확장하고 팩토리 메소드를 오버라이드해야 한다. 자신이 사용할 추상 형식만 알면 되므로 클라이언트와 구상 형식을 분리하게된다. 한 가지 제품만 생산하므로 복잡한 인터페이스도 필요하지 않고 메소드도 하나만 있으면 된다. 추상 팩토리 패턴: 클라이언트에서 서로 연관된 일련의 제품을 만들어야 할 때, 즉 제품군을 만들어야 할 때 활용하기 좋다. 객체를 써서 제품을 만든다. 객체 구성(composition)을 이용하여 객체를 만든다. 제품군을 만드는 추상 형식을 제공하고 제품이 생상되는 방법을 이 형식의 서브클래스에서 정의한다. 팩토리를 사용하려면 인스턴스를 만든 다음 추상 형식을 써서 만든 코드에 전달하는 방식으로 클라이언트와 구상 제품을 분리한다. 제품군에 제품을 추가하는 등의 관련 제품을 확대해야 할 경우에 인터페이스를 바꿔야한다. 많은 제품들을 포함하는 제품 군을 생성하기 때문에 인터페이스가 아주 큰 편이다. 디자인 도구상자 안에 들어가야할 도구들 객체지향 원칙 추상화된 것에 의존하게 만드록 구상 클래스에 의존하지 않게 만든다. 추상 팩토리 패턴 구상 클래스에 의존하지 않고도 서로 연관되거나 의존적인 객체로 이루어진 제품군을 생성하는 인터페이스를 제공한다. 구상 클래스는 서브클래스에서 만든다. 팩토리 메소드 패턴 객체를 생성할 때 필요한 인터페이스를 만든다. 어떤 클래스의 인스턴스를 만들지는 서브클래스에서 결정한다. 팩토리 메소드를 사용하면 인스턴스 만드는 일을 서브클래스에 맡길 수 있다. 핵심 정리 팩토리를 쓰면 객체 생성을 캡슐화할 수 있다. 간단한 팩토리는 엄밀하게 말해서 디자인 패턴은 아니지만, 클라이언트와 구상 클래스를 분리하는 간단한 기법으로 활용할 수 있다. 팩토리 메소드 패턴은 상속을 활용한다. 객체 생성을 서브클래스에게 맡기고, 서브클래스는 팩토리 메소드를 구현해서 객체를 생산한다. 추상 팩토리 패턴은 객체 구성을 활용한다. 팩토리 인터페이스에서 선언한 메소드에서 객체 생성이 구현된다. 모든 팩토리 패턴은 애플리케이션의 구상 클래스 의존성을 줄여줌으로써 느슨한 결합을 도와준다. 팩토리 메소드 패턴은 특정 클래스에서 인스턴스를 만드는 일을 서브클래스에게 넘긴다. 추상 팩토리 패턴은 구상 클래스에 직접 의존하지 않고도 서로 관련된 객체로 이루어진 제품군을 만드는 용도로 쓰인다. 의존성 뒤집기 윈칙을 따르면 구상 형식 의존을 피하고 추상화를 지향할 수 있다. 팩토리는 구상 클래스가 아닌 추상 클래스와 인터페이스에 맞춰서 코딩할 수 있게 해주는 강력한 기법이다. ","date":"2023-10-29T13:29:08+09:00","image":"https://codemario318.github.io/post/gof/4/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/4/","title":"4. 팩토리 패턴"},{"content":"데코레이터 패턴은 객체 작성이라는 형식으로 실행 중에 클래스를 꾸미는 방법이다.\n데코레이터 패턴을 활용하면 기존 클래스 코드를 바꾸지 않고도 객체에 새로운 임무를 추가할 수 있다.\n데코레이터 패턴 데코레이터 패턴은 기존 코드를 건드리지 않고 확장으로 새로운 행동을 추가하는 것이 목적이다. 이를 통해 새로운 기능을 추가할 때 급변하는 주변 환경에 작 적응하는 유연하고 튼튼한 디자인을 만들 수 있다.\nOCP(Open-Close Principle) 살펴보기 클래스는 확장에는 열려있어야 하지만 변경에는 닫혀 있어야 한다.\n확정에는 열려있고 변경에는 닫혀있다는 것은 모순처럼 보일 수 있으나 코드를 변경하지 않아도 시스템을 확장하게 해 주는 객체지향 기법은 많다.(ex. 옵저버 패턴)\n데코레이터 패턴도 이 중 하나로 확장하려고 코드를 직접 수정하는 일을 방지하는 방법을 제공한다.\n모든 부분에서 OCP를 준수하는 것은 불가능하다.\nOCP를 준수하는 객체지향 디자인을 만들려면 많은 노력이 필요하고, 디자인의 모든 부분을 깔끔하게 정돈할 만큼 여유가 있는 상황도 흔치 않다(굳이 그렇게 할 필요가 없다).\n따라서 디자인한 것 중에서 가장 바뀔 가능성이 높은 부분을 중점적으로 살펴보고 OCP를 적용하는 방법이 가장 좋다.\n코드에서 확장해야 할 부분을 선택할 때는 세심한 주의가 필요하다.\n무조건 OCP를 적용한다면 쓸데없는 일을 하며 시간을 낭비할 수 있다. 필요 이상으로 복잡하고 이해하기 힘든 코드를 만드렉 될 수 있다. 데코레이터 패턴 살펴보기 이후 예시에서 설명할 스타버즈에서 음료 가격과 첨가물 가격을 합해 총 가격을 산출하는 방법은 좋은 방법이 아니었다.\n클래스가 매우 많아진다. 일부 서브클래스에는 적합하지 않은 기능을 추가해야 한다. 일단 기본 단위에서 시작하여 추가되는 요소들로 최소 단위를 장식(decorate)하는 방법을 고려할 수 있다.\n가장 기본이 되는 객체를 가져온다. 추가되는 요소들로 장식한다. 최종 결과물을 만들어내는 메소드를 호출한다. 추가 요소들의 결과를 만드는 일은 해당 객체에게 위임한다. 데코레이터 패턴은 위와 같은 방식을 추가되는 요소 객체로 래핑하는 방식으로 구현한다.\n데코레이터의 슈퍼클래스는 자신이 장식하고 있는 객체의 슈퍼클래스와 같다. 한 객체를 여러개의 데코레이터로 감쌀 수 있다. 데코레이터는 자신이 감싸고 있는 객체와 같은 슈퍼클래스를 가지고 있기에 원래 객체가 들어갈 자리에 데코레이터 객체를 넣어도 상관없다. 데코레이터는 자신이 장식하고 있는 객체에게 어떤 행동을 위임하는 일 말고도 추가 작업을 수행할 수 있다. 객체는 언제든 감쌀 수 있으므로 실행 중에 필요한 데코레이터를 마음대로 적용할 수 있다. 데코레이터 패턴의 정의 데코레이터 패턴으로 객체에 추가 요소를 동적으로 더할 수 있다.\n데코레이터를 사용하면 서브클래스를 만들 때보다 훨씬 유연하게 기능을 확장할 수 있다.\nclassDiagram class Component { methodA() methodB() } class ConcreteComponent { methodA() methodB() } class Decorator { Component wrappedObj methodA() methodB() } class ConcreteDecoratorA { methodA() methodB() newBehavior() } class ConcreteDecoratorB { Object newState methodA() methodB() newBehavior() } Component \u003c-- ConcreteComponent Component \u003c-- Decorator Component \u003c.. Decorator : 구성 요소 Decorator \u003c-- ConcreteDecoratorA Decorator \u003c-- ConcreteDecoratorB Component: 각 구성 요소는 직접 쓰일 수 있고 데코레이터에 감싸여 쓰일 수도 있다. ConcreteComponent: 새로운 행동을 동적으로 추가한다. Decorator: 자신이 장식할 구성 요소와 같은 인터페이스 또는 추상 클래스를 구현한다. 각 데코레이터 안에는 Component 객체가 들어있어야 하므로 구성 요소의 레퍼런스를 포함한 인스턴스 변수가 있다. ConcreteDecorator: 데코레이터가 감싸고 있는 Component 객체용 인스턴스 변수가 있으며 Component의 상태를 확장할 수 있다. 데코레이터가 사로운 메소드를 추가할 수도 있으나 일반적으로 새로운 메소드를 추가하는 대신 Component에 원래 있던 메소드를 별도의 작업으로 처리하여 새로운 기능을 추가한다. 데코레이터의 상속과 구성 데코레이터에서는 전략 패턴처럼 구성을 이용하여 행동을 분리하지 않고, 데코레이터로 감싸는 객체의 형식과 같게 구성하여, 상속을 통해 형식을 맞추게 된다.\n상속으로 행동을 물려받지 않고, 어떠한 구성 요소를 가지고 데코레이터를 만들 때 새로운 행동을 추가한다. 추상 클래스, 인터페이스 등으로 형식을 맞추고 상속을 통해 행동을 구현하도록 하여 구성을 이용하게 된다. 데코레이터를 감싸기 위해 만든 인스턴스 변수에 저장되는 데코레이터에 행동을 구현하여 할당하는 방식으로 구성을 이용한다. 객체의 구성(인스턴스 변수로 다른 객체를 저장하는 방식)을 이용하고 있으므로, 데코레이터가 다양하게 추가되어도 유연성을 잃지 않을 수 있게 된다. 상속만 써야 했다면 행동이 컴파일 시 슈퍼클래스에서 받은 것과 코드로 오버라이드 한 것만 쓸수 있게 정적으로 결정되어 버린다. 데코레이터가 감싼 데코레이터의 동작을 구현하는 것으로 구성을 활용하면 실행 중에 원하는 데코레이터를 마음대로 조합해서 사용할 수 있다. 데코레이터의 단점 java.io 패키지\njava.io 패키지는 파일에서 데이터를 읽어오는 스트림에 기능을 더하는 데코레이터를 사용하는 객체로 구성되어 있어 굉장히 많은 클래스가 있다.\n추상 데코레이터 클래스(추상 구성 요소) 역할을 수행하는 InputStream을 꾸미는 FilterInputStream와 FilterInputStream를 꾸미는 BufferedInputStream, ZipInputStream 을 꾸미는 형식으로 계속 확장하는 방식으로 설계되어 있다.\nflowchart BT a[InputStream] aa[FileInputStream] b[StringBufferInputStream] c[ByteArrayInputStream] d[FilterInputStream] e[PushbackInputStream] f[BufferedInputStream] g[DataInputStream] h[InflatorInputStream] i[ZipInputStream] i --\u003e h e --\u003e d f --\u003e d g --\u003e d h --\u003e d aa --\u003e a b --\u003e a c --\u003e a d --\u003e a 결과적으로 만들어져 있는 많은 클래스들이 InputSteram을 감싸 주는 래퍼 클래스로 작동하게 된다.\n잡다한 클래스가 너무 많아진다.\n데코레이터가 어떤 식으로 작동하는지 이해하면 다른 사람이 구현한 데코레이터 패턴을 활용해도 개발하기 쉽다. 잘 모든다면 활용하기 어렵다. 클래스가 어떤 식으로 구성되어 있는지를 먼저 파악해야 사용하기 쉽다. 특정 형식에 의존하는 코드에 데코레이터 패턴을 적용하면 엉망이 될 수 있다.\n데코레이터의 장점인 데코레이터를 끼워 넣어도 클라이언트는 데코레이터를 사용하고 있다는 사실을 전혀 알 수 없다는 장점을 누릴 수 없다. 구성 요소를 초기화하는 데 필요한 코드가 훨씬 복잡해진다.\n구성 요소 인스턴스만 만든다고 끊나지 않고 많은 데코레이터로 감싸야 하는 경우가 있다. 구상 구성 요소로 어떤 작업을 처리하는 코드에 데코레이터 패턴을 적용하면 코드가 제대로 작동하지 않고, 반대로 추상 구성 요소로 돌아가는 코드에는 데코레이터 패턴을 적용해야만 제대로된 결과를 얻을 수 있다.\n구상 구성 요소로 돌아가는 코드를 만들어야 한다면 데코레이터 패턴 사용을 다시 한번 생각해봐야한다. 데코레이터 패턴을 쓰면 관리해야 할 객체가 늘어나므로 실수할 가능성도 높아질 수 있다.\n실제로는 팩토리나 빌더 같은 다른 패턴으로 데코레이터를 만들고 사용하게된다. 이러한 패턴을 배운다면 데코레이터로 장식된 구상 구성 요소는 캡슐화가 잘 되므로 실수할 가능성을 줄이게된다. 초대형 커피 전문점, 스타버즈 최초 주문 시스템 classDiagram class Beverage { description getDescription() cost() } class HouseBlend { cost() } class DarkRoast { cost() } class Decaf { cost() } class Espresso { cost() } Beverage \u003c-- HouseBlend Beverage \u003c-- DarkRoast Beverage \u003c-- Decaf Beverage \u003c-- Espresso Beverage: 음료를 나타내는 추상 클래스 매장에서 판매되는 모든 음료는 이 클래스의 서브클라스임 description: 각 서브클래스에서 설정되는 음료 설명 cost(): 추상메소드로서 서브클래스에서 구현해야함 상속을 이용한 개선 고객이 커피를 주문할 때 우유나 두유, 모카 등 추가 항목을 얹기도 하므로 그때마다 가격이 올라가야한다.\n각각에 대응하는 서브클래스를 만드는 방법도 있지만, 인스턴스 변수와 슈퍼클래스 상속을 통해 개선될 수 있다.\nclassDiagram class Beverage { description +milk +soy +mocha +whip getDescription() cost() +has...() +set...() } class HouseBlend { cost() } class DarkRoast { cost() } class Decaf { cost() } class Espresso { cost() } Beverage \u003c-- HouseBlend Beverage \u003c-- DarkRoast Beverage \u003c-- Decaf Beverage \u003c-- Espresso 상속을 이용한 개선의 문제점 각각 옵션에 대응하는 서브클래스를 만드는 방법보다는 나은 방법이지만 여전히 문제점은 존재한다.\n첨가물 가격이 바뀔 때마다 기존 코드를 수정해야한다. 첨가물의 종류가 많아지면 새로운 메소드를 추가해야 하고, 슈퍼클래스의 cost() 메소드도 고쳐야한다. 특정 첨가물이 들어가면 안되는 음료가 추가된다면 막는 작업이 필요하다. 등 데코레이터 패턴 적용하기 다크로스트에 모카, 휘핑 추가를 수행하게 될 경우 아래와 같은 과정을 거쳐야한다.\nDarkRoast 객체에서 시작한다. Mocha 객체를 만들고 그 객체로 DarkRoast 객체를 감싼다. Whip 데코레이터를 만들어 Mocha를 감싼다. 가격을 계산한다. 가장 바깥쪽에 있는 데코레이터인 Whip의 cost()를 호출한다. Whip은 장식하고 있는 객체에게 가격 계산을 위임하고, 가격이 구해지면 계산된 가격에 휘핑크림의 가격을 더한 다음 결과값을 리턴한다. Berverage 클래스 장식하기 classDiagram class Beverage { description getDescription() cost() } class HouseBlend { cost() } class DarkRoast { cost() } class Decaf { cost() } class Espresso { cost() } Beverage \u003c-- HouseBlend Beverage \u003c-- DarkRoast Beverage \u003c-- Decaf Beverage \u003c-- Espresso class CondimentDecorator { getDescription() } Beverage \u003c-- CondimentDecorator Beverage \u003c.. CondimentDecorator : 구성 요소 class Milk { Beverage beverage cost() getDescription() } class Mocha { Beverage beverage cost() getDescription() } class Soy { Beverage beverage cost() getDescription() } class Whip { Beverage beverage cost() getDescription() } CondimentDecorator \u003c-- Milk CondimentDecorator \u003c-- Mocha CondimentDecorator \u003c-- Soy CondimentDecorator \u003c-- Whip 커피 주문 시스템 코드 만들기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 public abstract class Beverage { String description = \u0026#34;제목 없음\u0026#34;; public String getDescription() { return description; } public abstract double cost(); } /** * Beverage 객체가 들어갈 자리에 들어갈 수 있어야 하므로 Beverage 클래스를 확장한다. */ public abstract class CondimentDecorator extends Beverage { /** * 데코레이터가 감쌀 음료를 나타내는 Beverage 객체를 지정한다. * 음료를 지정할 때는 데코레이터에서 어떤 음료든 감쌀 수 있도록 슈퍼 클래스 유형을 이용한다. */ Beverage beverage; public abstract String getDescription(); } /** * Beverage 객체를 확장하여 기본 음료를 만든다. */ public class Espresso extends Beverage { public Espresso() { description = \u0026#34;에스프레소\u0026#34;; } @java.lang.Override public double cost() { return 1.99; } } /** * CondimentDecorator를 확장하여 데코레이터를 만든다. */ public class Mocha extends CondimentDecorator { /** * 감싸고자 하는 음료를 저장하는 데코레이터 인스턴스 변수를 초기화 한다. * @param beverage */ public Mocha(Beverage beverage) { this.beverage = beverage; } @java.lang.Override public String getDescription() { return beverage.getDescription() + \u0026#34;, 모카\u0026#34;; } @java.lang.Override public double cost() { return beverage.cost() + .20; } } 핵심 정리 객체지향 원칙 + 클래스는 확장에는 열려있어야 하지만 변경에는 닫혀 있어야 한다(OCP). 객체지향 패턴 + 데코레이터 패턴 객체에 추가 요소를 동적으로 더할 수 있다. 서브클래스를 만들 때보다 훨씬 유연하게 기능을 확장할 수 있다. 디자인의 유연성 면에서 보면 상속으로 확장하는 일은 별로 좋은 선택은 아니다. 기존 코드 수정 없이 행동을 확장해야 하는 상황도 있다. 구성과 위임으로 실행 중에 새로운 행동을 추가할 수 있다. 상속 대신 데코레이터 패턴으로 행동을 확장할 수 있다. 데코레이터 패턴은 구상 구성 요소를 감싸 주는 데코레이터를 사용한다. 데코레이터 클래스의 형식은 그 클래스가 감싸는 클래스 형식을 반영한다. 상속이나 인터페이스 구현으로 자신이 감쌀 클래스와 같은 형식을 가진다. 데코레이터는 자기가 감싸고 있는 구성 요소의 새로운 기능을 더함으로써 행동을 확장한다. 구성 요소를 감싸는 데코레이터의 개수에는 제한이 없다. 구성 요소의 클라이언트는 데코레이터의 존재를 알 수 없다. 클라이언트가 구성 요소의 구체적인 형식에 의존하는 경우는 예외 데코레이터 패턴을 사용하면 자잘한 객체가 매우 많이 추가될 수 있다. 데코레이터를 너무 많이 사용하면 코드가 필요 이상으로 복잡해진다. ","date":"2023-10-02T14:29:08+09:00","image":"https://codemario318.github.io/post/gof/3/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/3/","title":"3. 데코레이터 패턴"},{"content":"옵저버 패턴은 중요한 일이 일어났을 때 객체에게 새 소식을 알려 줄 수 있는 패턴이다.\n일대 다 관계나 느슨한 결합같은 개념을 통해 구성되며 자주 사용되는 패턴 중 하나이다.\n옵저버 패턴 옵저버 패턴은 주제(Subject), 옵저버(observer)로 구성된다.\nflowchart LR a((주제 객체)) subgraph 옵저버 객체 b((객체 1)) c((객체 2)) d((객체 3)) end a-.-\u003eb a-.-\u003ec a-.-\u003ed 이 책에서는 옵저버 패턴을 신문 구독을 예시로 들고 있다.\n신문사가 신문을 찍어낸다. 독자가 특정 신문사에 구독 신청을 하면 구독 해지 전까지 새로운 신문이 나올 때마다 배달을 받을 수 있다. 신문을 보고싶지 않으면 구독 해지 신청을 한다. 신문사가 망하지 않는 이상 여러 구독자들은 신문을 구독하거나 해지하는 것을 반복한다. 옵저버 패턴의 정의 옵저버 패턴은 일련의 객체 사이에서 일대다 관계를 정의하고, 한 객체의 상태가 변경되면 그 객체에 의존하는 모든 객체에 연락이 간다.\n옵저버 패턴은 한 객체의 상태가 바뀌면 그 객체에 의존하는 다른 객체에게 연락이 가고 자동으로 내용이 갱신되는 방식으로 일대다(one-two-many) 의존성을 정의한다.\n옵저버는 주제에 딸려 있으며 주제의 상태가 바뀌면 옵저버에게 정보가 전달된다. 보통 주제 인터페이스와 옵저버 인터페이스가 들어있는 클래스 디자인으로 구현한다. 옵저버 패턴의 구조 classDiagram direction LR class Subject { \u003c\u003c interface \u003e\u003e registerObserver() removeObserver() notifyObservers() } class Observer { \u003c\u003c interface \u003e\u003e update() } class ConcreteSubject { registerObserver() removeObserver() notifyObservers() getState() setState() } class ConcreteObserver { update() } Subject --\u003e Observer : 옵저버 ConcreteObserver ..\u003e Observer ConcreteSubject \u003c-- ConcreteObserver : 주제 Subject \u003c.. ConcreteSubject Subject 주제를 나타내는 인터페이스로 객체에서 옵저버로 등록하거나 옵저버 목록에서 탈퇴하고 싶을 때 해당 인터페이스의 메소드를 사용한다. Observer 옵저버가 될 가능성이 있는 객체는 만드시 Observer 인터페이스를 구현해야 한다. 주제의 상태가 바뀌었을 때 호출되는 update() 메소드로만 구성된다. ConcreteSubject 주제 역할을 하는 구상 클래스는 항상 Subject 인터페이스를 구현해야 한다. 주제 클래스에는 등록 및 해지용 메소드와 상태가 바뀔 때마다 모든 옵저버에게 연락하는 notifyObservers() 메소드도 구현해야 한다. Concreteobserver Observer 인터페이스만 구현한다면 무엇이든 옵저버 클래스가 될 수 있다. 각 옵저버는 특정 주제에 등록해서 연략 받을 수 있다. 출판-구독(publish-Subscribe) 패턴과의 차이점\n출판-구독 패턴은 구독자가 서로 다른 유형의 메시지에 관심을 가질 수 있고, 출판사와 구독자를 더 세세하게 분리할 수 있는 복잡한 패턴이다.\n느슨한 결합의 위력 느슨한 결합(Loose Coupling)은 객체들이 상호작용할 수는 있지만, 서로 잘 모르는 관계를 의미한다.\n느슨한 결합을 활용하면 유연성이 좋아진다. 옵저버 패턴은 느슨한 결합의 좋은 예시이다. 옵저버 패턴의 느슨한 결합\n주제는 옵저버가 특정 인터페이스(Observer)를 구현한다는 사실만 알고있다. 옵저버는 언제든지 추가할 수 있다. 주제는 Observer 인터페이스를 구현하는 객체의 목록에만 의존하므로 엔제든지 새로운 옵저버를 추가할 수 있다. 실행 중에 하나의 옵저버를 다른 옵저버로 바꿔도 주제는 계속해서 다른 옵저버에게 데이터를 보낼 수 있다. 새로운 형식의 옵저버를 추가할 때도 주제를 변경할 필요가 없다. 새로운 옵저버 클래스를 추가할 때 변경 없이 Observer 인터페이스만 구현한다면 어떤 객체에도 연락할 수 있다. 주제와 옵저버는 서로 독립적으로 재사용 할 수 있다. 둘이 서로 단단하게 결합되어 있지 않기 때문에 손쉽게 재사용 할 수 있다. 주제나 옵저버가 달라져도 서로에게 영향을 미치지는 않는다. 느슨하게 결합되어 있으므로 주제나 옵저버 인터페이스를 구현한다는 조건만 만족한다면 어떻게 고쳐도 문제가 생기지 않는다. +디자인 원칙: 상호작용하는 객체 사이에는 가능하면 느슨한 결합을 사용해야 한다.\n느슨하게 결합하는 디자인을 사용하면 상호의존성을 최소화 할 수 있기 때문에 변경 사항이 생겨도 무난히 처리할 수 있는 유연한 객체지향 시스템을 구축할 수 있다.\n풀 방식 기본적인 옵저버 패턴은 주제가 변경되었을 때 옵저버에게 알리는 방식(푸시)으로 처리되었다. 하지만 이러한 방식은 변경 반영이 필요없는 옵저버에게도 알리게 된다.\n이러한 문제가 존재한다면 옵저버가 필요할 때마다 데이터를 끌어오는 풀 방식을 통해 개선될 수 있다.\n값이 변했다는 알림을 옵저버가 받았을 때 주제에 있는 게터 메소드를 호출해서 필요한 값을 당겨오도록 변경한다.\n푸시와 풀은 구현 방법의 문제이지만 시간이 지남에 따라 애플리케이션이 계속 바뀌고 복잡해지므로, 대체로 옵저버가 필요한 데이터를 골라서 가져가도록 만드는 방법이 더 좋다.\n주제에서 알림 보내기\n1 2 3 4 5 6 7 class Subject { public void notifyObservers() { for (Observer observer: observers) { observer.update(); } } } 옵저버에서 알림 받기\nObserver 인터페이스에서 update() 메소드에 매개변수가 없도록 서명을 바꾼다. 1 2 3 public interface Observer { public void update(); } update() 메소드의 서명을 바꾸고 Subject 구상 주제 메소드의 게터로 날씨를 받아오도록 Observer 구상 클래스를 수정한다. 1 2 3 4 5 6 class ExtendObserver extends Observer { public void update() { this.item = ExtendSubject.getItem(); 이후_동작(); } } 디자인 원칙 달라지는 부분을 찾아내고 달라지지 않는 부분과 분리한다. 옵저버 패턴에서 변하는 것은 주제의 상태와 옵저버의 개수, 형식이다. 옵저버 패턴에서는 주제를 바꾸지 않고도 주제의 상태에 의존하는 객체들을 바꿀 수 있다. 나중에 바뀔 것을 대비해 두면 편하게 작업할 수 있다. 구현보다는 인터페이스에 맞춰 프로그래밍한다. 주제와 옵저버에서 모두 인터페이스를 사용했다. 상속보다는 구성을 활용한다. 옵저버 패턴에서는 구성을 활용해서 옵저버들을 관리한다. 주제와 옵저버 사이의 관계가 상속이 아닌 구성으로 이루어진다. 기상 모니터링 애플리케이션 요구 사항 분석 flowchart LR a((습도 센서)) b((온도 센서)) c((기압 센서)) d([기상 스테이션]) e((WeatherData 객체)) f[[디스플레이 장비]] a-.-\u003ed b-.-\u003ed c-.-\u003ed d\u003c-.데이터 취득.-e e-.화면에 표시.-\u003ef WatherData 객체를 바탕으로 만들어짐 현재 기상 조건(온도, 습도 기압) 추적 WatherData 객체를 바탕으로 3개 항목을 화면에 표시함 현재 조건, 기상 통계, 간단한 기상 예보 해당 항목들이 최신 측정치를 수집할 때마다 실시간으로 갱신 다른 개발자가 직접 날씨 디스플레이를 만덜어 바로 넣을 수 있도록 확장 가능해야 함 정보가 화면에 표시되는 횟수로 고객에가 요금 부과 구현 목표 디스플레이를 구현하고 새로운 값이 들어올 때마다(measurementsChanged() 호출 시) WeatherData에서 디스플레이를 업데이트 해야한다.\nclassDiagram direction RL class WeatherData { getTemperature() getHumidity() getPressure() measurementsChanged() } note for WeatherData \"기상 관측값 갱신시 measurementsChanged() 호출\" WeatherData 클래스에는 3가지 측정값(온도, 습도 기압)의 게터 메소드가 있다. 새로운 기상 측정 데이터가 들어올 때마다 measurementsChanged() 메소드가 호출됨 이 메소드가 어떤 식으로 호출되는지 모른다(알 필요도 없다) 가상 데이터를 사용하는 디스플레이 요소 3가지를 구현해야 한다. 현재 조건, 기상 통계, 기상 예보 디스플레이를 업데이트하도록 measurementsChanged()에 코드를 추가해야 한다. 추가 목표\n기상 스테이션이 성공하면 디스플레이가 더 늘어날 수도 있고, 디스플레이를 추가할 수 있는 마켓플레이스가 만들어질지도 모른다. 따라서 확장성을 고려한다면 좋을 수 있다.\n가상 스테이션용 코드 추가 1차적으로 다음과 같이 구현될 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class WeatherData { // 인스턴스 변수 선언 public void measurementsChanged() { // 최신 측정값 가져오기 float temp = getTemperature(); float humidity = getHumidity; float pressure = getPressure(); // 각 디스플레이 업데이트 currentConditionsDisplay.update(temp, humidity, pressure); statisticsDisplay.update(temp, humidity, pressure); forecastDisplay.update(temp, humidity, pressure); } // 기타 메소드 } 원칙으로 추가 코드 살펴보기 구체적인 구현(Bad) 각 디스플레이를 업데이트 하는 로직이 구체적인 구현에 맞춰져 있으므로 프로그램을 고치지 않고는 다른 디스플레이를 추가, 제거할 수 없다. 캡슐화 부재(Bad) 디스플에이를 업데이트 하는 로직은 바뀔 수 있는 부분으로 캡슐화가 필요함 공통된 인터페이스(Good) {객체}.update 메소드를 호출하는 것으로 업데이트를 하는 공통적인 인테페이스를 구성했음 기상 스테이션 설계하기 classDiagram direction LR class Subject { \u003c\u003c interface \u003e\u003e registerObserver() removeObserver() notifyObservers() } class Observer { \u003c\u003c interface \u003e\u003e update() } class DisplayElement { \u003c\u003c interface \u003e\u003e display() } class WeatherData { registerObserver() removeObserver() notifyObservers() getTemperature() getHumidity() getPressure() measurementsChanged() } class CurrentConditionsDisplay { update() display() } class StatisticsDisplay { update() display() } class ThirdPartyDisplay { update() display() } class ForecastDisplay { update() display() } Subject --\u003e Observer : 옵저버 WeatherData ..\u003e Subject CurrentConditionsDisplay --\u003e WeatherData : 주제 CurrentConditionsDisplay ..\u003e Observer StatisticsDisplay ..\u003e DisplayElement StatisticsDisplay ..\u003e Observer ThirdPartyDisplay ..\u003e DisplayElement ThirdPartyDisplay ..\u003e Observer ForecastDisplay ..\u003e DisplayElement ForecastDisplay ..\u003e Observer Subject: 주제 인터페이스 Observer: 옵저버 인터페이스, 주제에서 옵저버에게 갱신된 정보를 전달하는 방법 제공 DisplayElement: 모든 디스플레이 요소의 구현 인터페이스 WeatherData: Subject 인터페이스를 구현할 기상 정보 CurrentConditionsDisplay: WatherData 객체로부터 얻은 현재 측정값을 보여줄 옵저버 이면서 디스플레이 요소 StatisticsDisplay: 측정치의 통계치를 표시할 옵저버 이면서 디스플레이 요소 ForecastDisplay: 측정치를 바탕으로 기상 예보를 화면에 보여줄 디스플레이 요소 ThirdPartyDisplay: 새롭게 구현될 디스플레이 요소\u0026hellip; 디자인 도구상자 안에 들어가야 할 도구들 객체지향 기초 추상화 캡슐화 다형성 상속 객체지향 원칙 바뀌는 부분은 캡슐화힌다. 상속보다는 구성을 활용한다. 구현보다는 인터페이스에 맞춰 프로그래밍한다. + 상호작용을 하는 객체 사이에는 가능하면 느슨한 결합을 사용한다. 객체지향 패턴 전략패턴 + 옵저버 패턴 한 객체의 상태가 바뀌면 그 객체에 의존하는 다른 객체에게 연락이 가고 자동으로 내용이 갱신되는 방식으로 일대다 의존성을 정의한다. 핵심 정리 옵저버 패턴은 객체들 사이에 일대다 관계를 정의한다. 주제는 동일한 인터페이스를 써서 옵저버에게 연락한다. Observer 인터페이스를 구현하기만 하면 어떤 구상 클래스의 옵저버라도 패턴에 참여할 수 있다. 주제는 옵저버들이 Observer 인터페이스를 구현한다는 것을 제외하면 옵저버에 관해 전혀 모른다.(느슨한 결합) 옵저버 패턴을 사용하면 주제가 데이터를 보내거나(푸시) 옵저버가 데이터를 가져올(풀) 수 있다. 일반적으로 풀 방식을 옳은 방식으로 간주함 옵저버 패턴은 여러 개의 주제와 메시지 유형이 있는 복잡한 상황에서 사용하는 출판-구독 패턴과 친척이다. 옵저버 패턴은 자주 쓰이는 패턴으로 모델-뷰-컨트롤러(MVC)를 배울 때 다시 볼 수 있을것이다. GUI 프레임 워크들이 옵저버 패턴을 많이 사용한다. RxJava, 자바빈, RMI 외 코코아, 스위프트, JS 같은 다른 언어의 프레임워크에서도 옵저버 패턴을 많이 사용한다. ","date":"2023-09-27T13:09:08+09:00","image":"https://codemario318.github.io/post/gof/2/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/2/","title":"2. 옵저버 패턴"},{"content":"디자인 패턴 만나기 누군가가 이미 우리의 문제를 해결해 놓았다.\n디자인 패턴은 다른 개발자가 똑같은 문제를 경험하고 해결하면서 익혔던 지혜와 교훈이 담겨있다.\n디자인 패턴은 코드가 아닌 경험을 재사용 하는 것과 같다.\n디자인 원칙 애플리케이션에서 달리지는 부분을 찾아내고, 달라지지 않는 부분과 분리한다.\n달라지는 부분을 찾아 나머지 코드에 영향을 주지 않도록 캡슐화 한다. 이로 인해 코드를 변경하는 과정에서 의도치 않게 발생하는 일을 줄이며 시스템의 유연성을 향상시킬 수 있다. 코드에 새로운 요구 사항이 있을 때마다 바뀌는 부분이 있다면 분리해야한다.\n이 디자인 원칙은 다음과 같이 해석할수도 있다.\n바뀌는 부분은 따로 뽑아 캡슐화한다. 그러면 나중에 바뀌지 않는 부분에는 영향을 미치치 않고 그 부분만 고치거나 확장할 수 있다.\n이 개념은 매우 간단하지만 다른 모든 디자인 패턴의 기반을 이루는 원칙이다.\n모든 패턴은 시스템의 일부분을 다른 부분과 독립적으로 변화시킬 수 있는 방법을 제공한다.\n패턴과 전문 용어 패턴으로 소통하면 일상어로 구구절절 말할 때보다 훨씬 효율적인 의사소통을 할 수 있다.\n디자인 패턴은 개발자 사이에서 서로 모두 이해할 수 있는 용어를 제공한다.\n용어를 이애하고 나면 다른 개발자와 더 쉽게 대화할 수 있고, 패턴을 아직 모르는 사람들에게는 패턴을 배우고 싶은 생각이 들도록 자극을 수 있다.\n또한 자질구레한 객체 수준에서의 생각이 아닌, 패턴 수준에서 생각할 수 있기에 아키텍처를 생각하는 수준도 끌어올릴 수 있다.\n패턴으로 의사소통하면 패턴 이름과 그 패턴에 담겨있는 모든 내용, 특성, 제약조건 등을 함께 이야기 할 수 있다. 전략 패턴을 사용했다는 말은, 대상의 동작을 쉽게 확장하거나 변경할 수 있는 클래스들의 집합으로 캡슐화 했다는 내용을 간략하게 설명할 수 있다. 패턴을 사용하면 간단한 단어로 많은 얘기를 할 수 있다. 뭔가를 설명할 때 패턴을 사용하면 생각하고 있는 디자인을 다른 개발자가 빠르고 정확하게 파악할 수 있다. 패턴 수준에서 이야기하면 디자인에 더 오랫동안 집중할 수 있다. 패턴을 사용하여 객체와 클래스를 구현하는 것과 관련된 자질구레한 내용에 시간을 버릴 필요가 없어 디자인 수준에서 초점을 맞출 수 있다. 전문 용어를 사용하면 개발팀의 능력을 극대화 할 수 있다. 디자인 패턴 용어를 모든 팀원이 잘 알고 있다면 오해의 소지가 줄어 작업을 빠르게 진행할 수 있다. 전문 용어는 신입 개발자에게 훌륭한 자극제가 될 수 있다. 선배가 디자인 패턴을 사용하면 디자인 패턴을 배울 동기가 부여될 수 있다. 디자인 패턴 사용법 라이브러리와 프레임워크는 개발 모델 전반에 걸쳐서 많은 영향을 미친다. 하지만 라이브러리와 프레임워크를 사용한다는 것이 이해하기 쉽고, 관리하기 쉬운 유연한 방법으로 애플리케이션의 구조를 만드는 데 도움을 주지는 못한다.\n디자인 패턴은 라이브러리보다 더 높은 단계에 속한다.\n디자인 패턴은 클래스와 객체를 구성해서 어떤 문제를 해결하는 방법을 제공하는데, 그런 디자인을 특정 애플리케이션에 맞게 적용하는 일은 개발자의 몫이다.\n디자인 패턴은 라이브러리나 프레임워크가 도와주지 못하는 부분을 해결하는데 도움을 줄 수 있다.\n패턴을 완전히 익혀 두면 어떤 코드가 유연성 없이 엉망으로 꼬여있는 스파게티 코드라는 사실을 금방 깨닳을 수 있다. 코드를 수정할 때 패턴을 적용하여 코드를 개선할 수 있다. 객체지향 디자인 원칙과 디자인 패턴 캡슐화, 추상화, 상속, 다형성을 잘 알고 있고 활용할 수 있다고 하더라도 유연하고, 재사용이 용이하고, 관리하기 쉬운 시스템을 쉽게 만들기는 어려운 일이다.\n디자인 패턴은 간단하지만은 않은 객체지향 시스템 구축 방법들을 모아서 정의한 말그대로 패턴이다.\n따라서 디자인 패턴을 잘 알고 있다면, 비교적 적은 수고로 제대로 작동하는 디자인을 만들 수 있게된다.\n디자인은 예술이다.\n장점이 있으면 단점도 있지만, 많은 사람이 오랜 시간 동안 고민해서 찾아낸 디자인 패턴을 잘 따른다면 훨씬 좋은 디자인을 만들 수 있다.\n또한 패턴의 밑바탕에는 객체지향 패턴이 있으므로 원칙을 알고 있다면 문제에 딱 맞는 패턴을 찾을 수 없을 때에 적절한 디자인을 만드는데 도움을 줄 수 있다.\n디자인 도구상자 안에 들어가야 할 도구들 객체지향 기초 추상화 캡슐화 다형성 상속 객체지향 원칙 바뀌는 부분은 캡슐화 한다. 상속보다는 구성을 활용한다. 구현보다는 인터페이스에 맞춰서 프로그래밍한다. 등 객체지향 패턴 전략패턴 등 시나리오 시스템을 처음 디자인 할때 표준 객체지향 기법을 사용하여 슈퍼클래스를 만든 다음, 그 클래스를 확장하여 서로 다른 종류의 클래스를 만들었다.\n이후 새로운 기능을 요구하는 상황이 발생하여 슈퍼클래스에 해당 기능을 수행하는 메소드를 추가하였으나, 슈퍼클래스를 상속받는 모든 클래스들이 메소드가 적용됨에 따라 해당 기능이 필요없는 모든 클래스에서 해당 기능이 적용되 결국 오류를 만들게 되었다.\n이에 따라 해당 기능이 필요없는 서브 클래스의 추가된 슈퍼클래스의 메소드를 오버라이드하여 사용할 수 없게 막았다.\n하지만 이런 상황으로 인해 서브클래스에서 중복이 많이 발생할 수 있었고, 지속적으로 새로운 기능을 추가하기로 함에 따라 특정한 기능을 묶어 인터페이스를 설계하였다.\n문제 서브클래스에 새로운 기능을 추가할 때 모든 서브클래스에 필요한 기능이 아니므로 상속으로 처리하는 것은 올바른 방법이 아니다.\n서브클래스에서 인터페이스를 구현하여 일부 문제점은 해결할 수 있지만, 코드를 재사용하지 않으므로 코드 관리에 커다란 문제가 생긴다. 서브클래스에 필요한 모든 기능들에 대해서 인터페이스를 만드는 방식도 서브클래스마다 구현이 필요하므로 좋은 해결방법이 아니다. 이러한 문제는 디자인 패턴의 적용으로 해결할 수 있다.\n문제를 명확하게 파악하기 위 시나리오에서 새로운 기능 추가를 위해 상속을 활용하는 것은 모든 서브클래스에서 한 가지 기능만 사용하도록 하는 방법이기 때문에 최선의 해결책이 아니다.\n또한 인테페이스를 사용하는 방법은 괜찮아 보이지만, 인터페이스에는 구현된 코드가 없으므로 코드를 재사용할 수 없다는 문제점이 있다.\n한가지 행동을 바꿀때마다 그 행동이 정의되어 있는 서로 다른 서브 클래스를 전부 찾아서 코드를 일일이 고쳐야한다. 그 과정으로 인해 새로운 버그가 생길 가능성이 높다. 바뀌는 부분과 그렇지 않은 부분 분리하기 변화하는 부분과 그대로 있는 부분을 분리하려면 2개의 클래스 집합을 만든다. 각 클래스의 집합에는 각각의 행동을 구현한 것을 전부 집어넣는다. 변화하는 기능을 슈퍼클래스에서 모두 분리하여 각 행동을 나타낼 클래스 집합을 새로 만들어야 한다.\nflowchart LR a((슈퍼클래스))--\u003eb a--\u003ec subgraph 달라지는 기능 b([메소드 구현 1]) c([메소드 구현 2]) end 달라지는 기능을 디자인하는 방법 달라지는 기능을 구현하는 클래스 집합은 최대한 유연하게 만들고, 인스턴스에 기능을 할당할 수 있어야 한다. 이를 위해 각 행동은 인터페이스로 표현하고 인테페이스를 사용해 행동을 구현한다.\n시나리오에서는 슈퍼클래스에서 구체적으로 구현하거나 서브클래스 자체에서 별도로 구현하는 방법에서 항상 특정 구현에 의존한다. 서브클래스는 인터페이스로 표현되는 행동을 사용하기 때문에 실제 행동 구현이 슈퍼클래스를 활용하는 서브클래스에게만 국한되지 않는다. 인터페이스로 인해 서브클래스마다 해당 기능을 구현해야하는 문제점이 있다. 실제 실행 시에 쓰이는 객체가 코드에 고정되지 않도록 상속이 아닌 상위 형식(super type)에 맞춰 프로그래밍해서 다형성을 활용해야 한다는 점에서 인터페이스에 맞춰서 프로그래밍한다는 말은 사실 상위 형식에 맞춰 프로그래밍한다는 의미이다.\n변수를 선언할 때 보통 추상 클래스나 인테페이스 같은 상위 형식으로 선언해야 한다. 객체를 변수에 대입할 때 상위 형식을 구체적으로 구현한 형식이라면 어떤 객체든 넣을 수 있다. 행동 통합하기 특정 행동을 슈퍼클래스 또는 서브클래스에서 정의한 메소드를 써서 구현하지 않고 다른 클래스에 위임한다.\nclassDiagram class Duck { + FlyBehavior flyBehavior + QuackBehavior quackBehavior + performQuack() - quack() swim() display() + performFly() - performFly() } 1 2 3 4 5 6 7 public abstract class Duck { QuackBehavior quackBehavior; public void performQuack() { quackBehavior.quack(); } } 특정 행동을 해고 싶을땐 인테페이스에 의해 참조되는 객체에서 동작하는 방식으로 위임할 수 있다.\n동적으로 행동 지정하기 Setter를 이용하여 위임한 다른 클래스를 설정한다면 동적으로 변경될 수 있다.\n전략 패턴 알고리즘군을 정의하고 캡슐화하여 각각의 알고리즘군을 수정해서 쓸 수 있게 해준다. 전략패턴을 사용하면 클라이언트로부터 알고리즘을 분리해서 독립적으로 변경할 수 있다.\n여러 알고리즘 또는 동작을 동적으로 선택하고 사용해야 할 때 클래스의 행동을 변경하고 확장하기 쉬운 구조를 갖추고자 할 때 코드 중복을 방지하고 재사용성을 높힐때 구조 전략(Strategy): 다양한 알고리즘 또는 동작을 나타내는 인터페이스 또는 추상 클래스 이 인터페이스를 구현하는 여러 전략 클래스가 존재함 전략 컨텍스트(Strategy Context): 전략 객체를 사용하는 클래스로, 전략을 변경하고 실행하는 역할을 수행 컨텍스트 객체는 전략 객체를 가지며 필요에 따라 전략을 바꿀 수 있다. 전략 구체 클래스(Concrete Strategy): 전략 인터페이스를 구현한 구체 클래스들 각 클래스는 특정한 알고리즘이나 동작을 구현 캡슐화된 행동 살펴보기 classDiagram class Duck { FlyBehavior flyBehavior QuackBehavior quackBehavior swim() display() performQuack() performFly() setFlyBehavior() setQuackBehavior() } class MallardDuck { display() } class RedheadDuck { display() } class RubberDuck { display() } class DecoyDuck { display() } Duck \u003c|-- MallardDuck Duck \u003c|-- RedheadDuck Duck \u003c|-- RubberDuck Duck \u003c|-- DecoyDuck class ImpFlyBehavior { fly() } class FlyWithWings { fly() // 나는 행동 구현 } class FlyNoWay { fly() // 아무것도 하지 않음 } ImpFlyBehavior\u003c|--Duck ImpFlyBehavior\u003c|--FlyWithWings ImpFlyBehavior\u003c|--FlyNoWay class ImpQuackBehavior { quack() } class Quack { quack() // 소리 내는 행동 구현 } class Squeak { quack() // 고무 오리 소리 구현 } class MuteQuack { quack() // 아무 소리 내지 못함 } ImpQuackBehavior\u003c|--Duck ImpQuackBehavior\u003c|--Quack ImpQuackBehavior\u003c|--Squeak ImpQuackBehavior\u003c|--MuteQuack 행동들을 알고리즘군(family of algorithms)으로 생각하고 위처럼 행동을 상속받는 대신 올바른 행동 객체로 구성되는 행동을 부여받기 위해 두 클래스를 합치는 것을 구성을 이용한다라고 표현한다.\n정리 객체지향 기초 지식만 가지고는 휼륭한 객체지향 디자이너가 될 수 없다. 휼륭한 객체지향 디자인이라면 재사용성, 확장성, 관리의 용이성을 갖출 줄 알아야 한다. 패턴은 훌륭한 객체지향 디자인 품질을 갖추고 있는 시스템을 만드는 방법을 제공한다. 패턴은 검증받은 객체지향 경험의 산물이다. 패턴이 코드를 바로 제공하는 것은 아니나, 디자인 문제의 보편적인 해법을 제공한다. 패턴은 발명되는 것이 아니라 발견되는 것이다. 대부분의 패턴과 원칙은 소프트웨어의 변경 문제와 연관되어 있다. 대부분 패턴은 시스템의 일부분을 나머지 부분과 무관하게 변경하는 방법을 제공한다. 많은 경우에 시스템에서 바뀌는 부분을 골라내서 캡슐화해야 한다. 패턴은 다른 개발자와의 의사소통을 극대화하는 전문 용어 역할을 한다. ","date":"2023-09-20T15:25:08+09:00","image":"https://codemario318.github.io/post/gof/1/cover_hu5fe9e632d31d6204170abf166c9a2927_244577_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/gof/1/","title":"1. 디자인 패턴 소개와 전략 패턴"},{"content":"MySQL 복제에서는 소스 서버의 특정 이벤트들만 레플리카 서버에 적용될 수 있도록 필터링 기능을 제공한다.\n필터링의 주체는 소스 서버와 레플리카 서버 둘 다 될 수 있다.\n레플리카 서버에서 좀 더 다양한 형태의 필터링을 사용할 수 있다. 소스 서버에서는 발생한 이벤트들 중 특정 이벤트들만 바이너리 로그에 기록하거나 혹은 기록하지 않음으로써 복제에서 이벤트가 필터링 될 수 있게 한다. 소스 서버에서의 필터링은 데이터베이스 단위로만 가능하며, 옵션을 사용해 특정 데이터베이스에 대한 이벤트들만 바이너리 로그에 기록되게 하거나 혹은 기록되지 않게끔 설정할 수 있다.\nbinlog-do-db 바이너리 로그에 기록할 데이터베이스명을 지정한다. 지정된 데이터베이스에 대한 이벤트들만 바이너리 로그에 기록된다. binlog-ignore-db 바이너리 로그에 기록하지 않을 데이터베이스명을 지정한다. 지정된 데이터베이스에 대한 이벤트들은 바이너리 로그에 기록되지 않는다. 두 옵션은 MySQL 서버를 시작할 때 커맨드 라인이나 설정 파일에 지정해서 사용 가능하며, 동적으로 변경할 수 없어 재시작이 필요하다.\n1 2 3 4 binlog-do-db=production binlog-do-db=db1 binlog-do-db=db2 binlog-ignore-db=test 레플리카 서버에서는 소스 서버에서보다 유연한 형태로 필터링 설정이 가능하며, 동적으로 필터링 설정을 변경할 수도 있다.\n레플리카 서버에서의 필터링은 릴레이 로그에 저장된 이벤트들을 실행하는 시점에 적용된다. 소스 서버로부터 이벤트를 가져올 때 미리 필터링해서 이벤트들을 가져오는 것이 아니라 일단 모든 이벤트들을 가져온 다음 이벤트를 실행할 때 필터링을 적용한다. 사용자는 레플리카 서버를 시작할 때 커맨드 라인 혹은 설정 파일에 옵션을 지정하거나 현재 구동중인 레플리가 서버에서 CHANGE REPLICATION FILTER 구문으로 필터링을 설정할 수 있다.\n1 2 3 4 5 6 7 8 9 10 CHANGE REPLICATION FILTER filter[, filter, ...] [FOR CHANNEL channel_name] filter: REPLICATE_DO_DB = (db_name[, db_name, ...]) REPLICATE_IGNORE_DB = (db_name[, db_name, ...]) REPLICATE_DO_TABLE = (db_name.table_name[, db_name.table_name, ...]) REPLICATE_IGNORE_TABLE = (db_name.table_name[, db_name.table_name, ...]) RELICATE_WILD_DO_TABLE = (\u0026#39;db_pattern.table_pattern\u0026#39;[, \u0026#39;db_pattern.table_pattern\u0026#39;, ...]) REPLICATE_WILD_IGNORE_TABLE = (\u0026#39;db_pattern.table_pattern\u0026#39;[, \u0026#39;db_pattern.table_pattern\u0026#39;, ...]) REPLICATE_REWRITE_DB = ((from_db, to_db) [, (from_db, to_db), ...]) 옵션 커맨드 라인 및 설정 파일 옵션 설명 REPLICATE_DO_DB replicate-do-db 복제 대상 데이터베이스 지정 REPLICATE_IGNORE_DB replicate-ignore-db 복제에서 제외할 데이터베이스 지정 REPLICATE_DO_TABLE replicate-do-table 복제 대상 테이블을 지정 REPLICATE_IGNORE_TABLE replicate-ignore-table 복제에서 제외할 테이블을 지정 RELICATE_WILD_DO_TABLE replicate-wild-do-table 복제 대상 테이블을 와일드카드 패턴을 사용해 지정 REPLICATE_WILD_IGNORE_TABLE replicate-wild-ignore-table 복제에서 제외할 테이블을 와일드 카드 패턴으로 지정 REPLICATE_REWRITE_DB replicate-rewrite-db 특정 데이터베이스에 대한 이벤트들을 지정한 데이터베이스로 치환해서 적용 복제 필터링을 적용하려면 복제를 시작하기 전 해당 구문을 실행하거나 SQL 스레드를 멈춘 후 구문을 실행한 후 SQL 스레드를 재시작한다.\n이미 설정된 필터링 옵션들을 해제하려면 CHANGE REPLICATION FILTER 구문에 해제하고자 하는 필터링 옵션들을 명시적으로 빈 값으로 설정해서 실행한다.\n1 2 CHANGE REPLICATION FILTER REPLICATE_DO_DB = (), REPLICATE_IGNORE_DB = (); 복제 필터링이 적용된 레플리카 서버에서는 복제된 이벤트 실행 시 다음 단계를 거친다.\n데이터베이스 수준으로 설정된 필터링 옵션들을 바탕으로 1차적으로 필터링한다. 테이블 수준으로 설정된 필터링 옵션들을 체크해서 최종적으로 이벤트의 적용 여부를 결정한다. 데이터베이스 수준의 필터링 옵션들의 경우 복제되어 넘어온 이벤트의 바이너리 로그 포맷별로 해당 이벤트가 속한 데이터베이스를 식별하는 방식이 다르기 때문이다.\n이벤트가 Statement 포맷인 경우에는 USE 문에 의해 지정된 디폴트 데이터베이스를 바탕으로 필터링이 적용된다. ROW 포맷인 경우 DML 이벤트들은 변경된 테이블이 속한 데이터베이스를 바탕으로 필터링이 적용된다. DDL 이벤트들은 Row 포맷이라도 Statement 포맷으로 로깅되므로 USE 문에 의해 지정된 데이터 베이스가 필터링 기준이 된다. 바이너리 로그 포맷에 따라 데이터베이스 식별 방식이 달라지므로 사용자가 예상했던 것과는 다르게 필터링이 처리될 수 있다.\n사용자는 데이터베이스 수준의 필터링 옵션을 사용할 때 서버에 설정된 바이너리 로그 포맷이 무엇인지 확인해야 한다. 데이터베이스를 직접 명시해서 사용하는 쿼리가 있는지 반드시 사전에 확인해야 한다. 테이블 수준의 필터링 옵션을 사용하는 경우에도 복제 대상 테이블과 복제 제외 대상 테이블을 함께 변경하는 형태의 쿼리를 사용하는 경우 바이너리 로그 포맷에 따라 필터링 결과가 달라질 수 있으므로 주의해야 한다.\nROW 포맷 DDL문에 의해서 USE 문을 사용해 디폴트 데이터베이스가 설정되게 하고 쿼리에서 데이터베이스명을 지정하지 않는다. STATEMENT or MIXED DML, DDL 모두 USE 문을 사용해 디폴트 데이터베이스가 설정되게 하고 쿼리에서 데이터베이스명을 지정하지 않는다. 복제 대상 테이블과 복제 제외 대상 테이블을 모두 변경하는 DML을 사용하지 않는다. ","date":"2023-09-16T20:01:10+09:00","image":"https://codemario318.github.io/post/real-mysql/16/7/4/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/16/7/4/","title":"16.7.4 복제 고급 설정 - 필터링된 복제(Filterd Replication)"},{"content":"MySQL 레플리카 서버가 비정상 종료되는 경우 서버를 다시 구동시켰을 때, 소스 서버와의 복제 동기화가 실패할 수 있고, 이로 인해 최악의 경우 서비스가 중단될 수 있다.\n레플리카 서버 재구축하는 동안 또 다른 레플리카 서버가 존재하지 않는다면 소스 서버는 레플리카 서버가 복구될 때까지 예비 서버 없이 운영되어야 하며 복구되는 동안 소스 서버에도 장애가 발생하는 경우 서비스가 중단될 수 있다. 이러한 상황을 대비하기 위해 비정상 종료된 후 재구동됐을 때도 복제가 원활하게 재개될 수 있게 여러 설정을 제공하며, 이를 통해 사용자는 서버 장애 이후에도 문제없이 복제가 진행되는 크래시 세이프 복제를 실현할 수 있다.\n크래시 세이프 복제는 옵션을 활성화해서 적용하는 기능이 아니라 여러 가지 복제 관련 옵션들을 복제 사용 형태에 따라 적절히 설정했을 때 얻게 되는 효과라 할 수 있다. 서버 장애와 복제 실패 MySQL 복제는 레플리케이션 I/O 스레드와 SQL 스레드가 협업하여 소스 서버와의 동기화를 수행한다.\nI/O 스레드 소스 서버로부터 바이너리 로그 이벤트를 네트워크를 통해 가져온 후 레플리카 서버의 로컬 디스크에 파일로 저장하는 역할을 담당. SQL 스레드 I/O 스레드가 가져온 바이너리 로그 이벤트를 실제 MySQL 서버에서 재실행하는 역할 담당. 동기화 과정에서 I/O 스레드와 SQL 스레드는 각각 자신이 어느 시점까지의 바이너리 로그를 가져왔는지, 어느 트랜잭션까지 재실행했는지에 대한 포지션 정보를 남기며, 서버가 종료되었다가 재실행되면 포지션 정보들을 참조해서 복제를 어느 시점 부터 다시 시작해야 할지를 판단하게 된다.\n실행 포지션 정보는 시스템 변수들에 설정된 값에 따라 파일, 테이블 형태로 관리된다. 파일 형태로 관리될 경우 각 스레드가 동작할 때 실제 자신이 처리 중인 내용과 포시션 정보를 원자적으로 동기화된 상태로 관리할 수 없어 처리한 내역과 포지션 정보 간에 불일치가 발생할 수 있었다.\nI/O 스레드가 릴레이 로그에 이벤트를 기록한 후 포지션 정보 파일에 업데이트를 하지 않은 상태에서 비정상 종료시 재구동할 때 릴레이 로그에 동일한 이벤트가 기록될 수 있다. SQL 스레드가 릴레이 로그에 기록된 트랜잭션을 커밋한 후 포지션 정보 파일에 업데이트를 하지 않은 상태에서 비정상 종료되면 재구동시 동일한 트랜잭션이 재실행될 수 있다. 이러한 불일치로 인해 동리한 INSERT 쿼리가 두번 실행되었을 때 발생할 수 있는 Duplicate key 에러가 발생하게된다.\n상황에 따라 다양한 에러가 발생할 수 있으며 최악의 경우 에러 없이 데이터가 잘못될 수도 있다. 테이블로 관리하는 경우 InnoDB 엔진을 사용하므로 SQL 스레드가 트랜잭션 적용과 포지션 정보 업데이트를 한 트랜잭션으로 묶어 원자적으로 처리할 수 있어 SQL 스레드가 동일한 쿼리를 재실행하는 문제는 방지할 수 있게 되었다.\nI/O 스레드의 포지션 불일치 문제는 relay_log_recovery 옵션이 도입되며 해결되었다.\n재구동 시 I/O 스레드의 포지션을 마지막으로 실행했던 포지션으로 초기화한다. 새로운 릴레이 로그 파일을 생성해서 SQL 스레드가 읽어야 할 릴레이 로그 포지션 위치를 초기화한다. 1 2 relay_log_recover=ON relay_log_info_repository=TABLE 설정한다고 하더라도 운영체제의 비정상 종료시 데이터 파일 자체가 손상될 수 있으므로 무용지물일 수 있다.\n복제 사용 형태별 크래시 세이프 복제 설정 MySQL 복제는 사용자가 설정한 내용에 따라 복제 타입 및 동기화 방식이 다르므로 그에 따라 크래시 세이프 복제 설정도 조금씩 달라질 수 있다.\n바이너리 로그 파일 위치 기반 복제 + 싱글 스레드 동기화 최소 옵션 설정 셋과 동일하다.\n바이너리 로그 파일 위치 기반 복제 + 멀티 스레드 동기화 레플리카 서버에서 복제된 트랜잭션들의 커밋 순서가 소스 서버에서와 동일하도록 설정됐는지 여부에 따라 크래시 세이프 복제를 위한 옵션 셋이 달라진다.\n소스 서버와 트랜잭션 커밋 순서 일치 여부 크래시 세이프 복제 설정 일치 최소 설정 불일치 최소 설정 + sync_relay_log=1 sync_relay_log 릴레이 로그를 디스크와 얼마나 자주 동기화할 것인지 제어(기본값 10000) 0: MySQL에서는 동기화 X, 운영체제 설정에 따라 도익화 수행 1이상: 릴레이 로그에 지정된 수만틈 이벤트가 기록됐을 때 동기화 수행 1이 아닌 값을 사용하게 되면 비정상 종료시 동기화되지 못한 릴레이 로그의 내용이 유실될 수 있어 갭이 발생 가능하고 이로인해 복제가 실패할 수 있다. 옵션을 1로 설정하면 이벤트가 릴레이 로그에 기록될 때마다 디스크에도 동기화가 처리되므로 이벤트 손실을 최소화 할 수 있으나 디스크에 부하를 주게 되며, 복제 시 성능이 저하될 수 있다.\n따라서 멀티 스레드 복제를 사용할 때는 LOGICAL_LOCK 방식을 사용하고 slave_preserve_commit_order 옵션을 1로 설정하여 갭이 발생 하지 않게 하여 sync_relay_log=1 설정 없이도 크래시 세이프한 복제가 될 수 있게 하는 것을 권장한다.\nGTID 기반 복제 + 싱글 스레드 동기화 mysql.gtid_executed 테이블 데이터가 복제된 트랜잭션이 적용될 때마다 매번 함께 갱신되는지 여부에 따라 옵션 셋이 달라진다.\ngtid_executed 테이블 데이터가 매 트랜잭션 적용 시 갱신되는 경우\n1 2 3 4 relay_log_recovery=ON ## 복제 AUTO_POSITION 활성화 [MASTER | SOURCE]_AUTO_POSITION=1 gtid_excuted 테이블 데이터가 매 트랜잭션 적용 시 갱신되지 않는 경우\n1 2 3 4 5 6 7 8 relay_log_recovery=ON ## 복제 AUTO_POSITION 활성화 [MASTER | SOURCE]_AUTO_POSITION=1 ## sync_binlog=1 innodb_flush_log_at_trx_commit=1 GTID 기반 복제에서는 [MASTER | SOURCE]_AUTO_POSITION=1 옵션을 사용하는데, 이 경우 SQL 스레드가 마지막으로 적용한 트랜잭션의 GTID를 얻기 위해 복구 시 mysql.slave_relay_log_info 테이블이 아닌 mysql.gtid_excuted 테이블 데이터를 참조한다.\n이에 따라 relay_log_info_repositiory=TABLE 옵션이 제외된다. gtid_executed 테이블이 매 트랜잭션이 적용될 때마다 항상 함께 갱신되어야 정상적으로 복구사 수행된다.\ngtid_executed 테이블 테이터는 MySQL 8.0.17 버전부터는 기본적으로 매 트랜잭션 적용 시 함께 갱신된다. MySQL 8.0.17 미만 버전을 테이블 설정에 따라 달라진다. gtid_executed 테이블의 데이터가 매 트랜잭션이 적용될 때마다 갠싱되지 않는다면 MySQL 서버의 바아너리 로그를 통해 누락된 트랜잭션들의 GTID들을 복구해야 하므로 sync_binlog=1, innodb_flush_log_at_trx_commit=1 옵션이 반드시 설정돼 있어야 한다.\nGTID 기반 복제 + 멀티 스레드 동기화 싱글 스레드로 동기화되는 경우와 동일하다.\n유의할 점은 멀티 스레드 복제인 경우 앞에서 언급한 것처럼 비정상적으로 종료된 후 relay_log_recovery=ON 설정으로 재구동 시 트랜잭션 갭을 메우는 작업이 수행된다.\n이는 SOURCE_AUTO_POSITION 옵션을 사용하는 GTID 기반 복제에서는 불필요하지만, 옵션 사용 여부와 관계없시 실행되는데 이러한 처리로 인해 오히려 크래시가 발생해 릴레이 로그에 이벤트가 손실되면 앞서 언급한 것과 동일하게 복구 작업 시 갭 메우기 작업이 실패하면서 복제 연결이 실패하는 현상이 발생할 수 있다.\nMySQL 8.0.18/5.7.28 버전부터는 GTID 기반이면서 SOURCE_AUTO_POSITION 옵션을 사용하는 경우에 대해서 이러한 작업이 자동으로 생략되도록 갯너되었다.\n","date":"2023-09-16T19:21:10+09:00","image":"https://codemario318.github.io/post/real-mysql/16/7/3/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/16/7/3/","title":"16.7.3 복제 고급 설정 - 크래시 세이프 복제(Crash-safe Replication)"},{"content":"MySQL 복제에서는 레플리카 서버에서 소스 서버로부터 복제된 트랜잭션들을 하나의 스레드가 아닌 여러 스레드로 처리할 수 있게 하는 멀티 스레드 복제 기능을 제공한다.\n소스 서버에서는 여러 세션에서 실행된 DML 쿼리들이 동시에 처리되는데, 소스 서버에서 짧은 시간 동안 다량의 DML 쿼리가 실행되는 경우 MySQL 5.6 버전 미만의 레플리카 서버에서는 하나의 스레드가 모든 트랜잭션을 처리하므로 복제 동기화에 지연이 발생한다.\n또한 하나의 스레드로만 복제가 동기화 되는것은 멀티코어 CPU가 장착된 서버를 사용하는 환경에서는 서버의 자원을 충분히 활용하지 못하는 비효율적인 방식이었다.\nflowchart LR a[Relay Log]--\u003eb([Coordinator Thread]) subgraph Worker 1 direction LR c[[Queue]] --\u003e d([Worker Thread]) end subgraph Worker 2 direction LR e[[Queue]] --\u003e f([Worker Thread]) end subgraph Worker N direction LR g[[Queue]] --\u003e h([Worker Thread]) end b--\u003ec b--\u003ee b--\u003eg 기존 단일 스레드 복제에서는 레플리케이션 SQL 스레드가 릴레이 로그 파일을 읽어서 바로 트랜잭션을 적용하는 형태였다면 멀티 스레드 복제에서는 SQL 스레드는 코디네이터 스레드(Coordinator Thread)로 불리며, 실제로 이벤트를 실행하는 스레드인 워커 스레드와 협업해서 동기화를 진행한다.\n코디네이터 스레드는 릴레이 로그 파일에서 이벤트들을 읽은 뒤 설정된 방식에 따라 스케줄링 해서 워커 스레드에 각 이벤트를 할당한다. 각 이벤트는 워커 스레드들의 큐에 적재되며 워커 스레드는 큐에서 이벤트들을 꺼내 순차적으로 레플리카 서버에 적용한다. 멀티 스레드 복제는 소스 서버로부터 복제된 트랜잭션들을 어떻게 병렬로 처리할 것인가에 따라 데이터베이스 기반과 LOGICAL CLOCK 기반 처리 방식으로 나뉜다.\nslave_parallel_type 병렬 처리 방식 설정 기본적으로 데이터베이스 기반 방시긍로 설정 parallel_workers 시스템 변수를 통해 워커 스레드의 개수를 지정할 수 있다. 0 ~ 1024 까지 설정 가능 1로 설정하면 스레드 복제를 위한 코드 블록이 모두 실행되면서 실제 복제는 단을 스레드처럼 처리됨 0으로 설정시 단일 스레드 모드로 복제 수행, 부가적인 작업 처리를 수행하지 않음 slave_pending_jobs_size_max 시스템 변수를 통해 워커 스레드의 큐에 할당할 수 있는 최대 메모리 크기를 설정 MySQL 8.0 기본값은 128MB 작은 이벤트들이 빈번하게 발생하는 OLTP 환경에서는 기본값도 문제 없음 소스 서버로부터 전달받은 이벤트 크기가 설정된 값을 초과하는 경우 모든 워커 스레드들의 큐가 바위질 때 까지 대기후 처리됨 복제 지연이 발생할 수 있으므로 적절히 큰 값 설정 필요 SHOW PROCESSLIST 명령을 통해 코디네이터 스레드와 워커 스레드를 확인할 수 있다.\n데이터 베이스 기반 멀티 스레드 복제 데이터베이스 기반 멀티 스레드 복제 방식은 스키마 기반 처리 방식이라고도 하며, 멀티 스레드 복제가 처음 도입됐을 때 유일하게 사용할 수 있었던 방식이다.\n데이터 베이스 기반 멀티 스레드 복제는 MySQL 내의 데이터베이스 단위로 병렬 처리를 수행하는 형태를 말한다.\nMySQL 서버에 데이터베이스가 하나밖에 존재하지 않는다면 장점을 가지지 못한다. 여러 개의 데이터베이스가 있다면 레플리카 서버에서는 데이터베이스 개수 만큼 워커 스레드 수를 설정하는 것이 좋다. 코디네이터 스레드는 릴레이 로그 파일에서 이벤트를 읽어 데이터베이스 단위로 분리하고 각 워커 스레드에게 이벤트들을 할당한다.\nflowchart LR a[트랜잭션 3: UPDATE DB3\nUPDATE DB1\n트랜잭션 2: UPDATE DB2\n트랜잭션 3: UPDATE DB1] b([코디네이터 스레드]) c([워커 스레드 1\n1. UPDATE DB1\n트랜잭션 1]) d([워커 스레드 2\n2. UPDATE DB2\n트랜잭션 2]) e([워커 스레드 3\n3. UPDATE DB3\n트랜잭션 3]) a--4. UPDATE DB1\n트랜잭션 3--\u003eb b--\u003ec b--\u003ed b--\u003ee 위의 트랜잭션 3의 마지막 처리인 UPDATE DB1 같은 경우 워커 스레드 1이 이미 처리중이므로 해당 작업을 완료될때까지 기다리게 된다.\n데이터베이스 기반 멀티 스레드 복제에서는 테이블이나 레코드 수준까지의 충돌 여부는 고려하지 않고 데이터베이스가 동일한지만 비교하기때문에, 변경 대상 테이블이나 레코드는 다르다고 하더라도 대기한다. 트랜잭션 3의 UPDATE DB1이 대기하고 있는 상황에서, 워커 스레드 2의 작업이 이미 완료되고 네 번째 트랜잭션으로 또 다른 DB2 변경 작업이 들어온다고 하더라도 대기하게 된다. 이러한 처리로 인해 서로 다른 데이터베이스를 참조하는 쿼리나 트랜잭션이 빈번하게 실행되는 경우 예상했던 것보다 멀티 스레드 처리 효율이 낮아질 수 있다.\n하지만 MySQL 서버에 여려 개의 데이터베이스가 존재하고 각 데이터베이스에 유입되는 DML이 서로 독립적이면서 균등하게 실행되는 환경이라면 데이터기반 멀티 스레드 복제가 단일 스레드 복제보다 월등한 처리량을 보인다. 데이터베이스 기반 멀티 스레드 복제를 사용하려면 레플리카 서버를 다음과 같이 설정 후 복제를 연결하면 된다.\n기존에 이미 단일 스레드 복제가 진행되고 있는 상황에서 데이터베이스 기반 멀티 스레드 복제로 전환하고 싶다면 SQL 스레드만 멈춘 후 멀티 스레드 복제를 설정하고 다시 시작하면 된다. 1 2 3 [mysqld] slave_parallel_type=\u0026#39;DATABASE\u0026#39; slave_parallel_workers=N (N\u0026gt;0) 데이터베이스 기반 멀티 스레드 복제에서는 레플리카 서버가 자체적으로 바이너리 로그를 가지고 log_slave_updates 있고 옵션이 활성화되어 있을 때 소스 서버의 바이너리 로그에 기록된 트랜잭션 순서와 레플리카 서버의 바이너리 로그에 기록된 트랜잭션 순서가 다를 수 있다.\n레플리카 서버의 멀티 스레드 복제 동기화에서 각 트랜잭션이 처리된 시점에 따라 실제 소스 서버에서 실행된 것과는 순서가 달라질 수 있기 때문 소스 서버에서 가장 최근에 싫애된 트랜잭션을 레플리카 서버에서 확인한다고 했을 때 소스 서버에서 그 트랜잭션 이전에 실행된 모든 트랜잭션들이 레플리카 서버에도 전부 실행되었다고 보장하긴 어렵다. LOGICAL CLOCK 기반 멀티 스레드 복제 MySQL 5.7 버전부터 소스 서버로부터 넘어온 전체 트랜잭션들을 데이터베이스에 종속되지 않고 멀티 스레드로 처리하는, 즉 같은 데이터베이스 내에서도 멀티 스레드 동기화 처리가 가능한 LOGICAL CLOCK 방식이 도입되어 데이터 베이스 기반 멀티 스레드 복제의 아쉬움을 해소할 수 있게 되었다.\nLOGICAL CLOCK 기반 멀티 스레드 복제는 소스 서버에서 트랜잭션들이 바이너리 로그로 기록될 때 각 트랜잭션별로 논리적인 순번 값을 부여해 레플리카 서버에서 트랜잭션의 순번 값을 바탕으로 정해진 기준에 따라 병렬로 실행하는 방식이다.\n트랜잭션이 병렬로 처리될 수 있다고 여겨지는 기준은 같은 상황에서도 세부 처리 방식에 따라 달라딘다.\nCommit-parent 기반 방식 잠금(Lock) 기반 방식 WriteSet 기반 방식 바이너리 로그 그룹 커밋 MySQL 5.5 버전까지는 InnoDB 스토리지 엔진에서 한 시점에 하나의 트랜잭션만 커밋될 수 있었으며, 바이너리 로그에 트랜잭션을 기록하고 디스크와 동기화하는 부분도 마찬가지로 여러 트랜잭션이 동시에 진행될 수 없었다.\nsequenceDiagram 클라이언트-\u003e\u003eMySQL: COMMIT; MySQL-\u003e\u003e스토리지 엔진: prepare; MySQL-\u003e\u003e바이너리 로그: write; MySQL-\u003e\u003e바이너리 로그: fsync; MySQL-\u003e\u003e스토리지 엔진: commit; MySQL--\u003e\u003e클라이언트: OK; 클라이언트로부터 커밋 요청이 들어오면 MySQL 서버에서는 Prepare, Commit 두 단계를 거쳐 머닛을 처리하는데, 이를 분산 트랜잭션 이라고 한다.\n분산 트랜잭션은 트랜잭션을 커밋할 때 스토리지 엔진에 적용된 내용과 바이너리 로그에 기록된 내용 간의 일관성을 유지하기 위해 사용된다. 커밋을 처리하는 과정에서 바이너리 로그에 기록한 내용을 디스크와 동기화하는 fsync 작업은 sync_binlog 시스템 변수에 설정된 값에 따라 실행 여부와 실행 빈도수가 결정된다\n1: 트랜잭션이 커밋될 때마다 디스크 동기화를 수행 빈번하게 수행되는 디스크 동기화 작업은 서버에 부하가 발생한다. 트랜잭션 처리량 저하를 야기한다. 이 같은 처리 성능 저하 문제를 개선하기 위해 MySQL 5.6 버전에서는 여러 트랜잭션에 대한 커밋을 동시에 진행할 수 있게 바뀌었고, 바이너리 로그 단의 처리 또한 여러 트랜잭션을 함께 처리할 수 있도록 바이너리 로그 그룹 커밋 기능이 도입되었다.\n바이너리 로그 그룹 커밋에서 트랜잭션들은 커밋 처리 과정 중 Prepare이후 바이너리 로그 관련 처리를 진핼할 때 세 단계를 거치면서 최종적으로 그룹 커밋된다.\n트랜잭션들은 순서대로 대기 큐에 등록된다. 비어있는 대기 큐에 첫번째로 등록된 트랜잭션을 리더(Leader)라고 하며, 이후 등록된 다른 트랜잭션들은 팔로워(Follower) 라고 한다. 팔로워는 리더에게 자신의 트랜잭션 처리에 대한 모든 것을 일임하며, 리더는 큐에 등록된 모든 팔로워들을 가져와 처리하고 다음 단계의 대기 큐에 등록한다. 다음 단계의 큐에 등록될 때는 큐가 비어있지 않은 경우 리더는 팔로워가되고 해당 큐의 리더가 그 단계에서의 처리를 주도하게 된다. 리더는 팔로워가 될 수 있으나 팔로워는 절대 리더가 될 수 없다. 각 단계의 트랜잭션들은 다음과 같이 처리된다.\nFlush 단계 대기 큐에 등록된 각 트랜잭션들을 순서대로 바이너리 로그에 기록한다. Sync 단계 앞서 기록된 바이너리 로그 내용들을 디스크와 동기화하는 fsync() 시스템 콜이 수행된다. sync_binlog 옵션에 설정된 값에 따라 디스크 동기화 수행 트랜잭션 별로 동기화가 수행되는 것이 아닌 트랜잭션 그룹에 대해 동기화가 수행됨 대기 큐에 트랜잭션들이 많을수록 효율적으로 처리됨 Sync 단계의 대기 큐에 많은 트랜잭션을 쌓기 위해 시스템 변수를 설정하여 실행을 지연할 수 있음 binlog_group_commit_sync_delay:\n동기화 작업을 얼마정도 지연시킬지 제아하는 변수(마이크로 초) binlog_group_commit_sync_no_delay_count:\n동기화 작업이 진행되기 전에 지연되어 대기할 수 있는 최대 트랜잭션 수 Commit 단계 대기 큐에 등록된 트랜잭션들에 대해 스토리지 엔진 커밋을 진행한다. 스토리지 엔진 커밋은 대기 큐에 등록된 순서대로 혹은 병렬로도 처리될 수 있다. 대기 큐에 등록된 순서대로 커밋되는 경우 대기 큐의 리더에 의해 처리가 진행 이러한 경우 트랜잭션들은 바이너리 로그에 기록된 순서와 스토리지 엔진에 커밋된 순서가 일치하게 된다. 병렬로 처리되는 경우 리더가 아닌 각 트랜잭션들이 커밋을 수행하게 된다. 시스템 변수를 통해 트랜잭션들이 커밋되는 순서를 제어할 수 있다. binlob_order_commit: 바이너리 로그 파일에 기록된 순서대로 스토리지 엔진에 커밋됨 Commit-parent 기반 방식 멀티 스레드 복제 동기화가 처음 도입됐던 버전에서 적용된 방식으로, 동일 시점에 커밋된 트랜잭션들을 레플리카 서버에서 병렬로 실행될 수 있게 한다.\n커밋 시점이 같은 트랜잭션들은 잠금 경합 등과 같이 서로 충돌하는 부분이 없는 트랜잭션들이므로 병렬로 실행될 수 있다는 부분에서 착안됨. Commit-parent 기반 방식이 적용된 MySQL 버전을 사용하는 레플리카 서버에서 LOGICAL CLOCK 멀티 스레드 동기화가 활성화되어 있는 경우 소스 서버에서 같은 시점에 커밋된 트랜잭션들을 복제 동기화할 때 병렬로 처리한다.\nMySQL 서버는 같은 시점에 커밋 처리된 트랜잭션들을 식별할 수 있도록, 바이너리 로그에 트랜잭션을 기록할 때 commit_clock이라는 64비트 정수값을 기반으로 한다. 각 트랜잭션이 커밋을 위해 Prepare 단계에 진입했을 때 설정되며 그 당시의 commit clock 값이 저장된다.(최종적으로 커밋되기 전에 값이 증가함) 같은 시점에 커밋 처리가 시작된 트랜잭션들은 동일한 commit_seq_no 값을 가지며, 복제된 트랜잭션들의 해당 값을 바탕으로 같은 값을 가진 트랜잭션들을 병렬로 수행한다. 병렬 처리 기준인 commit_seq_no 값이 앞서 커밋된 트랜잭션 순번 값에 해당하므로 이 방식을 Commit-parent 기반 방식이라고 한다.\nsequenceDiagram 0-\u003e\u003e1:Tx1 C=0; 1-\u003e\u003e2:Tx2 C=1; 2-\u003e\u003e3:Tx3 C=2; 3-\u003e\u003e4:Tx4 C=3; 4-\u003e\u003e5:Tx5 C=4; 4-\u003e\u003e6:Tx6 C=4; 4-\u003e\u003e7:Tx7 C=4; 4-\u003e\u003e8:Tx8 C=6; 6-\u003e\u003e9:Tx9 C=6; 8-\u003e\u003e10:Tx10 C=8; Commit-parent 기반에서는 소스 서버에서 같은 그룹으로 커밋된 트랜잭션 수가 많을수록 레플리카 서버에서의 트랜잭션 병렬 처리율이 향상된다.\n사용자는 소스 서버에서 binlog_group_commit_sync_delay 시스템 변수와 binlog_group_commit_sync_no_delay_count 시스템 변수에 적절한 값을 설정해 그룹 커밋되는 트랜잭션 수를 늘릴 수 있다.\n소스 서버에서 트랜잭션들의 처리 속도를 느리게 하는 대신, 레플리카 서버에서의 처리 속도를 높이는 방법이다. 소스 서베에서 트랜잭션을 실행하는 클라이언트들이 영향을 받을 수 있으므로 설정을 변경한 후 주의 깊게 모니터링 해야한다. 잠금 기반 방식 기존 Commit-parent 기반 방식에서는 단순하게 마지막으로 커밋된 선행 트랜잭션의 순번 값이 동일한 트랜잭션들만 레플리카 서버에서 병렬로 처리할 수 있었다.\n그러나 잠금 기반 방식에서는 선행 트랜잭션의 순번 값이 동일하지 않더라도 커밋 처리 시점이 겹친다면 그 트랜잭션들은 레플리카 서버에서 병렬로 처리될 수 있다.\n이를 위해 MySQL 서버에서는 트랜잭션을 바이너리 로그에 기록할 때 sequence_number와 last_committed라는 값을 함께 기록한다.\nsequence_number 커밋된 트랜잭션에 대한 논리적인 순번 값으로, 매 트랜잭션이 커밋될 때마다 값이 증가한다. last_committed 현 트랜잭션 이전에 커밋된 가장 최신 트랜잭션의 sequence_number 값이 저장된다. 바이너리 파일이 새로운 파일로 로테이션 되는 경우 sequence_number 값은 1, last_committed 값은 0으로 초기화된다. 잠금 기반 LOGICAL CLOCK 방식이 적용된 MySQL 버전을 사용하는 레플리카 서버에서는 LOGICAL CLOCK 멀티 스레드 동기화가 활성화돼 있는 경우 별렬로 트랜잭션을 실행할 때 다음과 같은 조건을 기준으로 트랜잭션들의 실행 가능 여부를 결정한다.\n실행하려는 트랜잭션의 last_committed \u0026lt; 현재 실행중인 트랜잭션들이 가지는 가장 작은 sequence_number sequenceDiagram 0-\u003e\u003e1:Tx1 C=0, S=1; 1-\u003e\u003e2:Tx2 C=1, S=2; 2-\u003e\u003e3:Tx3 C=2, S=3; 3-\u003e\u003e4:Tx4 C=3, S=4; 4-\u003e\u003e5:Tx5 C=4, S=5; 4-\u003e\u003e6:Tx6 C=4, S=6; 4-\u003e\u003e7:Tx7 C=4, S=7; 4-\u003e\u003e8:Tx8 C=6, S=7; 6-\u003e\u003e9:Tx9 C=6, S=9; 8-\u003e\u003e10:Tx10 C=8, S=10; 잠금 기반 방식에서는 트랜잭션들이 레플리카 서버로 복제됐을 때 다음과 같이 처리된다.\n트랜잭션 처리 Tx1, Tx2, Tx3, Tx4 병렬 처리 가능 조건을 충족하지 않으므로 순차적으로 하나씩 실행됨 Tx5, Tx6, Tx7 병렬 처리 기능 조건을 충족하므로 병렬로 실행 가능 Tx8, Tx9 Tx6이 종료되고 Tx7이 아직 실행 중인 경우 Tx8, Tx9는 Tx7과 같이 실행될 수 있음(Tx8, 9의 last_committed 값보다 Tx7의 sequence_number 값이 크므로) Tx10 Tx8이 종료되고 Tx9가 실행 중일 때 Tx9과 함께 실행 가능 잠금 기반 방식은 소스 서버에서 커밋 처리 시점이 겹치는 트랜잭션들의 수가 많을 수록 레플리카 서버에서 최대한 병렬로 처리되므로 Commit-parent 기반 방식과 동일하게 소스 서버에서 그룹 커밋되는 트랜잭션 수에 영향의 받는다.\n잠금 기반 방식에서도 소스 서버의 시스템 변수 값을 적절히 조정해서 레플리카 서버에서의 병렬 처리율을 향상시킬 수 있다. WriteSet 기반 방식 WriteSet 기반 방식은 트랜잭션의 커밋 처리 시점이 아닌 트랜잭션이 변경한 데이터를 기준으로 병렬 처리 가능 여부를 결정한다.\n기존 잠금 기반 방식에서는 다음과 같이 커밋 처리 시점이 전혀 겹치지 않는 두 트랜잭션은 병렬로 실행될 수 없었다.\n1 2 Tx1 : ---P-------C----------------\u0026gt; Tx2 : --------------P--------C----\u0026gt; WriteSet 기반 방식에서는 두 트랜잭션이 서로 다른 데이터를 변경하는 것이라면 병렬로 실행할 수 있다.\n동일한 데이터를 변경하지 않는 트랜잭션들은 레플리카 서버에서 모두 병렬로 실행될 수 있다. WriteSet 기반에서는 같은 세션에서 실행된 트랜잭션들의 병렬 처리 여부에 따라 WRITESET과 WRITESET_SESSTION 타입으로 나눠진다.\nbinlog_transaction_dependency_tracking 시스템 변수로 설정 가능하다. COMMIT_ORDER 기본값으로 잠금 기반 방식과 동일하게 동작한다. WRITESET 서로 다른 데이터를 변경한 트랜잭션들은 모두 병렬로 처리될 수 있다. WRITE_SESSION WRITE_SET 처리에서 동일한 세션에서 실행된 트랜잭션들은 병렬로 처리될 수 없다는 점만 다르다. WriteSet 기반 방식에서는 각 트랜잭션에서 변경한 데이터를 기주능로 병렬 처리를 위한 트랜잭션들의 종속 관계를 정의하므로 이를 위해 내부적으로 트랜잭션에 의해 변경된 데이터들의 목록을 관리한다.\n변경된 데이터들은 하나하나가 전부 해시값으로 표현된다. 해싱된 변경 데이터를 WriteSet이라고 한다. WriteSet은 테이블에 존재하는 유니크한 키의 개수만큼 만들어진다. 따라서 하나의 변경 데이터는 여러 개의 WriteSet을 가질 수 있다. 1 WriteSet = hash(index_name, db_name, db_name_length, table_name, table_name_length, value, value_length) 트랜잭션들의 WriteSet은 MySQL 서버 메모리에서 해시맵 테이블로 그 히스토리가 관리된다.\n히스토리 테이블에는 변경된 데이터의 해시값인 WriteSet과 해당 데이터를 변경한 트랜잭션의 sequence_number 값이 Key-Value 형태로 저장된다. 사용자는 binlog_transaction_dependency_histroy_size 시스템 변수를 통해 히스토리 테이블이 최대로 가질 수 있는 WriteSet 개수를 정할 수 있다(기본값 25000). 저장된 데이터 수가 지정된 최대 개수만큼 도달하면 히스토리 테이블은 초기화된다. DDL 쿼리가 실행된 경우에도 초기화된다. WriteSet 기반 방식에서도 마찬가지로 트랜잭션이 커밋되면 바이너리 로그에 트랜잭션 정보와 함께 last_committed 값과 sequence_number 값이 기록되며, 레플리카 서버에서는 이를 바탕으로 병렬 처리를 수행한다.\nWRITESET, WRITESET_SESSION 타입 모두 트랜잭션 커밋을 처리할 때 트랜잭션의 last_committed 값을 1차적으로 COMMIT_ORDER 타입 기반으로 설정한다. 이후 WriteSet 히스토리 테이블 데이터를 조회해서 트랜잭션의 WriteSet과 충돌하는 WriteSet의 존재 여부를 확인 후 다시 last_committed 값을 설정하게 된다. 처리 과정\nWriteSet 히스토리 테이블에서 가장 작은 sequence_number 값을 자신의 last_committed 값으로 설정한다. WriteSet을 히스토리 테이블에서 찾는다. 충돌하는 데이터가 있을 경우 이 해시값에 매핑된 sequence_number 값을 자신의 sequence_number 값으로 변경한다. 기존 sequence_number 값은 자신의 last_committed 값으로 설정한다. 기존 sequence_number 값이 자신의 last_committed 값보다 크지 않을 경우 변경하지 않는다. 충돌하는 데이터가 없는 경우 히스토리 테이블에 자신의 sequence_number 값과 함께 해당 데이터를 저장한다. 트랜잭션에서는 WriteSet 히스토리 테이블에 자신의 WriteSet과 충돌되는 WriteSet 데이터가 존재하는 경우 그 WriteSet에 매핑된 sequence_number 값을 가져와 자신의 last_committed에 저장하고 해상 WriteSet의 sequence_number를 자신의 sequence_number 값으로 업데이트한다.\n충돌하는 WriteSet이 여러개이고 WriteSet이 다른 sequence_number 값을 가지는 경우 가장 큰 sequence_number로 업데이트된다. 충돌하는 WriteSet이 없다면 해당 트랜잭션이 가진 WriteSet들이 히스토리 테이블에 새로 저장되고 WriteSet 중 가장 작은 sequence_number로 저장된다. WRITESET_SESSION 타입에서는 이렇게 결정된 last_committed 값을, 같은 세션에서 커밋된 마지막 트랜잭션의 sequnece_number 값과 한번 더 비교해서 둘 중 더 큰값을 선택해 last_committed에 저장한다.\nWRITESET과 WRITESET_SESSION 타입 모드 트랜잭션에서 변경된 데이터들이 속하는 테이블이 유니크한 키를 가지고 있지 않은 경우 해당 트랜잭션의 WriteSet은 생성되지 않으며, last_committed에는 COMMIT_ORDER 타입 기반으로 결정된 값이 그대로 사용된다. 변경된 데이터들이 속하는 테이블의 유니크한 키들이 다른 테이블에서 외래키로 참조되는 경우에도 같은 방식으로 처리된다. WriteSet 기반 방식에서는 레플리카 서버에서의 트랜잭션 병렬 처리가 소스 서버에서 동시에 커밋되는 트랜잭션 수에 의존적이지 않으므로 그룹 커밋되는 트랜잭션 수를 늘리기 위해 의도적으로 소스 서버의 트랜잭션 커밋 처리 속도를 저하시킬 필요가 없다.\n체인 형태로 복제가 구성돼 있는 경우 하위 계층의 레플리카 서버로 갈수록 병렬로 처리되는 트랜잭션 수가 점점 줄어드는 문제는 WRITESET 타입을 사용하면 병렬 처리량 감소 문제를 해결할 수 있다. WriteSet 기반 방식은 어느 방식보다도 레플리카 서버에서의 병렬 처리성을 높이는 방식이지만 트랜잭션 커밋 시 추가적인 메모리 공간이 필요하며 매 트랜잭션마다 WriteSet 히스토리 테이블에 저장된 값들을 계속 비교해야 하므로 이에 따른 오버헤드가 발생한다.\n멀티 스레드 복제와 복제 포지션 정보 멀티 스레드 복제에서 각 워커 스레드들이 실행한 바이너리 로그 이벤트의 포지션 정보는 relay_log_info_repository 시스템 변수에 지정된 값에 따라 mysql 데이터베이스 내 slave_worker_info 테이블 혹은 데이터 디렉터리 내 worker-relay-log.info 접두사를 가지는 파일들에 각 스레드별로 저장되며, 워커 스레드들은 이벤트를 실행 완료할 때마다 해당 데이터를 갱신한다.\n현재 복제 이벤트의 처리 현황을 보여주는 어플라이어 메타데이터에는 워커 스레드들이 실행한 이벤트들에서 로우 워터마크에 해당하는 이벤트의 포지션 값이 저장된다.\n코디네이터 스레드가 수행하는 체크포인트 작어베 의해 주기적으로 갱신된다. 갭이 존재하는 경우 체크포인트 지점은 항상 갭 이전에 실행 완료된 이벤트에서만 나타날 수 있다. 먼저 실행된 트랜잭션보다 나중에 실행된 트랜잭션이 먼저 처리되었을 때 발생하는 포지션 간격을 갭(Gap)이라고 한다. 멀티 스레드 복제에서 slave_preserve_commit_order 시스템 변수가 비활성화되어있는 경우 발생한다. 레플리카 서버에서 복제를 통해 넘어온 이벤트를 소스 서버에서 커밋된 순서와 동일한 순서로 커밋할 것인지를 제어 1로 설정시 여러 이벤트들이 동시에 처리돼도 릴레이 로그에 기록된 수서대로 커밋되어 갭이 발생하지 않는다. 갭이 발생하지 않는다 하더라도 어플라이어 메타데이터는 워커 스레드들이 처리한 이벤트 내역이 실시가능로 반영된 데이터가 아니라 체크포인트 주기마다 갱신되는 데이터므로 실제 적용된 이벤트의 포지션 값보다 이전의 포지션 값을 보여준다. (명령을 통한 조회도 동일함) 코디네이터 스레드는 다음 시스템 변수들에 설정된 값을 바탕으로 워커 스레드들에서 실행된 이벤트들에 대해 체크포인트를 수행해 어플라이어 메타데이터를 갱신한다.\nslave_checkpoint_period 어플라이어 메타데이터 갱신 작업의 주기를 결정하는 시스템 변수(기본값 300 (밀리초)) slave_checkpoint_group 어플라이어 메타데이터 갱신 작업의 주기를 결정하는 시스템 변수로 실행한 트랜잭션의 개수를 지정함 ","date":"2023-09-16T17:44:10+09:00","image":"https://codemario318.github.io/post/real-mysql/16/7/2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/16/7/2/","title":"16.7.2 복제 고급 설정 - 멀티 스레드 복제"},{"content":"복제는 최대한 빠르게 동기화해서 소스 서버와 레플르키 서버 가느이 데이터를 동일한 상태로 만드는 것이 원래의 목적이다.\n소스 서버와 레플리카 서버 간의 데이터 동기화 지연이 없으면 없을수록 레플리카 서버를 이용한 장애 복구가 용이해지며, 이러한 부분은 레플리카 서버를 서버스의 읽기 요청 처리 용도로 사용할 때도 매우 유용하다.\n하지만 때로는 의도적으로 소스 서버와 레플리카 서버 간의 복제를 지연시켜야 할 때도 있다.\n개발자나 DBA가 소스 서버에서 실수로 중요한 테이블이나 데이터를 삭제하여 서비스를 멈추고 데이터를 복구해야 하는 경우 이전 시점의 데이터 값에 대해 확인이 필요한 경우 데이터 반영에 지연이 있을 때 어떻게 서비스가 동작하는지 테스트를 위한 시뮬레이션이 필요한 경우 1 2 3 4 5 /* 8.0.23 이전 */ CHANGE MASTER TO MASTE_DELAY=86400; /* 8.0.23 이후 */ CHANGE REPLICATION SOURCE TO MASTE_DELAY=86400; MySQL 8.0 개선점 MySQL의 지연된 복제 기능은 5.6 버전에서 처음 도입됐으며, 8.0 버전에서 바이너리 로그에 타임 스탬프가 추가되는 등 몇 가지 부분들이 개선되었다.\noriginal_commit_timestamp(OCT)\n트랜잭션이 원본 소스 서버(트랜잭션이 시작된 소스 서버)에서 커밋된 시각으로, 밀리초 단위의 유닉스 타임스탬프 값으로 저장된다. immediate_commit_timestamp(ICT)\n트랜잭션이 직계 소스 서버(바로 위 소스 서버)에서 커밋된 시각으로, 밀리초 단위의 유닉스 타임스탬프 값으로 저장된다. OCT 값은 복제 구성에서 해당 트랜잭션이 복제되는 모든 레플리카 서버들이 동일한 값을 가지며, 원본 소스서버는 OCT 값과 ICT 값이 동일하다.\nlog_slave_updates 옵션이 활성화 되어있는 경우 ICT에는 레플리카 서버에서 복제된 트랜잭션이 커밋된 시점으로 값이 저장된다.\n두 타임 스탬프 값은 소스 서버의 바이너리 로그에 기록되어 레플리카 서버로 그대로 복제되어 릴레이 로그에 저장되는데, 레플리카 서버에서는 SOURCE_DELAY 옵션에 값이 지정되면 ICT를 참조하여 각 트랜잭션별로 실행을 지연시킬 것인지 결정한다.\nMySQL 8.0 미만 버전의 지연된 복제에서는 이벤트 그룹(트랜잭션) 단위가 아닌 개별 이벤트 단위로 지연 실행 여부를 결정하였다.\n이로 인해 동일한 트랜잭션 내의 이벤트들이라 하더라도 각 이벤트 사이에 지연 대기가 발생하였고, 지연 측정을 위한 기준 시각도 각 이벤트가 종료된 시각이 아닌 시작된 시각이 기준이었다.\n원본 소스 서버의 바이너리 로그에 기록된 이벤트 시작 값이 레플리카 서버들로 그대로 복제되므로 최하위 계층의 레플리카 서버에서는 자신의 직계 소스 서버가 아닌 원본 소스 서버의 시각을 기준으로 지연이 계산되었으며, 체인 복제를 구성하는 서버들의 타임존이 서로 다른경우 지연 시간이 예상한 것과는 다른 값으로 계산되어 표시되었는데 타임 스탬프를 활용하게 되며 이러한 문제점이 사라졌다.\n지연된 복제가 활성화되면 SHOW [REPLICA | SLAVE] STATUS 명령의 결과에서 레플리카 서버가 얼마나 지연되어 실행되고 있는지, 다음 트랜잭션을 실행할 때까지 얼마나 시간이 남았는지를 확인할 수 있다.\n1 2 3 SHOW GLOBAL STATUS LIKE \u0026#39;%semi_sync_master%\u0026#39;; SHOW GLOBAL STATUS LIKE \u0026#39;%semi_sync_slave%\u0026#39;; 지연된 복제를 사용한다고 하더라도 소스 서버의 바이너리 로그는 즉시 레플리카 서버의 릴레이 로그 파일로 복사된다. 따라서 소스 서버가 장애로 디스크의 데이터가 복구 불가능하다 하더라도 레플리카 서버의 릴레이 로그를 이용해서 복구한다면 지연된 복제를 사용하지 않는 복제와 같이 레플리카 서버로의 장애 복구가 가능하다. 복제를 지연되지 않도록 다시 설정하고 싶은 경우 다음 명령을 실행한다.\n1 2 3 4 STOP [SLAVE | REPLICA] SQL_THREAD; CHANEGE [MASTER | REPLICATION SOURCE] TO [MASTER_DELAY | SOURCE_DELAY]=0; START [SLAVE | REPLICA] SQL_THREAD; ```제 ","date":"2023-09-12T10:29:10+09:00","image":"https://codemario318.github.io/post/real-mysql/16/7/1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/16/7/1/","title":"16.7.1 복제 고급 설정 - 지연된 복제"},{"content":"싱글 레플리카 복제 구성 flowchart LR a([웹 서버]) b[(소스 서버)] c[(레플리카 서버)] a-. 읽기 + 쓰기 .-\u003e b a-. 읽기 .-\u003e c b--\u003ec 하나의 소스 서버에 하나의 레플리카 서버반 연결돼 있는 복제 형태를 말한다.\n가장 기본적인 형태로, 보통 애플리케이션 서버는 소스 서버에만 직접적으로 접근해 사용하고 레플리카 서버에는 접근하지 않으며, 레플리카 서버는 소스 서버에서 장애가 발생했을 때 사용될 수 있는 예비 서버 및 데이터 백업 수행을 위한 용도로 많이 사용된다.\n이 같은 형태에서 애플리케이션 서버가 레플리카 서버에서도 서비스용 읽기 쿼리를 실행한다고 하면 레플리카 서버에 문제가 발생한 경우 서비스 장애 상황이 도래할 수 있다.\n이렇게 소스 서버와 레플리카 서버가 일대일로 구성된 형태에서는 레플리카 서버를 정말 예비용 서버로서만 사용하는 게 제일 적합하다.\n멀티 레플리카 복제 구성 flowchart LR a([웹 서버]) b[(소스 서버)] c[(레플리카 서버 1)] d[(레플리카 서버 2)] a-. 읽기 + 쓰기 .-\u003e b a-. 읽기 .-\u003e d b--\u003ec b--\u003ed 멀티 레플리카 복제는 하나의 소스 서버에 2개 이상의 레플리카 서버를 연결한 복제 형태로, 보통 싱글 레플리카 복제 구성에서 추가적인 용도를 위해 여분의 레플리카 서버가 필요해졌을 때 자주 사용되는 형태다.\n새로 오픈될 서비스에서 사용할 MySQL 서버를 설정할 때는 유입되는 쿼리 요청이 적기 때문에 보통 싱글 레플리카 복제 구성으로 구축하지만, 서비스의 트래픽이 크게 증가하면 소스 서버 한 대에서만 쿼리 요청을 처리하기에는 벅찰 수 있다.\n이렇게 증가된 쿼리 요청은 대부분의 경우 쓰기보다는 읽기 요청이 더 많으므로 사용자는 멀티 레플리카 형태로 복제 구성을 전환해 읽기 요청 처리를 분산시킬 수 있다.\n서비스의 읽기 요청 처리를 분산하는 용도로 사용하는 경우 레플리카 서버 한 대는 예비용으로 남겨두는 것이 좋다.\n체인 복제 구성 flowchart LR a([웹 서버]) b[(소스 서버 1)] c[(레플리카 서버 1-1)] d[(레플리카 서버 1-2)] e[(레플리카 서버 1-3 / 소스 서버 2)] f([배치/분석 서버]) g[(레플리카 서버 2-1)] h[(레플리카 서버 2-2)] a-. 읽기 + 쓰기 .-\u003e b a-. 읽기 .-\u003e d b--\u003ec b--\u003ed b--\u003ee e--\u003eg e--\u003eh f-. 읽기 .-\u003e h 멀티 레플리카 복제 구성에서 레플리카 서버가 너무 많아 소스 서버의 성능에 악영향이 예상된다면 1:N:M 구조의 체인 복제 구성을 고려해 볼 수 있다.\nMySQL 복제에서 소스 서버는 레플리카 서버가 요청할 때마다 계속 바이너리 로그를 읽어서 전달해야 한다.\n따라서 만약 하나의 소스 서버에 연결된 레플리카 서버 수가 많다면 바이너리 로그를 읽고 전달하는 작업 자체가 부하가 될 수도 있다.\n이러한 경우 레플리카 서버 1-3 / 소스 서버 2 형태로 소스 서버가 해야 할 바이너리 로그 배포 역할을 새로운 MySQL 서버로 넘길 수 있다.\n체인 복제 구성을 이용한 서버 교체 이 복제 형태는 MySQL 서버를 업그레이드하거나 장비를 일괄 교체할 때도 사용될 수 있다.\nflowchart subgraph 구 장비 direction LR a([웹 서버]) b[(소스 서버 1)] c[(레플리카 서버 1-1)] d[(레플리카 서버 1-2)] a-. 읽기 + 쓰기 .-\u003e b a-. 읽기 .-\u003e d b--\u003ec b--\u003ed end 위와 같은 구 장비를 신 장비로 교체하기 위해 아래와 같이 새로운 장비를 구성한다.\nflowchart LR subgraph 구 장비 direction LR a([웹 서버]) b[(소스 서버 1)] c[(레플리카 서버 1-1)] d[(레플리카 서버 1-2)] a-. 읽기 + 쓰기 .-\u003e b a-. 읽기 .-\u003e d b--\u003ec b--\u003ed b--\u003ee end subgraph 신 장비 direction LR e[(레플리카 서버 1-3 / 소스 서버 2)] f[(레플리카 서버 2-1)] g[(레플리카 서버 2-2)] e--\u003ef e--\u003eg end 위와 같이 복제를 구성하여 기존 구 장비에서 진행되는 복제를 통해 같이 동기화 될 수 있게 구성한다.\nflowchart LR subgraph 구 장비 direction LR a([웹 서버]) b[(소스 서버 1)] c[(레플리카 서버 1-1)] d[(레플리카 서버 1-2)] a-. 읽기 + 쓰기 (변경 전) .-\u003e b a-. 읽기 .-\u003e d b--\u003ec b--\u003ed b--\u003ee end subgraph 신 장비 direction LR e[(레플리카 서버 1-3 / 소스 서버 2)] f[(레플리카 서버 2-1)] g[(레플리카 서버 2-2)] a-. 읽기 + 쓰기 (변경 후) .-\u003e e e--\u003ef e--\u003eg end 복제가 완료되면 구 장비 그룹 서버를 모두 복제 그룹에서 제외시킨다.\nflowchart LR subgraph 신 장비 direction LR a([웹 서버]) e[(레플리카 서버 1-3 / 소스 서버 2)] f[(레플리카 서버 2-1)] g[(레플리카 서버 2-2)] a-. 읽기 + 쓰기 .-\u003e e a-. 읽기 + 쓰기 .-\u003e f e--\u003ef e--\u003eg end 체인 형태의 복제를 구성하려면 중간 계층에서 레플리카 서버이면서 동시에 소스 서버 역할을 하는 서버에서 바이너리 로그와 log_slave_updates 시스템 변수가 반드시 활성화 되어 있어야 한다. 체인 복제 구성을 사용할 때는 중간 계층의 서버에서 장애가 발생하는 경우 하위 계층의 레플리카 서버들도 복제가 중단되므로 장애 처리시 복잡도가 조금 더 높을 수 있다. 듀얼 소스 복제 구성 듀얼 소스 복제 구성은 두 개의 MySQL 서버가 서로 소스 서버이자 레플리카 서버로 구성되어 있는 형태이다.\nflowchart LR a([웹 서버]) b[(소스/레플리카 서버 1)] c[(소스/레플리카 서버 2)] a-. 읽기 + 쓰기 .-\u003e b a-. 읽기 + 쓰기 .-\u003e c b\u003c--\u003ec 듀얼 소스 구성은 두 MySQL 서버 모두 쓰기가 가능하다는 것이 큰 특징이다.\n각 서버에서 변경한 데이터는 복제를 통해 다시 각 서버에 적용되므로 양쪽에서 쓰기가 발생하지만 두 서버는 서로 동일한 데이터를 갖는다. 목적에 따라 ACTIVE-PASSIVE 또는 ACTIVE-ACTIVE 형태로 사용할 수 있다. ACTIVE-PASSIVE 하나의 MySQL 서버에서만 쓰기 작업이 수행되는 형태이다.\n싱글 레플리카 구성과 다른점은 예비 서버인 다른 MySQL 서버가 바로 쓰기 작업이 가능한 상태이므로 작업이 수행되고 있는 서버에서 문제 발생 시 별도의 설정 변경 없이 바로 예비용 서버로 쓰기 작업을 전환할 수 있다. 한 서버에서 다른 서버로 바로 쓰기가 전활될 수 있는 환경이 필요한 경우 사용된다. ACTIVE-ACTIVE 두 서버 모두에 쓰기 작업을 수행하는 형태이다.\n주로 지리적으로 매우 떨어진 위치에서 유입되는 쓰기 요청도 원활하게 처리하기 위해 사용된다. 각 지역에 위치한 MySQL 서버로 수행되게 끔 구현하여 처리하며, 최종적으로 같은 데이터들을 갖게 된다. 하지만 복제를 통해 다른 지역의 서버로부터 넘어온 트랜잭션이 적용되기까지는 다소 시간이 걸려 적용 전까지 일관되지 않은 데이터를 가질 수 있다. 주의사항 동일한 데이터를 각 서버에서 변경 두 서버 모두에서 쓰기가 발행하는 ACTIVE-ACTIVE 형태 동일한 데이터에 대한 변경 트랜잭션이 서버에 동시점에 유입되는 경우 나중에 처리된 트랜잭션의 내용이 최종적으로 반영함 이러한 경우 예상하지 못한 방향으로 데이터가 처리될 수 있음 테이블에서 Auto-Increment 사용 거의 동일한 시점에 새로운 데이터가 각 서버로 유입되었을 때 같은 키 값을 가질 수 있다. ACTIVE-ACTIVE 형태에서는 동시점에 동일한 데이터를 변경하는 트랜잭션이 있어서는 안되며, 테이블의 Auto-Increment 키 사용을 지양하고 애플리케이션 단에서 글로벌하게 값을 생성해서 사용하는 것이 좋다.\nACTIVE-PASSIVE 형태라도동시에 쓰기 요청이 유입된다면 발생한다.\n멀티 소스 복제 구성 멀티 소스 복제 구성은 하나의 레플리카 서버가 둘 이상의 소스 서버를 갖는 형태를 말한다.\nflowchart TD a[(소스 서버 1)] b[(소스 서버 2)] c[(소스 서버 3)] d[(소스 서버 4)] e[(레플리카 서버)] a--\u003ee b--\u003ee c--\u003ee d--\u003ee 멀티 소스 복제 기능은 MySQL 5.7.6 버전에서 처음 도입되었으며, 다음과 같은 목적으로 사용된다.\n여러 MySQL 서버에 존재하는 각기 다른 데이터를 하나의 MySQL 서버로 통합 데이터 분석 같은 경우 조금 더 빠르고 편리하게 분석을 수행하고자 할 때 멀티 소스 복제 형태를 사용하면 효과적이다. 여러 MySQL 서버에 샤딩돼 있는 테이블 데이터를 하나의 테이블로 통합 늘어날 서비스 트래픽에 대비해 서버들을 동일한 테이블 스키마 구조를 가지는 샤드 형태로 구성했지만 트래픽이 유입되지 않은 경우 멀티 소스 복제를 구성하여 샤딩된 테이블들의 데이터를 통합해 MySQL 서버 수를 줄일 수 있다. 여러 MySQL 서버에 데이터들을 모아 하나의 MySQL 서버에서 백업을 수행 다수의 서버의 데이터를 하나의 서버에서 백업하고자 할 때도 멀티 소스 복제 구성을 통해 손쉽게 구현할 수 있다. 멀티 소스 복제 형태를 사용할 때는 각 소스 서버로부터 유입되는 변경 이벤트들이 레플리카 서버로 복제되었을 때 서로 충돌을 일으킬 만한 부분이 없는지 사전에 충분한 검토가 필요하다.\n멀티 소스 복제의 레플리카 서버는 각 소스 서버들의 대체 서버로 사용하기에는 어려움이 있으므로 장애 대비용 레플리카 서버는 멀티 소스가 아닌 각 소스 서버와 일대일 복제로 연결된 별도의 서버로 구축하는 것이 좋다.\n멀티 소스 복제 동작 멀티 소스 복제에서 레플리카 서버는 자신과 연결된 소스 서버들의 변경 이벤트들을 동시점에 병렬로 동기화한다.\n각 소스 서버들에 대한 복제가 독립적으로 처리되는 것을 의미하며, 각각의 독립된 복제 처리를 채널(Channel)이라고 한다.\n각 복제 채널은 개별적인 레플리케이션 I/O 스레드, 릴레이 로그, 레플리케이션 SQL 스레드를 가지며, 채널의 이름은 어느 소스 서버와의 복제연결인지를 구별할 수 있는 식별자 역할을 한다.\n최대 256개 복제 채널을 이용할 수 있다.\n멀티 소스 복제 구축 기존의 단일 소스 복제와 달리 구축하는 과정에 큰 차이가 있는 것은 아니다.\n복제를 연결하기 위해 소스 서버들의 백업 데이터를 레플리카 서버로 적재해야 하는데, 기존과 달리 여러 대의 소스 서버의 백업 데이터를 가져와야 하므로 이부분의 작업이 조금 번거롭고 까다롭게 느껴질 수 있다.\n","date":"2023-09-07T12:47:10+09:00","image":"https://codemario318.github.io/post/real-mysql/16/6/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/16/6/","title":"16.6 복제 - 복제 토폴로지"},{"content":"MySQL 서버에서는 소스 서버와 레플리카 서버 간의 복제 동기화에 대해 두가지 방식을 제공한다.\n비동기 복제(Asynchronous replication): 오래전부터 사용되어왔다. 반동기 복제(Semi-synchronous replication): MySQL 5.5 버전부터 도입되었다. 비동기 복제 MySQL 복제는 기본적으로 비동기 방식으로 동작한다.\n비동기 방식이란 소스 서버가 자신과 복제 연결된 레플리카 서버에서 변경 이벤트가 정상적으로 전달되어 적용됐는지를 확인하지 않는다.\n소스 서버에서 커밋된 트랜잭션은 바이너리 로그에 기록되며, 레플리카 서버에서는 주기적으로 신규 트랜잭션에 대한 바이너리 로그를 소스 서버에 요청한다.\n비동기 복제 방식에서 소스 서버는 레플리카 서버로 변경 이벤트가 잘 전달됐는지, 실제로 적용됐는지 알지 못하며 이에 대한 어떠한 보장도 하지 않는다.\n이로 인해 소스 서버에 장애가 발생하면 소스 서버에서 최근까지 적용된 트랜잭션이 레플리카 서버로 전송되지 않아 누락된 트랜잭션이 발생할 수 있다.\n만약 소스 서버 장애로 인해 레플리카 서버를 새로운 소스 서버로 승격시키는 경우 사용자는 이 레플리카 서버가 소스 서버로부터 전달받지 못한 트랜잭션이 있는지 직접 확인하고 그런 것들이 있다면 필요 시 레플리카 서버에 수동으로 다시 적용해야 한다.\n이처럼 비동기 복제는 소스 서버가 레플리카 서버의 동기화 여부를 보장하지 않는다는 것이 가장 큰 단점이지만 소스 서버가 각 트랜잭션에 대해 레플리카 서버로 전송되는 부분을 고려하지 않기 때문에 몇가지 장점이 있다.\n트랜잭션 처리시 좀 더 빠른 성능을 보인다. 레플리카 서버에 문제가 생기더라도 소스 서버에는 아무런 영향도 받지 않는다. 비동기 복제 방식은 소스 서버에 레플리카 서버를 여러 대 연결한다고 해도 소스 서버에서 큰 성능 저하가 없다. 10대 이상 연결하는 경우 성능 저하가 있을 수 있다. 레플리카 서버에 무거운 쿼리가 실행되어 성능 저하가 있더라도 소스 서버와는 관계가 없으므로 분석 용도 등으로 사용하기 좋다. 이러한 장점으로 인해 레플리카 서버를 확장해서 읽기 트래픽을 분산하는 용도로 사용하기 적합하다.\n반동기 복제 반동기 복제는 비동기 복제보다 좀 더 향상된 데이터 무결성을 제공하는 복제 동기화 방식이다.\n소스 서버가 레플리카 서버에 변경 이벤트를 보냄 레플리카 서버가 전달받은 변경 이벤트를 릴레이 로그에 기록 후 응답(ACK)을 보냄 소스 서버가 레플리카 서버에게 응답을 받으면 트랜잭션을 완전히 커밋 클라이언트에 결과를 반환 이러한 과정을 통해 소스 서버에서 커밋되어 정상적으로 결과가 반환된 모든 트랜잭션들에 대해 적어도 하나의 레플리카 서버에는 해당 트랜잭션들이 전송되었음을 보장한다.\n하지만 레플리카 서버에 전송이 되었음을 보장한다는 것이지, 실제로 복제된 트랜잭션이 레플리카 서버에 적용된 것까지 보장하는 것은 아니다.\n반동기 복제에서는 소스 서버가 트랜잭션 처리 중 어느 지점에서 레플리카 서버의 응답을 기다리느냐에 따라 소스 서버에서 장애가 발생했을 때 사용자가 겪을 수 있는 문제 상황이 조금 다를 수 있다.\n사용자는 rpl_semi_sync_master_wait_point 시스템 변수를 통해 소스 서버가 레플리카 서버의 응답을 기다리는 지점을 제어할 수 있다.\nAFTER_SYNC:\n소스 서버에서는 각 트랜잭션을 바이너리 로그에 기록하고 난 후 스토리지 엔진에 커밋하기 전 단계에서 레플리카 서버의 응답을 기다리게 된다. 레플리카 서버로부터 정상적으로 응답이 내려오면 소스 서버는 그때 스토리지 엔진을 커밋해서 트랜잭션에 대한 처리를 완전히 끝내고 트랜잭션을 실행한 클라이언트에 그 처리 결과를 반환한다. AFTER_COMMIT\n소스 서버에서 트랜잭션을 바이너리 로그에 기록하고 스토리지 엔진에서의 커밋도 진행하고 나서 최종적으로 클라이언트에 결과를 반환하기 전에 레플리카 서버의 응답을 기다린다. 레플리카 서버로부터 응답이 내려오면 그때 클라이언트는 처리 결과를 얻고 그 다음 쿼리를 실행할 수 있다. 처음 반동기 복제가 도입됐을 때는 스토리지 엔진 커밋까지 완료 후 대기하는 AFTER_COMMIT 방식으로만 동작했으나, MySQL 5.7 버전에서 AFTER_SYNC 방식이 도입된 후 MySQL 8.0 버전에서 기본 동작 방식으로 채택되었다.\nAFTER_SYNC와 AFTER_COMMIT 비교 AFTER_SYNC 방식은 AFTER_COMMIT보다 무결성이 강화된 방식으로 다음과 같은 장점이 있다.\n팬텀 리드 AFTER_COMMIT에서는 트랜잭션이 스토리지 엔진 커밋까지 처리된 후 레플리카 서버의 응답을 기다리는데, 스토리지 엔진 커밋까지 완료된 데이터는 다른 세션에서도 조회가 가능하다.\n따라서 소스 서버가 어떤 트랜잭션에 대해 스토리지 엔진 커밋 후 레플리카 서버로부터 응답을 기다리고 있는 상황에서 소스 서버의 장애가 발생한 경우, 사용자는 이후 새로운 소스 서버로 승격된 레플리카 서버에서 데이터를 조회할 때 자신이 이전 소스 서버에서 조회했던 데이터를 보지 못할 수도 있다.\nAFTER_SYNC는 소토리지 엔진 커밋 전에 레플리카 서버의 응답을 기다리므로 응답을 기다리던 중에 소스 서버에 장애가 발생하더라도 팬텀 리드 현상을 겪지 않게 한다.\n장애 복구 AFTER_COMMIT은 소스 서버에서 커밋되었으나 레플리카 서버로 복제는 되지 않은 상황에서 장애가 발생한 소스 서버를 재사용하는 경우, 사용자가 수동으로 트랜잭션들을 롤백시켜야한다.\nAFTER_SYNC 방식은 레플리카 서버에 복제되지 않았지만 소스 서버에는 커밋되어 실제 데이터에 반영된 트랜잭션들이 존재하는 경우가 발생하지 않기 때문에 별도 롤백 처리를 하지 않아도 된다.\n반동기 복제라 하더라도 소스 서버에 장애가 발생했을 때 소스 서버의 데이터와 레플리카 서버의 데이터가 달라지는 경우가 발생할 수 있으므로 장애가 발생한 소스 서버를 바로 재사용하지 않는 것이 좋다.\n유입된 트랜잭션의 수가 많다면 레플리카 서버의 트랜잭션 반영 여부를 확인하는 것은 거의 불가능하므로, 백업된 데이터를 새로 구축해 사용하는 것이 좋다.\n성능 및 주의사항 반동기 복제는 트랜잭션을 처리하는 중에 레플리카 서버의 응답을 기다리므로 비동기 방식과 비교했을 때 트랜잭션의 처리 속도가 더 느릴 수 있다. 최소 레플리카 서버로 응답을 요청하고 전달받기 까지의 네트워크 왕복 시간만큼 더 걸림 레플리카 서버에서 응답이 늦어지는 경우 더 큰 지연 발생 이처럼 네트워크 통신으로 인한 지연을 예방하기 위해 레플리카 서버 물리적으로 가깝게 위치하는게 유리할 수 있다. 소스 서버에서 설정된 타임아웃 시간을 초과하면 자동으로 비동기 복제 방식으로 전환한다. 반동기 복제 설정 방법 MySQL 서버에서 반동기 복제 기능은 플러그인 형태로 구현돼 있으므로 이를 사용하려면 먼저 관련 플러그인을 설치해야 한다.\n1 2 3 4 5 /* 소스 서버 */ INSTALL PLUGIN rpl_semi_sync_master SONAME \u0026#39;semisync_mastser.so\u0026#39;; /* 레플리카 서버 */ INSTALL PLUGIN rpl_semi_sync_slave SONAME \u0026#39;semisync_slave.so\u0026#39;; 플러그인이 정상적으로 설치됐는지는 information_schema.PLUGINS 테이블을 조회하거나 SHOW PLUGINS 명령을 통해 확인할 수 있다.\n1 2 3 4 5 SELECT PLUGIN_NAME, PLUGIN_STATUS FROM information_schema.PLUGINS WHERE PLUGIN_NAME LIKE \u0026#39;%semi%\u0026#39;; SHOW PLUGINS; 반동기 복제 사용을 위해 관련 시스템 변수들을 적절히 설정해야 하는데, 이 시스템 변수들은 플러그인이 정상적으로 설치된 이후 SHOW GLOBAL VARIABLES 명령 등에서 확인할 수 있다.\nrpl_semi_sync_master_enabled: 소스 서버에서 반동기 복제 활성화 여부를 제어한다. ON(1), OFF(0) rpl_semi_sync_master_timeout: 소스 서버에서 레플리카 서버의 응답이 올 때까지 대기하는 시간으로, 밀리초 단위로 설정할 수 있다. 레플리카 서버의 응답이 지정된 시간을 초과하면 비동기 복제로 전환한다. 기본값: 10000 (10초) rpl_semi_sync_master_trace_level: 소스 서버에서 반동기 복제에 대해 디버깅 시 출력되는 로그 수준에 대한 레벨을 지정한다. 1, 16, 32, 64 지정 가능 rpl_semi_sync_master_wait_for_slave_count: 소스 서버에서 반드시 응답을 받아야하는 레플리카 수를 결정한다. 기본값: 1, 설정 가능 값: 1 ~ 655535 응답을 받아야 하는 레플리카 수가 많을수록 소스 서버에서의 처리 성능은 저하될 수 있다. rpl_semi_sync_master_wait_no_slave: 설정된 타임아웃 시간 동안 소스 서버에서 반동기 복제로 연결된 레플리카 서버 수가 지정된 수 보다 적어졌을 때 어떻게 처리할것인지 결정한다. ON(1): 반동기 복제를 그대로 유지, OFF(0): 즉시 비동기 복제로 전환 rpl_semi_sync_master_wait_point: 소스 서버가 트랜잭션 처리 단계 중 레플리카 서버의 응답을 대기하는 지점을 설정.(AFFTER_SYNC(기본값), AFFTER_COMMIT) rpl_semi_sync_slave_enabled: 레플리카 서버에서 반동기 활성화 여부를 제어. ON(1), OFF(0) rpl_semi_sync_slave_trace_level: 레플리카 서버에서 반동기 복제에 대한 디버깅 수준 레벨을 설정한다. 1, 16, 32, 64 지정 가능 rpl_semi_sync_master_wait_point 시스템 변수는 MySQL 5.7.2 버전에서 AFFTER SYNC 방식이 도입되며 추가된 변수로, 이전 버전과 이후 버전 간의 반동기 복제시 설정시 반동기 복제가 제대로 동작하지 않을 수 있다.\n소스 서버와 레플리카 서버가 기존에 복제가 실행 중인 상태라면 반동기 복제 적용을 위해 다음과 같이 레플리카 서버에서 레플리케이션 I/O 스레드를 재시작 해야한다.\n반동기 복제가 잘 적용되었는지는 SHOW GLOBAL STATUS 명령을 통해 확일할 수 있다.\n","date":"2023-09-06T12:47:10+09:00","image":"https://codemario318.github.io/post/real-mysql/16/5/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/16/5/","title":"16.5 복제 - 복제 동기화 방식"},{"content":"개요 이번에는 깃 브랜치 전략을 바탕으로 깃 브랜치를 구성해보고, 이에 맞는 CI/CD를 구성해보겠습니다.\n이전 직장에서는 Git flow 브랜치 전략과 Gitlab flow 브랜치 전략과 유사한 방식으로 브랜치를 운용했습니다.\n관련 문서에는 Git flow 브랜치 전략으로 구성했다고 되어있었는데, 지속적으로 사용하면서 개발자들이 편하게 느끼는 방식으로 자연스럽게 바뀐 듯 했습니다.\n그리고 Gitlab CI/CD를 통해 CI/CD가 구성되어 있었는데, 서버 로직에 jQuery 뿐만 아니라 Vue 등이 한 프로젝트에 존재하는 조금 복잡한 모놀리식 구조에서 Vue를 직접 빌드해서 푸시하는 것이 너무 불편해서 관련 작업을 했었습니다.\nCI/CD가 온프레미스 환경에서 이미 구축되어있던 운영, 테스트, 개발 환경을 유지하기 위해 깃 전환 전(3~4년 전?) SVN에서 사용하던 스크립트를 동작시키는 형태로 사용되고 있었는데 스크립트가 SPA 활용을 고려하지 않았고, 많이 복잡해서 고생했었던 기억이 나네요.\n8년 동안 제대로 진행하지 못한 회사의 계획인 언어 전환을 위해 MSA로 구조를 변경해야한다는 의견을 피력했었습니다.\n이를 위해 MSA 구조 변경을 위한 컨테이너 인프라에서 개발, 테스트, 운영 환경을 어떻게 배포해야 할까, CI/CD는 어떻게 구성하면 좋을까 고민했었는데 퇴사 기념으로 한번 정리해보고 직접 구성해보려고 합니다.\nGitlab CI/CD 작업할 때 자료가 많이 없어 고생했던 기억도 있고, 새로운 내용을 찍먹하기 위해 이번에는 Jenkins으로 해보겠습니다.\n개발 프로세스 다시 이전 직장 이야기를 해보면 개발 프로세스는 다음과 같았습니다.(git-flow 기준으로 설명하겠습니다.)\nrelease 브랜치에서 feature 브랜치를 생성한다. CI/CD로 온프레미스 테스트용 서버들로 feature 브랜치를 복제하고 nginx 설정 추가를 통해 개발 서버를 배포 feature 브랜치에서 작업한다. 개발자가 여러 명 투입되면 한 브랜치를 같이 사용 feature 브랜치로 release 브랜치에 대해 MR을 만들고 개발 테스트와 선택적으로 코드 리뷰를 진행한다. feature 브랜치를 통해 배포된 devlopment, staging, production 환경에서 QA를 진행한다. feature 브랜치를 release 브랜치에 머지하고 release 브랜치를 이용해 devlopment, staging, production 환경에서 QA를 진행한다. production 테스트 진행시 사용자에게 영향을 줄 수 있는 테스트는 금지 매주 화, 목 release 브랜치를 master 브랜치와 머지한다. CI/CD를 통해 배포 문제 위와 같은 개발 프로세스를 거치면서 아래와 같은 불편함을 느꼈습니다.\n작업 영역이 비슷한 경우, release 브랜치에서 발생한 이슈가 내가 작업한 부분 때문에 발생한 이슈인지, 다른 feature에서 발생한 이슈인지 확인하기 까다롭다. 작업을 최대한 겹치지 않게 배정해주시긴 했습니다. release 브랜치에 새로운 feature가 머지되면 새로운 이슈가 발생하여 다른 feature에도 영향을 줄 수 있다. 충돌 처리 실수, 사이드 이펙트 등 release 브랜치에서의 QA 시나리오가 클 경우 정기 배포로 인해 충분히 검증할 시간이 부족할 수 있다. feature 브랜치들이 온프레미스 환경에 배포되어 진행중인 업무가 많다면 서버의 리소스가 부족해진다. 저는 이러한 문제의 원인이 feature가 통합되는 release 브랜치라고 생각했습니다.\n많은 회사에서 Git flow를 채택하여 버전 별 release 환경을 구성하는 방식을 채택하고 있으며, 특히 feature 브랜치를 많이 구성해야하는 경우 이러한 선택을 통해 리스크를 줄이고 있는 것으로 보입니다.\n개인적으로 Git flow 브랜치 전략에서 release 브랜치를 구성하는 이유는 production 버저닝이 가장 크다고 생각합니다.\n많은 소프트웨어 제품, 라이브러리처럼 버전마다 많은 변경을 한번에 적용하고 이전 버전을 계속 사용해야 하는 경우 release를 통해 여러 장점을 취할 수 있지만 웹 서비스처럼 작은 변경 사항들을 지속적으로 빠르게 반영해야고, 이전 버전이 큰 의미가 없는 경우 이러한 버저닝이 주는 장점은 크지 않으며 오히려 부작용이 더 크다고 생각되었습니다.\n그럼에도 불구하고 이전 직장에서 이러한 방식으로 브랜치를 구성한 이유도 납득은 됩니다.\n큰 모놀리식 레거시로 인해 발생하는 이슈를 release를 통해 확인할 수 있어 안심할 수 있음 비교적 최근까지 활용했던 SVN 개발 프로세스를 그대로 활용하기 위해 온프레미스 환경에 맞추어 작성된 복잡한 배포 스크립트 수정 부담 기존 개발자들의 학습 부담 등 Github flow Git flow와 같은 복잡한 브랜치 전략을 단순화 하기 위해 GitHub flow와 이를 기본으로 한 여러 브랜치 전략이 제안되었습니다.\nGithub flow는 Github에서 제안된 배포 전략으로 크게 아래와 같은 프로세스를 가집니다.\nmain 브랜치를 기준으로 새로운 기능 또는 버그 수정을 개발하기 위한 브랜치 생성. 변경 사항을 커밋하고, 이를 Pull Request (PR)로 병합 요청. PR에서 코드 검토와 논의 진행. PR 승인 시 main 브랜치로 병합하고 자동 배포. Github Flow는 단순한 구조로 버전 관리가 필요 없으면서 빠른 배포가 필요한 작은 규모의 프로젝트에서 유용하게 활용될 수 있으나 너무 단순한 구조로 인해 비교적 규모가 있는 프로젝트에서는 적합하지 않을 수 있습니다.\n복잡한 워크플로우 지원 부족 긴 작업 처리의 어려움 명시적인 staging 환경 부재 등 이러한 단점으로 아래와 같은 경우 적용이 어려울 수 있습니다.\n다수의 인원이 협업을 하는 경우 작업 수가 많은 경우 등 모놀리식 구조 Mono-repo Gitlab flow release 브랜치를 통해 feature 브랜치를 통합하는 과정이 없이 리뷰 완료 후 배로 배포되는 Github flow에서는 기존 Git 복잡함을 해소하였지만 단순한 구조로 아쉬운 부분들이 있는데, 이러한 부분들을 해소할 수 있는 브랜치 전략으로 Gitlab flow가 있습니다.\nGit flow는 develop 브랜치를 기준으로 새로운 기능들이 추가되고 이러한 변경 내용들을 release 브랜치를 통해 통합되지만, Gitlab flow는 master 브랜치를 기준으로 feature 브랜치를 만들고 feature 브랜치를 통해 만들어지는 기능을 직접 반영하는 방식입니다.\n이러한 기능 중심의 개발을 통해 릴리스 머지에서 발생하는 오버헤드를 예방할 수 있습니다.\nfeature 브랜치를 통해 개발 환경 테스트를 진행하고, master 브랜치를 통해 staging 환경에 대한 배포와 테스트를 진행하며, pre-production 브랜치를 통해 운영 환경에서의 테스트를 완료하고 실제 운영 환경으로 배포되는 흐름을 가지고 있습니다.\n그래서 저는 Gitlab flow 브랜치 전략을 활용해서 프로세스를 구성하고, CI/CD로 테스트 환경까지 만들어보는 작업을 해보려고 합니다. 대략적인 프로세스는 다음과 같습니다.\nmaster 브랜치에서 feature 브랜치를 생성한다. feature 브랜치를 기준으로 CI/CD를 활용하여 dev 환경을 배포한다. feature 브랜치에서 작업을 완료 후 master 브랜치에 MR를 만들고 리뷰를 진행하고 리뷰가 완료되면 master 브랜치에 머지한다. master 기준으로 stg 환경을 배포한다. stg QA가 완료되면 master 브랜치를 pre-production 브랜치로 머지한다. pre-production 기준으로 운영 환경을 배포한다. pre-production QA가 완료되면 실제로 배포한다. Best practices Gitlab flow 브랜치 전략을 사용할 때 아래와 같은 그라운드 룰을 설정하고 지킬것을 권하고 있습니다.\nmaster 브랜치에 대해 직접 커밋하지 않고 꼭 feature 브랜치들을 사용하여 반영 머지 전 코드 리뷰를 쉽게 할 수 있다. 모든 커밋에 대해 모든 테스트를 수행 master 브랜치에 존재하는 모든 기능들이 기본적으로 동작한다고 확신할 수 있어야 한다. feature 브랜치에 새롭게 커밋할 때 master에 병합할 때 master 브랜치에 feature 브랜치를 머지하기 전 코드리뷰 수행 배포는 branch 또는 tag 기준으로 자동을 수행 master에 이미 반영된 커밋에 대해 rebase를 수행 X 기존 테스트와 수정 내용을 확인하기 힘들어 질 수 있다. cherry pick이 힘들어질 수 있다. 모든 feature 브랜치의 시작과 병합 대상은 master 브랜치 길어지는 브랜치 분기를 막을 수 있다. 커밋 메시지는 무슨 역할을 했는지 표현해야함. 핫픽스는 master 먼저 반영할 것 ","date":"2023-08-31T23:21:25+09:00","image":"https://codemario318.github.io/post/infra/container-env/2/cover_hu2346df674ac5a4d10745571793c062e9_26488_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/infra/container-env/2/","title":"2. 깃 브랜치 전략과 CI/CD"},{"content":"MySQL의 복제는 소스 서버의 바이너리 로그에 기록된 변경 내역(바이너리 로그 이벤트)들을 식별하는 방식에 따라 바이너리 로그 파일 위치 기반 복제(Binary Log File Position Based Replication)과 글로벌 트랜잭션 ID 기반 복제(Global Transaction Identifiers Based Replication)로 나뉘는데, 각 방식의 동작 원리와 구축 방법이 있다.\n바이너리 로그 파일 위치 기반 보겢는 MySQL에 복제 기능이 처음 도입됐을 때부터 제공된 방식으로, 레플리카 서버에서 소스 서버의 바이너리 로그 파일명과 파일 내에서의 위치(Offset 또는 Position)로 개별 바이너리 로그 이벤트를 식별하여 복제되는 형태를 말한다.\n바이너리 로그 파일 위치 기반 복제는 소스 서버에서 발생한 각 이벤트에 대한 식별이 반드시 필요하다.\n복제를 처음 구축할 때 레플리카 서버에 소스서버의 어떤 이벤트로부터 동기화를 수행할 것인가에 대한 정보를 설정해야 한다. 복제가 설정된 레플리카 서버는 소스 서버의 어느 이벤트까지 로컬 디스크로 가져왔고 또 적용했는지에 대한 정보를 관리하며 소스 서버에 해당 정보를 전달해 그 이후 바이너리 이벤트로 가져온다. 바이너리 로그 파일 위치 기반 복제에서는 이러한 이벤트 하나하나를 소스 서버의 바이너리 로그 파일명과 파일 내에서의 위치 값(File Offset)의 조합으로 식별하고 이를 통해 자신의 적용 내역을 추적할 수 있다.\n복제를 일시적으로 중단할 수 있다. 재개할 때도 자신이 마지막으로 적용했던 이벤트 이후의 이벤트들부터 다시 읽어올 수 있다. 바이너리 로그에는 각 이벤트별로 이 이벤트가 최초로 발생한 MySQL 서버를 식별하기 위해 server_id를 함께 저장하기 때문에 바이너리 로그 파일 위치 기반 복제에 참여한 MySQL 서버들이 모두 고유한 server_id 값을 가지고 있어야 한다.\nserver_id는 MySQL 서버의 시스템 변수 중 하나로, 사용자가 서버마다 원하는 값으로 설정할 수 있다. (기본값은 1) server_id가 동일할 경우 자신의 서버에서 발생한 이벤트로 간주해서 발생한 이벤트를 적용하지 않는다. 바이너리 로그 파일 위치 기반의 복제 구축 MySQL 서버 간에 복제를 설정할 때는 각 서버에 데이터가 이미 존재하는지 여부와 복제를 어떻게 활용할 것인지 등에 따라 복제 설정 과정 및 구축 방법이 달라진다.\n설정 준비 소스 서버에서 반드시 바이너리 로그가 활성화돼 있어야 한다. 복제 구성원이 되는 각 MySQL 서버가 고유한 server_id값을 가져야한다. MySQL 8.0 에서는 바이너리 로그가 기본적으로 활성화돼 있어, 서버 시작 시 데이터 디렉터리 밑에 binlog라는 이름으로 바이너리 로그 파일이 자동으로 생성된다. server_id 값도 기본적으로 1로 설정되는데, 서버마다 고유한 값을 가져야하므로 다른 값을 설정해주는 것이 좋다. 소스 서버 설정 바이너리 로그 파일 위치나 파일명을 따로 설정하고 싶다면 log_bin 시스템 변수를 통해 원하는 값으로 설정할 수 있다. 필요에 따라 로그 동기화 방식, 바이너리 로그를 캐시하기 위한 메모리 크기, 바이너리 로그 파일 크기, 보관 주기 등도 지정할 수 있다. 1 2 3 4 5 6 7 8 [mysqld] server_id=1 log_bin=/binary-log-dir-path/binary-log-name sync_binlog=1 binlog_cache_size=5M max_binlog_size=512M binlog_expire_logs_seconds=1209600 ... 소스 서버에서 바이너리 로그가 정상적으로 기록되고 있는지는 소스 서버에 로그인하여 SHOW MASTER STATUS 명령을 실행해보면 된다.\n레플리카 서버 설정 레플리카 서버에서 복제를 위해 생성하는 릴레이 로그 파일도 복제 설정 시 기본적으로 데이터 디렉터리 밑에 자동으로 생성된다. 릴레이 로그에 기록된 이벤트는 레플리카 서버에 적용되면 더이상 필요하지 않게 되는데, 필요없어진 릴레이 로그 파일은 레플리카 서버가 자동으로 삭제한다. 릴레이 로그 파일을 자동으로 삭제하지 않고 유지하려면 relay_log_purge 시스템 변수를 OFF로 설정하면 된다. OFF로 설정하면 레플리카 서버의 디스크 여유 공간이 부족하지 않은지 모니터링하는 것이 좋다. 레플리카 서버는 일반적으로 읽기 전용으로 사용되므로 read_only 설정을 사용하는 것이 좋다. 소스 서버의 장애로 레플리카 서버가 소스 서버로 승격될 수 있음을 고려하면 log_slave_updates 시스템 변수도 명시하는 것이 좋다. 기본적으로 레플리카 서버는 복제에 의한 데이터 변경 사항은 자신의 바이너리 로그에 기록하지 않는데 log_slave_updates 시스템 변수를 설정하면 복제에 의한 데이터 변경 내용도 자신의 바이너리 로그에 기록하게 된다. 1 2 3 4 5 6 7 [mysqld] server_id=2 relay_log=/relay-log-dir-path/relay-log-name relay_log_purge=ON read_only log_slave_updates ... 복제 계정 준비 레플리카 서버가 소스 서버로부터 바이너리 로그를 가져오려면 소스 서버에 접속해야 하므로 접속 시 사용할 DB 계정이 필요하다.(복제용 계정이라 함)\n복제를 위해 특별히 새로운 계정을 만들 필요 없이 기존 사용 중인 계정에 복제 관련 권한을 추가로 부여해도 되지만 복제에서 사용되는 계정의 비밀번호는 레플리카 서버의 커넥션 메타데이터에 평문으로 저장되므로 보안 측변을 고려하여 복제에 사용되는 권한만 주어진 별도의 계정을 생성해 사용하는 것이 좋다.\n복제용 계정은 복제를 시작하기 전 소스 서버에 미리 준비돼 있어야 하며, 이 계정은 반드시 REPLICATION SLAVE 권한을 가지고 있어야 한다.\n1 2 CREATE USER \u0026#39;repl_user\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;repl_user_password\u0026#39;; GRANT REPLICATION SLAVE ON *.* TO \u0026#39;repl_user\u0026#39;@\u0026#39;%\u0026#39;; 설명을 위해 복제 계정의 호스트 제한을 \u0026ldquo;%\u0026ldquo;로 설정했지만 보안을 위해 꼭 필요한 IP 대역에서만 복제 연결이 가능하도록 설정하는 것이 좋다.\n데이터 복사 소스 서버의 데이터를 레플리카 서버로 가져와서 적재할때는 MySQL 엔터프라이즈 백업이나 mysqldump 같은 툴을 이용해 소스 서버에서 데이터를 내려받아 레플리카 서버로 복사한다.\nmysqldump를 사용해 소스 서버의 데이터를 덤프할 때는 --single-transaction, --master-data 옵션을 반드시 사용해야 한다.\n--single-transaction: 데이터를 덤프할 때 하나의 트랜잭션을 사용해 덤프가 진행되게 해서 테이블이나 레코드에 잠금을 걸지 않고 InnoDB 테이블들에 대해 일관된 데이터를 덤프받을 수 있게 한다. --master-data: 덤프 시작 시점의 소스 서버의 바이너리 로그 파일명과 위치 정보를 포함하는 복제 설정 구문(CHANGE REPLICATION SOURCE TO, CHANGE MASTER TO)이 덤프 파일 헤더에 기록될 수 있게 한다. 해당 옵션을 사용하면 MySQL 서버에서 바이너리 로그의 위치를 순간적으로 고정하기 위해 FLUSH TABLES WITH READ LOCK 명령을 실행해 글로벌 락(모든 테이블에 대한 읽기 잠금)을 건다. 1: 덤프 파일 내의 복제 설정 구문이 실제 실행 가능한 형태로 기록된다. 2: 해당 구문이 주석으로 처리되어 참조만 할 수 있는 형태로 기록된다. 1 2 mysqldump -uroot -p --single-transaction --master-data=2 \\ --opt --routines --triggers --hex-blob --all-databases \u0026gt; source_data.sql 데이터 덤프가 완료되면 source_data.sql 파일을 레플리카 서버로 옮겨 데이터 적재를 진행한다.\n1 2 ## 서버에 직접 접속해 데이터 적재 명령을 실행 SOURCE /tmp/master_data.sql 1 2 3 4 ## MySQL 서버에 로그인하지 않고 데이터 적재 명령을 실행 ## 다음 두 명령어 중 하나를 사용 mysql -uroot -p \u0026lt; /tmp/source_data.sql cat /tmp/source_data.sql | mysql -uroot -p mysqldump에 지정된 \u0026ndash;master-data 옵션으로 FLUSH TABLES WITH READ LOCK 명령이 실행되기 전에 이미 장시간 실행중인 쿼리가 있다면 글로벌 락 명령어가 실행 중인 쿼리에서 참조하고 있는 테이블들에 대한 잠금을 획득할 수 없어 대기하게 된다.\n글로벌 락 명령어가 대기하는 상황이 발생하면 그 뒤로 유입되는 다른 쿼리들도 연달아 대기해서 쿼리가 실행되지 못하고 적체되어 서비스에 문제가 될 수 있으므로 장시간 실행 중인 쿼리가 있는지, 명령 실행 후에도 대기 현상이 발생하고 있지 않은지 미리 확인하는 것이 좋다.\n복제 시작 mysqldump를 이용해 데이터를 덤프하고, 덤프한 데이터를 레플리카 서버에 적재가 완료되는 기간까지 소스 서버의 데이터가 반영이 되지 않은 상태이다.\n복제를 설정하는 명령은 CHANGE REPLICARTION SOURCE TO(CHANGE MASTER TO) 명령으로, mysqldump로 백업 받은 파일의 헤더 부분에서 해당 명령어를 참조할 수 있다.\n백업받은 파일을 열어 24 라인쯤에 위치한 CHANGE MASTER로 시작하는 내용을 복사해둔다.\n1 2 3 4 5 6 7 less /tmp/source_data.sql ... -- Position to start replication or point-in-time recovery from -- -- CHANGE MASTER TO MASTER_LOG_FILE=\u0026#39;binary-log.000002\u0026#39;, MASTER_LOG_POS=2708; 복사해둔 내용(CHANGE MASTER TO MASTER_LOG_FILE='binary-log.000002', MASTER_LOG_POS=2708;)에 소스 서버 MySQL 서버의 호스트명, 포트, 복제용 사용자 계정, 비밀번호 등을 다음과 같이 추가해 복제 설정 명령을 준비한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 -- // MySQL 8.0.23 이상 CHANGE REPLICATION SOURCE TO SOURCE_HOST=\u0026#39;source_server_host\u0026#39;, SOURCE_PORT=3306, SOURCE_USER=\u0026#39;repl_user\u0026#39;, SOURCE_PASSWORD=\u0026#39;repl_user_password\u0026#39;, SOURCE_LOG_FILE=\u0026#39;binary-log.000002\u0026#39;, SOURCE_LOG_POS=\u0026#39;2708, GET_SOURCE_PUBLIC_KEY=1; -- // MySQL 8.0.23 이하 CHANGE MASTER TO MASTER_HOST=\u0026#39;source_server_host\u0026#39;, MASTER_PORT=3306, MASTER_USER=\u0026#39;repl_user\u0026#39;, MASTER_PASSWORD=\u0026#39;repl_user_password\u0026#39;, MASTER_LOG_FILE=\u0026#39;binary-log.000002\u0026#39;, MASTER_LOG_POS=2708, GET_MASTER_PUBLIC_KEY=1; GET_[SOURCE | MASTER]_PUBLIC_KEY: RSA 키 기반 비밀번호 교환 방식의 통신을 위해 공개키를 소스 서버에 요청할 것인지 여부\n복제 설정에 보안된 연결과 관련된 옵션들을 명시하지 않아 레플리카 서버가 소스 서버와 함호화되지 않는 통신 방식으로 연결되는 경우 별도로 설정하지 않으면 에러가 발생한다. 위 명령을 레플리카 서버의 MySQL에 로그인하여 실행한 후 SHOW [REPLICA | SLAVE] STATUS 명령을 실행해보면 복제 관련 정보가 등록되고 동기화 명령을 대기하는 상태가 되어 [Replica | Slave]_IO_Running과 [Replica | Slave]_SQL_Running 컬럼이 No 표시된다.\n이 상태에서 START [Replica | Slave] 명령을 실행하면 두 컬럼 값이 Yes로 바뀌며 동기화되지 않았던 변경사항들을 소스 서버로 부터 가져와 적용한다.\nSHOW [REPLICA | SLAVE] STATUS 명령의 결과에 나타나는 Seconds_Behind_[Source | Master]의 값이 0이 되면 완전히 동기화됐음을 의미한다.\nSTART [REPLICA | SLAVE] 명령을 실행했는데도 Yes로 변경되지 않는다면 아래 내용들을 확인한다.\n소스 서버의 호스트명 MySQL의 포트 레플리카 서버에서 사용하는 복제용 접속 계정과 비밀번호 소스 서버와 레플리카 서버 간에 네트워크사으이 문제 바이너리 로그 파일 위치 가반의 복제에서 트랜잭션 건너뛰기 복제로 구성돼 있는 MySQL 서버들을 운영하다 보면 레플리카 서버에서 소스 서버로부터 넘어온 트랜잭션이 제대로 실행되지 못하고 에러가 발생해 복제가 멈추는 현상이 발생하기도 한다.\n대부분 사용자의 실수로 인해 발생하는 경우가 많다.(중복 키 에러) 경우에 따라 레플리카 서버에서 문제되는 소스 서버의 트랜잭션을 무시하고 넘어가도록 처리해도 괜찮다면 sql_slave_skip_counter 시스템 변수를 이용해 문제되는 트랜잭션을 건너뛸 수 있다.\n1 2 3 STOP [SLAVE | REPLICA] SQL_THREAD; SET GLOBAL sql_slave_skip_counter=1; START [SLAVE | REPLICA] SQL_THREAD; sql_slave_skip_counter 시스템 변수에는 적용하지 않고 건너뛸 바이너리 로그 이벤트 그룹 수를 지정한다.\n1로 설정했을때 DML 쿼리 문장 하나를 가진 바이너리 로그 이벤트 1개를 무시하는 것이 아닌 현재 이벤트를 포함한 이벤트 그룹을 무시한다. 이벤트 그룹은 트랜잭션을 지원하는 테이블의 경우에는 트랜잭션이 하나의 이벤트 그룹이 되며, 트랜잭션을 지원하지 않는 테이블에서는 DML 문장 하나하나가 이벤트 그룹이 된다.\n하나의 트랜잭션에 여러 개의 DML 쿼리들이 포함되는 경우가 존재한다면 에러가 발생한 쿼리 외에 다른 쿼리들이 예상치 못하게 함께 무시될 수 있으므로 주의해야한다.\n","date":"2023-08-30T23:27:10+09:00","image":"https://codemario318.github.io/post/real-mysql/16/2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/16/2/","title":"16.3.3 복제 (2) - 바이너리 로그 파일 위치 기반 복제"},{"content":"개인적으로 해보고 싶었던 쿠버네티스를 이용하여 컨테이너 인프라 환경을 구축해보려고합니다.\n이전 회사에서 거대한 모놀리식 구조로 운영되던 서비스를 회사 장기 비전인 Java 전환과 MSA 전환을 위해 꼭 필요하다고 졸라서 해보라고 허락을 받고 공부하고 있었는데 결국 퇴사를 하게 되었네요.\n기본적인 내용들은 컨테이너 인프라 환경 구축을 위한 쿠버네티스/도커 책을 통해 학습하였고, 추가로 참고한 내용들은 따로 공유드리겠습니다.\n컨테이너 인프라 환경을 구축하기 위한 첫 걸음으로 Docker를 이용하여 컨테이너에 배포하는 방법을 알아보겠습니다.\n준비 이전에 코프링 찍먹 해보려고 만들었던 프로젝트을 배포하는 실습을 해보겠습니다.\n도커 준비 실습을 위해 로컬 환경에 Docker를 설치해야 합니다.\n설치하는 방법은 다양한데 공식 문서를 따라서 차분하게 진행하시면 어렵지 않게 수행하실 수 있으므로 관련 링크만 남깁니다.\nInstall Docker Desktop 프로젝트 준비 제가 찍먹을 위해 만들었던 코프링 프로젝트는 스프링 공식 가이드의 Building web applications with Spring Boot and Kotlin입니다.\n1 2 mkdir blog \u0026amp;\u0026amp; cd blog curl https://start.spring.io/starter.zip -d language=kotlin -d type=gradle-project-kotlin -d dependencies=web,mustache,jpa,h2,devtools -d packageName=com.example.blog -d name=Blog -o blog.zip 위 명령을 사용하셨다면 Kotlin DSL + Gradle로 빌드할 수 있는 SpringBoot 프로젝트가 아래 구조로 만들어졌을 겁니다.\n1 2 3 4 5 6 7 8 9 10 . ├── Dockerfile ├── HELP.md ├── build ├── build.gradle.kts ├── gradle ├── gradlew ├── gradlew.bat ├── settings.gradle.kts └── src 저는 가이드 문서대로 이미 구현을 한 상태라 추가 작업 없이 확인할 수 있지만, 잘 배포 되었는지 확인하기 위한 간단한 REST 컨트롤러를 만들어주겠습니다.\n패키지 경로 내에 있다면 어떤 이름으로 만들어도 상관은 없습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 package com.example.blog import org.springframework.web.bind.annotation.GetMapping import org.springframework.web.bind.annotation.RequestMapping import org.springframework.web.bind.annotation.RestController @RestController @RequestMapping(\u0026#34;/api/hello\u0026#34;) class HelloController { @GetMapping(\u0026#34;/\u0026#34;) fun hello() = \u0026#34;Hello\u0026#34; } 네트워크 환경 준비 (공유기 사용시) 저와 같이 공유기를 사용한다면 외부 네트워크에서 정상 동작을 확인하기 위해 추가 설정이 필요합니다. 테스트를 환경이 공유기를 사용하지 않는다면 넘기셔도 좋습니다.\n외부 네트워크를 통해 웹 애플리케이션에 접근하려면 애플리케이션이 배포된 서버의 IP와 웹 애플리케이션이 사용 중인 PORT 정보를 알아야 하는데, 저처럼 공유기를 사용하는 환경에서는 공유기가 IP를 할당 받게 됩니다.\n라우터(공유기 등)으로 구성된 내부 네트워크를 서브넷 이라고 하며, 서브넷 자체적으로 내부 IP를 할당하기 때문에 서브넷 내부에서 서로 통신할 수 있습니다.\n따라서 공유기에 들어온 요청을 연결된 내부 네트워크를 통해 웹 애플리케이션을 배포한 로컬 환경으로 요청을 전달해야 하는데, 이러한 처리를 포트 포워딩이라고 합니다.\n포트 포워딩은 공유기가 받은 요청을 내부적으로 연결된 목적지에 전달하는 것이기 때문에 요청을 받는 공유기에 설정해야 하고, 대부분 공유기가 이러한 기능을 제공하고 있습니다.\n현재 연결된 무선 인터넷의 IP 정보가 내부 IP이고, 외부 IP는 mac 기준 아래 명령으로 확인할 수 있습니다.\n1 curl ifconfig.me 공유기 관리자 페이지는 내부 IP에서 마지막이 1로 바꿔 접근하면 됩니다. 저는 같은 경우는 http://192.168.0.1/ 이었습니다.\nIpTime 공유기는 이런 식으로 나올 텐데, 구성은 대부분 같습니다.\n외부 IP, 내부 IP 맞춰 입력하고 외부 포트는 포트 포워딩할 대상 포트 번호를 입력하고, 내부 포트는 로컬 환경에서 배포될 컨테이너의 호스트 포트 번호를 입력하면 됩니다.\n외부 포트 같은 경우 http 포트 번호가 80이라서 설정했는데, 8비트 정수값이라면 어떤 값이 들어가도 상관없습니다.\n다만 브라우저를 통해 접근할 때 외부_IP:외부_포트로 접근하시면 됩니다.\n컨테이너 빌드 Docker 컨테이너 배포를 위해 배포하고자 하는 프로젝트를 컨테이너 이미지로 빌드해야합니다.\n프로젝트 빌드 일반적으로 Docker 컨테이너를 빌드할 때 이미지 크기 및 빌드 시간 최적화를 위해 실행 가능한 애플리케이션만을 이미지에 포함해 빌드합니다.\n실습 같은 Spring boot 프로젝트 같은 경우 빌드를 완료한 후 생성되는 실행 파일인 .jar 파일만을 포함하는 것을 의미합니다.\n1 ./gradlew build 해당 명령을 실행하여 프로젝트를 빌드하면 아래와 같은 구조로 빌드 결과물들이 나옵니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . ├── classes │ └── kotlin ├── generated │ └── source ├── kotlin │ ├── compileKotlin │ ├── compileTestKotlin │ ├── kaptGenerateStubsKotlin │ └── kaptGenerateStubsTestKotlin ├── libs │ ├── \u0026lt;프로젝트명\u0026gt;-\u0026lt;build.gradle에 설정한 버전\u0026gt;-plain.jar │ └── \u0026lt;프로젝트명\u0026gt;-\u0026lt;build.gradle에 설정한 버전\u0026gt;.jar ├── reports │ └── tests ├── resolvedMainClassName ├── resources │ └── main ├── snapshot │ └── kotlin ├── test-results │ └── test └── tmp ├── bootJar ├── jar ├── kapt3 └── test Gradle 같은 경우 별도 설정을 하지 않았다면 build/libs 디렉터리에 \u0026lt;프로젝트명\u0026gt;-\u0026lt;build.gradle에 설정한 버전\u0026gt; 형태로 실행 파일을 만듭니다.\nPlain JAR는 애플리케이션 코드만을 JAR 파일로 패키징 하는 방식으로 배포되는 파일의 크기가 작지만, 실행 시에 종속성을 관리해야 하는 번거로움이 있습니다.\n일반적으로 SpringBoot 애플리케이션을 빌드할 때는 Fat JAR 방식으로 종속성을 내장하여 실행 가능한 형태로 배포하므로 Fat JAR를 컨테이너 이미지에 포함하면 됩니다.\n지금은 직접 명령어를 통해서 프로젝트를 빌드하고 있지만, CI/CD 통해 변경된 내용을 반영하고 빌드해서 배포하는 과정을 자동화할 수 있습니다.\nDockerfile 작성 Dockerfile은 Docker 컨테이너 이미지를 빌드하기 위해 필요한 명령과 필요한 환경 변수 등을 기술한 텍스트 파일입니다.\nDocker 이미지는 실행 가능한 애플리케이션과 해당 애플리케이션을 실행하는 데 필요한 모든 환경 및 종속성을 포함하는데, Dockerfile을 통해 이미지를 어떻게 구성하고 빌드해야 하는지에 대한 명령을 정의할 수 있습니다.\nDockerfile을 프로젝트 최상위 디렉터리에 만들고 docker build 명령을 실행하면 작성 내용에 맞춰 빌드 작업을 수행하게 됩니다.\n그럼 간단하게 Dockerfile을 작성해보겠습니다. 위에서 언급한 것처럼 애플리케이션을 직접 실행할 수 있는 항목들만 컨테이너에 기술해주면 됩니다.\n1 2 3 FROM bellsoft/liberica-runtime-container:jre-17-slim-musl COPY build/libs/*.jar app/app.jar ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;,\u0026#34;/app/app.jar\u0026#34;] Dockerfile을 해석해보면\nFROM: 베이스 이미지를 선택합니다. Ubuntu 기반 Java 17 런타임 환경 이미지인 bellsoft/liberica-runtime-container:jre-17-slim-musl로 설정하였습니다. 베이스 이미지는 Docker Hub 에서 찾아볼 수 있고, 저는 jre 17 중 가장 용량이 작은 이미지로 선택했습니다. 기본적인 환경이 구성된 이미지는 대부분 Docker Hub에서 찾을 수 있고 해당 이미지의 이미지 레이어도 확인 가능합니다. COPY: 호스트 시스템의 디렉터리에서 컨테이너 내부 지정 위치로 복사합니다. Java 실행파일을 컨테이너 환경 최상위 디렉터리에 app.jar 이름으로 복사하게 됩니다. ENTRYPOINT: 컨테이너가 실행될 때 실행할 명령어를 정의합니다. ENTRYPOINT는 CMD로 대체할 수도 있는데 명령어가 실행되는 방식이 조금 다릅니다. 이는 추후에 다뤄보겠습니다. Docker 이미지 빌드 이제 Docker 이미지를 빌드해보겠습니다. 기본 사용법은 아래와 같습니다.\n1 docker build [OPTIONS] PATH | URL | - 여러 옵션이 있지만 -t(--tag) 옵션을 사용하여 이름과 태그를 설정할 수 있습니다. 이름:태그 형식으로 작성하고, 태그는 생략될 수 있습니다.\n1 docker build -t \u0026lt;이미지 이름:태그\u0026gt; \u0026lt;Dockerfile이 위치한 디렉터리 경로\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [+] Building 9.0s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 281B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/bellsoft/liberica-runtime-container:jre-17-slim-musl 2.6s =\u0026gt; [auth] bellsoft/liberica-runtime-container:pull token for registry-1.docker.io 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 344B 0.0s =\u0026gt; [1/2] FROM docker.io/bellsoft/liberica-runtime-container:jre-17-slim-musl@sha256:95ebc03f0f27568a915cc2616a8e9f12e3eb33f4c3197c71606c186 6.1s =\u0026gt; =\u0026gt; resolve docker.io/bellsoft/liberica-runtime-container:jre-17-slim-musl@sha256:95ebc03f0f27568a915cc2616a8e9f12e3eb33f4c3197c71606c186 0.0s =\u0026gt; =\u0026gt; sha256:95ebc03f0f27568a915cc2616a8e9f12e3eb33f4c3197c71606c186e84096969 529B / 529B 0.0s =\u0026gt; =\u0026gt; sha256:216addf7a20c81782208e43c78219ebf492c4ac44dc6ebd2fa4f04b7fe5573c9 1.77kB / 1.77kB 0.0s =\u0026gt; =\u0026gt; sha256:a53839d7740238f431dd368e69ab2fde8df89f7fe7df9fc1f1bb50aa59ccd6a1 39.73MB / 39.73MB 5.1s =\u0026gt; =\u0026gt; extracting sha256:a53839d7740238f431dd368e69ab2fde8df89f7fe7df9fc1f1bb50aa59ccd6a1 0.9s =\u0026gt; [2/2] COPY build/libs/*.jar app/app.jar 0.1s =\u0026gt; exporting to image 0.1s =\u0026gt; =\u0026gt; exporting layers 0.1s =\u0026gt; =\u0026gt; writing image sha256:9830437234c832dc3ad45e9833a27e89449889296bdf9ec9091f6680d648d8a8 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/kt-spring-docker-edit 0.0s Use \u0026#39;docker scan\u0026#39; to run Snyk tests against images to find vulnerabilities and learn how to fix them 저는 kt-spring-docker로 이름을 설정하고 태그는 설정하지 않았습니다. 명령어를 실행하면 위와 같은 형식으로 빌드 과정을 출력합니다.\n결과는 docker images 명령으로 확인할 수 있습니다.\n1 docker images 1 2 3 4 REPOSITORY TAG IMAGE ID CREATED SIZE ... kt-spring-docker-edit latest 9830437234c8 7 minutes ago 164MB ... 컨테이너 배포 컨테이너 배포를 위한 기본 명령어는 아래와 같습니다.\n1 docker run [OPTIONS] IMAGE [COMMAND] [ARG...] 다양한 옵션이 존재하지만 그 중 -p(--publish) 옵션은 호스트 포트와 컨테이너 포트를 연결하여 컨테이너를 실행할 수 있습니다.\n1 docker run -p \u0026lt;호스트_포트번호:컨테이너_포트번호\u0026gt; \u0026lt;컨테이너 식별자\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 WARNING: The requested image\u0026#39;s platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested . ____ _ __ _ _ /\\\\ / ___\u0026#39;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | \u0026#39;_ | \u0026#39;_| | \u0026#39;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) \u0026#39; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v3.1.2) 2023-08-30T17:29:41.102Z INFO 1 --- [ main] c.m.k.KotlinTutorialApplicationKt : Starting KotlinTutorialApplicationKt v0.0.1-SNAPSHOT using Java 17.0.8 with PID 1 (/app/app.jar started by root in /) 2023-08-30T17:29:41.124Z INFO 1 --- [ main] c.m.k.KotlinTutorialApplicationKt : No active profile set, falling back to 1 default profile: \u0026#34;default\u0026#34; 2023-08-30T17:29:46.689Z INFO 1 --- [ main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode. 2023-08-30T17:29:47.122Z INFO 1 --- [ main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 375 ms. Found 2 JPA repository interfaces. 2023-08-30T17:29:51.800Z INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http) 2023-08-30T17:29:51.850Z INFO 1 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat] 2023-08-30T17:29:51.851Z INFO 1 --- [ main] o.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/10.1.11] 2023-08-30T17:29:52.320Z INFO 1 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext 2023-08-30T17:29:52.334Z INFO 1 --- [ main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 10170 ms 2023-08-30T17:29:53.238Z INFO 1 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Starting... 2023-08-30T17:29:54.047Z INFO 1 --- [ main] com.zaxxer.hikari.pool.HikariPool : HikariPool-1 - Added connection conn0: url=jdbc:h2:mem:71313eab-1da9-46b3-bcd5-df59c1121b0b user=SA 2023-08-30T17:29:54.057Z INFO 1 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Start completed. 2023-08-30T17:29:54.322Z INFO 1 --- [ main] o.hibernate.jpa.internal.util.LogHelper : HHH000204: Processing PersistenceUnitInfo [name: default] 2023-08-30T17:29:54.538Z INFO 1 --- [ main] org.hibernate.Version : HHH000412: Hibernate ORM core version 6.2.6.Final 2023-08-30T17:29:54.576Z INFO 1 --- [ main] org.hibernate.cfg.Environment : HHH000406: Using bytecode reflection optimizer 2023-08-30T17:29:55.140Z INFO 1 --- [ main] o.h.b.i.BytecodeProviderInitiator : HHH000021: Bytecode provider name : bytebuddy 2023-08-30T17:29:56.491Z INFO 1 --- [ main] o.s.o.j.p.SpringPersistenceUnitInfo : No LoadTimeWeaver setup: ignoring JPA class transformer 2023-08-30T17:29:58.111Z INFO 1 --- [ main] o.h.b.i.BytecodeProviderInitiator : HHH000021: Bytecode provider name : bytebuddy 2023-08-30T17:30:01.316Z INFO 1 --- [ main] o.h.e.t.j.p.i.JtaPlatformInitiator : HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform] 2023-08-30T17:30:01.550Z INFO 1 --- [ main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit \u0026#39;default\u0026#39; 2023-08-30T17:30:04.136Z WARN 1 --- [ main] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning 2023-08-30T17:30:06.776Z INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path \u0026#39;\u0026#39; 2023-08-30T17:30:06.861Z INFO 1 --- [ main] c.m.k.KotlinTutorialApplicationKt : Started KotlinTutorialApplicationKt in 28.216 seconds (process running for 31.678) 2023-08-30T17:30:47.268Z INFO 1 --- [nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring DispatcherServlet \u0026#39;dispatcherServlet\u0026#39; 2023-08-30T17:30:47.272Z INFO 1 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Initializing Servlet \u0026#39;dispatcherServlet\u0026#39; 2023-08-30T17:30:47.288Z INFO 1 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Completed initialization in 14 ms docker ps 명령을 통해 현재 실행 중인 컨테이너를 확인할 수 있고, 앞서 설명해 드린 외부 IP를 통해 접근해보면 정상적으로 배포된 것을 확인할 수 있습니다.\n1 2 3 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9d85dbbbf374 kt-spring-docker-edit-2 \u0026#34;java -jar /app/app.…\u0026#34; About an hour ago Up About an hour 0.0.0.0:8080-\u0026gt;8080/tcp bold_chaum ","date":"2023-08-29T14:52:25+09:00","image":"https://codemario318.github.io/post/infra/container-env/1/cover_hu2346df674ac5a4d10745571793c062e9_26488_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/infra/container-env/1/","title":"1. Docker를 이용한 컨테이너 배포"},{"content":"데이터베이스를 사용하고 운영할 때 가장 중요한 두 가지 요소를 꼽으라면 바로 확장성과 가용성이다.\n서비스에서 발생하는 대용량 트래픽을 안정적으로 처리하기 위해서는 데이터베이스 서버의 확장이 필수적이며, 사용자가 언제든지 안정적인 서비스를 이용할 수 있게 하려면 DBMS 서버를 포함함 하위 시스템들의 가용성이 반드시 뒷받침돼야 한다.\n이 두 요소를 위해 가장 일반적으로 사용되는 기술이 복제(Replication)이다.\n개요 복제는 한 서버에서 다른 서버로 데이터가 동기화되는 것을 말하며, 원본 데이터를 가진 서버를 소스(Source) 서버, 복제된 데이터를 가지는 서버를 레플리카(Replica) 서버라고 부른다.\n소스 서버에서 데이터 및 스키마에 대한 변경이 최초로 발생하며, 레플리카 서버에서는 이러한 변경 내역을 소스 서버로부터 전달받아 자신이 가지고 있는 데이터에 반영함으로써 소스 서버에 저장된 데이터와 동기화 시킨다.\n대부분의 DBMS에서 복제 기능을 제공하며, 일반적으로 서비스에서 사용될 DB 서버를 구축할 때는 메인으로 사용될 소스 서버 한 대와 복제를 통해 소스 서버와 동일한 데이터를 가진 레플리카 서버를 한 대 이상 함께 구축한다.\n이는 서비스의 메인 DB 서버인 소스 서버에 문제가 생겼을 때를 대비해려는 목적이 제일 크지만 복제를 통해 레플리카 서버를 구축하는 데는 여러가지 목적이 있다. 스케일 아웃(Scale-out) 서비스를 운영하다 보면 사용자가 늘어나고, 이에 따라 DB 서버로 유입되는 트래픽도 자연히 증가해 DB 서버의 부하가 높아진다.\n이러한 경우 서버의 성능을 올리는 스케일 업(Scale-up)으로 애플리케이션의 큰 변화 없이 쉽게 해결할 수 있지만, 서버 한 대에서 처리할 수 있는 트래픽은 한계가 있으므로 일시적인 해결 방법이다.\n이러한 경우 동일한 데이터를 가진 DB 서버를 여러개 사용하는 스케일 아웃 방식으로 트래픽 증가를 조금 더 유연하게 대응할 수 있다.\n데이터 백업 DB 서버에는 다양한 종류의 데이터가 저장되며, 사용자의 실수로 데이터가 삭제되었을 때 서비스 운영에 치명적인 영향을 줄 수도 있다.\n이러한 경우를 대비하기 위해 DB 서버에 저장된 데이터들을 주기적으로 백업하는 것이 필수적인데, 일반적으로 데이터가 저장돼 있는 DB 서버에서 백업 프로그램이 실행되어 백업을 진행한다.\n이처럼 동일한 서버 내에서 백업이 실행되는 경우 백업 프로그램과 DBMS가 서버의 자원을 공유하기 때문에 백업으로 인해 실행 중인 쿼리들이 영향을 받아 서비스에 문제가 발생할 가능성이 있다.\n이러한 문제를 방지하기 위해 레플리카 서버를 구축하여 백업을 실행한다.\n데이터 분석 DB 서버에서는 기본적으로 서비스에 사용되는 쿼리들이 실행되지만, 데이터 분석을 위한 분석용 쿼리들을 실행하기도 하는데, 이러한 쿼리들의 경우 대량의 데이터를 조회, 집계 연산을 위한 조회 등 부하가 큰 쿼리들이 주로 실행되어 서비스에서 사용되는 DB 서버를 사용하면 서비스에 장애가 발생할 수 있다.\n이러한 문제를 예방하기 위해 데이터 분석을 위한 레플리카 서버를 구축하여 전용으로 활용하기도 한다.\n데이터의 지리적 분산 DB 서버와 애플리케이션 서버가 물리적으로 서로 먼 거리에 있는 경우 두 서버간의 통신 시간은 거리에 비례하여 늘어나게된다.\n서비스 응답 속도는 애플리케이션 서버의 처리 속도와 더불어 서버 간의 통신 속도에도 영향을 받으므로 서버의 물리적인 위치를 가깝게 구성하는게 좋은데, DB 서버의 위치를 이동시키지 못한다면 복제를 통해 가까운 곳에 레플리카 서버를 구축해 사용하여 개선될 수 있다.\n복제 아키텍처 MySQL 복제는 바이너리 로그를 기반으로 구현되었으며, 소스 서버에서 생성된 바이너리 로그가 레플리카 서버로 전송되고 레플리카 서버에서는 해당 내용을 로컬 디스크에 저장한 뒤 자신이 가진 데이터에 반영함으로써 소스 서버와 레플리카 서버 간의 데이터 동기화가 이루어진다.\nMySQL 서버에서 발생하는 모든 변경 사항은 별도의 로그 파일에 순서대로 기록되는데, 이를 바이너리 로그(Binary Log)라고 한다. 바이너리 로그에는 데이터 변경 내역 뿐만 아니라 테이블의 구조 변경과 계정이나 권한의 변경 정보까지 모두 저장되며, 바이너리 로그에 기록된 각 변경 정보들을 이벤트라고 한다. 레플리카 서버에서 소스 서버의 바이너리 로그를 읽어 들여 따로 로컬 디스크에 저장해둔 파일을 릴레이 로그라고 한다. MySQL 복제는 세 개의 스레드에 의해 작동한다.\n바이너리 로그 덤프 스레드(Binary Log Dump Thread) 소스 서버에 위치한 스레드이며, 레플리카 서버가 데이터 동기화를 위해 소스 서버에 접속해 바이너리 로그 정보를 요청하면, 소스 서버에서 레플리카 서버가 연결될 때 내부적으로 바이너리 로그 덤프 스레드를 생성해서 바이너리 로그의 내용을 레플리카 서버로 전송한다.\n바이너리 로그 덤프 스레드는 레플리카 서벌오 보낼 각 이벤트를 읽을 때 일시적으로 바이너리 로그에 잠금을 수행하며, 이벤트를 읽고난 후에는 바로 잠금을 해제한다.\n이 스레드는 소스 서버에서 SHOW PROCESSLIT 명령을 통해 확인할 수 있다. 레플리케이션 I/O 스레드(Replication I/O Thread) 복제가 시작되면(START REPLICA, START SLAVE) 레플리카 서버는 I/O 스레드를 생성하고, 복제가 멈추면(STOP REPLICA, STOP SLAVE)되면 I/O 스레드는 종료된다.\nI/O 스레드는 소스 서버의 바이너리 로그 덤프 스레드로부터 바이너리 로그 이벤트를 가져와 로컬 서버의 파일(릴레이 로그)로 저장하는 역할을 담당한다.\n소스 서버의 바이너리 로그를 읽어 파일로 쓰는 역할만 하기 때문에 I/O 스레드라 부른다. 이 스레드의 상태는 MySQL의 복제 현황을 보여주는 SHOW REPLICA STATUS(SHOW SLAVE STATUS) 명령의 결과에서 Replica_IO-Running(Slave_SQL_running) 컬럼에 표시된 값을 통해 확인할 수 있다. 레플리케이션 SQL 스레드(Replication SQL Thread) 레플리케이션 I/O 스레드에 의해 작성된 릴레이 로그 파일의 이벤트를 읽고 실행한다.\n레플리케이션 I/O 스레드와 같은 방식으로 상태를 확인할 수 있다.\n주의사항 레플리카 서버에서 레플리케이션 I/O 스레드와 SQL 스레드는 서로 독립적으로 동작하기 때문에 SQL 스레드에서 이벤트를 적용하는 게 느리더라도 I/O 스레드는 그것과 무관하게 정상적으로 빠르게 소스 서버로부터 이벤트를 읽어올 수 있다.\n레플리카 서버에서 소스 서버의 변경 사항들이 적용되는 것은 소스 서버가 동작하는 것과는 별개로 진행되므로 레플리카 서버에 문제가 생기더라도 소스 서버는 전혀 영향을 받지 않는다.\n그러나 소스 서버에 문제가 생겨 정상적으로 동작하지 않게 되면 복제는 에러를 발생시키고 중단된다.(복제 기능만)\n이에 따라 레플리카 서버에서 처리되는 쿼리는 정상적으로 동작하지만, 동기화가 안되어 예전 상태 데이터를 보게 된다.\n이러한 문제를 예방하기 위해 복제가 시작되면 레플리카 서버는 릴레이 로그를 비롯해 총 세가지 유형의 복제 관련 데이터를 생성하고 관리한다.\n릴레이 로그(Relay Log) 레플리케이션 I/O 스레드에 의해 작성되는 파일로, 소스 서버의 바이너리 로그에서 읽어온 이벤트(트랜잭션) 정보가 저장된다. 바이너리 로그와 마찬가지로 현재 존재하는 릴레이 로그 파일들의 목록이 담긴 인덱스 파일과 실제 이벤트 정보가 저장돼 있는 로그 파일들로 구성된다. 릴레이 로그에 저장된 트랜잭션 이벤트들은 레플리케이션 SQL 스레드에 의해 레플리카 서버에 적용된다. 커넥션 메타데이터(Connection Metadata) 레플리케이션 I/O 스레드에서 소스 서버에 연결할 때 사용하는 DB 계정 정보 및 현재 읽고 있는 소스 서버의 바이너리 파일명과 파일 내 위치 값 등이 담겨있다. 이러한 정보는 기본적으로 mysql.slave_master_info 테이블에 저장된다. 어플라이어 메타데이터(Applier Metadata) 레플리케이션 SQL 스레드에서 릴레이 로그에 저장된 소스 서버의 이벤트들을 레플리카 서버에 적용하는 컴폰언트이다. 최근 적용된 이벤트에 대해 해당 이벤트가 저장돼 있는 릴레이 로그 파일명과 파일 내 위치 정보 등을 담고 있다. 레플리케이션 SQL 스레드가 이러한 정보들을 바탕으로 레플리카 서베에 나머지 이벤트들을 적용한다. 이러한 정보는 기본적으로 mysql.slave_relay_log_info 테이블에 저장된다. 커넥션 및 어플라이어 메타데이터는 MySQL 시스템 변수 master_info_repository, relay_log_info_repository를 통해 어떤 태로 데이터를 관리할 지 결정할 수 있다.\nFILE: 커넥션 메타데이터와 어플라이어 메타데이터는 각각 MySQL의 데이터 디렉터리에서 파일로 관리된다. TABLE: MySQL의 mysql 데이터 베이스 내 테이블에 각각 데이터가 저장된다. FILE 타입은 레플리케이션 I/O 스레드로 SQL 스레드가 동작할 때 파일의 내용이 동기화되지 않는 경우가 빈번하게 발생하여 여러 문제를 발생시켰다.\n이로인해 MySQL 8.0.2 버전부터 기본값으로 TABLE이 되었고, FILE 타입은 삭제 예정이다.\n","date":"2023-08-29T10:51:10+09:00","image":"https://codemario318.github.io/post/real-mysql/16/1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/16/1/","title":"16. 복제 (1)"},{"content":"4가지 기본 파티션 기법을 제공하고 있으며, 해시와 키 파티션에대해서는 리니어 파티션과 같은 추가적인 기법도 제공한다.\n레인지 파티션 리스트 파티션 해시 파티션 키 파티션 레인지 파티션 파티션 키의 연속된 범위로 파티션을 정의하는 방법으로, 가장 일반적으로 사용되는 파티션 방법 중 하나다.\nMAXVALUE 키워드를 이용해 명시되지 않은 범위의 키 값이 담긴 레코드를 저장하는 파티션을 정의할 수 있다.\n레인지 파티션의 용도 다음과 같은 성격을 지는 테이블에서는 레인지 파티션을 사용하는 것이 좋다.\n날짜를 기반으로 데이터가 누적되고 연도나 월, 또는 일 단위로 분석하고 삭제해야 할 때 범위 기반으로 데이터를 여러 파티션에 균등하게 나눌 수 있을 때 파티션 키 위주로 검색이 자주 실행될 때 데이터베이스에서 파티션의 장점은 다음 2가지로 구분할 수 있다.\n큰 테이블을 작은 크기의 파티션으로 분리 필요한 파티션만 접근(쓰기, 읽기) 위 2가지 장점 중 두 번째 장점의 효과가 매우 큰 편이다.\n파티션을 적용하면서 두 번째 장점은 취하지 못하고 첫 번째 장점에만 집중하다보니, 결과적으로 파티션 때문에 오히려 서버의 성능을 더 떨어뜨리게 된다.\n실제 데이터베이스 서버의 파티션에서 두가지 장점을 모두 취하기는 매우 어렵지만, 이력을 저장하는 테이블에서 레인지 파티션은 두 가지 장점을 모두 어렵지 않게 취할 수 있다.\n레인지 파티션 테이블 생성 1 2 3 4 5 6 7 8 9 10 11 12 CREATE TAVLE employees ( id INT NOT NULL, first_name VARCHAR(30), last_name VARCHAR(30), hired DATE NOT NULL DEFAULT \u0026#39;1970-01-01\u0026#39;, ... ) PARITITON BY RANGE( YEAR(hired) ) ( PARTITION p0 VALUES LESS THAN (1991), PARTITION p1 VALUES LESS THAN (1996), PARTITION p2 VALUES LESS THAN (2001), PARTITION p3 VALUES LESS THAN MAXVALUE ); PARTITION BY RANGE 키워드로 레인지 파티션을 정의한다. PARTITION BY RANGE 뒤에 컬럼 또는 내장 함수를 이용해 파티션 키를 명시한다. VALUES LESS THAN 으로 명시된 값보다 작은 값만 해당 파티션에 저장하게 설정한다. LESS THAN 절에 명시된 값은 그 파티션에 포함되지 않는다. VALUE LESS THAN MAXVALUE로 명시되지 않은 레코드를 저장할 파티션을 지정한다. 선택 사항이므로 지정하지 않아도 괜찮다. VALUE LESS THAN MAXVALUE가 정의되지 않으면 hired 컬럼의 값이 \u0026lsquo;2011-02-30\u0026rsquo;인 레코드가 INSERT될 때 에러가 발생한다. Table has no partition for value 2011 메시지 표시 테이블과 각 파티션은 같은 스토리지 엔진으로 정의할 수 있다. 하지만 MySQL 8.0 에서는 InnoDB 스토리지 엔진이 기본 스토리지 엔진이므로 별도로 명시하지 않아도 InnoDB 테이블로 생성된다. 레인지 파티션의 분리와 병합 단순 파티션의 추가 1 2 ALTER TABLE employees ADD PARTITION (PARTITION p4 VALUES LESS THAN (2011)); 현재 employees 테이블에는 LESS THAN MAXVALUE 파티션을 가지고 있으므로 새로운 파티션을 추가하려고 하면 에러가 발생한다.\n이미 MAXVALUE 파티션이 2001년 이후의 모든 레코드를 가지고 있는 상태에서 2011년 파티션이 추가되면 2011년 레코드는 2개의 파티션에 나뉘어 저장되는 결과를 만들어낸다.\n이는 하나의 레코드는 반드시 하나의 파티션에만 저장돼야 한다는 기본 조건을 벗어나는 것이므로 이러한 경우 ALTER TABLE ... REORGANIZE PARTITION 명령을 사용해야 한다.\n1 2 3 4 5 6 ALTER TABLE employees ALGORITHM=INPLACE, LOCK=SHARED, REORGANIZE PARTITION p3 INTO ( PARTITION p3 VALUES LESS THAN (2011), PARTITION p4 VALUES LESS THAN MAXVALUE ) ; 해당 명령은 파티션의 레코드를 모두 새로운 두 개의 파티션으로 복사하는 작업을 필요로 한다. p3 파티션의 레코드가 매우 많다면 매우 오랜 시간이 걸리게 된다.\n레인지 파티션에서는 일반적으로 LESS THAN MAXVALUE 절을 사용하는 파티션을 추가하지 않고, 미래에 사용될 파티션을 미리 2~3개 정도 더 만들어 두는 형태로 테이블을 생성하기도 한다. 그리고 배치 스크립트를 이용해 주기적으로 파티션 테이블의 여유 기간을 판단해서 파티션을 자동으로 추가하는 방법을 사용한다.\n하지만 필요한 파티션을 배치 스크립트에 의존하는 경우 배치 스크립트의 오류로 파티션이 자동으로 추가되지 못할 수도 있다.\nLESS THAN MAXVALUE 파티션이 존재한다고 하더라도 이 파티션이 데이터를 가지고 있지 않다면 파티션을 수정하는 명령은 매우 빨리 완료될 것이므로 성능 관련해서 걱정은 하지 않아도 된다.\n파티션 삭제 레인지 파티션을 삭제하려면 다음과 같이 DROP PARTITION 키워드에 삭제하려는 파티션의 이름을 지정하면 된다.\n레인지, 리스트 파티션을 삭제하는 작업은 아주 빠르게 처리되므로 날짜 단위로 파티션된 테이블에서 오래된 데이터를 삭제하는 용도로 자주 사용된다.\n1 ALTER TABLE employees DROP PARTITION p0; 레인지 파티션을 사용하는 테이블에서 파티션을 삭제할 때 항상 가장 오래된 파티션 순서로만 삭제할 수 있다.\n기존 파티션의 분리 하나의 파티션을 두 개 이상의 파티션으로 분리하고자 할 때는 REORGANIZE PARTITION 명령을 사용하면 된다. 다음 예제는 MAXVALUE 파티션인 p3을 두개로 나누는 명령이다.\n1 2 3 4 5 6 ALTER TABLE employees ALGORITHM=INPLACE, LOCK=SHARED, REORGANIZE PARTITION p3 INTO ( PARTITION p3 VALUES LESS THAN (2011), PARTITION p4 VALUES LESS THAN MAXVALUE ) ; MAXVALUE 파티션뿐만 아니라 다른 파티션들도 명령을 이용해 분리할 수 있다. 기존 파티션의 레코드를 새로운 파티션으로 복사하기 때문에 기존 파티션의 레코드 건수에 따라 오랜 시간이 걸릴 수 있다. 기존 파티션의 레코드가 많다면 온라인 DDL로 실행될 수 있도록 ALGORITHM, LOCK 절을 사용하면 좋다. 파티션 재구성 명령은 INPLACE 알고리즘을 사용할 수 있지만 최소한 읽기 잠금으로 인해 테이블 쓰기가 불가능하므로 재구성 작업은 쿼리 처리가 많지 않은 시간대에 진행하는 것이 좋다. 기존 파티션 병합 REORGANIZE PARTITION 명령으로 처리할 수 있다.\n1 2 3 4 ALTER TABLE employees ALGORITHM=INPLACE, LOCK=SHARED, REORGANIZE PARTITION p2, p3 INTO ( PARTITION p23 VALUES LESS THAN (2011), ); 파티션을 병합하는 경우도 파티션 재구성이 필요하여, 테이블에 대해 읽기 잠금을 한다.\n리스트 파티션 리스트 파티션은 레인지 파티션과 흡사하게 동작한다.\n둘의 가장 큰 차이는 레인지 파티션은 파티션 키 값의 범위로 파티션을 구성할 수 있지만, 리스트 파티션은 파티션 키 값 하나하나를 리스트로 나열해야 한다는 점이다. 따라서 레인지 파티션과 같이 MAXVALUE 파티션을 정의할 수 없다. 리스트 파티션의 용도 테이블이 다음과 같은 특성을 지닐 때는 리스트 파티션을 사용하는 것이 좋다.\n파티션 키 값이 코드값이나 카테고리와 같이 고정적일 때 키 값이 연속되지 않고 정렬 순서와 관계없이 파티션을 해야 할 때 파티션 키 값을 기준으로 레코드의 건수가 균일하고 검색 조건에 파티션 키가 자주 사용될 때 리스트 파티션 테이블 생성 1 2 3 4 5 6 7 8 9 10 11 CREATE TABLE product( id INT NOT NULL, name VARCHAR(30), category_id INT NOT NULL, ... ) PARTITION BY LIST( category_id ) ( PARTITION p_appliance VALUES IN (3), PARTITION p_computer VALUES IN (1, 9), PARTITION p_sports VALUES IN (2, 6, 7), PARTITION p_etc VALUES IN (4, 5, 8, NULL) ); INT 타입의 category_id 컬럼 값을 그대로 파티션 키로 사용한다. VALUES IN (...)을 통해 파티션별로 저장할 파티션 키 값의 목록을 나열한다. 키 값중에 NULL을 명시할 수도 있다. MAXVALUE 파티션은 정의할 수 없다. 1 2 3 4 5 6 7 8 9 10 11 CREATE TABLE product( id INT NOT NULL, name VARCHAR(30), category_id VARCHAR(20) NOT NULL, ... ) PARTITION BY LIST( category_id ) ( PARTITION p_appliance VALUES IN (\u0026#39;TV\u0026#39;), PARTITION p_computer VALUES IN (\u0026#39;Notebook\u0026#39;, \u0026#39;Desktop\u0026#39;), PARTITION p_sports VALUES IN (\u0026#39;Tennis\u0026#39;, \u0026#39;Soccer\u0026#39;), PARTITION p_etc VALUES IN (\u0026#39;Magazine\u0026#39;, \u0026#39;Socks\u0026#39;, NULL) ); 리스트 파티션의 분리와 병합 파티션을 정의하는 부분에서 VALUE LESS THAN이 아닌 VALUES IN을 사용한다는 것 외에는 레인지 파티션의 추가, 삭제, 병합 작업이 모두 같다.\n리스트 파티션 주의사항 명시되지 않은 나머지 값을 저장하는 MAXVALUE 파티션을 정의할 수 없다. 레인지 파티션과는 달리 NULL을 저장하는 파티션을 별도로 생성할 수 있다. 해시 파티션 해시 파티션은 MySQL 에서 정의한 해시 함수에 의해 레코드가 저장될 파티션을 결정하는 방법이다.\nMySQL에서 정의한 해시 함수는 복잡한 알고리즘이 아니라 파티션의 표현식의 결괏값을 파티션의 개수로 나눈 나머지로 저장될 파티션을 결정하는 방식이다. 해시 파티션의 파티션 키는 항상 정수 타입의 컬럼이거나 정수를 반환하는 표현식만 사용할 수 있다. 해시 파티션에서 파티션의 개수는 레코드를 각 파티션에 할당하는 알고리즘과 연관되기 때문에 파티션을 추가하거나 삭제하는 작업에는 테이블 전체적으로 레코드를 재분배하는 작업이 따른다. 해시 파티션의 용도 해시 파티션은 다음과 같은 특성을 지닌 테이블에 적합하다.\n레인지 파티션이나 리스트 파티션으로 데이터를 균등하게 나누는 것이 어려울 때 테이블의 모든 레코드가 비슷한 사용 빈도를 보이지만 테이블이 너무 커서 파티션을 적용해야 할 때 테이블의 데이터가 특정 값에 영향을 받지 않고 전체적으로 비슷한 사용 빈도를 보일 때 적합하다.\n해시 파티션, 키 파티션의 대표적인 용도로는 회원 테이블을 들 수 있다. 회원 정보는 가입 일자가 오래돼서 사용되지 않거나 최신이어서 더 빈번하게 사용하지 않고, 지역이나 취미 같은 정보 또한 사용 빈도에 미치는 영향이 거의 없다. 해시 파티션 테이블 생성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 /* 파티션의 개수만 지정 */ CREATE TABLE employees ( id INT NOT NULL, first_name VARCHAR(30), last_name VARCHAR(30), hired DATE NOT NULL DEFAULT \u0026#39;1970-01-01\u0026#39;, ... ) PARTITION BY HASH(id) PARTITIONS 4; /* 파티션의 이름을 별도로 지정 */ CREATE TABLE employees ( id INT NOT NULL, first_name VARCHAR(30), last_name VARCHAR(30), hired DATE NOT NULL DEFAULT \u0026#39;1970-01-01\u0026#39;, ... ) PARTITION BY HASH(id) PARTITIONS 4 ( PARTITION p0 ENGINE=INNODB, PARTITION p1 ENGINE=INNODB, PARTITION p2 ENGINE=INNODB, PARTITION p3 ENGINE=INNODB, ); 해시 파티션의 파티션 키 또는 파티션 표현식은 반드시 정수 타입의 값을 반환해야한다. 해시나 키 파티션에서는 특정 파티션을 삭제하거나 병합하는 작업이 거의 불필요하므로 파티션의 이름을 부여하는 것이 크게 의미는 없다. 파티션의 개수만 지정하면 각 파티션의 이름은 기본적으로 \u0026ldquo;p0, p1, \u0026hellip;\u0026ldquo;과 같은 규칙으로 생성된다. 해시 파티션의 분리와 병합 파티션의 분리나 병합으로 인해 파티션의 개수가 변경된다는 것은 해시 함수의 알고리즘을 변경하는 것 이므로, 해시 파티션의 분리와 병합은 모든 파티션에 저장된 레코드를 재분배 하는 작업이 필요하다.\n해시 파티션 추가 해시 파티션 키 값을 테이블의 파티션 개수로 MOD 연산한 결괏값에 의해 각 레코드가 저장될 파티션을 결정한다.\n따라서 새로운 파티션이 추가된다면 기존의 각 파티션에 저장된 모든 레코드가 재배치돼야 한다.\n1 2 3 4 5 6 7 /* 파티션 1개만 추가하면서 파티션 이름 부여 */ ALTER TABLE employees ALGORITHM=INPLACE, LOCK=SHARED, ADD PARTITION(PARTITION p5 ENGINE=INNODB); /* 파티션 6개를 이름 없이 추가 */ ALTER TABLE employees ALGORITHM=INPLACE, LOCK=SHARED, ADD PARTITION PARTITIONS 6; 위의 예제같이 해시 파티션에서 파티션을 추가하는 작업은 INPLACE 알고리즘으로 실행된다고 하더라도 레코드 리빌드 작업이 필요하며 테이블에 대한 읽기 잠금이 필요하다. 따라서 해시 파티션에서 파티션을 추가하거나 생성하는 작업은 많은 부하를 발생시키며, 다른 트랜잭션에서 동일 테이블에 데이터를 변경하는 작업은 허용되지 않는다.\n해시 파티션 삭제 해시나 키 파티션은 파티션 단위로 레코드를 삭제하는 방법이 없다.\n1 2 3 4 5 6 ALTER TABLE employees DROP PARTITION p0; /* Error Code : 1512 DROP PARTITION can only be used on RANGE/LIST partitions */ MySQL 서버가 지정한 파티션 키 값을 가공하여 데이터를 각 파티션으로 분산한 것이므로 각 파티션에 저장된 레코드가 어떤 부류의 데이터인지 사용자가 예측할 수 없다.\n해시 파티션이나 키 파티션을 사용한 테이블에서 파티션 단위로 데이터를 삭제하는 작업은 의미도 없으며 해서도 안될 작업이다.\n해시 파티션 분할 해시 파티션이나 키 파티션에서 특정 파티션을 두 개 이상의 파티션으로 분할하는 기능은 없으며, 테이블 전체적으로 파티션의 개수를 늘리는 작업만 가능하다.\n해시 파티션 병합 2개 이상의 파티션을 하나의 파티션으로 통합하는 기능을 제공하지 않고, 파티션의 개수를 줄이는 것만 가능하다.\n1 2 ALTER TABLE employees ALGORITHM=INPLACE, LOCK=SHARED COALESCE PARTITION 1; COALESCE PARTITION 뒤에 명시한 숫자 값은 줄이고자 하는 파티션 개수를 의미한다.\n마찬가지로 테이블의 모든 레코드가 재배치되는 작업이 수행돼야한다.\n해시 파티션 주의사항 특정 파티션만 삭제 하는 것은 불가능하다. 새로운 파티션을 추가하는 작업은 단순히 파티션만 추가하는 것이 아니라 기존 모든 데이터의 재배치 작업이 필요하다. 해시 파티션은 레인지 파티션이나 리스트 파티션과는 다른 방식으로 괸리하기 때문에 용도에 적합한 해결책인지 확인이 필요하다. 일반적으로 익숙한 파티션 조작이나 특성은 대부분 리스트 파티션이나 레인지 파티션에만 해당하는 것들이 많다. 해시, 키 파티션을 사용하거나 조작할 때는 주의가 필요하다. 키 파티션 키 파티션은 해시 파티션과 사용법과 특성이 거의 같다.\n키 파티션에서는 정수 타입이나 정수값을 반환하는 표현식 뿐만아니라 대부분의 데이터 타입에 대해 파티션을 적용할 수 있다. 선정된 파티션 키의 값을 MD5() 함수를 이용해 해시 값을 계산하고, 그 값을 MOD 연산하여 데이터를 각 파티션에 분배한다. 키 파티션 생성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 /* 프라이머리 키가 있는 경우 자동으로 프라이머리 키가 파티션 키로 사용됨 */ CREATE TABLE k1 ( id INT NOT NULL, name VARCHAR(20), PRIMARY KEY (id) ) /* 괄호의 내용을 비워두면 자동으로 프라이머리 키의 모든 컬럼이 파티션 키가 됨, 프라이머리 키의 일부만 명시할 수도 있음 */ PARTITION BY KEY() PARTITIONS 2; /* 프라이머리 키가 없을 때 유니크 키가 존재하면 파티션 키로 사용됨 */ CREATE TABLE k1 ( id INT NOT NULL, name VARCHAR(20), UNIQUE KEY (id) ) PARTITION BY KEY() PARTITIONS 2; /* 프라이머리 키나 유니크 키의 컬럼 일부를 파티션 키로 명시적으로 설정 */ CREATE TABLE dept_emp ( emp_no INT NOT NULL, dept_no CHAR(4) NOT NULL, ... PRIMARY KEY (dept_no, emp_no) ) /* 괄호 내용에 프라이머리 키나 유니크 키를 구성하는 컬럼중에서 일부만 선택하여 파티션 키로 설정하는 것도 가능 */ PARTITION BY KEY(dept_no) PARTITIONS 2; 키 파티션의 주의하항 및 특이사항 키 파티션은 MySQL 서버가 내부적으로 MD5() 함수를 이용해 파티션하기 때문에 파티션 키가 반드시 정수 타입이 아니어도 된다. 프라이머리 키나 유니크 키를 구성하는 컬럼 중 일부만으로도 파티션 할 수 있다. 유니크 키를 파티션 키로 사용할 때 해당 유니크 키는 반드시 NOT NULL이어야 한다. 해시 파티션에 비해 파티션 간의 레코드를 더 균등하게 분할할 수 있기 때문에 키 파티션이 더 효율적이다. 리니어 해시 파티션/리니어 키 파티션 해시, 키 파티션은 새로운 파티션을 추가하거나 파티션을 통합해서 개수를 줄일 때 대상 파티션만이 아니라 테이블의 전체 파이션에 저장된 레코드의 재분배 작업이 발생하는데, 이러한 단점을 최소화 하기 위해 리니어 해시 파티션/리니어 키 파티션 알고리즘이 고안되었다.\n각 레코드 분배를 위해 \u0026ldquo;Power-of-two(2의 승수)\u0026rdquo; 알고리즘을 이용하며, 파티션의 추가나 통합 시 다른 파티션에 미치는 영향을 최소화한다.\nPower-of-two 알고리즘 해시 테이블의 크기가 2의 거듭제곱 수로 정의되는 경우 사용되는 해시 충돌 해결 방법 중 하나로, 해시 테이블의 크기가 2^n이라면 적절하게 동작할 수 있다.\n해시 테이블의 크기를 2^n으로 설정한다. 키를 해시 함수에 넣어 해시 값을 계산한다. 계산된 해시 값을 2^n으로 나눈 나머지를 버킷 인덱스로 사용한다. 해시 충돌이 발생할 경우, 다음 빈 버킷으로 이동하여 저장하는 방식으로 충돌을 해결한다.\n만약 충돌이 발생한 위치가 이미 사용중인 경우, 다음 버킷을 검사하고 비어있는 버킷을 찾을 때까지 반복하며, 이러한 과정을 선형 탐사라고 한다.\n리니어 해시 파티션/리니어 키 파티션의 추가 및 통합 리니어 파티션의 경우 단순히 나머지 연산으로 레코드가 저장될 파티션을 결정하는 것이 아닌, \u0026ldquo;Power-of-two\u0026rdquo; 분배 방식을 사용하기 때문에 파티션의 추가나 통합 시 특정 파티션의 데이터에대해서만 이동 작업을 하면 된다.\n파티션을 추가하거나 통합하는 작업에서 나머지 파티션의 데이터는 재분배 대상이 되지 않는다.\n리니어 해시 파티션/리니어 키 파티션의 추가 리니어 파티션에 새로운 파티션을 추가하는 명령은 일반 해시 파티션이나 키 파티션과 동일하며 키 분배 알고리즘에만 차이가 있다.\n리니어 파티션은 \u0026ldquo;Power-of-two\u0026rdquo; 알고리즘으로 레코드가 분배돼 있기 때문에 새로운 파티션을 추가할 때도 특정 파티션의 레코드만 재분배 하면 된다.\n다른 파티션 데이터는 레코드 재분배 작업과 관련이 없기 때문에 일반 해시 파티션이나 키 파티션의 파티션 추가보다 매우 빠르게 처리할 수 있다.\n리니어 해시 파티션/리니어 키 파티션 통합 새로운 파티션을 추가할 때와 같이 일부 파티션에 대해서만 레코드 통합 작업이 필요하다. 통합되는 파티션만 레코드 이동이 필요하며, 나머지 파티션의 레코드는 레코드 재분배 작업에서 제외된다.\n리니어 해시 파티션/리니어 키 파티션과 관련된 주의사항 파티션을 추가하거나 통합할 때 작업의 범위를 최소화하는 대신 각 파티션이 가지는 레코드의 건수는 일반 해시 파티션이나 키 파티션보다는 덜 균등해질 수 있다.\n해시 파티션이나 키 파티션을 사용하는 테이블에 대해 새로운 파티션을 추가하거나 삭제해야 할 요건이 많다면 리니어 해시 파티션 또는 리니어 키 파티션을 적용하는 것의 좋으나, 파티션을 조정할 필요가 거의 없다면 일반 해시 파티션이나 키 파티션을 사용하는 것이 좋다.\n파티션 테이블의 쿼리 성능 파티션 테이블에 쿼리가 실행될 때 테이블의 모든 파티션을 읽을지 아니면 일부 파티션만 읽을지는 성능에 아주 큰영향을 미친다.\n쿼리의 성능은 테이블에서 얼마나 많은 파티션을 프루닝할 수 있는지가 관건이며, 옵티마이저가 수립하는 실행 계획에서 어떤 파티션에 제외되고 선택되는지 확인할 수 있다.\n일반적으로 레인지 파티션이나 리스트 파티션을 사용하는 테이블에서 개별 파티션을 명시해야하므로, 레인지 파티션이나 리스트 파티션이 사용되는 테이블의 개수는 파티션 개수는 적은편이다. 해시나 키 파티션의 경우 파티션의 개수만 지정하면 되므로 많은 파티션을 가진 테이블도 쉽게 생성할 수 있어 주의가 필요하다. 인덱스가 있다고 하더라도 파티션 테이블 조회가 필요하므로 파티션 조건에 따라 파티션 테이블을 모두 확인해야 할 수 있다. 일부 파티션만 집중적으로 사용한다면 문제 없겠지만, 균등하게 파티션 되었다면 오버헤드가 커질 수 있다. MySQL 서버의 파티션은 샤딩이 아니므로 파티션을 사용할 때는 반드시 파티션 프루닝이 얼마나 도움이 될지 예측해보고 적용해야한다. ","date":"2023-08-28T09:54:10+09:00","image":"https://codemario318.github.io/post/real-mysql/13/3/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/13/3/","title":"13.3 파티션 - MySQL 파티션의 종류"},{"content":"파티션은 5.1 버전부터 도입되어 8.0 버전까지 많은 발전이 있었지만, 아직도 많은 제약을 지니고있다.\n대부분 파티션의 태생적인 한계이기 때문에 MySQL 서버가 아무리 업그레이드 된다고 하더라도 여전히 가질 제약사항일 수도 있다.\n파티션의 제약 사항 1 2 3 4 5 6 7 8 9 10 11 CREATE TABLE tb_article ( article_id INT NOT NULL AUTO_INCREMENT, reg_date DATETIME NOT NULL, ... PRIMARY KEY(article_id, reg_date) ) PARTITION BY RANGE ( YEAR(reg_Date) ) ( PRATITION p2009 VALUES LESS THAN (2010), PRATITION p2010 VALUES LESS THAN (2011), PRATITION p2011 VALUES LESS THAN (2012), PRATITION p9999 VALUES LESS THAN MAXVALUE ); PARTITION BY RANGE 절은 이 테이블이 레인지 파티션을 사용한다는 것을 의미한다.\n스토어드 루틴이나 UDF, 사용자 변수 등을 파티션 표현식에 사용할 수 없다. 파티션 표현식은 일반적으로 컬럼 그 자체 또는 MySQL 내장 함수를 사용할 수 있는데, 일부 함수들은 파티션 생성은 가능하지만 프루닝을 지원하지 않을 수 있다. 프라이머리 키를 포함해서 테이블의 모든 유니크 인덱스는 파티션 키 컬럼을 포함해야 한다. 파티션 테이블의 인덱스는 모두 로컬 인덱스이며, 동일 테이블에 소속된 모든 파티션은 같은 구조의 인덱스만 가질 수 있다. 파티션 개별로 인덱스를 변경하거나 추가할 수 없다. 동일 테이블에 속한 모든 파티션은 동일 스토리지 엔진만 가질 수 있다. 서브 파티션까지 포함하여 8192개의 파티션을 가질 수 있다. 파티션 생성 이후 MySQL sql_mode 시스템 변수 변경은 데이터 파티션의 일관성을 깨뜨릴 수 있다. 파티션 테이블에서는 외래키를 사용할 수 없다. 파티션 테이블은 전문 검색 인덱스 생성이나 전문 검색 쿼리를 사용할 수 없다. 공간 데이터를 저장하는 컬럼 타입은 파티션 테이블에서 사용할 수 없다. 임시 테이블은 파티션 기능을 사용할 수 없다. 일반적으로 파티션 테이블을 생성할 때 가장 크게 영향을 미치는 제약 사항은 모드 유니크 인덱스에 파티션 키 컬럼이 포함돼야 한다는 것이다.\n파티션 표현식에는 기본적인 산술 연산자를 사용할 수 있고, 추가로 MySQL 내장 함수를 사용할 수 있다.\n내장 함수들을 파티션 표현시게 사용할 수 있다고 해서 모두 파티션 프루닝을 지원하는 것은 아니므로 메뉴얼을 참조하자. 파티션 사용 시 주의사항 파티션 테이블의 경우 프라이머리 키를 포함한 유니크 키에 대해서는 상당히 머리 아픈 제약사항이 있다.\n파티션의 목적은 작업 범위를 좁히는 것인데, 유니크 인덱스는 중복 레코드에 대한 체크 작업 때문에 범위가 좁혀지지 않는다는 점이다.\nMySQL 파티션은 일반 테이블과 같이 별도의 파일로 관리되는데, 이와 관련하여 서버가 조작할 수 있는 파일의 개수와 연관된 제약도 있다.\n파티션과 유니크 키(프라이머리 키 포함) 종류와 관계없이 테이블에 유니크 인덱스가 있으면 파티션 키는 모든 유니크 인덱스의 일부 또는 모든 컬럼을 포함해야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 CREATE TABLE tb_parition ( fd1 INT NOT NULL, fd2 INT NOT NULL, fd3 INT NOT NULL, UNIQUE KEY (fb1, fb2) ) PARTITION BY HASH (fd3) PARTITIONS 4 ; CREATE TABLE tb_parition ( fd1 INT NOT NULL, fd2 INT NOT NULL, fd3 INT NOT NULL, UNIQUE KEY (fb1), UNIQUE KEY (fb2) ) PARTITION BY HASH (fd1 + fd2) PARTITIONS 4 ; CREATE TABLE tb_parition ( fd1 INT NOT NULL, fd2 INT NOT NULL, fd3 INT NOT NULL, PRIMARY KEY (fb1), UNIQUE KEY (fb2, fb3) ) PARTITION BY HASH (fd1 + fd2) PARTITIONS 4 ; 위 예시는 모두 잘못된 테이블 파티션을 생성하는 방법이다.\n유니크 키에 대해 파티션 키가 제대로 설정됐는지 확인하려면 각 유니크 키에 대해 값이 주어졌을 때 해당 레코드가 어느 파티션에 저장돼 있는지 계산할 수 있어야 한다는 점을 기억하면 된다.\n첫 번째 쿼리는 유니크 키와 파티션 키가 전혀 연관이 없기 때문에 불가능 두 번째 쿼리는 첫 번째 유니크 키 컬럼인 fd1만으로 파티션 결정이 되지 않는다(fd2 컬럼도 같이 있어야 파티션 위치를 판단할 수 있다.) 세 번째 쿼리 또한 fd1 값 만으로 파티션 판단이 되지 않으며, 유니크 키인 fd2, fd3로도 파티션 위치를 결정할 수 없다. 아래는 옳은 예시이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 CREATE TABLE tb_parition ( fd1 INT NOT NULL, fd2 INT NOT NULL, fd3 INT NOT NULL, UNIQUE KEY (fb1, fb2, fd3) ) PARTITION BY HASH (fd1) PARTITIONS 4 ; CREATE TABLE tb_parition ( fd1 INT NOT NULL, fd2 INT NOT NULL, fd3 INT NOT NULL, UNIQUE KEY (fb1, fd2) ) PARTITION BY HASH (fd1 + fd2) PARTITIONS 4 ; CREATE TABLE tb_parition ( fd1 INT NOT NULL, fd2 INT NOT NULL, fd3 INT NOT NULL, PRIMARY KEY (fb1, fd2, fd3), UNIQUE KEY (fb3) ) PARTITION BY HASH (fd3) PARTITIONS 4 ; 파티션과 open_files_limit 시스템 변수 설정 MySQL에서는 일반적으로 테이을 파일 단위로 관리하기 때문에 MySQL 서버에서 동시에 오픈된 파일의 개수가 상당히 많아질 수 있다.\n이를 제한하기 위해 시스템 변수에 동시에 오픈할 수 있는 적절한 파일의 개수를 설정하는데, 파티션 되지 않은 일반 테이블은 테이블 1개당 오픈된 파일의 개수가 2~3개 수준이지만 파티션 테이블에서는 파티션의 개수 * 2~3 개가 된다.\n따라서 파티션을 많이 사용하는 경우 시스템 젼수를 절절히 높은 값으로 다시 설정할 필요가 있다.\n","date":"2023-08-27T20:24:10+09:00","image":"https://codemario318.github.io/post/real-mysql/13/2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/13/2/","title":"13.2 파티션 - 주의사항"},{"content":"파티션 기능은 테이블을 논리적으로는 하나의 테이블이지만 물리적으로는 여러 개의 테이블로 분리해서 관리할 수 있게 해준다.\n주로 대용량 테이블을 물리적으로 여러 개의 소규모 테이블로 분산하는 목적으로 사용한다. 파티션 기능을 사용하더라도 어떤 쿼리를 사용하느냐에 따라 오히리 성능이 더 나빠지는 경우도 발생할 수 있다. MySQL 파티션이 적용된 테이블에서 INSERT나 SELECT 같은 쿼리가 어떻게 실행되는지 이해한다면 파티션을 어떻게 사용하는 것이 가장 최적일지 쉽게 이해할 수 있다.\n파티션을 사용하는 이유 테이블의 데이터가 많아진다고 해서 무조건 파티션을 적용하는 것이 효율적인 것은 아니다.\n하나의 테이블이 너무 커서 인덱스의 크기가 물리적인 메모리보다 훨씬 큰 경우 데이터 특성상 주기적인 삭제 작업이 필요한 경우 단일 INSERT와 단일 또는 범위 SELECT의 빠른 처리 데이터베이스에서 인덱스는 일반적으로 SELECT를 위한 것으로 보이지만 UPDATE나 DELETE 쿼리를 위해 필요한 때도 많다.\n물론 레코드를 변경하는 쿼리를 실행하면 인덱스 변경을 위한 부가적인 작업이 발생하지만 처리 대상 레코드를 검색하려면 인덱스가 필수적이지만 인덱스가 커지면 커질수록 느려지는 단점이 있다.\n특히 한 테이블의 인덱스 크기가 물리적으로 MySQL이 사용 가능한 메모리 공간보다 크다면 영향은 더 심각하다.\n테이블의 데이터는 실질적인 물리 메모리보다 큰 것이 일반적이겠지만, 인덱스의 워킹 셋이 실질적인 물리 메모리보다 크다면 쿼리 처리가 상당히 느려진다.\n파티션하지 않고 하나의 큰 테이블로 사용하면 인덱스도 커지고 그만큼 물리적인 메모리 공간도 많이 필요하므로, 파티션을 통해 데이터와 인덱스를 조각화해서 물리적 메모리를 효율적으로 사용할 수 있게 만들어준다.\n데이터의 물리적인 저장소를 분리 데이터 파일이나 인덱스 파일이 파일 시스템에서 차지하는 공간이 크다면 그만큼 백업이나 관리 작업이 어려워진다. MySQL은 테이블의 데이터나 인덱스를 파일 단위로 관리하기 때문에 더 치명적인 문제가 될 수 있다.\n이러한 문제는 파티션을 통해 파일의 크기를 조절하거나 파티션별 파일들이 저장될 위치나 디스크를 구분해서 지정해 해결하는 것도 가능하다.\n하지만 MySQL에서는 테이블의 파티션 단위로 인덱스를 생성하거나 파티션별로 다른 인덱스를 가지는 형태는 지원하지 않는다.\n이력 데이터의 효율적인 관리 거의 모든 애플리케이션이 로그라는 이력 데이터를 가지고 있는데, 이는 단기간에 대량으로 누적됨과 동시에 일정 기간이 지나면 쓸모가 없어진다.\n로그 데이터는 결국 시간이 지나면 별도로 아카이빙하거나 백업한 후 삭제해버리는 것이 일반적이며, 특히 다른 데이터에 비해 라이프 사이클이 상당히 짧다.\n로그 테이블에서 불필요해진 데이터를 백업하거나 삭제하는 작업은 일반 테이블에서는 상당히 고부하 작업에 속한다.\n로그 테이블을 파티션 테이블로 관리한다면 풀필요한 데이터 삭제 작업은 단순히 파티션을 추가하거나 삭제하는 방식으로 간단하고 빠르게 해결할 수 있다. MySQL 파티션의 내부 처리 1 2 3 4 5 6 7 8 9 10 11 CREATE TABLE tb_article ( article_id INT NOT NULL, reg_date DATETIME NOT NULL, ... PRIMARY KEY(article_id, reg_date) ) PARTITION BY RANGE ( YEAR(reg_Date) ) ( PRATITION p2009 VALUES LESS THAN (2010), PRATITION p2010 VALUES LESS THAN (2011), PRATITION p2011 VALUES LESS THAN (2012), PRATITION p9999 VALUES LESS THAN MAXVALUE ); 파티션 테이블의 레코드 INSERT INSERT 쿼리가 실행되면 MySQL 서버는 INSERT되는 컬럼의 값 중에서 파티션 키인 reg_date 컬럼의 값을 이용해 파티션 표현식을 평가하고, 그 결과를 이용해 레코드가 저장될 적절한 파티션을 결정한다.\n새로 INSERT되는 레코드를 위한 파티션이 결정되면 나머지 과정은 파티션 되지 않은 일반 테이브로가 동일하게 처리된다.\n파티션 테이블의 UPDATE UPDATE 쿼리를 실행하려면 변경 대상 레코드가 어느 파티션에 저장돼 있는지 찾아야 하는데, 이때 WHERE 조건에 파티션 키 컬럼이 조건으로 존재한다면 그 값을 이용해 레코드가 저장된 파티션에서 빠르게 대상 레코드를 검색할 수 있다.\n하지만 WHERE 조건에 파티션 키 컬럼의 조건이 면시되지 않았다면 변경 대상 레코드를 찾기 위해 모든 파티션을 검색한다. 실제 레코드를 변경하는 작업의 절차는 UPDATE 쿼리가 어떤 컬럼의 값을 변경하느냐에 따라 큰 차이가 생긴다.\n파티션 키 이외의 컬럼만 변경될 때는 파티션이 적용되지 않은 일반 테이블과 마찬가지로 컬럼 값만 변경한다. 파티션 키 컬럼이 변경될 때는 기존 레코드가 저장된 파티션에서 해당 레코드를 삭제하고, 변경되는 파티션 키 컬럼의 표현식을 평가하여 새로운 파티션을 결정하여 새로 저장한다. 파티션 테이블의 검색 파티션 테이블을 검색할 때 성능에 크게 영향을 미치는 조건은 다음과 같다.\nWHERE 절의 조건으로 검색해야 할 파티션을 선택할 수 있는가? WHERE 절의 조건이 인덱스를 효율적으로 사용(인덱스 레인지 스캔)할 수 있는가? 파티션 테이블에서는 첫 번째 선택사항의 결과에 의해 두 번째 선택사항의 작업 내용이 달라질 수 있다.\n파티션 선택 가능 + 인덱스 효율적 사용 가능 가장 효율적으로 처리 될 수 있다. 파티션 개수와 관계없이 검색을 위해 꼭 필요한 파티션의 인덱스만 레인지 스캔한다. 파티션 선택 불가 + 인덱스 효율적 사용 가능 모든 파티션을 대상으로 검색해야 한다. 각 파티션에 대해서는 인덱스 레인지 스캔을 사용할 수 있기 때문에 최종적으로 테이블에 존재하는 모든 파티션의 개수만큼 인덱스 레인지 스캔을 수행해서 검색하게된다. 파티션 개수 만큼의 테이블에 대해 인덱스 레인지 스캔 후 결과를 병합해서 가져오는 것 과 같다. 파티션 선택 가능 + 인덱스 효율적 사용 불가 검색을 위해 필요한 파티션만 읽는다. 대상 파티션에 대해 풀 테이블 스캔을 한다. 각 파티션의 레코드 건수가 많다면 상당히 느리게 처리될것이다. 파티션 선택 불가 + 인덱스 효율적 사용 불가 모든 파티션에 대해 풀 테이블 스캔을 한다. 파티션 테이블의 인덱스 스캔과 정렬 파티션 테이블에서 인덱스는 전부 로컬 인덱스에 해당한다.\n모든 인덱스는 파티션 단위로 생성된다. 파티션과 관계없이 테이블 전체 단위로 글로벌하게 하나의 통합된 인덱스는 지원하지 않는다. 파티션이 되지 않는 테이블에서는 인덱스를 순서대로 읽으면 크 컬럼으로 정렬된 결과를 바로 얻을 수 있지만, 파티션된 테이블은 그렇지 않다.\n1 2 3 4 5 6 SELECT * FROM tb_article WHERE reg_userid BETWEEN \u0026#39;brew\u0026#39; AND \u0026#39;toto\u0026#39; AND reg_date BETWEEN \u0026#39;2009-01-01\u0026#39; AND \u0026#39;2010-12-31\u0026#39; ORDER BY reg_userid ; MySQL 서버는 여러 파티션에 대해 인덱스 스캔을 수행할 때 각 파티션으로부터 조건에 일치하는 레코드를 정렬된 순서대로 읽으면서 우선순위 큐에 임시로 저장한다. 파티션 테이블에서 인덱스 스캔을 통해 레코드를 읽을 때 별도의 정렬 작업을 수행하지는 않으며, 내부적으로 큐 처리가 필요하다. 파티션 프루닝 옵티마이저에 의해 불필요한 파티션에는 전혀 접근하지 않는데, 이렇게 최적화 단계에서 필요한 파티션만 골라내고 불필요한 것들은 실행 계획에서 배제하는 것을 파티션 프루닝이라 한다.\n","date":"2023-08-27T19:54:10+09:00","image":"https://codemario318.github.io/post/real-mysql/13/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/13/","title":"13.1 파티션"},{"content":"Minikube는 로컬 환경에서 쿠버네티스 클러스터를 간편하게 구축하고 실행하기 위한 도구입니다.\n단일 머신 위에서 쿠버네티스 클러스터를 실행하므로 개발, 테스트 및 학습을 위한 로컬 환경에서 매우 유용합니다.\nminikube는 컨테이너, 가상머신 환경에서 동작하므로 이와 같은 환경을 먼저 구성해야하며, 저는 사전에 docker를 설치, 실행했습니다.\nminikube 클러스터 만들기 Homebrew를 사용하여 minikube를 간단하게 설치할 수 있습니다.\n1 brew install minikube 설치가 완료되었다면 클러스터를 시작해줍니다.\n1 minikube start 명령이 정상적으로 수행되었다면 쿠버네티스 대시보드를 열어보겠습니다.\n1 minikube dashboard 대시보드 애드온과 프록시가 활성화되고 해당 프록시로 접속하는 기본 웹 브라우저 창이 열립니다.\n대시보드에서 디플로이먼트나 서비스와 같은 쿠버네티스 자원을 생성할 수 있습니다.\n1 minikube addons enable metrics-server 대시보드가 열리지 않는다면 애드온을 추가해보세요.\n디플로이먼트 만들기 쿠버네티스 디플로이먼트는 헬스 체크를 통해 파드의 컨테이너가 종료되었다면 재시작해줍니다. 공식문서에서는 파드의 생성 및 스케일링을 관리하는 방법으로 디플로이먼트를 권장하고 있습니다.\n쿠버네티스 파드는 관리와 네트워킹 목적으로 함께 묶여 있는 하나 이상의 컨테이너 그룹입니다.\nkubectl create 명령을 실행하여 파드를 관리할 디플로이먼트를 만듭니다.\n1 kubectl create deployment NAME --image=image -- [COMMAND] [args...] [options] 1 kubectl create deployment hello-node --image=registry.k8s.io/e2e-test-images/agnhost:2.39 -- /agnhost netexec --http-port=8080 만들어지는 파드는 공식문서에서 제공된 Docker 이미지를 기반으로 한 컨테이너를 실행합니다.\n디플로이먼트 보기 1 kubectl get deployments 1 2 NAME READY UP-TO-DATE AVAILABLE AGE hello-node 1/1 1 1 3m38s 파드 보기 1 kubectl get pods 1 2 NAME READY STATUS RESTARTS AGE hello-node-67949d9db-7xxk9 1/1 Running 0 5m50s 클러스터 이벤트 보기 1 kubectl get events 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 LAST SEEN TYPE REASON OBJECT MESSAGE 7m28s Normal Scheduled pod/hello-node-67949d9db-7xxk9 Successfully assigned default/hello-node-67949d9db-7xxk9 to minikube 7m29s Normal Pulling pod/hello-node-67949d9db-7xxk9 Pulling image \u0026#34;registry.k8s.io/e2e-test-images/agnhost:2.39\u0026#34; 7m18s Normal Pulled pod/hello-node-67949d9db-7xxk9 Successfully pulled image \u0026#34;registry.k8s.io/e2e-test-images/agnhost:2.39\u0026#34; in 10.747873713s 7m18s Normal Created pod/hello-node-67949d9db-7xxk9 Created container agnhost 7m18s Normal Started pod/hello-node-67949d9db-7xxk9 Started container agnhost 7m29s Normal SuccessfulCreate replicaset/hello-node-67949d9db Created pod: hello-node-67949d9db-7xxk9 7m29s Normal ScalingReplicaSet deployment/hello-node Scaled up replica set hello-node-67949d9db to 1 57m Normal Starting node/minikube Starting kubelet. 57m Normal NodeHasSufficientMemory node/minikube Node minikube status is now: NodeHasSufficientMemory 57m Normal NodeHasNoDiskPressure node/minikube Node minikube status is now: NodeHasNoDiskPressure 57m Normal NodeHasSufficientPID node/minikube Node minikube status is now: NodeHasSufficientPID 57m Normal NodeAllocatableEnforced node/minikube Updated Node Allocatable limit across pods 57m Normal Starting node/minikube 57m Normal RegisteredNode node/minikube Node minikube event: Registered Node minikube in Controller kubectl 환경설정 보기 1 kubectl config view 해당 명령으로 환경설정을 볼 수 있는데, 다른 클러스터를 사용했다면 minikube 외 다른 설정들도 표시될 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion: v1 clusters: - cluster: certificate-authority: /Users/mario/.minikube/ca.crt extensions: - extension: last-update: Thu, 24 Aug 2023 14:45:00 KST provider: minikube.sigs.k8s.io version: v1.31.2 name: cluster_info server: https://127.0.0.1:64072 name: minikube contexts: - context: cluster: minikube extensions: - extension: last-update: Thu, 24 Aug 2023 14:45:00 KST provider: minikube.sigs.k8s.io version: v1.31.2 name: context_info namespace: default user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /Users/mario/.minikube/profiles/minikube/client.crt client-key: /Users/mario/.minikube/profiles/minikube/client.key 서비스 만들기 기본적으로 파드는 쿠버네티스 클러스터 내부의 IP 주소로만 접근할 수 있기때문에 hello-node 컨테이너를 쿠버네티스 가상 네트워크 외부에서 접근하려면 파드를 쿠버네티스 서비스로 노출해야 합니다.\nkubectl expose 명령으로 퍼블릭 인터넷에 파드를 노출할 수 있습니다.\n1 2 kubectl expose (-f FILENAME | TYPE NAME) [--port=port] [--protocol=TCP|UDP|SCTP] [--target-port=number-or-name] [--name=name] [--external-ip=external-ip-of-service] [--type=type] [options] 여러 옵션을 사용할 수 있으며, 공식 문서에서 제공하는 명령어를 실행해봅시다.\n1 kubectl expose deployment hello-node --type=LoadBalancer --port=8080 --type=LoadBalancer: 클러스터 밖의 서비스로 노출하기 원한다는 뜻 입니다. 이미지 내 애플리케이션 코드는 TCP 포트 8080에서만 수신합니다. kubectl expose를 사용하여 다른 포트를 노출한 경우, 클라이언트는 다른 포트에 연결할 수 없습니다. 1 kubectl get services 1 2 3 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-node LoadBalancer 10.97.121.119 \u0026lt;pending\u0026gt; 8080:30180/TCP 4m44s ... 로드 밸런서를 지원하는 클라우드 공급자의 경우에는 서비스에 접근할 수 있도록 외부 IP 주소가 프로비저닝 하지만, minikube에서 LoadBalancer 타입은 minikube service 명령어를 통해서 해당 서비스를 접근할 수 있게합니다.\n1 minikube service hello-node 1 2 3 4 5 6 7 8 9 10 11 12 13 |-----------|------------|-------------|---------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|------------|-------------|---------------------------| | default | hello-node | 8080 | http://192.168.49.2:30180 | |-----------|------------|-------------|---------------------------| 🏃 hello-node 서비스의 터널을 시작하는 중 |-----------|------------|-------------|------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|------------|-------------|------------------------| | default | hello-node | | http://127.0.0.1:64781 | |-----------|------------|-------------|------------------------| 🎉 Opening service default/hello-node in default browser... ❗ Because you are using a Docker driver on darwin, the terminal needs to be open to run it. 제거하기 클러스터에서 만들어진 리소스를 제거할 수 있습니다.\n1 2 kubectl delete service hello-node kubectl delete deployment hello-node 필요시 minikube 가상 머신을 정지하거나 살제할 수 있습니다.\n1 minikube stop 1 minikube delete ","date":"2023-08-24T12:51:03+09:00","image":"https://codemario318.github.io/post/kubernetes/tutorials/1/cover_hu15504362c54454204bbf321d3bfe3873_12586_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/kubernetes/tutorials/1/","title":"쿠버네티스 튜토리얼 - 1. Hello Minikube"},{"content":"작성된 쿼리가 얼마나 효율적이고 더 개선할 부분이 있는지 확인하려면 아래의 과정을 거친다.\n실행 계획을 살펴보고 문제될 만한 부분이 있는지 검토한다. 실행 계획에 특별히 문제될 부분이 없다면 쿼리를 직접 실행해본다. 실행 계획상 보이지 않는 부분을 확인하기 위해 쿼리를 직접 실행해보면서 눈으로 성을 체크할 때 여러 방해 요소가 있는데, 이러한 방해 요소를 간과하고 쿼리의 성능을 판단하는 것은 매우 위험하다.\n쿼리의 성능에 영향을 미치는 요소 직접 작성한 쿼리를 실행해 보고 성능을 판단할 때 가장 큰 변수는 서버가 가지고 있는 여러 종류의 버퍼나 캐시이다.\n운영체제의 캐시 MySQL 서버는 운영체제의 파일 시스템 관련 기능(시스템 콜 등)을 이용해 데이터 파일을 읽어온다.\n일반적으로 대부분 운영체제는 한 번 읽어은 데이터를 관리하는 별도 캐시 영역에 보관했다가 다시 해당 데이터가 요청되면 디스크를 읽지 않고 캐시의 내용을 바로 MySQL 서버로 전달한다. InnoDB 스토리지 엔진은 일반적으로 파일 시스템의 캐시나 버퍼를 거치지 않는 Direct I/O를 사용하므로 운영체제의 캐시가 큰 영향을 미치지는 않는다. MyISAM 스토리지 엔진은 운영체제의 캐시 의존도가 높기 때문에 캐시에 따라 성능 차이가 크다. 운영체제가 관리하는 캐시나 버퍼는 공용 공간으로 MySQL 서버와 같은 응용 프로그램이 종료된 후에도 남아있을 수 있다. 따라서 운영체제가 가지고 있는 캐시나 버퍼가 전혀 없는 상태에서 쿼리의 성능을 테스트하려면 캐시 삭제 명령을 실행하고 테스트 하는 것이 좋다.\n1 2 3 4 5 6 ## linux 기준! ## 캐스나 버퍼의 내용을 디스크와 동기화 sync ## 운영체제의 포함된 캐시 내용을 초기화한다. echo 3 \u0026gt; /proc/sys/vm/drop_caches MySQL 서버의 버퍼풀(InnoDB 버퍼풀과 MyISAM의 키 캐시) 운영 체제의 버퍼나 캐시와 마찬가지로 MySQL 서버에서도 InnoDB 버퍼풀, MyISAM 키 캐시 등으로 데이터 파일의 내용을 페이지(또는 블록) 단위로 캐시하는 기능을 제공한다.\nInnoDB 버퍼풀은 인덱스 페이지와 데이터 페이지까지 캐시하며, 쓰기 작업을 위한 버퍼링 작업까지 처리한다. MyISAM 키 캐시는 읽기를 위한 캐시 역할을 수행하며, 제한적으로 인덱스 변경만을 위한 버퍼 역할을 수행한다. 인덱스를 제외한 테이블 데이터는 모두 운영체제의 캐시에 의존한다. MySQL 서버가 한번 시작되면 버퍼풀과 키 캐시를 강제로 퍼지할 수 있는 방법이 없으므로 초기화 하려면 MySQL 서버를 재시작 해야한다.\nInnoDB 버펄풀은 서버가 종료될 때 자동으로 덤프됐다가 사작될 때 자동으로 적재하므로 InnoDB 버퍼풀이 자동으로 처리도지 않게 innodb_buffer_pool_load_at_startup 시스템 변수를 OFF로 설정한 후 재시작 해야 한다.\n독립된 MySQL 서버 MySQL 서버가 기동 중인 장비에 웹 서버나 다른 배치용 프로그램이 실행되고 있다면 테스트하려는 쿼리의 성능이 영향을 받는다.\n이와 마찬가지로 테스트 쿼리를 실행하는 클라이언트 프로그램이나 네트워크의 영향 요소도 고려해야 한다. MySQL 서버가 설치된 서버에 직접 로그인해서 테스트해볼 수 있다면 이러한 요소를 쉽게 배제할 수 있다. 쿼리 테스트 횟수 실제 쿼리의 성능 테스트를 MySQL 서버의 상태가 워밍업된 상태에서 진행할지 아니면 콜드 상태에서 진행할지도 고려햐야 한다.\n일반적으로 쿼리의 성능 테스트는 워밍업된 상태를 가정한다. 어느정도 사용량이 있는 서비스라면 워밍업 상태로 전환하는 데 오래 걸리지 않는다. 운영체제의 캐시나 버퍼풀, 키 캐시는 크기가 제한적이므로 쿼리에서 필요로 하는 데이터나 인덱스 페이지보다 크기가 작으면 플러시 작업과 캐시 작업이 반복해서 발생하므로 쿼리를 한 번 실행해서 나온 결과를 그대로 신뢰해서는 안된다.\n쿼리를 번갈아가면서 6~7회 실행한 후, 처음 한두 번의 결과는 버리고 나머지 결과의 평균값을 기준으로 비교하는 것이 좋다. 버퍼풀과 키캐시가 준비되지 않을 때가 많아 대체로 많은 시간이 걸려 편차가 클 수 있기 때문이다. 이 같은 사항을 고려해 쿼리의 성능을 비교하는 것은 결국 상대적인 비교이므로 쿼리가 어떤 서버에서도 그 시간 내에 처리된다고 보장할 수는 없다.\n실제 서비스용 MySQL 서버에서는 동시에 여러개의 쿼리가 실행 중인 상태이다. 쿼리가 자원을 점유하기 위한 경함 등이 발생하므로 항상 테스트보다는 느린 처리 성능을 보이는 것이 일반적이다. ","date":"2023-08-23T13:02:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/8/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/8/","title":"11.8 쿼리 작성 및 최적화 - 쿼리 성능 테스트"},{"content":"쿼리가 오랜 시간 실행되고 있는 경우도 문제지만 트랜잭션이 오랜 시간 완료되지 않고 활성 상태로 남아있는 것도 MySQL 서버의 성능에 영향을 미칠 수 있다.\nMySQL 서버의 트랜잭션 목록은 information_schema.innodb_trx 테이블을 통해 확인 가능하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SELECT trx_id ,( SELECT CONCAT(user, \u0026#39;@\u0026#39;, host) FROM information_schema.processlist WHERE id=trx_mysql_thread_id ) AS soucre_info ,trx_state ,trx_started ,now() ,(unix_timestamp(now() - unix_timestamp(trx_started))) AS lasting_sec ,trx_requested_lock_id ,trx_wait_started ,trx_mysql_thread_id ,trx_tables_in_use ,trx_tables_in_use ,trx_tables_locked FROM information_schema.innodb_trx WHERE (unix_timestamp(now()) - unix_timestamp(trx_started)) \u0026gt; 5 \\G 어떤 레코드를 잠그고 있는지는 performance_schema.locks 테이블을 참조하면 된다.\n1 SELECT * FROM performance_schema.data_locks \\G ","date":"2023-08-23T12:57:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/7/9/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/7/9/","title":"11.7.9 스키마 조작(DDL) - 활성 트랜잭션 조회"},{"content":"MySQL 서버에 접속된 사용자의 목록이나 각 클라이언트 사용자가 현재 어떤 쿼리를 실행하고 있는지는 SHOW PROCESSLIST 명령으로 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 SHOW PROCESSLIST; /* +----+-----------------+-----------+-----------+---------+--------+------------------------+------------------+ | Id | User | Host | db | Command | Time | State | Info | +----+-----------------+-----------+-----------+---------+--------+------------------------+------------------+ | 5 | event_scheduler | localhost | NULL | Daemon | 514030 | Waiting on empty queue | NULL | | 9 | root | localhost | employees | Query | 0 | init | SHOW PROCESSLIST | +----+-----------------+-----------+-----------+---------+--------+------------------------+------------------+ */ 활용 MySQL 서버가 어떤 상태인지를 판단하는 데도 많은 도움이 된다.\n일반적으로 쾌적한 상태로 서비스되는 MySQL에서는 대부분의 프로세서의 Command 컬럼이 Sleep 상태로 표시된다. 컬럼 값이 Query 이면서 Time이 상당히 큰 값을 가지고 있다면 쿼리가 장시간 실행되고 있음을 의미한다. State 컬럼 State 컬럼에 표시될 수 있는 값의 종류는 다양하며 주의깊게 살펴야하는 항목은 다음과 같다.\nCopying ... Sorting ... 해당 값으로 시작하는 문장이 표시되는 경우 주의 깊게 살펴봐야 한다.\n강제 종료 Id 컬럼값은 접속된 커넥션의 요청을 처리하는 전용 스레드의 번호를 의미하며 KILL ... 으로 쿼리나 커넥션을 강제 종료할 수 있다.\n1 2 3 4 5 /* 쿼리 강제 종료, 커넥션 유지 */ KILL QUERY 4228; /* 쿼리 및 커넥션 강제 종료 */ KILL 4228; ","date":"2023-08-23T12:45:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/7/8/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/7/8/","title":"11.7.8 스키마 조작(DDL) - 프로세스 조회 및 강제 종료"},{"content":"하나의 테이블에 대해 여러가지 스키마 변경을 해야 하는 경우 온라인 DDL을 사용할 수 있다면 개별 ALTER TABLE 명령을 차례대로 실행하는 방법 보다 모아서 실행하는 것이 일반적으로 효율적이다.\n1 2 3 4 5 ALTER TABLE employees ADD INDEX ix_lastname (last_name, first_name), ALGORITHM=INPLACE, LOCK=NONE; ALTER TABLE employees ADD INDEX ix_birthdate (birth_date), ALGORITHM=INPLACE, LOCK=NONE; 위처럼 2개의 ALTER TABLE 명령으로 인덱스를 각각 생성하면 인덱스를 생성할 때마다 테이블의 레코드를 풀 스캔해서 인덱스를 생성하게 된다.\n1 2 3 4 ALTER TABLE employees ADD INDEX ix_lastname (last_name, first_name), ADD INDEX ix_birthdate (birth_date), ALGORITHM=INPLACE, LOCK=NONE; 위와 같이 모아서 실행하면 레코드를 한 번만 풀스캔해서 2개의 인덱스를 한꺼번에 생성할 수 있게 되어 2개의 인덱스를 각각 생성하는 데 걸리는 시간보다 많은 시간을 단축할 수 있다.\n스키마 변경 작업들이 다른 알고리즘을 사용한다면 모아서 실행할 필요는 없다. 가능하면 같은 알고리즘을 사용하는 스키마 변경 작업을 모아서 실행한다. 같은 INPLACE 알고리즘을 사용하더라도 테이블 리빌딩 여부에 따라 모아서 실행할 수 있다면 더 효율적으로 관리할 수 있다. ","date":"2023-08-23T12:32:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/7/7/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/7/7/","title":"11.7.7 스키마 조작(DDL) - 테이블 변경 묶음 실행"},{"content":"MySQL 8.0 버전에서는 대부분의 인덱스 변경 작업이 온라인 DDL로 처리 가능하도록 개선됐다.\n인덱스 추가 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ALTER TABLE employees ADD PRIMARY KEY (emp_no), ALGORITHM=INPLACE, LOCK=NONE; ALTER TABLE employees ADD UNIQUE INDEX ux_empno (emp_no), ALGORITHM=INPLACE, LOCK=NONE; ALTER TABLE employees ADD INDEX ix_lastname (last_name), ALGORITHM=INPLACE, LOCK=NONE; ALTER TABLE employees ADD FULLTEXT INDEX fx_firstname_lastname (first_name, last_name), ALGORITHM=INPLACE, LOCK=SHARED; ALTER TABLE employees ADD SPATIAL INDEX fx_loc (last_location), ALGORITHM=INPLACE, LOCK=SHARED; 전문 검색 인덱스와 공간 검색 인덱스는 INPLACE 알고리즘으로 인덱스 생성이 가능하지만 SHARED 잠금이 필요하다. B-Tree 자료 구조를 사용하는 인덱스의 추가는 프라이머리 키라고 하더라도 INPLACE 알고리즘에 잠금 없이 온라인으로 생성이 가능하다. 인덱스 조회 인덱스의 목록을 조회할 때는 SHOW INDEXES 명령을 사용하거나 SHOW CREATE TABLE 명령으로 표시되는 테이블의 생성 명령을 참조한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SHOW INDEX FROM employees; /* +-----------+------------+-----------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+ | Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | Visible | Expression | +-----------+------------+-----------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+ | employees | 0 | PRIMARY | 1 | emp_no | A | 297022 | NULL | NULL | | BTREE | | | YES | NULL | | employees | 0 | ux_empno | 1 | emp_no | A | 280034 | NULL | NULL | | BTREE | | | YES | NULL | | employees | 1 | ix_hiredate | 1 | hire_date | A | 5332 | NULL | NULL | | BTREE | | | YES | NULL | | employees | 1 | ix_gender_birthdate | 1 | gender | A | 5 | NULL | NULL | | BTREE | | | YES | NULL | | employees | 1 | ix_gender_birthdate | 2 | birth_date | A | 10970 | NULL | NULL | | BTREE | | | YES | NULL | | employees | 1 | ix_firstname | 1 | first_name | A | 1619 | NULL | NULL | | BTREE | | | YES | NULL | | employees | 1 | ix_lastname | 1 | last_name | A | 1681 | NULL | NULL | | BTREE | | | YES | NULL | | employees | 1 | fx_firstname_lastname | 1 | first_name | NULL | 280034 | NULL | NULL | | FULLTEXT | | | YES | NULL | | employees | 1 | fx_firstname_lastname | 2 | last_name | NULL | 280034 | NULL | NULL | | FULLTEXT | | | YES | NULL | +-----------+------------+-----------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+ */ SHOW INDEXES 명령은 테이블의 인덱스만 인덱스 컬럼별로 한 줄씩 표시한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 SHOW CREATE TABLE employees; /* +-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Table | Create Table | +-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | employees | CREATE TABLE `employees` ( `emp_no` int NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NOT NULL, `last_name` varchar(16) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NOT NULL, `gender` enum(\u0026#39;M\u0026#39;,\u0026#39;F\u0026#39;) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`), UNIQUE KEY `ux_empno` (`emp_no`), KEY `ix_hiredate` (`hire_date`), KEY `ix_gender_birthdate` (`gender`,`birth_date`), KEY `ix_firstname` (`first_name`), KEY `ix_lastname` (`last_name`), FULLTEXT KEY `fx_firstname_lastname` (`first_name`,`last_name`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci STATS_PERSISTENT=0 | +-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ */ 인덱스 이름 변경 쿼리에서 인덱스의 이름을 힌트로 사용하면 MySQL 서버에서 해당 인덱스를 삭제하거나 다른 인덱스로 대체하는 경우 응용프로그램의 코드를 변경해야 했다.\n인덱스를 삭제하고 동일 이름으로 새로운 인덱스를 생성하면 되지만 새로운 인덱스를 생성하는 동안 필요한 인덱스가 없어지므로 손쉽게 작업하기가 어려웠다.\n이는 인덱스의 이름만 변경할 수 있다면 새로운 인덱스로 기존 인덱스를 대체할 수 있는데, MySQL 5.7 버전부터는 인덱스의 이름을 변경할 수 있게 되었다.\n1 2 ALTER TABLE salaries RENAME INDEX ix_salary TO ix_salary2, ALGORITHM=INPLACE, LOCK=NONE; 인덱스의 이름을 변경하는 작업은 INPLACE 알고리즘을 사용하지만 실제 테이블 리빌드를 필요로 하지 않는다. 응용 프로그램에서 힌트로 해당 인덱스의 이름을 사용 중이라고 하더라도 짧은 시간 안에 교체가 가능하다. 1 2 3 4 5 6 7 8 9 10 /* 새로운 인덱스 생성 */ ALTER TABLE employees ADD INDEX index_new (first_name, last_name), ALGORITHM=INPLACE, LOCK=NONE; /* 기존 인덱스 삭제 및 새로운 인덱스의 이름을 변경 */ ALTER TABLE employees DROP INDEX ix_firstname, RENAME INDEX index_new TO ix_firstname, ALGORITHM=INPLACE, LOCK=NONE; 인덱스 가시성 변경 MySQL 서버에서 인덱스를 삭제하는 작업은 ALTER TABLE DROP INDEX 명령으로 즉시 완료된다.\n하지만 한 번 삭제된 인덱스를 새로 생성하는 것은 매우 많은 시간이 걸릴 수도 있다.\n최악의 경우 응용 프로그램의 서비스를 멈추고, 인덱스를 다시 생성하고 응용 프로그램을 다시 시작해야 한다. 인덱스나 테이블을 삭제하는 작업은 부담스러운 작업이므로 한 번 생성된 인덱스는 삭제하지 못하는 경우가 많다. MySQL 8.0 버전부터는 인덱스의 가시성을 제어할 수 있는 기능이 도입되었으며, 이로 인해 쿼리를 실행할 때 해당 인덱스를 사용할 수 있는지를 결정할 수 있게 되었다.\n1 ALTER TABLE employees ALTER INDEX ix_firstname [INVISIBLE | VISIBLE]; 인덱스가 INVISIBLE 상태로 변경되는 옵티마이저는 해당 인덱스가 없는 것으로 간주하고 실행 계획을 수립한다.\n해당 명령은 메타데이터만 변경하기 때문에 온라인 DDL로 실행되는지 여부를 고려하지 않아도 된다. 인덱스를 삭제하기 전 먼저 해당 인덱스를 비활성하여 상황을 모니터링한 후 안전하게 인덱스를 삭제할 수 있게 됐다. 최초 인덱스를 생성할 때도 가시성을 설정할 수 있다.\n1 2 3 4 5 6 7 8 SHOW CREATE TABLE employees \\G /* CREAET TABLE employees \\G ( ... KEY \u0026#39;ix_firstname_lastname\u0026#39; (\u0026#39;first_name\u0026#39;, \u0026#39;last_name\u0026#39;) /* !80000 INVISIBLE */ ) ENGINE=InnoDB */ 비슷한 컬럼으로 구성된 인덱스가 많아지면 MySQL 옵티마이저가 다른 인덱스를 선택하여 실행계획이 바뀌고, 이로인해 성능이 악화될 수도 있다.\n이러한 문제가 우려되는 경우 처음 INVISIBLE로 생성 후 부하가 낮을 때 VISIBLE로 변경하여 모니터링하고 적용 여부를 결정 및 성능이 떨어지는 원인을 분석 할 수 있다.\n인덱스 삭제 1 2 3 ALTER TABLE employees DROP PRIMARY, ALGORITHM=COPY, LOCK=SHARED; ALTER TABLE employees DROP INDEX ux_empno, ALGORITHM=COPY, LOCK=NONE; ALTER TABLE employees DROP fx_loc, ALGORITHM=INPLACE, LOCK=NONE; MySQL 서버의 인덱스 삭제는 일반적으로 매우 빨리 처리된다.\n세컨더리 인덱스 삭제 작업은 INPLACE 알고리즘을 사용하지만 테이블 리빌딩은 발생하지 않는다. 프라이머리 키의 삭제 작업은 모든 세컨더리 인덱스의 리프 노드에 저장된 프라이머리 키 값을 삭제해야 한다. 임시 테이블로 레코드를 복사해서 테이블을 재구성 해야한다. ","date":"2023-08-23T12:05:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/7/6/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/7/6/","title":"11.7.6 스키마 조작(DDL) - 인덱스 변경"},{"content":"테이블 구조 변경 작업은 대부분 컬럼을 추가하거나 타입을 변경하는 작업이다.\n컬럼 추가 MySQL 8.0 버전으로 업그레이드되면서 테이블 컬럼 추가 작업은 대부분 INPLACE 알고리즘을 사용하는 온라인 DDL로 처리가 가능하다.\n또한 컬럼을 테이블의 제일 마지막 컬럼으로 추가하는 작업은 INSTANT 알고리즘으로 즉시 추가된다.\n1 2 3 4 5 ALTER TABLE employees ADD COLUMN emp_telno VARCHAR(20), ALGORITHM = INSTANT; ALTER TABLE employees ADD COLUMN emp_telno VARCHAR(20) AFTER emp_no, ALGORITHM = INPLACE, LOCK = NONE; 사용하여 기존 컬럼 중간에 새로 추가하는 경우 테이블의 리빌드가 필요하여 INPLACE 알고리즘을 사용해야 한다.\n따라서 테이블이 큰 경우라면 테이블의 마지막 컬럼으로 추가하는 것이 좋다.\n컬럼 삭제 컬럼을 삭제하는 작업은 항상 테이블 리빌드를 필요로 하기 때문에 INPLACE 알고리즘만 사용 가능하다.\n1 2 ALTER TABLE employees DROP COLUMN emp_telno, ALGORITHM = INPLACE, LOCK = NONE; 이름 및 타입 변경 컬럼의 타입 변경은 현재 컬럼의 타입과 변경하고자 하는 데이터 타입에 따라 매우 다양한 형태가 될 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 /* 이름 변경 */ ALTER TABLE salaries CHANGE to_date end_date DATE NOT NULL, ALGORITHM = INPLACE, LOCK = NONE; /* INT를 VARCHAR로 변경 */ ALTER TABLE salaries MODIFY salary VARCHAER(20), ALGORITHM = COPY, LOCK = SHARED; /* VARCHAR 확장 */ ALTER TABLE employees MODIFY last_name VARCHAER(30) NOT NULL, ALGORITHM = COPY, LOCK = NONE; /* VARCHAR 축소 */ ALTER TABLE employees MODIFY last_name VARCHAER(10) NOT NULL, ALGORITHM = COPY, LOCK = SHARED; 이름 변경: 컬럼의 이름만 변경하는 작업은 INPLACE 알고리즘을 사용하지만 실제 데이터 리빌드 작업은 필요치 않으므로 빠르게 완료된다. 데이터 타입 변경: 데이터 타입이 변경되는 경우 COPY 알고리즘이 필요하며 온라인 DDL로 실행되어도 스키마 변경중에는 테이블 쓰기 작업은 불가능하다. 크기 확장: 확장하는 경우는 현재 길이와 확장하는 길이의 관계에 따라 리빌드가 필요할 수 있다. 크기 축소: 길이를 축소하는 경우 완전히 다른 타입으로 변경되는 경우과 같이 COPY 알고리즘을 사용해야한다. 스키마를 변경하는 중 테이블의 데이터 변경은 허용되지 않으므로 SHERED로 사용돼야한다. 컬럼 타입을 변경 중 가장 빈번한 것은 VARCHAR, VARBINARY 타입의 길이를 확장하는 것이다.\n해당 타입은 컬럼의 쵀대 허용 사이자가 메타데이터에 저장되지만, 실제 컬럼이 가지는 값의 길이는 데이터 레코드의 컬럼 헤더에 저장되는데, 동일한 값이더라도 다른 용량이 필요할 수 있다. 값의 길이를 위해서 사용하는 공간의 크기는 최대 가질 수 있는 바이트 수 만큼 필요하기 때문에 컬럼값의 길이 저장용 공간은 컬럼의 값이 최대 가질수 있는 바이트 수가 255 이하인 경우 1 바이트만 사용하며, 256 바이트 이상인 경우 2바이트를 사용한다. INPLACE 알고리즘으로 필요로 하는 같은 용량으로 변경하는 경우 리빌드가 필요 없지만, 문자 셋이나 크기 변경으로 인해 필요로 하는 용량이 변경된다면 리빌딩이 필요하다. ","date":"2023-08-18T16:00:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/7/5/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/7/5/","title":"11.7.5 스키마 조작(DDL) - 컬럼 변경"},{"content":"테이블은 사용자의 데이터를 가지는 주체로서, 서버의 많은 옵션과 인덱스 등의 기능이 테이블에 종속되어 사용된다.\n테이블 생성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tb_test ( member_id BIGINT [UNSIGNED] [AUTO_INCREMENT], nickname CHAR(20) [CHARACTER SET \u0026#39;utf-8\u0026#39;] [COLLATE \u0026#39;utf8_general_ci\u0026#39;] [NOT NULL], home_url VARCHAR(200) [COLLATE \u0026#39;latin1_general_cs\u0026#39;], birth_year SMALLINT [(4)] [UNSIGNED] [ZEROFILL], member_point INT [NOT NULL] [DEFAULT 0], registerd_dttm DATETIME [NOT NULL], modified_ts TIMESTAMP [NOT NULL] [DEFAULT CURRENT_TIMESTAMP], gender ENUM(\u0026#39;Female\u0026#39;, \u0026#39;Male\u0026#39;) [NOT NULL], hobby SET(\u0026#39;Reading\u0026#39;, \u0026#39;Game\u0026#39;, \u0026#39;Sports\u0026#39;), profile TEXT [NOT NULL], session_data BLOB, PRIMARY KEY (member_id), UNIQUE INDEX ux_nickname (nickname), INDEX ix_registereddttm (registered_dttm) ) ENGINE=INNODB; TEMPORARY 키워드를 사용하면 해당 커넥션에서만 사용 가능한 임시 테이블을 생성한다. 이미 같은 테이블이 있으면 에러가 발생하며 옵션을 사용하여 무시할 수 있다. 테이블을 정의한 스크립트 마지막에 스토리지 엔진을 결정하기 위한 ENGINE 키워드를 사용할 수 있다. 별도로 정의되지 않으면 MySQL 8.0에서는 InnoDB 스토리지 엔진이 기본으로 사용된다. 각 컬럼은 컬럼명 + 컬럼타입 + [타입 별 옵션] + [NULL 여부] + [기본값] 순서로 명시하고, 타입별로 여러 옵션을 추가로 사용할 수 있다.\n15장 참조 테이블 구조 조회 테이블 구조를 확인하는 방법은 SHOW CREATE TABLE, DESC 명령이 있다.\nSHOW CREATE TABLE CREATE TABLE 문장을 표시해준다. 최초로 생성할 때 사용자가 실행한 내용이 아닌 테이블의 메타 정보를 읽어 CREATE TABLE 명령으로 재작성하여 보여준다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 SHOW CREATE TABLE employees; /* +-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Table | Create Table | +-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | employees | CREATE TABLE `employees` ( `emp_no` int NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NOT NULL, `last_name` varchar(16) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NOT NULL, `gender` enum(\u0026#39;M\u0026#39;,\u0026#39;F\u0026#39;) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`), KEY `ix_hiredate` (`hire_date`), KEY `ix_gender_birthdate` (`gender`,`birth_date`), KEY `ix_firstname` (`first_name`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci STATS_PERSISTENT=0 | +-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ */ 컬럼의 목록, 인덱스, 외래키 정보를 동시에 보여주기 때문에 SQL을 튜닝하거나 테이블의 구조를 확인할 때 주로 사용한다.\nDESC DESCRIBE의 약어 형태의 명령으로 같은 결과를 보여준다. 테이블이 컬럼 정보를 보기 편한 표 형태로 표시해준다.\n인덱스 컬럼의 순서나 외래키, 테이블 자체의 속성을 보여주지는 않으므로 테이블의 전체적인 구조를 한 번에 확인하기는 어렵다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 DESC employees; /* +------------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +------------+---------------+------+-----+---------+-------+ | emp_no | int | NO | PRI | NULL | | | birth_date | date | NO | | NULL | | | first_name | varchar(14) | NO | MUL | NULL | | | last_name | varchar(16) | NO | | NULL | | | gender | enum(\u0026#39;M\u0026#39;,\u0026#39;F\u0026#39;) | NO | MUL | NULL | | | hire_date | date | NO | MUL | NULL | | +------------+---------------+------+-----+---------+-------+ */ 테이블의 구조 변경 ALTER TABLE 명령을 사용한다. 테이블의 자체의 속성을 변경할 수 있을뿐만 아니라 인덱스의 추가 삭제나 컬럼을 추가/삭제하는 용도로도 사용된다.\n테이블 자체에 대한 속성 변경은 주로 테이블의 문자 집합이나 스토리지 엔진, 파티션 구조 등의 변경이다.\n파티션은 13장 참조 1 2 3 4 5 6 ALTER TABLE employees CONVERT TO CHARACTER SET UTF8MB4 COLLATE UTF8MB4_GENERAL_CI, ALGORITHM=INPLACE, LOCK=NONE; ALTER TABLE employees ENGINE=InnoDB, ALGORITHM=INPLACE, LOCK=NONE; 스토리지 엔진을 변경하는 명령은 내부적인 테이블의 저장소를 변경하는 것이라 항상 테이블의 모든 레코드를 복사하는 작업이 필요하다.\nALTER TABLE 문장에 명시된 ENGINE이 기존과 동일하더라도 테이블의 데이터를 복사하는 작업은 실행되기 때문에 주의해야 한다. 테이블 데이터를 리블드 하는 목적으로도 사용된다. 주로 레코드의 삭제가 자주 발생하는 테이블에서 데이터가 저장되지 않은 빈 공간을 제거해 디스크 사용 공간을 줄이는 역할을 한다. 테이블 명 변경 RENAME TABLE 명령으로 처리할 수 있으며, 당순히 테이블의 이름 변경뿐만 아니라 다른 데이터베이스로 테이블을 이동할 때도 사용할 수 있다.\n1 2 RENAME TABLE table1 TO table 2; RENAME TABLE db1.table1 TO db2.table 2; 이름만 변경하는 경우 첫 번째 명령 처럼 동일 데이터베이스 내에서 이름만 변경하는 작업은 메타 정보만 병경하기 때문에 매우 빨리 처리된다. 테이블 복사 두 번째 명령과 같이 데이터베이스를 변경하는 경우는 메타 정보뿐만 아니라 테이블이 저장된 파일까지 다른 디렉터리로 이동해야 한다. 운영체제에서 서로 다른 파티션으로 파일을 이동할 때는 데이터 파일을 먼저 복사하고 복사를 완료하면 원본 파티션의 파일을 삭제하는 형태로 처리하는데 동일하게 처리된다. 서로 다른 운영체제의 파일 스스템을 사용하고 있었다면 데이터 파일의 크기에 비례해서 시간이 소요된다. 테이블을 교체하는 경우 일시적으로 테이블이 없어지는 시점이 발생하기 때문에 에러를 발생 시킬 수 있다. 여러 테이블의 RENAME명령을 하나의 문장으로 묶어 실행하여 예방할 수 있다. 테이블 상태 조회 모든 테이블은 만들어진 시간, 대략적인 레코드 건수, 데이터 파일의 크기 등 정보를 가지고 있다. 또한 데이터 파일의 버전이나 레코드 포맷 등과 같이 자주 사용되지는 않지만 중요한 정보도 가지고 있는데, 이러한 정보를 조회할 수 있는 명령이 SHOW TABLE STATUS ...다.\nLIKE '패턴' 과 같은 조건을 사용해 특정 테이블의 상태만 조회하는 것도 가능하다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 SHOW TABLE STATUS LIKE \u0026#39;employees\u0026#39; \\G; /* *************************** 1. row *************************** Name: employees Engine: InnoDB Version: 10 Row_format: Dynamic Rows: 297022 Avg_row_length: 51 Data_length: 15220736 Max_data_length: 0 Index_length: 22593536 Data_free: 5242880 Auto_increment: NULL Create_time: 2023-07-25 10:31:33 Update_time: 2023-08-17 14:39:48 Check_time: NULL Collation: utf8mb4_general_ci Checksum: NULL Create_options: stats_persistent=0 Comment: */ SELECT 쿼리를 이용해서 조회할 수도 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 SELECT * FROM information_schema.TABLES WHERE TABLE_SCHEMA=\u0026#39;employees\u0026#39; AND TABLE_NAME=\u0026#39;employees\u0026#39; \\G /* *************************** 1. row *************************** TABLE_CATALOG: def TABLE_SCHEMA: employees TABLE_NAME: employees TABLE_TYPE: BASE TABLE ENGINE: InnoDB VERSION: 10 ROW_FORMAT: Dynamic TABLE_ROWS: 297022 AVG_ROW_LENGTH: 51 DATA_LENGTH: 15220736 MAX_DATA_LENGTH: 0 INDEX_LENGTH: 22593536 DATA_FREE: 5242880 AUTO_INCREMENT: NULL CREATE_TIME: 2023-07-25 10:31:33 UPDATE_TIME: 2023-08-17 14:39:48 CHECK_TIME: NULL TABLE_COLLATION: utf8mb4_general_ci CHECKSUM: NULL CREATE_OPTIONS: stats_persistent=0 TABLE_COMMENT: */ 테이블 구조 복사 SHOW CREATE TABLE\n테이블 구조는 같지만 이름만 다른 테이블을 생성할 때는 SHOW CREATE TABLE 명령을 이용해 테이블의 생성 DDL을 조회한 후에 조금 변경해서 만들 수도 있다. CREATE TABLE ... AS SELECT ... LIMIT 0\nSHOW CREATE TABLE 명령을 사용하면 내용을 조금 변경해야 할 수도 있다 인덱스가 생성되지 않는다. CREATE TABLE ... LIKE\n데이터는 복사하지 않고 테이블의 구조만 동일하게 복사한다. 데이터가지 복사하려면 생성 후 INSERT INTO 명령을 추가로 사용한다. 테이블 삭제 MySQL 8.0 에서는 특정 테이블을 삭제하는 작업이 다른 테이블의 DML이나 쿼리를 직접 방해하지는 않는다.\n레코드가 많지 않은 테이블을 삭제하는 작업은 서비스 도중이라고 하더라도 문제가 되지는 않지만 용량이 매우 큰 테이블을 삭제하는 작업은 상당히 부하가 크다.\n서버는 해당 테이블이 사용하던 데이터 파일을 삭제해야 한다. 파일의 크기가 매우 크고 디스크에서 파일 조각들이 너무 분산되어 저장돼 있다면 많은 디스크 읽기/쓰기 작업이 필요하다. 이로인해 다른 커넥션의 쿼리 처리 성능이 떨어질 수도 있다. InnoDB 스토리지 엔진의 어댑티브 해시 인덱스는 버퍼불의 각 페이지가 가진 레코드에 대한 해시 인덱스 기능을 제공하는데, 어댑티브 해시 인덱스가 활성화돼있는 경우 테이블이 삭제되면 관련 정보들을 모두 삭제해야 한다.\n많이 가지고 있다면 삭제 작업으로 인해 서버의 부하가 높아지고 간접적으로 다른 쿼리 처리에 영향을 미칠 수도 있다. 테이블의 스키마 변경에도 영향을 미칠 수 있다. ","date":"2023-08-18T14:32:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/7/4/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/7/4/","title":"11.7.4 스키마 조작(DDL) - 테이블 변경"},{"content":"MySQL 서버에는 전통적으로 테이블별로 전용 테이블 스페이스를 사용했었다.\nInnoDB 스토리지 엔진의 시스템 테이블 스페이스만 제너럴 테이블스페이스를 사용했는데, 제너럴 테이블스페이스는 여러 테이블의 데이터를 한꺼번에 저장하는 테이블스페이스를 의미한다.\nMySQL 8.0 버전이 되면서 사용자 테이블을 제너럴 테이블스페이스로 저장하는 기능이 추가되고 테이블스페이스를 관리하는 DDL 명령이 추가됐다.\n제약 사항 MySQL 8.0에서도 제너럴 테이블스페이스는 여러가지 제약 사항을 가진다.\n파티션 테이블은 제너럴 테이블 스페이스를 사용하지 못함 복제 소스와 레플리카 서버가 동일 호스트에서 실행되는 경우 ADD DATAFILE 문장은 사용 불가 테이블 암호화는 테이블스페이스 단위로 설정됨 테이블 압축 가능 여부는 테이블스페이스의 블록 사이즈와 InnoDB 페이지 사이즈에 의해 결정됨 특정 테이블을 삭제(DROP TABLE)해도 디스크 공간이 운영체제로 반납되지 않음 장점 그럼에도 불구하고 사용자 테이블이 제너럴 테이블 스페이스를 이용할 수 있게 개선된 것은 다음과 같은 장점이 있기 때문이다.\n제너럴 테이블스페이스를 사용하면 파일 핸들러(Open file descriptor)를 최소화 테이블스페이스 관리에 필요한 메모리 공간 최소화 위 장점은 테이블의 개수가 매우 많은 경우에 유용하며, 일반적인 환경에서 제너럴 테이블 스페이스의 장점을 취하기가 어렵다.\n사용 MySQL 서버에서 테이블이 어떤 테이블스페이스를 사용할지는 innodb_file_per_table 시스템 변수로 제어할 수 있다.\nMySQL 8.0에서는 해당 시스템 변수 기본값이 ON이므로 테이블은 자동으로 개별 테이블스페이스를 사용한다.\n데이터베이스에 작은 테이블이 매우 많이 필요한 응용 프로그램을 개발 중이라면 제너럴 테이블 스페이스에 대한 활용을 고려해보는 것도 좋은 방법이다.\n","date":"2023-08-18T14:23:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/7/3/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/7/3/","title":"11.7.3 스키마 조작(DDL) - 테이블 스페이스 변경"},{"content":"MySQL에서 하나의 인스턴스는 1개 이상의 데이터베이스를 가질 수 있다.\n다른 RDBMS에서는 스키마와 데이터베이스를 구분해서 관리하지만 MySQL 서버에서는 스키마와 데이터베이스는 동격 개념이므로 굳이 스키마를 명시적으로 사용하지는 않는다. MySQL의 데이터베이스는 디스크의 물리적인 저장소를 구분하기도 하지만 여러 데이터베이스의 테이블을 묶어서 조인 쿼리를 사용할 수도 있기 때문에 단순히 논리적인 개념이기도 하다. 데이터베이스는 객체에 대한 권한을 구분하는 용도로 사용되기도 하지만 그 이상의 큰 의미를 가지지는 않는다. 데이터베이스 생성 데이터베이스에 설정할 수 있는 옵션은 기본 문자 집합이나 콜레이션을 설정하는 정도이므로 간단하다.\n1 2 3 4 CREATE DATABASE [IF NOT EXISTS] employees; CREATE DATABASE [IF NOT EXISTS] employees CHARACTER SET utf8mb4; CREATE DATABASE [IF NOT EXISTS] employees CHARACTER SET utf8mb4 COLLATE utf8mb_general_ci; 데이터베이스 목록 1 2 SHOW DATABASES; SHOW DATABASES LIKE \u0026#39;%emp%\u0026#39;; 접속된 MySQL 서버가 가지고 있는 데이터베이스의 목록을 나열한다.\n권한을 가지고 있는 데이터 베이스 목록만 표시한다. SHOW DATABASES 권한이 있어야 한다. 데이터베이스 선택 1 USE employees; 기본 데이터베이스를 선택하는 명령이다.\nSQL 문장에서 데이터베이스를 명시하지 않고 테이블 이름이나 프로시저의 이름만 명시하면 현재 커넥션의 기본 데이터베이스에서 주어진 테이블이나 프로시저를 검색한다. 기본 데이터베이스에 존재하지 않는 테이블이나 프로시저를 사용하려면 이름 앞에 데이터베이스 이름을 반드시 명시해야한다. 데이터베이스 속성 변경 1 2 ALTER DATABASE employees CHARACTER SET = euckr; ALTER DATABASE employees CHARACTER SET = euckr COLLATE = euckr_korean_ci; 생성할 때 지정한 문자 집합이나 콜레이션을 변경한다.\n데이터베이스 삭제 1 DROP DATABASE [IF EXISTS] employees; 데이터베이스를 삭제한다.\n지정한 이름의 데이터베이스가 존재하지 않는다면 에러가 발생한다. IF EXISTS 키워드를 사용하면 존재할 때만 삭제한다. ","date":"2023-08-18T14:10:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/7/2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/7/2/","title":"11.7.2 스키마 조작(DDL) - 데이터베이스 변경"},{"content":"DBMS 서버의 모든 오브젝트를 생성하거나 변경하는 쿼리를 DDL(Data Definition Language)라고 한다.\n스토어드 프로시저나 함수, DB나 테이블 등을 생성하거나 변경하는 대부분의 명령이 DDL에 해당한다. MySQL 서버가 업그레이드 되면서 많은 DDL이 온라인 모드로 처리될 수 있게 개선됐지만, 스키마를 변경하는 작업은 오랜 시간이 걸리고 서버에 많은 부하를 발생시키는 작업들이 있으므로 주의해야 한다. MySQL 5.5 이전 버전까지는 서버에서 테이블의 구조를 변경하는 동안 다른 커넥션에서 DML을 실행할 수 없었기 때문에 Percona에서 개발한 pt-online-schema-change라는 도구를 사용해야했다.(MySQL 5.5 버전에서도 성능상 이유로 사용했다.)\nMySQL 8.0 버전으로 업그레이드되면서 대부분의 스키마 변경 작업은 서버에 내장된 온라인 DDL 기능으로 처리가 가능하기 때문에 위와 같은 도구는 거의 사용되지 않는다.\n온라인 DDL 알고리즘 온라인 DDL은 스키마를 변경하는 작업 도중에도 다른 커넥션에서 해당 테이블의 데이터를 변경하거나 조회하는 작업을 가능하게 해준다.\n온라인 DDL 기능은 테이블의 구조를 변경하거나 인덱스 추가와 같은 대부분의 작업에 대해 작동한다. ALGORITHM, LOCK 옵션을 통해 어떤 모드로 스키마 변경을 실행할지 결정할 수 있다. MySQL 서버는 old_alter_table 시스템 변수를 이용해 ALTER TABLE 명령이 온라인 DDL 또는 예전 방식(테이블의 읽고 쓰기를 막고 스키마를 변경하는 방식)으로 처리할지를 결정할 수 있다.\n해당 시스템 변수의 기본값은 OFF로 설정되어 있어 기본적으로 온라인 DDL이 활성화된다. ALTER TABLE 명령이 실행되면 다음과 같은 순서로 스키마 변경에 적합한 알고리즘을 찾는다.\nALGORITHM=INSTANT로 스키마 변경이 가능한지 확인 후, 가능하면 선택 ALGORITHM=INPLACE로 스키마 변경이 가능한지 확인 후, 가능하면 선택 ALGORITHM=COPY 알고리즘 선택 스키마 변경 알고리즘의 우선순위가 낮을수록 MySQL 서버는 스키마 변경을 위해 더 큰 잠금과 많은 작업을 필요로 하고 서버의 부하도 많이 발생시킨다.\nINSTANT 테이블의 데이터는 전혀 변경하지 않고, 메타데이터만 변경하고 작업을 완료한다.\n테이블이 가진 레코드 건수와 무관하게 작업 시간이 매우 짧다. 스키마 변경 도중 테이블의 읽고 쓰기는 대기하게 되지만 스키마 변경 시간이 매우 짧기 때문에 다른 커넥션의 쿼리 처리에는 크게 영향을 미치지 않는다. INPLACE 임시 테이블로 데이터를 복사하지 않고 스키마 변경을 실행한다.\n내부적으로는 테이블의 리빌드를 실행할 수 있다. 레코드의 복사 작업은 없지만 테이블의 모든 레코드를 리빌드해야 하기 때문에 테이블의 크기에 따라 많은 시간이 소요될 수 있다. 스키마 변경 중에도 테이블의 읽기와 쓰기 모두 가능하다. 최초 시작 시점과 마지막 종료 시점에는 테이블의 읽고 쓰기가 불가능하다. 하지만 시간이 매우 짧기 때문에 다른 커넥션의 쿼리 처리에 대한 영향도는 높지 않다. 대부분 잠금은 NONE으로 설정 가능하지만, COPY 변경된 스키마를 적용한 임시 테이블을 생성하고 테이블을 모두 입시 테이블로 복사한 후 최종적으로 임시 테이블을 RENAME하여 스키마 변경을 완료한다.\n테이블 읽기만 가능하고 DML은 실행할 수 없다. 잠금 수준 온라인 DDL 명령은 알고리즘과 함께 잠금 수준도 함께 명시할 수 있다.\n1 2 ALTER TABLE salaries CHANGE to_date end_date DATE NOT NULL, ALGORITHM=INPLACE, LOCK=NONE; ALGORITHM, LOCK 옵션이 명시되지 않으면 서버가 적절한 수준의 알고리즘과 잠금 수준을 선택한다.\nINSTANT 알고리즘은 메타데이터만 변경하기 때문에 매우 짧은 시간 동안의 메타데이터 잠금을 필요로 하기 때문에 LOCK 옵션을 지원하지 않는다.\nNONE: 아무런 잠금을 걸지 않음 SHARED: 읽기 잠금을 걸고 스키마 변경을 실행한다. 스키마 변경 중 읽기는 가능하지만 쓰기는 불가 INPLACE 알고리즘이 사용되는 경우 대부분 NONE으로 설정 가능하지만 SHARED 수준까지 설정해야 할 수 있다. EXCLUSIVE: 쓰기 잠금을 걸고 스키마 변경을 실행한다. 테이블의 읽고 쓰기 불가 전통적인 ALTER TABLE과 동일하므로 명시할 필요는 없다. 온라인 스키마 변경 작업이 INPLACE 알고리즘을 사용하더라도 내부적으로는 테이블의 리빌드가 필요할 수도 있다.\n테이블의 프라이머리 키를 추가하는 작업은 데이터 파일에서 레코드의 저장 위치가 바뀌어야 하기 때문에 테이블 리빌드가 필요하다. 단순히 컬럼의 이름만 변경하는 경우 INPLACE 알고리즘을 사용해야 하지만 테이블 레코드의 리빌드는 필요하지 않다. 프라이머리 키를 추가하는 경우와 같이 테이블 레코드의 리빌드가 필요한 경우를 Data Reorganizing(데이터 재구성) 또는 Table Rebuild(테이블 리빌드) 라고 부른다.\n결론적으로 INPLACE 알고리즘을 사용하는 경우는 데이터 재구성(테이블 리빌드) 여부에 따라 다음과 같이 구분할 수 있다.\n필요한 경우: 잠금을 필요로 하지 않기 때문에 읽고 쓰기는 가능하지면 테이블의 레코드 건수에 따라 상당히 많은 시간이 소요될 수도 있다. 필요치 않은 경우: INPLACE 알고리즘을 사용하지만 INSTANT 알고리즘처럼 매우 빨리 작업이 완료될 수 있다. MySQL 서버의 온라인 DDL 기능은 버전별로 많은 차이가 있으므로 사용 중인 버전이 8.0이 아니라면 메뉴얼을 살펴보고 테이블 리빌드가 필요한지 확인이 필요하다.\n또한 스키마 변경 작업을 실행하기 전 먼저 메뉴얼과 테스트를 진행해본다면 안전하게 처리할 수 있다.\n온라인 처리 가능한 스키마 변경 MySQL 서버의 모든 스키마 변경 작업이 온라인으로 가능한 것이 아니기 때문에 필요한 스키마 변경 작업의 형태가 온라인으로 처리될 수 있는지, 일고 쓰기가 대기(Waiting)하게 되는지 확인한 후 실행하는 것이 좋다.\nMySQL 서버에서 사용할 수 있는 스키마 변경 작업은 매우 다양하기 때문에 모든 명령이 온라인 DDL을 지원하는지는 기억하기 어려우므로 가이드나 책을 참고한다.\nALTER TABLE 문장에 LOCK과 ALGORITHM절을 명시해서 온라인 스키마 변경의 처리 알고리즘을 강제할 수 있으나, 무조건 명시한대로 처리하지는 않는다. 명시된 알고리즘으로 온라인 DDL이 처리되지 못한다면 에러를 발생시키고 작업은 수행되지 않기 때문에 의도하지 않은 잠금과 대기는 발생하지 않는다. 온라인 DDL이라 하더라도 서버에 부하를 유발할 수 있으며, 그로 인해 다른 커넥션의 쿼리들이 느려질 수도 있다. 따라서 스키마 변경 작업이 직접 다른 커넥션의 DML을 대기하게 만들지는 않더라도 주의해서 사용한다.\nINPLACE 알고리즘 INPLACE 알고리즘은 임시 테이블로 레코드를 복사하지는 않더라도 내부적으로 테이블의 모든 레코드를 리빌드해야 하는 경우가 많다.\n이러한 경우 다음과 같은 과정을 거친다.\n스키마 변경이 지원되는 스토리지 엔진의 테이블인지 확인 스키마 변경 준비 스키마 변경에 대한 정보를 준비해서 온라인 DDL 작업 동안 변경되는 데이터를 추적할 준비 테이블 스키마 변경 및 새로운 DML 로깅 실제 스키마 변경을 수행하는 과정으로, 작업이 수행되는 동안 다른 커넥션의 DML 작업이 대기하지 않는다. 스키마를 온라인으로 변경함과 동시에 다른 스레드에서는 사용자에 의해서 발생한 DML들에 대해서 별도의 로그로 기록 로그 적용 온라인 DDL 작업 동안 수집된 DML들에 대해 별도 로그로 기록 스키마 변경 완료(COMMIT) INPLACE 알고리즘으로 스키마가 변경된다고 하더라도 2, 4번 단계에서는 짧은 배다적 잡금이 필요하며, 이 시점에는 다른 커넥션의 DML들이 잠깐 대기한다. 실제 변경 작업이 실행되며 많은 시간이 필요한 3번 단계는 DML 작업이 대기 없이 즉시 처리된다. INPLACE 알고리즘으로 온라인 스키마 변경이 진행되는 동안 새로 유입된 DML 쿼리들에 의해 변경되는 데이터를 온라인 변경 로그라는 메모리 공간에 쌓아 두었다가 완료되면 로그의 내용을 실제 테이블로 일괄 적용한다. 온라인 변경 로그는 메모리에만 생성되며, 메모리 공간의 크기는 innodb_online_alter_log_max_size시스템 설정 변수에 의해 결정된다. 온라인 스키마 변경이 오랜 시간 걸리거나, 스키마 변경 중에 유입되는 DML 쿼리가 많다면 메모리 공간을 크게 설정하는 것이 좋다. 해당 시스템 변수는 세션 단위의 동적 변수이므로 언제든 변경할 수 있다. 온라인 DDL의 실패 케이스 온라인 DDL이 INSTANT 알고리즘을 사용하는 경우 시작과 동시에 작업이 완료되기 때문에 작업 도중 실패할 가능성은 거의 없지만, INPLACE 알고리즘으로 실행되는 경우 내부적으로 테이블 리빌드 과정이 필요할 수 있고, 최종 로그 적용 과정이 필요해서 중간에 실패할 가능성이 높은 편이다.\nALTER TABLE 명령이 장시간 실행되고 동시에 다른 커넥션에서 DML이 많이 실행되는 경우 온라인 변경 로그의 공간이 부족한 경우 ALTER TABLE 명령이 실행되는 동안 이전 버전의 테입르 구조에서는 아무런 문제가 안되지만 이후 테이블 구조에는 적합하지 않은 레코드가 INSERT, UPDATE 됐다면 스키마 변경 작업은 마지막 과정에서 실패 스키마 변경을 위해서 필요한 잠금 수준보다 낮은 잠금 옵션이 사용된 경우 잠금 없이 실행되는데 변경 작업의 처음과 마지막 과정에서 잠금이 이 수행되는 과정에서 잠금을 획득하지 못하는 경우 인덱스를 생성하는 작업의 경우 정렬을 위해 디스크의 임시 디렉터리를 사용하는데, 이 공간이 부족한 경우 온라인 DDL 진행 상황 모니터링 온라인 DDL을 포함한 모든 ALTER TABLE 명령은 MySQL 서버의 performance_schema를 통해 진행 상황을 모니터링 할 수 있다.\nperformance_schema를 이용해 ALTER TABLE의 진행 상황을 모니터링 하려면 옵션(Instrument, Consumer)이 활성화돼야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 /* 시스템 변수 활성화 - 서버 재시작 필요 */ SET GLOBAL performance_schema = ON; UPDATE performance_schema.setup_instruments SET ENABLED = \u0026#39;YES\u0026#39;, TIMED = \u0026#39;YES\u0026#39; WHERE NAME LIKE \u0026#39;stage/innodb/alter%\u0026#39; ; UPDATE performance_schema.setup_consumers SET ENABLED = \u0026#39;YES\u0026#39; WHERE NAME LIKE \u0026#39;%stages%\u0026#39; ; 스키마 변경 작업의 진행 상황은 performance_schema.events_stages_current 테이블을 통해 확인할 수 있는데 실행중인 스키마 변경 종류에 따라 기록되는 내용이 조금씩 달라진다.\n","date":"2023-08-17T14:51:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/7/1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/7/1/","title":"11.7.1 스키마 조작(DDL) - 온라인 DDL"},{"content":"일반적인 온라인 트랜잭션 프로그램에서 UPDATE, DELETE 문장은 주로 하나의 테이블에 대해 한 건 이상의 레코드를 변경, 삭제하기위해 사용되지만, MySQL 서버에서는 여러 테이블을 조인해서 한 개 이상의 테이블의 레코드를 변경하거나 삭제하는 기능도 제공한다.\nJOIN UPDATE, JOIN DELETE로 처리하며 잘못된 데이터 보정, 일괄로 많은 레코드를 변경 및 삭제하는 경우에 유용하다. UPDATE \u0026hellip; ORDER BY \u0026hellip; LIMIT n MySQL 에서는 UPDATE, DELETE 문장에 ORDER BY 절과 LIMIT절을 동시에 사용해 특정 컬럼으로 정렬해서 상위 몇 건만 변경 및 삭제하는 것도 가능하다.\n한 번에 너무 많은 레코드를 변경 및 삭제하는 작업은 MySQL 서버에 과부하를 유발하거나 다른 커넥션의 쿼리를 방해할 수도 있는데, 이 때 LIMIT절을 이용해 조금씩 잘라서 변경하거나 삭제하는 방식을 손쉽게 구현할 수 있다.\n복제 소스 서버에서 문장을 실행하면 경고 메시지가 발생할 수 있다. ORDER BY에 의해 정렬되더라도 중복된 값의 순서가 복제 소스 서버와 레플리카 서버에서 달라질 수도 있기 때문이다. 프라이머리 키로 정렬하면 문제는 없지만 오류는 기록된다. 바이너리 로그의 포맷이 로우(ROW)일 경우 문제가 되지 않지만 문장(STATEMENT) 기반의 복제에서는 주의가 필요하다. JOIN UPDATE 두 개 이상의 테이블을 조인해 조인된 결과 레코드를 변경 및 삭제하는 쿼리를 JOIN UPDATE라고 한다.\n조인된 테이블 중에서 특정 테이블의 컬럼 값을 다른 테이블의 컬럼에 업데이트해야 할 때 주로 사용한다. 꼭 다른 테이블의 컬럼값을 참조하지 않더라도 조인되는 양쪽 테이블에 공통으로 존재하는 레코드만 찾아서 업데이트하는 용도로도 사용할 수 있다. 일반적으로 조인되는 모든 테이블에 대해 읽기 참조만 되는 테이블은 읽기 잠금이 걸리고, 컬럼이 변경되는 테이블은 쓰기 잠금이 걸린다.\n웹 서비스 같은 OLTP 환경에서는 데드락을 유발할 가능성이 높으므로 빈번하게 사용하는 것은 피하는게 좋다. 배치 프로그램이나 통계용 UPDPATE 문장에서는 유용하게 사용할 수 있다. JOIN UPDATE 쿼리도 2개 이상의 테이블을 먼저 조인해야 하므로 테이블의 조인 순서에 따라 UPDATE 문장의 성능이 달라질 수 있으므로 사용하기 전에 실행 계획을 확인하는 것이 좋다.\n파생 테이블을 사용하는 JOIN UPDATE 1 ALTER TABLE departments ADD emp_count INT; 테스트를 위해 departments에 해당 부서에 소속된 사원의 수를 저장하기위한 emp_count컬럼을 추가한다.\n1 2 3 4 5 6 7 8 9 UPDATE departments d, dept_emp de SET d.emp_count=COUNT(*) WHERE de.dept_no=d.dept_no GROUP BY de.dept_no ; /* ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near \u0026#39;GROUP BY de.dept_no\u0026#39; at line 4 */ 위 쿼리는 dept_emp 테이블에서 부서별로 사원의 수를 departments 테이블의 emp_count를 업데이트 하기 위해 만든 쿼리다.\nGROUP BY를 이용해서 해당 부서의 사원을 업데이트 할 수 있을 것 처럼 보이지만 JOIN UPDATE 문장에서는 GROUP BY, ORDER BY절을 사용할 수 없기 때문에 위와 같은 에러가 발생한다.\n이렇게 문법적으로 지원하지 않는 쿼리는 서브쿼리를 이용한 파생 테이블을 사용하여 의도한 대로 처리할 수 있다.\n1 2 3 4 5 6 7 8 UPDATE departments d, ( SELECT de.dept_no, COUNT(*) AS emp_count FROM dept_emp de GROUP BY de.dept_no ) dc SET d.emp_count = dc.emp_count WHERE dc.dept_no = d.dept_no ; 서브쿼리로 dept_dmp 테이블을 dept_no로 그루핑하고, 그 결과를 파생 테이블로 저장한 후, departments 테이블을 조인하여 업데이트 처리하게 된다.\n여러 레코드 UPDATE 하나의 UPDATE 문장으로 여러 개의 레코드를 업데이트하는 경우 아래와 같이 모든 레코드 값을 동일한 값으로만 업데이트 할 수 있었다.\n1 2 UPDATE departments SET emp_count = 10; UPDATE departments SET emp_count = emp_count + 10; 하지만 MySQL 8.0 버전부터는 레코드 생성(Row Constructor) 문법을 이용해 레코드별로 서로 다른 값을 업데이트 할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 CREATE TABLE user_level ( user_id BIGINT NOT NULL, user_lv INT NOT NULL, created_at DATETIME NOT NULL, PRIMARY KEY (user_id) ); UPDATE user_level ul INNER JOIN (VALUES ROW(1, 1), ROW(2, 4)) new_user_level (user_id, user_lv) ON new_user_level.user_id = ul.user_id SET ul.user_lv = ul.user_lv + new_user_level.user_lv ; VALUES ROW(...), ROW(...), ... 문법을 사용하면 SQL 문장 내에서 임시 테이블을 생성하는 효과를 낼 수 있다.\n위 예제는 2건의 레코드를 가지는 임시 테이블을 생성하고, 임시 테이블과 조인하여 업데이트를 수행하는 JOIN UPDATE 효과를 낼 수 있다.\nJOIN DELETE JOIN DELETE 문장을 사용하려면 단일 테이블 처리시와 조금 다른 문법으로 쿼리를 작성해야한다.\n한개 테이블 데이터 삭제 1 2 3 4 5 6 DELETE e FROM employees e JOIN dept_emp de ON de.emp_no = e.emp_no JOIN departments d ON d.dept_no = de.dept_no WHERE d.dept_no = \u0026#39;d001\u0026#39; ; 위 예제는 3개의 테이블을 조인한 후 조인이 성공한 레코드에 대해 employees 테이블의 레코드만 삭제한다.\n일반적으로 하나의 테이블에서 레코드를 삭제할 때는 DELETE FROM table ... 같은 문법으로 사용하지만 JOIN DELETE 문장에서는 DELETE와 FROM절 사이에 삭제할 테이블을 명시해야 한다. 여러개 테이블 데이터 삭제 1 2 3 4 5 6 DELETE e, de, d FROM employees e JOIN dept_emp de ON de.emp_no = e.emp_no JOIN departments d ON d.dept_no = de.dept_no WHERE d.dept_no = \u0026#39;d001\u0026#39; ; DELETE와 FROM 사이에 표시된 테이블에 대해 삭제 처리된다.\n","date":"2023-08-16T11:01:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/6/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/6/","title":"11.6 쿼리 작성 및 최적화 - UPDATE와 DELETE"},{"content":"일반적으로 온라인 트랜잭션 서비스에서 INSERT 문장은 대부분 1건 또는 소량의 레코드를 처리하므로 그다지 성능에 대해 고려할 부분이 많지 않다.\n오히려 많은 INSERT 문장이 동시에 실행되는 경우 INSERT 문장 자체보다는 테이블의 구조가 성능에 더 영향을 미친다.\n하지만 많은 경우 ISNERT의 성능과 SELECT의 성능을 동시에 빠르게 만들 수 있는 테이블 구조는 없기 때문에, INSERT와 SELECT 성능을 어느 정도 타협하면서 테이블 구조를 설계해야 한다.\n고급 옵션 INSERT IGNORE INSERT 문장의 IGNORE 옵션은 특정 상황에 대한 오류를 모두 무시하고 다음 레코드를 처리할 수 있게 한다.\n저장하는 레코드의 프라이머리 키나 유니크 인덱스 컬럼의 값이 이미 테이블에 존재하는 레코드와 중복되는 경우 에러를 경고 수준의 메시지로 바꾸고 나마저 레코드 처리를 진행한다. 저장하는 레코드의 컬럼이 테이블의 컬럼과 호환되지 않는 경우 에러를 경고 수준의 메시지로 바꾸고 해당 컬럼의 기본 값으로 INSERT 하도록 만든다. 프로그램 코드에서 중복을 무시하기 위해 INSERT IGNORE 옵션을 사용한다면 데이터 중복 이외의 에러가 발생할 여지가 없는지 면밀히 확인한 후 적용하는 것이 좋다.\n제대로 검증되지 않은 INSERT IGNORE 문장은 의도하지 않은 에러까지 모두 무시해버릴 수도 있다.\nINSERT \u0026hellip; ON DUPLICATE KEY UPDATE 프라이머리 키나 유니크 인덱스의 중복이 발생하면 UPDATE 문장의 역할을 수행하게 해준다.\nMySQL 서버의 REPLACE 문장도 비슷한 역할을 하지만 REPLACE는 내부적으로 DELETE 후 INSERT로 작동한다. 1 2 3 4 5 6 7 8 INSERT INTO daily_statistic SELECT DATE(visited_at), \u0026#39;VISIT\u0026#39;, COUNT(*) FROM access_log GROUP BY DATE(visited_at) ON DUPLICATE KEY UPDATE stat_value = stat_value + COUNT(*) ; # ERROR 1111 (HY000): Invalid use of group function 위 예제에서 stat_value 컬럼에는 GROUP BY 결과 건수를 저장하고 있다\n중복으로 실행 된 ON DUPLICATE KEY UPDATE 절에서 GROUP BY 결과인 COUNT(*)를 참조할 수 없어 에러가 발생한다. 이러한 경우 VALUES() 함수를 사용하면 해결할 수 있다. 1 2 3 4 5 6 INSERT INTO daily_statistic SELECT DATE(visited_at), \u0026#39;VISIT\u0026#39;, COUNT(*) FROM access_log GROUP BY DATE(visited_at) ON DUPLICATE KEY UPDATE stat_value = stat_value + VALUES(stat_value) ; VALUES() 함수는 컬럼명을 인자로 사용하는데, 위의 예제같이 사용하면 INSERT 하려고 했던 값을 반환한다. MySQL 8.0.20 버전부터 VALUES() 함수가 지원되지 않을 예정이므로 아래와 같이 작성해야 한다.\n1 2 3 4 5 6 7 8 9 10 INSERT INTO daily_statistic SELECT target_date, stat_name, stat_value FROM ( SELECT DATE(visited_at) target_date, \u0026#39;VISIT\u0026#39; stat_name, COUNT(*) stat_value FROM access_log GROUP BY DATE(visited_at) ) stat ON DUPLICATE KEY UPDATE daily_statistic.stat_value = daily_statistic.stat_value + stat.stat_value ; 1 2 3 4 5 6 INSERT INTO daily_statistic (target_date, stat_name, stat_value) VALUES (\u0026#39;2020-09-01\u0026#39;, \u0026#39;VISIT\u0026#39;, 1) ,(\u0026#39;2020-09-02\u0026#39;, \u0026#39;VISIT\u0026#39;, 1) AS new ON DUPLICATE KEY UPDATE daily_statistic.stat_value = daily_statistic.stat_value + new.stat_value ; LOAD DATA 명령 주의 사항 LOAD DATA 명령은 내부적으로 MySQL 엔진과 스토리지 엔진의 호출 횟수를 최소화하고 스토리지 엔진이 직접 데이터를 적재하기 때문에 INSERT 명령과 비교했을때 매우 빠르다.\n단일 스레드로 실행 단일 트랜잭션으로 실행 적재하는 데이터가 아주 많지 않다면 큰 문제가 되지는 않지만, 데이터가 매우 커서 실행 시간이 아주 길어진다면 다른 온라인 트랜잭션 쿼리들의 성능이 영향을 받을 수 있다.\nLOAD DATA는 단일 스레드로 실행되기 때문에 테이블과 인덱스가 커질수록 INSERT 속도가 떨어진다. 테이블에 여러 인덱스가 있다면 LOAD DATA 문장이 레코드를 INSERT하고 인덱스에도 키 값을 INSERT 해야 한다. 레코드가 INSERT 될수록 테이블과 인덱스의 크기도 커진다. LOAD DATA는 단일 트랜잭션으로 처리되기 때문에 문장이 시작한 시점부터 언두로그가 삭제되지 못하고 유지되어야 한다. 언두로그가 많이 쌓이면 레코드를 읽는 쿼리들이 필요한 레코드를 찾는 데 더 많은 오버헤드를 만들어 내기도 한다. 가능하다면 LOAD DATA 문장으로 적재할 데이터 파일 하나보다는 여러 개의 파일로 준비하여 동시에 여러 트랜잭션으로 나뉘어 실행되게 하는 것이 좋다. 테이블 간 데이터 복사 작업이라면 INSERT ... SELECT ... 문장으로 조건절에서 데이터를 부분적으로 잘라 효율적으 INSERT할 수 있게 해주는 것이 좋다. 프라이머리 키 값을 기준으로 데이터를 잘라 여러 개의 스레드로 실행하기가 훨씬 용이하다. 성능을 위한 테이블 구조 INSERT 문장의 성능은 쿼리 문장 자체보다는 테이블의 구조에 의해 많이 결정된다.\n대부분 INSERT 문장은 단일 레코드를 저장하는 형태로 많이 사용되기 때문에 ISNERT 문장 자체는 튜닝할 수 있는 부분이 별로 없다. 실제 쿼리 튜닝을 할 때도 소량의 레코드를 INSERT 하는 문장 자체는 무시하는 경우가 많다. 대량 INSERT 성능 하나의 INSERT 문장으로 수백, 수천 건의 레코드를 INSERT한다면 대상 레코드들을 키 값 기준으로 미리 정렬하여 INSERT 문장을 구성하는 것이 성능에 도움이 될 수 있다.\n프라이머리 키\n레코드를 INSERT 할 때마다 InnoDB 스토리지 엔진은 프라이머리 키를 검색하여 레코드가 저장될 위치를 찾아야한다.\n정렬없이 랜덤한 레코드를 저장할 경우 프라이머리 키의 B-Tree에서 랜덤한 위치의 페이지를 메모리로 읽어와야 하기 때문에 처리가 더 느리다. InnoDB 스토리지 엔진의 버퍼 풀이 충분히 크다면 비교적 덜 느릴 수 있다. 세컨더리 인덱스\nSELECT의 성능을 높히지만 INSERT 성능은 떨어지므로 남용하는 것은 성능상 좋지 않다.\n테이블에 세컨더리 인덱스가 많을수록, 테이블이 클수록 INSERT 성능이 떨어지게 된다. 세컨더리 인덱스도 정렬된 순서대로 처리할 수 있다면 더 빠른 성능을 얻을 수 있지만, 세컨더리 인덱스가 저장되는 순서대로 정렬되게 보장하기 어렵다. 세컨더리 인덱스의 변경은 일시적으로 체인지 버퍼에 버퍼링됐다가 백그라운드 스레드에 의해 일괄 처리될 수 있으나, 너무 많은 세컨더리 인덱스는 백그라운드 작업의 부하를 유발하므로 전체적인 성능은 떨어진다. 프라이머리 키 선정 프라이머리 키의 선정은 INSERT 성능과 SELECT 성능의 대립되는 두가지 요소 중에서 하나를 선택해야 함을 의미한다.\n테이블의 프라이머리 키는 INSERT 성능을 결정하는 가장 중요한 부분이지만, 대부분의 온라인 트랜잭션 처리를 위한 테이블은 쓰기보다는 읽기 쿼리 비율이 압도적으로 높다. 이러한 부분을 모두 만족하는 프라이머리 키를 선택하는 것이 가장 좋지만, 매우 드문 경우이므로 적절히 타협해 프라이머리 키를 선정해야한다.\nINSERT\n프라이머리 키의 순서: 프라이머리 키를 단조 증가 또는 단조 감소하는 패턴의 값을 선택하는 것이 좋다. INSERT 하는 처리는 프라이머리 키의 B-Tree가 메모리에 적재돼 있어야 빠른 INSERT를 보장할 수 있다. 프라이머리 키의 전체 범위에 대해 랜덤하게 저장된다면 더 큰 범위의 키를 메모리에 적재하여 키를 탐색해야한다. 메모리가 부족한 경우 저장될 위치를 찾기 위해 디스크 읽기가 발생하게되어 성능이 매우 떨어질 수 있다. 인덱스의 개수를 최소화 해야한다. SELECT\n클러스터링 키의 특성: 프라이머리 키에 대한 클러스터링 키를 사용하는 InnoDB 스토리지 엔진은 세컨더리 인덱스를 이용하는 쿼리보다 프라이머리 키를 이용하는 쿼리의 성능이 훨씬 빠르다. 프라이머리 키를 데이터 조회에 최적화된 컬럼으로 구성하는 것이 좋을 수 있다. 쿼리에 맞게 필요한 인덱스들을 추가해도 시스템 전반겅으로 영향도가 크지 않다. Auto-Increment 컬럼 SELECT 보다는 INSERT에 최적화된 테이블을 생성하기 위해서는 다음 두가지 요소를 갖춰 테이블을 준비한다.\n단조증가 또는 단조 감소되는 값으로 프라이머리 키 선정 세컨더리 인덱스 최소화 InnoDB 스토리지 엔진을 사용하는 테이블은 자동으로 프라이머리 키로 클러스터링 된다. 하지만 자동 증가 컬럼을 이용하면 클러스터링 되지 않는 테이블의 효과를 얻을 수 있다.\n자동 증가 값을 프라이머리 키로 해서 테이블을 생성하는 것이 가장 빠른 INSERT를 보장한다. MySQL 서버에서는 자동 증가 값의 채번을 위해서 잠금이 필요한데, 이를 AUTO-INC 잠금이라고 하며, 시스템 변수를 통해 변경할 수 있다.\ninnodb_autoinc_lock_mode=0 항상 AUTO-INC 잠금을 걸고 한 번에 1 씩만 증가된 값을 가져온다. MySQL 5.1 호환성과 테스트 용도로만 사용하기 위한 설정이다. innodb_autoinc_lock_mode=1: Consecutive mode, MySQL 5.7 Default 레코드 한 건씩 INSERT하는 쿼리에서는 잠금을 사용하지 않고 뮤텍스를 이용해 처리한다. 하나의 INSERT 문으로 여러 레코드를 처리하거나 LOAD DATA 처리시 잠금을 걸고 필요한 만큼 자동 증가 값을 한꺼번에 가져와 사용한다. 순서대로 채번된 자동 증가 값은 일관되고, 연속된 번호를 갖게 된다. innodb_autoinc_lock_mode=2: Interleaved mode, MySQL 8 Default LOAD DATA나 벌크 INSERT를 포함한 문장을 실행할 때 잠금을 사용하지 않는다. 자동 증가 값을 적당히 미리 받아 처리할 수 있으므로 가장 빠르다. 채번된 번호는 단조 증가하는 유니크한 번호까지만 보장하며, 순서와 번호의 연속성은 보장하지 않는다. 쿼리 기반의 복제를 사용하는 MySQL 서버와 레플리카 서버의 자동 증가 값이 동기화되지 못할 수도 있으므로 주의해야한다. 복제를 STATEMENT 바이너리 로크 포맷으로 사용 중이라면 1로 설정해야한다.\n자동 증가 값이 반드시 연속이어야 한다면 2보다는 1로 설정하는 것이 좋지만, 0이나 1로 설정하더라도 시간이 지마면 연속된 값에 빈 공간이 생기므로 집착하지 않아도 괜찮다.\n","date":"2023-08-02T08:30:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/5/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/5/","title":"11.5 쿼리 작성 및 최적화 - INSERT"},{"content":"InnoDB 테이블에 대해서는 레코드를 SELECT할 때 레코드에 아무런 잠금도 걸지 않는데, 이를 잠금 없는 읽기(Non Locking Consistent Read) 라고 한다.\n하지만 SELECT 쿼리를 이용해 읽은 레코드의 컬럼값을 애플리케이션에서 가공해서 다시 업데이트하고자 할 때는 SELECT가 실행된 후 다른 트랜잭션이 그 컬럼의 값을 변경하지 못하게 해야 한다.\n이럴 때는 레코드를 읽으면서 강제로 잠금을 걸어 둘 필요가 있는데, 이때 사용하는 옵션이 FOR SHARE, FOR UPDATE 절이다.\nFOR SHARE: SELECT 쿼리로 읽은 레코드에 대해 읽기 잠금(공유 잠금, Shared lock)을 설정하고 다른 세션에서 해당 레코드를 변경하지 못하게 한다. 다른 세션에서 잠금이 걸린 레코드를 읽는 것은 가능. FOR UPDATE: SELECT 쿼리가 읽은 레코드에 대해 쓰기 잠금(배타 잠금, Exclusive lock)을 설정하고, 다른 트랜잭션에서는 그 레코드를 변경하는 것 뿐만 아니라 읽기도 수행할 수 없다. 1 2 SELECT * FROM employees WHERE emp_no = 10001 FOR SHARE; SELECT * FROM employees WHERE emp_no = 10001 FOR UPDATE; MySQL 8.0 이전 버전에서는 SELECT로 읽은 레코드에 대한 읽기 잠금을 위해 LOCK IN SHARE MODE 절을 사용했지만, 버전이 업그레이드되며 FOR SHARE로 변경되었다.\n기존 문법을 호환하기는 하지만, 8.0 버전 부터 제공하는 SELECT 쿼리 잠금을 위해 여러 새로운 기능을 위해 FOR SHARE, FOR UPDATE 문법을 사용하는 것이 좋다.\n언급한 잠금 옵션은 모두 자동 커밋이 비활성화된 상태 또는 BEGIN 명령이나 START TRANSACTION 명령으로 트랜잭션이 시작된 상태에서만 잠금이 유지된다.\n주의 사항 InnoDB 스토리지 엔진을 사용하는 테이블에서 FOR UPDATE, FOR SHARE 절을 가지지 않는 SELECT 쿼리는 잠금 없는 읽기가 지원되기 때문에 특정 레코드가 SELECT ... FOR UPDATE 쿼리에 의해 잠겨진 상태라 하더라도 아무런 대기 없이 실행된다.\n잠금 대기하지 않는 경우\n세션 1 세션 2 BEGIN; SELECT * FROM employees WHERE emp_no=10001 FOR UPDATE; SELECT * FROM employees WHERE emp_no=10001; =\u0026gt; 잠금 대기 없이 즉시 결과 반환 잠금 대기하는 경우\n세션 1 세션 2 BEGIN; SELECT * FROM employees WHERE emp_no=10001 FOR UPDATE; SELECT * FROM employees WHERE emp_no=10001 FOR SHARE; =\u0026gt; 세션 1의 잠금을 기다림 COMMIT; SELECT 쿼리 결과 반환 잠금 테이블 선택 1 2 3 4 5 6 SELECT * FROM employees e INNER JOIN dept_emp de ON de.emp_no=e.emp_no INNER JOIN departments d ON d.dept_no=de.dept_no FOR UPDATE ; 위 쿼리는 employees, dept_emp, departments 테이블을 조인해서 읽으며 FROM UPDATE절을 사용했으므로 InnoDB 스토리지 엔진은 3개 테이블에서 읽은 레코드에 대해 모두 쓰기 잠금을 걸게 된다.\n그런데 실제 쓰기 잠금은 employees 테이블만 필요하다면 잠금을 테이블을 선택할 수 있도록 MySQL 8.0 부터 개선이 되었다.\n1 2 3 4 5 6 SELECT * FROM employees e INNER JOIN dept_emp de ON de.emp_no=e.emp_no INNER JOIN departments d ON d.dept_no=de.dept_no FOR UPDATE OF e ; NOWAIT \u0026amp; SKIP LOCKED 지금까지 MySQL 잠금은 누군가 레코드를 잠그고 있다면 다른 트랜잭션은 그 잠금이 해제될 때까지 기다려야 했고, 때로는 일정 시간이 지나면서 잠금 획득 실패 에러 메시지를 받을 수도 있었다.\n이러한 처리는 서비스 상황에 따라 적절하지 않응 처리일 수 있으며, 이러하 부분을 개선하기 위해 MySQL 8.0 부터는 NOWAIT, SKIP LOCKED 옵션을 사용할 수 있게 기능이 추가되었다.\nNOWAIT\n해당 레코드가 다른 트랜잭션에 의해서 잠겨진 상태라면 에러를 반환하면서 즉시 종료된다. SELECT 쿼리가 해당 레코드에 대해 즉시 잠금을 획득했다면 옵션이 NOWAIT 없을때와 동일하게 실행된다. SKIP LOCKED\nSELECT 하려는 레코드가 다른 트랜잭션에 의해 이미 잠겨진 상태라면 에러를 반환하지 않고, 잠금이 걸리지 않은 레코드만 가져온다. SKIP LOCKED 구문을 사용하는 SELECT 구문은 잠금이 걸린 레코드를 무시하고 결과가 반환되므로 확정적이지 않은(NOT-DETERMINISTIC) 쿼리가 된다.\nNOWAIT, SKIP LOCKED를 이용한 큐 NOWAIT, SKIP LOCKED 기능을 활용하여 큐와 같은 기능을 구현할 때 유용하게 쓰일 수 있다.\n쿠폰 발급 기능 예시\n하나의 쿠폰은 한 사용자만 사용 가능하다. 쿠폰의 개수는 1000개 제한이며, 선착순으로 요청한 사용자에게 발급한다. 1 2 3 4 5 6 7 8 CREATE TABLE coupon ( coupon_id BIGINT NOT NULL, owned_user_id BIGINT NULL DEFAULT 0, coupon VARCHAR(15) NOT NULL, ... PRIMARY KEY (coupon_id), INDEX ix_owneduserid (owned_user_id) ); 응용 프로그램 코드에서 주인이 없는(owned_user_id=0) 쿠폰을 검색해서 하나를 가져온다. owned_user_id를 요청한 사용자의 id로 업데이트 한다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 BEGIN; SELECT * FROM coupon WHERE owned_user_id=0 ORDER BY coupon_id ASC LIMIT 1 FOR UPDATE ; /* 프로그램 연산 수행 */ UPDATE coupon SET owned_user_id=? WHERE coupon_id=? ; COMMIT; 위 처리에서 1000명의 사용자가 쿠폰을 요청하면 애플리케이션 서버는 그 요청만큼 프로세스를 생성해서 위의 트랜잭션을 동시에 실행. 각 트랜잭션에서 실행하는 SELECT ... FOR UPDATE 쿼리는 coupon 테이블에서 하나의 레코드로 집중해서 잠금 획득 시도. 처음으로 잠금을 획득하는 트랜잭션 외 999개의 트랜잭션은 첫번째 트랜잭션의 COMMIT 까지 대기 요청 순서대로 잠금을 획득 및 대기 트랜잭션의 처리 속도에 따라 일정 시점 이후 트랜잭션은 timeout 시간 내에 잠금을 획득하지 못해 에러처리됨 SELECT 쿼리의 ORDER BY coupon_id으로 인해 모든 트랜잭션이 하나의 레코드로 집중된다고 생각할 수 있지만, DBMS 서버는 쿼리의 실행이 항상 실행 계획을 기반으로 수행되기 때문에 ORDER BY와 무관하게 모든 트랜잭션은 항상 순서대로 레코드를 읽는다.\nMySQL 8.0 이전 버전에서는 이러한 문제를 해결하기 위해 레디스 같은 캐시 솔루션을 별도로 구축해서 쿠폰 발급 기능을 구현했다.\n하지만 MySQL 8.0 버전에서 제공하는 UPDATE SKIP LOCKED 절을 이용하면 트랜잭션이 수행되는 데 걸리는 시간과 관계없이 다른 트랜잭션에 의해서 이미 잠금된 레코드를 스킵하는 시간만 지나면 각자의 트랜잭션을 수행할 수 있다.\nNOWAIT, SKIP LOCKED 절은 SELECT ... FOR UPDATE 구문에서만 사용할 수 있다. UPDATE, DELETE 에서는 사용할 수 없다. NOWAIT, SKIP LOCKED 절은 쿼리 자체를 비확정적으로 만드릭 때문에 실행될 때마다 데이터베이스의 상태를 다른 결과로 만든다. 이로인해 정상적으로 실행되어도 어떤 레코드가 업데이트되거나 삭제됐는지 알 수 없게 되므로 사용자를 혼란에 빠트릴 수 있고, 서버 복제에서 큰 문제가 발생할 수 있다. ","date":"2023-08-01T09:28:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/4/5/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/4/5/","title":"11.4 쿼리 작성 및 최적화 - SELECT(5) - 잠금을 사용하는 SELECT"},{"content":"윈도우 함수는 현재 레코드를 기준으로 연관된 레코드 집합의 연산을 수행한다.\n집계 함수는 주어진 그룹(GROUP BY 절에 나열된 컬럼 값에 따른 그룹 또는 GROUP BY 절 없이 전체 그룹)별로 하나의 레코드로 묶어 출력하지만 윈도우 함수는 조건에 일치하는 레코드 건수는 변하지 않고 그대로 유지한다. 일반적인 SQL 문장에서 하나의 레코드를 연산할 때 다른 레코드의 값을 참조할 수 없는데, 예외적으로 GROUP BY 또는 집계 함수를 이용하면 다른 레코드의 컬럼 값을 참조할 수 있다. GROUP BY 또는 집계 함수를 사용하면 결과 집합의 모양이 바뀌나, 윈도우 함수는 결과 집합을 그대로 유지하면서 하나의 레코드 연산에 다른 레코드 연산에 따른 레코드 컬럼값을 참조할 수 있다. 쿼리 각 절의 실행 순서 윈도우 함수를 사용하는 쿼리의 결과에 보여지는 레코드는 FROM절과 WHERE 절, GROUP BY와 HAVING 절에 의해 결정되고, 그 이후 윈도우 함수가 실행된다.\nflowchart TB subgraph S1 [윈도우 함수 이전에 실행] direction LR A(WHERE) ~~~ B(FROM) ~~~ C(GROUP BY) ~~~ D(HAVING) end subgraph S2 [윈도우 함수 처리] end subgraph S3 [윈도우 함수 이후에 실행] direction LR E(SELECT 절) ~~~ F(ORDER BY 절) ~~~ G(LIMIT) end S1 --\u003e S2 --\u003e S3 쿼리에서 각 절의 실행 순서를 숙지하고 있어야 정확한 쿼리를 작성할 수 있다.\nex) 윈도우 함수를 GROUP BY 컬름으로 사용하거나 WHERE 절에 사용할 수 없다. 순서를 벗어나는 쿼리를 작성하려면 FROM 절의 서브쿼리를 사용해야 한다. FROM절 서브쿼리없이 LIMIT을 사용한 경우\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 SELECT emp_no ,from_date ,salary ,AVG(salary) OVER() AS avg_salary FROM salaries WHERE emp_no = 10001 LIMIT 5 ; /* +--------+------------+--------+------------+ | emp_no | from_date | salary | avg_salary | +--------+------------+--------+------------+ | 10001 | 1986-06-26 | 60117 | 75388.9412 | | 10001 | 1987-06-26 | 62102 | 75388.9412 | | 10001 | 1988-06-25 | 66074 | 75388.9412 | | 10001 | 1989-06-25 | 66596 | 75388.9412 | | 10001 | 1990-06-25 | 66961 | 75388.9412 | +--------+------------+--------+------------+ */ 조건에 일치하는 17건의 레코드를 모두 가져온 다음 윈도우 함수를 실행하고, 그 결과에서 5건만 반환한다.\nFROM절 서브쿼리에서 LIMIT을 사용한 경우\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SELECT emp_no ,from_date ,salary ,AVG(salary) OVER() AS avg_salary FROM (SELECT * FROM salaries WHERE emp_no = 10001 LIMIT 5) s2 ; /* +--------+------------+--------+------------+ | emp_no | from_date | salary | avg_salary | +--------+------------+--------+------------+ | 10001 | 1986-06-26 | 60117 | 64370.0000 | | 10001 | 1987-06-26 | 62102 | 64370.0000 | | 10001 | 1988-06-25 | 66074 | 64370.0000 | | 10001 | 1989-06-25 | 66596 | 64370.0000 | | 10001 | 1990-06-25 | 66961 | 64370.0000 | +--------+------------+--------+------------+ */ 최종 5건의 평균을 의미 한다.\n윈도우 함수 기본 사용법 1 AGGREGATE_FUNC() OVER(\u0026lt;partition\u0026gt; \u0026lt;order\u0026gt;) AS window_func_column 윈도우 함수는 용도별로 다양한 함수들을 사용할 수 있으며, 집계 함수와는 달리 함수 뒤에 OVER 절을 이용해 연산 대상을 파티션 하기 위한 옵션을 명시할 수 있다. 이렇게 OVER 절에 의해 만들어진 그룹을 파티션 또는 윈도우 라고 한다. 정렬 사용\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 SELECT e.* ,RANK() OVER(ORDER BY e.hire_date) AS hire_date_rank FROM employees e ; /* |... | 227544 | 1954-11-17 | Shahab | Demeyer | M | 2000-01-08 | 300018 | | 422990 | 1953-04-09 | Jaana | Verspoor | F | 2000-01-11 | 300019 | | 47291 | 1960-09-09 | Ulf | Flexer | M | 2000-01-12 | 300020 | | 222965 | 1959-08-07 | Volkmar | Perko | F | 2000-01-13 | 300021 | | 499553 | 1954-05-06 | Hideyuki | Delgrande | F | 2000-01-22 | 300022 | | 428377 | 1957-05-09 | Yucai | Gerlach | M | 2000-01-23 | 300023 | | 463807 | 1964-06-12 | Bikash | Covnot | M | 2000-01-28 | 300024 | +--------+------------+----------------+------------------+--------+------------+----------------+ */ 소 그룹을 별도로 구분하지 않고 전체 집합과 집합에서 e.hire_date 컬럼으로 정렬한 후 순위를 매긴다.\n파티션 및 정렬 사용\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 SELECT de.dept_no ,e.emp_no ,e.first_name ,e.hire_date ,RANK() OVER(PARTITION BY de.dept_no ORDER BY e.hire_date) AS hire_date_rank FROM employees e INNER JOIN dept_emp de ON de.emp_no = e.emp_no ORDER BY de.dept_no, e.hire_date ; /* | d009 | 461853 | Licheng | 1999-10-14 | 23572 | | d009 | 292220 | Danny | 1999-10-26 | 23573 | | d009 | 252656 | Moti | 1999-10-28 | 23574 | | d009 | 65535 | Aleksander | 1999-11-21 | 23575 | | d009 | 89511 | Kazuhiro | 1999-11-27 | 23576 | | d009 | 218859 | Arvin | 1999-12-03 | 23577 | | d009 | 13919 | Brewster | 1999-12-04 | 23578 | | d009 | 73925 | Vasilii | 1999-12-30 | 23579 | | d009 | 60134 | Seshu | 2000-01-02 | 23580 | +---------+--------+----------------+------------+----------------+ */ 부서 별로 입사 순위를 매기려고 한다면 부서 코드로 파티션을 하면 된다.\n파티션 및 정렬 미사용\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 SELECT emp_no ,from_date ,salary ,AVG(salary) OVER() AS avg_salary FROM salaries WHERE emp_no = 10001 ; /* +--------+------------+--------+------------+ | emp_no | from_date | salary | avg_salary | +--------+------------+--------+------------+ | 10001 | 1986-06-26 | 60117 | 75388.9412 | | 10001 | 1987-06-26 | 62102 | 75388.9412 | | 10001 | 1988-06-25 | 66074 | 75388.9412 | | 10001 | 1989-06-25 | 66596 | 75388.9412 | | 10001 | 1990-06-25 | 66961 | 75388.9412 | | 10001 | 1991-06-25 | 71046 | 75388.9412 | | 10001 | 1992-06-24 | 74333 | 75388.9412 | | 10001 | 1993-06-24 | 75286 | 75388.9412 | | 10001 | 1994-06-24 | 75994 | 75388.9412 | | 10001 | 1995-06-24 | 76884 | 75388.9412 | | 10001 | 1996-06-23 | 80013 | 75388.9412 | | 10001 | 1997-06-23 | 81025 | 75388.9412 | | 10001 | 1998-06-23 | 81097 | 75388.9412 | | 10001 | 1999-06-23 | 84917 | 75388.9412 | | 10001 | 2000-06-22 | 85112 | 75388.9412 | | 10001 | 2001-06-22 | 85097 | 75388.9412 | | 10001 | 2002-06-22 | 88958 | 75388.9412 | +--------+------------+--------+------------+ */ 소그룹 파티션이나 정렬이 필요하지 않은 경우 PARTITION이나 ORDER BY 없이 비어 있는 OVER() 절을 사용하면 된다.\n사원 번호가 10001인 사원의 모든 급여 변경 이력의 평균을 조회하므로 PARTITION 키워드를 생략했다. 평균을 계산하므로 정렬이 필요하지 않아 ORDER BY 키워드를 생략 했다. 따라서 avg_salary 컬럼의 값은 모두 동일하다. 프레임 사용\n윈도우 함수의 각 파티션 안에서도 연산 대상을 레코드별로 연산을 수행할 소그룹이 사용되는데, 이를 프레임이라 한다.\n윈도우 함수에서 프레임을 명시적으로 지정하지 않아도 상황에 맞게 묵시적으로 선택한다. 프레임은 레코드의 순서대로 현재 레코드 기준 앞뒤 몇 건을 연산 범위로 제한하는 역할을 한다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 AGGREGATER_FUNC() OVER(\u0026lt;partition\u0026gt; \u0026lt;order\u0026gt; \u0026lt;frame\u0026gt;) AS window_func_column frame: {ROWS | RANGE} {frame_start | frame_between} frame_between BETWEEN frame_start AND frame_end frame_start, frame_end: { CURRENT ROW | UNBOUNDED PRECEDING | UNBOUNDED FOLLOWING | expr PRECEDING | expr FOLLOWING } 프레임을 만드는 기준 ROWS: 레코드의 위치를 기준으로 프레임 생성 expr에는 레코드의 위치를 명시해야 한다. RANGE: ORDER BY 절에 면시된 컬럼을 기준으로 값의 범위로 프레임 생성 expr에는 컬럼과 비교할 값이 설정돼야 한다. 시작과 끝 의미 CURRENT ROW: 현재 레코드 UNBOUNDED PRECEDING: 파티션의 첫 번째 레코드 UNBOUNDED FOLLOWING: 파티션의 마지막 레코드 expr PRECEDING: 현재 레코드로부터 n번째 이전 레코드 expr FOLLOWING: 현재 레코드로부터 n번째 이후 레코드 윈도우 함수에서 프레임이 별도로 명시되지 않으면 무조건 파티션의 모든 레코드가 연산의 대상이 되는 것은 아니다.\nOVER() 절이 ORDER BY를 가지는지 여부에 따라 묵시적인 프레임 범위가 달라진다.\nOVER()가 ORDER BY를 가지는 경우 파티션의 첫 번째 레코드부터 현재 레코드까지가 프레임이 된다. OVER()가 ORDER BY를 가지지 않는 경우 파티션의 모든 레코드가 묵시적인 프레임으로 선택된다. 일부 윈도우 함수들은 프레임이 고정되어있어 SQL 문장에서 프레임을 별도로 명시하더라도 무시된다. 이러한 경우 에러는 발생하지 않기 때문에 혼란스러울 수 있으니 주의한다. CUME_DIST(), DENSE_RANK(), LAG(), LEAD(), NTILE(), PERCENT_RANK(), RANK(), ROW_NUMBER() 윈도우 함수 MySQL 서버의 윈도우 함수에는 집계 함수와 비 집계 함수를 모두 사용할 수 있다.\n집계함수는 GROUP BY 절과 함께 사용할 수 있는 함수를 의미하며 OVER() 절 없이 단독으로도 사용할 수 있다. 비 집계 함수는 반드시 OVER() 절을 가지고 있어야 하며 윈도우 함수로만 사용될 수 있다. DENSE_RANK()와 RANK(), ROW_NUMBER() DENSE_RANK()와 RANK() 모두 ORDER BY 기준으로 매겨진 순위를 반환하지만 조금 차이가 있다.\nRANK(): 동점인 레코드가 두 건 이상인 경우 그 다음 레코드를 동점인 레코드수 만큼 증가시킨 순위를 반환. DENSE_RANK(): 동점인 레코드를 1건으로 가정하고 순위를 매기기 때문에 연속된 순위를 가진다. ROW_NUMBER(): 똑같이 순위를 매기지만 각 레코드의 고유한 순번을 반홚나다. 동점에 대한 고려 없이 정렬된 순서대로 레코드 번호를 부여한다. LAG(), LEAD() LAG(): 파티션 내에서 현재 레코드를 기준으로 n번째 이전 레코드르 반환한다. LEAD(): 반대로 n번째 이후 레코드를 반환한다. 3개 파라미터를 필요로 하며, 1,2 는 필수이며, 3은 선택이다. 컬럼 이름 위치 증감 숫자 윈도우 함수와 성능 MySQL 서버의 윈도우 함수는 8.0 버전에 처음 도입됐으며, 아직 인덱스를 이용한 최적화가 부족한 부분도 있다.\n윈도우 함수 이용\n1 2 3 4 5 6 7 8 9 10 11 12 EXPLAIN SELECT MAX(from_date) OVER(PARTITION BY emp_no) AS max_from_date FROM salaries ; /* +----+-------------+----------+------------+-------+---------------+-----------+---------+------+---------+----------+-----------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+----------+------------+-------+---------------+-----------+---------+------+---------+----------+-----------------------------+ | 1 | SIMPLE | salaries | NULL | index | NULL | ix_salary | 4 | NULL | 2838426 | 100.00 | Using index; Using filesort | +----+-------------+----------+------------+-------+---------------+-----------+---------+------+---------+----------+-----------------------------+ */ GROUP BY 이용\n1 2 3 4 5 6 7 8 9 10 11 12 13 EXPLAIN SELECT MAX(from_date) FROM salaries GROUP BY emp_no ; /* +----+-------------+----------+------------+-------+-------------------+---------+---------+------+--------+----------+--------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+----------+------------+-------+-------------------+---------+---------+------+--------+----------+--------------------------+ | 1 | SIMPLE | salaries | NULL | range | PRIMARY,ix_salary | PRIMARY | 4 | NULL | 293568 | 100.00 | Using index for group-by | +----+-------------+----------+------------+-------+-------------------+---------+---------+------+--------+----------+--------------------------+ */ 두 쿼리의 결과는 차이가 있지만 사용자별로 MAX(from_date) 값을 구한다.\n윈도우 함수 이용 인덱스를 풀 스캔 레코드 정렬 작업 수행 GROUP BY 별도 정렬 없음 루스 인덱스 스캔으로 값을 찾아냄 이러한 차이로 인해 실제 두 쿼리가 연산에 사용한 레코드 건수도 상당히 차이가 난다.\n윈도우 함수는 테이블의 모든 레코드 건수만큼의 결과를 만들어야 하지만, GROUP BY 절을 사용하는 쿼리는 유니크한 emp_no별로 레코드 1건씩만 결과를 만들어 내면 되므로 레코드 건수에서 차이가 날 수 있다.\n하지만 앞의 결과를 보면 윈도우 함수를 사용한 쿼리는 예상보다 훨씬 많은 레코드를 가공했고, 그로 인해 레코드의 읽고 쓰기가 상당히 많이 발생했다.\n쿼리 요건에 따라 GROUP BY나 다른 기존 기능으로는 윈도우 함수를 대체할 수 없겠지만, 가능하다면 윈도우 함수에 너무 의존하지 않는 것이 좋다.\n소량의 레코드에 대해서라면 윈도우 함수를 사용해도 메모리에서 빠르게 처리될 것이므로 특별히 성능에 대해 고민하지 않아도 된다.\n","date":"2023-07-31T14:28:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/4/4/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/4/4/","title":"11.4 쿼리 작성 및 최적화 - SELECT(4) - 윈도우 함수(Window Function)"},{"content":"쿼리를 작성할 때 서브쿼리를 사용하면 단위 처리별로 쿼리를 독립적으로 작성할 수 있다.\n조인처럼 여러 테이블을 섞어 두는 형태가 아니어서 쿼리의 가독성도 높아지며, 복잡한 쿼리도 손쉽게 작성할 수 있다.\nMySQL 5.6 버전까지는 서브쿼리를 최적으로 실행하지 못할 때가 많았지만 서브쿼리 처리가 많이 개선되었다.\n서브쿼리는 쿼리의 여러 위치에 사용될 수 있으며 위치별로 최적화와 성능에 도움이 되는 방법이 조금씩 다르다.\nSELECT FROM WHERE 등 SELECT 절에 사용된 서브쿼리 SELECT 절에 사용된 서브쿼리는 내부적으로 임시 테이블을 만들거나 쿼리를 비효율적으로 실행하게 만들지는 않기 때문에 서브쿼리가 적절히 인덱스를 사용할 수 있다면 크게 주의할 사항은 없다.\n일반적으로 SELECT 절에 서브쿼리를 사용하면 그 서브쿼리는 항상 컬럼과 레코드가 하나인 겨로가를 반환해야 하지만, MySQL에서는 이 체크 조건이 조금 느슨하다.\n서브쿼리의 결과가 0건인 경우 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 SELECT emp_no , (SELECT dept_name FROM departments WHERE dept_name = \u0026#39;Sales1\u0026#39;) subquery FROM dept_emp LIMIT 10 ; /* +--------+----------+ | emp_no | subquery | +--------+----------+ | 110022 | NULL | | 110085 | NULL | | 110183 | NULL | | 110303 | NULL | | 110511 | NULL | | 110725 | NULL | | 111035 | NULL | | 111400 | NULL | | 111692 | NULL | | 110114 | NULL | +--------+----------+ */ 사용된 서브쿼리는 항상 결과가 0건이다. 하지만 에러를 발생시키지 않고 결과로 NULL을 채워 반환한다.\n2건 이상 레코드를 반환하는 경우 1 2 3 4 5 6 SELECT emp_no, (SELECT dept_name FROM departments) FROM dept_emp LIMIT 10 ; # ERROR 1242 (21000): Subquery returns more than 1 row 에러가 발생하며 쿼리가 종료된다.\n2개 이상 컬럼을 가져오려고 하는 경우 1 2 3 4 5 6 7 SELECT emp_no ,(SELECT dept_no, dept_name FROM departments WHERE dept_name = \u0026#39;Sales1\u0026#39;) FROM dept_emp LIMIT 10 ; # ERROR 1241 (21000): Operand should contain 1 column(s) 2건 이상 결과를 반환하는 서브쿼리처럼 에러가 발생한다.\nSELECT 절의 서브쿼리에는 로우(row) 서브쿼리를 사용할 수 없고, 오직 스칼라 서브쿼리만 사용할 수 있다.\n조인으로 처리해도 되는 쿼리를 SELECT 절의 서브쿼리를 사용하는 경우 조인으로 처리해도 되는 쿼리를 SELECT 절의 서브쿼리를 사용해서 작성하는 경우가 종종 있는데, 서브쿼리로 실행될 때보다 조인으로 처리할 때 조금 더 빠르기 때문에 가능하다면 조인으로 쿼리를 작성하는 것이 좋다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 SELECT COUNT( CONCAT( e1.first_name, (SELECT e2.first_name FROM employees e2 WHERE e2.emp_no=e1.emp_no) ) ) FROM employees e1; SELECT COUNT(CONCAT(e1.first_name, e2.first_name)) FROM employees e1 ,employees e2 WHERE e1.emp_no = e2.emp_no ; 처리해야 하는 레코드 건수가 많아지면 성능 차이가 커지므로 가능하면 조인으로 쿼리를 작성하는 방법을 권장한다.\n동일한 서브쿼리를 여러번 사용하는 경우 1 2 3 4 5 6 7 8 SELECT e.emp_no ,e.first_name ,(SELECT s.salary FROM salaries s WHERE s.emp_no = e.emp_no ORDER BY s.from_date DESC LIMIT 1) salary ,(SELECT s.from_date FROM salaries s WHERE s.emp_no = e.emp_no ORDER BY s.from_date DESC LIMIT 1) salary_from_date ,(SELECT s.to_date FROM salaries s WHERE s.emp_no = e.emp_no ORDER BY s.from_date DESC LIMIT 1) salary_to_date FROM employees e WHERE e.emp_no=499999 ; 위 예시의 경우 LIMIT 1 조건으로 인해 salaries 터이블을 조인으로 사용할 수 없는데, MySQL 8.0 버전부터 도입된 래터럴 조인을 이용하면 동일한 결과를 만들 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 SELECT e.emp_no ,e.first_name ,s2.salary ,s2.from_date ,s2.to_date FROM employees e INNER JOIN LATERAL ( SELECT * FROM salaries s WHERE s.emp_no=e.emp_no ORDER BY s.from_date DESC LIMIT 1 ) s2 ON s2.emp_no=e.emp_no WHERE e.emp_no = 499999 ; 3번 반복되던 서브쿼리를 래터럴 조인으로 변경하여 한법만 읽어 쿼리를 처리할 수 있다.\n래터럴 조인을 사용한 경우 인덱스를 이용해 충분히 정렬된 결과를 가져올 수 있음에도 저열ㄹ을 수행하는 버그가 있다.\n래터럴 조인은 내부적으로 임시 테이블을 생성하기 때문에 Handler_write 값과 Handler_read_key 값이 증가할 수 있다. 서브쿼리를 사용한 경우는 테이블을 여러번 읽기 때문에 Handler_read_key 값이 증가할 수 있는 실행계획이다. FROM 절에 사용된 서브쿼리 MySQL 5.7 전 MySQL 서비에서는 FROM 절에 서브쿼리가 사용되면 항상 서브쿼리의 결과를 임시 테이블로 저장하고 필요할 때 다시 임시 테이블을 읽는 방식으로 처리되어 FROM 절의 서브 외부 쿼리로 병합하는 형태로 튜닝을 했다.\n이에 따라 업그레이드 되며 옵티마이저가 FROM 절의 서브쿼리를 외부로 병합하는 최적화를 수행하도록 개선되었다.\nMySQL 서버가 서브쿼리를 병합해서 재작성했다면 EXPLAIN 명령을 실행한 후, SHOW WARNINGS 명령을 실행하면 어떻게 병합했는지 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 EXPLAIN SELECT * FROM (SELECT * FROM employees) y; /* +----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------+ | 1 | SIMPLE | employees | NULL | ALL | NULL | NULL | NULL | NULL | 300269 | 100.00 | NULL | +----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------+ */ SHOW WARNINGS; /* +-------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Level | Code | Message | +-------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Note | 1003 | /* select#1 */ select `employees`.`employees`.`emp_no` AS `emp_no`,`employees`.`employees`.`birth_date` AS `birth_date`,`employees`.`employees`.`first_name` AS `first_name`,`employees`.`employees`.`last_name` AS `last_name`,`employees`.`employees`.`gender` AS `gender`,`employees`.`employees`.`hire_date` AS `hire_date` from `employees`.`employees` | +-------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ */ 서브쿼리에서 발생하는 외부 쿼리 병합은 꼭 FROM 절에서만 적용되는 최적화는 아니고, FROM 사용된 뷰의 경우에도 MySQL 옵티마이저는 뷰 쿼리와 외부 쿼리를 병합해서 최적화된 실행 계획을 사용한다.\nFROM 절의 모든 서브쿼리를 외부 쿼리로 병합할 수 있는 것은 아니며, 대표적으로 다음과 같은 기느이 서브쿼리가 사용되면 FROM 절의 서브쿼리는 외부 쿼리로 병합되지 못한다.\n집합 함수 사용(SUM, MIN, MAX, COUNT 등) DISTINCT GROUP BY, HAVING LIMIT UNION(UNION DISTINCT), UNION ALL SELECT 절에 서브쿼리가 사용된 경우 사용자 변수 사용(사용자 변수에 값이 할당되는 경우) 외부 쿼리와 병합되는 FROM 절의 서브쿼리가 ORDER BY 절을 가진 경우 외부 쿼리가 GROUP BY나 DISTINCT같은 기능을 사용하지 않는다면 서브쿼리의 정렬 조건을 외부 쿼리로 같이 병합한다.\n외부 쿼리에서 GROUP BY, DISTINCT와 같은 기능이 사용되고 있다면, 서브쿼리의 정렬 작업은 무의미하기 때문에 ORDER BY는 무시된다.\nWHERE 절에 사용된 서브쿼리 WHERE 절의 서브쿼리는 SELECT 절이나 FROM 절보다는 다양한 형태(연산자)로 사용될 수 있다.\n동등 또는 크다 작다 비교 (= (subquery)) IN 비교(IN (subquery)) NOT IN 비교(NOT IN (subquery)) 동등 또는 크다 작다 비교 MySQL 5.5 이전 버전까지는 서브쿼리 외부의 조건으로 쿼리를 실행하고, 최종적으로 서브쿼리를 체크 조근으로 사용했으나, 이러한 방식은 풀 테이블 스캔이 필요한 경우가 많아 성능 저하가 심각했다.\nMySQL 5.5 이하 dept_emp 테이블을 풀 스캔하면서 서브쿼리 조건에 일치하는지 여부를 체크함 MySQL 5.5 이상 서브쿼리를 먼저 실행하여 상수로 변환 후 상수값으로 서브쿼리를 대체해여 나머지 쿼리 실행 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 EXPLAIN SELECT * FROM dept_emp de WHERE de.emp_no = ( SELECT e.emp_no FROM employees e WHERE e.first_name = \u0026#39;Georgi\u0026#39; AND e.last_name = \u0026#39;Facello\u0026#39; LIMIT 1 ) ; /* TABLE +----+-------------+-------+------------+------+-------------------+-------------------+---------+-------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+-------------------+-------------------+---------+-------+------+----------+-------------+ | 1 | PRIMARY | de | NULL | ref | ix_empno_fromdate | ix_empno_fromdate | 4 | const | 1 | 100.00 | Using where | | 2 | SUBQUERY | e | NULL | ref | ix_firstname | ix_firstname | 58 | const | 253 | 10.00 | Using where | +----+-------------+-------+------------+------+-------------------+-------------------+---------+-------+------+----------+-------------+ TREE +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | EXPLAIN | +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | -\u0026gt; Filter: (de.emp_no = (select #2)) (cost=0.35 rows=1) -\u0026gt; Index lookup on de using ix_empno_fromdate (emp_no=(select #2)) (cost=0.35 rows=1) -\u0026gt; Select #2 (subquery in condition; run only once) -\u0026gt; Limit: 1 row(s) (cost=65.8 rows=1) -\u0026gt; Filter: (e.last_name = \u0026#39;Facello\u0026#39;) (cost=65.8 rows=25.3) -\u0026gt; Index lookup on e using ix_firstname (first_name=\u0026#39;Georgi\u0026#39;) (cost=65.8 rows=253) | +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ */ 실행 계획을 살펴보면 dept_emp 테이블을 풀 스캔하지 않고 (emp_no, from_date) 조합의 인덱스를 사용했다.\nTREE 형식으로 출력한 실행 계획의 마지막 라인에서 Index lookup on e using ix_firstname (first_name='Georgi')를 통해 employees 테이블의 ix_firstname 인덱스로 서브쿼리를 처리한 후, 그 결과를 이용해 dept_emp 테이블의 ix_empno_fromdate 인덱스를 검색해 쿼리가 완료되었음을 알 수 있다.\n동등 비교 뿐만 아니라 대소비교가 사용되어도 동일한 실행 계획을 사용한다.\n단일 값 비교가 아닌 튜플 비교 방식이 사용되면 서브쿼리가 먼저 처리되어 상수화 되긴 하지만, 외부 쿼리는 인덱스를 사용하지 못하고 풀 테이블 스캔을 실행하는 것을 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 EXPLAIN SELECT * FROM dept_emp de WHERE (emp_no, from_date) = ( SELECT emp_no, from_date FROM salaries WHERE emp_no = 100001 LIMIT 1) ; +----+-------------+----------+------------+------+-------------------+---------+---------+-------+--------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+----------+------------+------+-------------------+---------+---------+-------+--------+----------+-------------+ | 1 | PRIMARY | de | NULL | ALL | NULL | NULL | NULL | NULL | 331143 | 100.00 | Using where | | 2 | SUBQUERY | salaries | NULL | ref | PRIMARY,ix_salary | PRIMARY | 4 | const | 4 | 100.00 | Using index | +----+-------------+----------+------------+------+-------------------+---------+---------+-------+--------+----------+-------------+ IN 비교 WHERE 절에 사용된 IN (subquery) 형태의 조건을 조인의 한 방식인 세미 조인으로 간주하여 처리한다.\n세미 조인(Semi-Join) 실제 조인은 아니지만 테이블의 레코드가 다른 테이블의 레코드를 이용한 표현식(또는 컬럼 그 자체)와 일치하는지를 체크하는 형태 1 2 3 4 5 6 7 SELECT * FROM employees e WHERE e.emp_no IN ( SELECT de.emp_no FROM dept_emp de WHERE de.from_date = \u0026#39;1995-01-01\u0026#39;) ; MySQL 5.5 버전까지는 세미 조인 최적화가 부족하여 대부분 풀 테이블 스캔이 발생하였으므로 사용하면 안되는 패턴으로 알려졌었으나, 현재는 세미 조인의 최적화가 많이 개선되면서 IN (subquery) 형태를 2개의 쿼리로 쪼개어 실행하거나 다른 우회 방법을 찾을 필요가 없어졌다.\nMySQL 서버의 세미 조인 최적화는 쿼리 특성이나 조인 관계에 맞게 5개 전략을 사용한다.\n테이블 풀-아웃(Table Pull-out) 퍼스트 매치(Firstmatch) 루스 스캔(Loosescan) 구체화(Materialization) 중복 제거(Duplicated Weed-out) MySQL 8.0을 사용한다면 세미 조인 최적화에 익숙해져야 한다.\n예전처럼 불필요하게 쿼리를 여러 조각으로 분리해서 실행하는 습관을 버리고 MySQL 8.0의 기능을 적극 활용해 개발 생산성을 높힐 수 있다.\nNOT IN 비교 IN (subquery) 와 비슷하지만 이 경우를 안티 세미 조인(Anti Semi-Join)이라고 명명한다.\n일반적인 RDBMS에서 Not-Equal 비교(\u0026lt;\u0026gt;)는 인덱스를 제대로 활용할 수 없듯이 안티 세미 조인 또한 최적화할 수 있는 방법이 많지 않다.\n안티 세미 조인 쿼리가 사용되면 2가지 방식으로 최적화를 수행한다.\nNOT EXISTS 구체화(Materialization) 두 가지 최적화 모두 그다지 성능 향상에 도움이 되지 않는 방법이므로 쿼리가 최대한 다른 조건을 활용해서 데이터 검색 범위를 좁힐 수 있게 하는 것이 좋다.\nWHERE 절에 안티 세미 조인 조건만 단독으로 있다면 풀 테이블 스캔을 피할 수 없으므로 주의해야한다.\nCTE(Common Table Expression) CTE(Common Table Expression)는 이름을 가지는 임시 테이블로서, SQL 문장 내에서 한 번 이상 사용될 수 있으며 SQL 문장이 종료되면 자동으로 CTE 임시 테이블은 삭제된다.\nCTE는 재귀적 반복 실행 여부를 기준으로 Non-recursive, Recursive CTE로 구분된다.\nMySQL 서버의 CTE는 재귀 여부에 관계없이 다음과 다양한 SQL 문장에서 사용할 수 있다.\nSELECT, UPDATE, DELETE 문장의 제일 앞쪽 WITH cte1 AS (SELECT ...) SELECT ... WITH cte1 AS (SELECT ...) UPDATE ... WITH cte1 AS (SELECT ...) DELETE ... 서브쿼리의 제일 앞쪽 SELECT ... FROM ... WHERE id IN (WITH cte1 AS (SELECT ...) SELECT ...) ... SELECT ... FROM (WITH cte1 AS (SELECT ...) SELECT ...) ... SELECT 절의 바로 앞쪽 INSERT ... WITH cte 1 AS (SELECT ...) SELECT ... REPLACE ... WITH cte 1 AS (SELECT ...) SELECT ... CREATE TABLE ... WITH cte 1 AS (SELECT ...) SELECT ... CREATE VIEW ... WITH cte 1 AS (SELECT ...) SELECT ... DECLARE CURSOR ... WITH cte 1 AS (SELECT ...) SELECT ... EXPLAIN ... WITH cte 1 AS (SELECT ...) SELECT ... 비 재귀적 CTE(Non-Recursive CTE) MySQL 서버에서는 ANSI 표준을 그대로 이용해서 WITH 절을 이용해 CTE를 정의한다.\n기본 사용\n1 2 3 WITH cte1 AS (SELECT * FROM departments) SELECT * FROM cte1 ; CTE는 WITH 절로 정의한다. CTE 쿼리로 생성되는 임시 테이블의 이름은 WITH 바로 뒤에 정의한다. 예시 쿼리의 cte1 임시 테이블은 한 번만 사용되기 때문에 FROM 절의 서브쿼리로 바꿔 사용할 수 있다. 실행 계획까지 동일하게 사용한다. 여러개의 임시 테이블을 하나의 쿼리에서 사용\n1 2 3 4 5 6 WITH cte1 AS (SELECT * FROM departments), cte2 AS (SELECT * FROM dept_emp) SELECT * FROM temp1 INNER JOIN cte2 ON cte2.dept_no = cte1.dept_no ; 여러 개의 CTE 임시 테이블을 사용하는 쿼리도 FROM 절의 서브쿼리로 대체해서 사용할 수 있지만, 임시 테이블이 여러 번 사용되는 쿼리는 둘의 실행 계획이 조금 달라진다.\nCTE를 이용한 실행 계획\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 EXPLAIN WITH cte1 AS (SELECT emp_no, MIN(from_date) FROM salaries GROUP BY emp_no) SELECT * FROM employees e INNER JOIN cte1 t1 ON t1.emp_no = e.emp_no INNER JOIN cte1 t2 ON t2.emp_no = e.emp_no ; /* +----+-------------+------------+------------+--------+-------------------+-------------+---------+-----------+--------+----------+--------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+------------+------------+--------+-------------------+-------------+---------+-----------+--------+----------+--------------------------+ | 1 | PRIMARY | \u0026lt;derived2\u0026gt; | NULL | ALL | NULL | NULL | NULL | NULL | 293568 | 100.00 | NULL | | 1 | PRIMARY | e | NULL | eq_ref | PRIMARY | PRIMARY | 4 | t1.emp_no | 1 | 100.00 | NULL | | 1 | PRIMARY | \u0026lt;derived2\u0026gt; | NULL | ref | \u0026lt;auto_key0\u0026gt; | \u0026lt;auto_key0\u0026gt; | 4 | t1.emp_no | 10 | 100.00 | NULL | | 2 | DERIVED | salaries | NULL | range | PRIMARY,ix_salary | PRIMARY | 4 | NULL | 293568 | 100.00 | Using index for group-by | +----+-------------+------------+------------+--------+-------------------+-------------+---------+-----------+--------+----------+--------------------------+ */ FROM 절의 서브쿼리를 이용한 실행 계획\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 EXPLAIN SELECT * FROM employees e INNER JOIN (SELECT emp_no, MIN(from_date) FROM salaries GROUP BY emp_no) t1 INNER JOIN (SELECT emp_no, MIN(from_date) FROM salaries GROUP BY emp_no) t2 ; /* +----+-------------+------------+------------+-------+-------------------+---------+---------+------+--------+----------+-------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+------------+------------+-------+-------------------+---------+---------+------+--------+----------+-------------------------------+ | 1 | PRIMARY | \u0026lt;derived2\u0026gt; | NULL | ALL | NULL | NULL | NULL | NULL | 293568 | 100.00 | NULL | | 1 | PRIMARY | \u0026lt;derived3\u0026gt; | NULL | ALL | NULL | NULL | NULL | NULL | 293568 | 100.00 | Using join buffer (hash join) | | 1 | PRIMARY | e | NULL | ALL | NULL | NULL | NULL | NULL | 300269 | 100.00 | Using join buffer (hash join) | | 3 | DERIVED | salaries | NULL | range | PRIMARY,ix_salary | PRIMARY | 4 | NULL | 293568 | 100.00 | Using index for group-by | | 2 | DERIVED | salaries | NULL | range | PRIMARY,ix_salary | PRIMARY | 4 | NULL | 293568 | 100.00 | Using index for group-by | +----+-------------+------------+------------+-------+-------------------+---------+---------+------+--------+----------+-------------------------------+ */ CTE를 이용한 쿼리에서는 임시 테이블을 한 번만 생성하지만, FROM 절에 서브쿼리를 이용한 쿼리에서는 2개의 임시 테이블을 생성하기 위해서 각 서브쿼리에서 salaries 테이블을 읽었다. 이뿐만 아니라 CTE로 생성된 임시 테이블은 다른 CTE 쿼리에서 참조할 수 있다는 장점도 있다.\n결론\nCTE 임시 테이블은 재사용 가능하므로 FROM 절의 서브쿼리보다 효율적이다. CTE로 선언된 임시 테이블을 다른 CTE 쿼리에서 참조할 수 있다. CTE는 임시 테이블의 생성 부분과 사용 부분의 코드를 분리할 수 있으므로 가독성이 높다. 재귀적 CTE(Recursive CTE) MySQL 8.0 부터 CTE를 이용한 재귀 쿼리가 가능해졌다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 WITH RECURSIVE cte (no) AS ( SELECT 1 UNION ALL SELECT (no + 1) FROM cte WHERE no \u0026lt; 5 ) SELECT * FROM cte; /* +------+ | no | +------+ | 1 | | 2 | | 3 | | 4 | | 5 | +------+ */ 재귀적 CTE 쿼리는 비 재귀적 쿼리 파트와 재귀적 파트로 구분되며, 이 둘을 UNION(UNION DISTINCT) 또는 UNION ALL로 연결하는 형태로 반드시 쿼리를 작성해야 한다.\n비재귀적 파트: SELECT 1 재귀적 파트: UNION ALL SELECT (no + 1) FROM cte WHERE no \u0026lt; 5 위 예시가 동작하는 방법은 아래와 같다.\nCTE 쿼리의 비 재귀적 파트의 쿼리를 실행 1번의 결과를 이용해 cte라는 이름의 임시 테이블 생성 1번의 결과를 cte라는 임시 테이블에 저장 1번 결과를 입력으로 사용해 CTE 쿼리의 재귀적 파트를 실행 4번의 결과를 cte라는 임시 테이블에 저장(이때 UNION, UNION DISTINCT의 경우 중복 제거를 실행) 전 단계의 결과를 입력으로 사용해 CTE 쿼리의 재귀적 파트 쿼리를 실행 6번 단계에서 쿼리 결과가 없으면 CTE 쿼리를 종료 6번의 결과를 cte라는 임시 테이블에 저장 6번으로 돌아가 반복 실행 1번 과정에서 매우 중요한 CTE 임시 테이블 구조가 결정된다.\nCTE 테이블의 구조(테이블의 컬럼명과 컬럼의 데이터 타입)는 CTE 쿼리의 비 재귀적 파트의 결과로 결정된다.\n비 재귀적 파트의 결과와 재귀적 파트의 결과에서 컬럼 개수나 컬럼 타입, 컬럼 이름이 다른 경우 비 재귀적 파트에 정의된 결과를 사용한다. CTE의 비 재귀적 쿼리 파트는 초기 데이터와 임시 테이블의 구조를 준비한다. 재귀적 쿼리 파트에서는 이후 데이터를 생성해내는 역할을 한다. 재귀적 쿼리 파트를 실행할 때는 모든 단계에서 만들어진 결과 셋이 아니라 직전 단계의 결과만 재귀 쿼리의 입력으로 사용된다.\n재귀적으로 실행되는 CTE에서 주의할 것은 반복 실행의 종료 조건이다.\n모든 재귀 쿼리에는 종료 조건이 필요할 것처럼 보이지만 실제 재귀 쿼리가 반복을 멈추는 조건은 재귀 파트의 결과가 0건일 때까지다. 실제 응용 프로그램의 쿼리에서 사용하는 데이터는 몇단계까지 재귀적으로 실행할지 알 수 없는 경우가 더 많다. 데이터의 오류나 쿼리 작성자의 실수로 종료 조건을 만족하지 못해 무한 반복하는 경우가 발생할 수 있다. cte_max_recursion_depth 시스템 변수를 이용해 최대 반복 실행횟수를 제한할 수 있다. 시스템 변수의 값을 적절히 낮은 값으로 변경하고, 꼭 필요한 쿼리에서만 SET_VAR힌트를 이용해 해당 쿼리에서만 반복 호출 횟수를 늘리는 방법을 추천한다. CTE 임시 테이블의 컬럼명을 변경하고자 한다면 CTE 별명 뒤에 (...)를 이용해 새로운 이름을 부여할 수 있다.\n","date":"2023-07-26T12:08:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/4/3/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/4/3/","title":"11.4 쿼리 작성 및 최적화 - SELECT - 서브쿼리 (3)"},{"content":"GROUP BY GROUP BY는 특정 컬럼 값으로 레코드를 그루핑하고, 그룹별로 집계된 결과를 하나의 레코드로 조회할 때 사용한다.\nWITH ROLLUP GROUP BY가 사용된 쿼리에서는 그루핑된 그룹별로 소계를 가져올 수 있는 롤업(ROLLUP) 기능을 사용할 수 있는데, ROLLUP으로 출력되는 소계는 단순히 최종 합만 가져오는 것이 아니라 사용된 컬럼의 개수에 따라 소계의 레벨이 달라진다.\nMySQL의 GROUP BY ... ROLLUP 쿼리는 엑셀의 피벗 테이블과 거의 동일한 기능이며, 결과의 소계 레코드의 컬럼값은 항상 NULL로 표현된다.\n컬럼 1개\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 SELECT dept_no, COUNT(*) FROM dept_emp GROUP BY dept_no WITH ROLLUP ; /* +---------+----------+ | dept_no | COUNT(*) | +---------+----------+ | d001 | 20211 | | d002 | 17346 | | d003 | 17786 | | d004 | 73485 | | d005 | 85707 | | d006 | 20117 | | d007 | 52245 | | d008 | 21126 | | d009 | 23580 | | NULL | 331603 | +---------+----------+ */ 컬럼 1개 이상\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SELECT first_name, last_name, COUNT(*) FROM employees GROUP BY first_name, last_name WITH ROLLUP ; /* +------------+-----------+----------+ | first_name | last_name | COUNT(*) | +------------+-----------+----------+ | Aamer | Anger | 1 | | Aamer | ... | ... | | Aamer | NULL | 228 | | Zvonko | Aamodt | 1 | | Zvonko | ... | ... | | Zvonko | NULL | 258 | | NULL | NULL | 300024 | +------------+-----------+----------+ */ MySQL 8.0 버전부터는 그룹 레코드에 표시되는 NULL을 GROUPING() 함수를 통해 변경할 수 있다.\n레코드를 컬럼으로 변환해서 조회 GROUP BY나 집합 합수를 통해 레코드를 그루핑할 수 있지만 하나의 레코드를 여러 개의 컬럼으로 나누거나 변환하는 SQL 문법은 없다.\n하지만 SUM(), COUNT() 같은 집합 함수와 CASE WHEN ... END 구분을 이용해 레코드를 컬럼으로 변환하거나 하나의 컬럼을 조건으로 구분하여 2개 이상의 컬럼으로 변환하는 것은 가능하다.\n레코드르 컬럼으로 변환 GROUP BY의 결과를 SUM(CASE WHEN ...) 구분을 사용해 한 번 더 변환하면 레코드를 컬럼으로 변환할 수 있다.\n1 2 3 4 5 6 7 8 9 10 SELECT SUM(CASE WHEN dept_no=\u0026#39;1001\u0026#39; THEN emp_count ELSE 0 END) AS count_d001 , ... SUM(CASE WHEN dept_no=\u0026#39;1001\u0026#39; THEN emp_count ELSE 0 END) AS count_d009 FROM ( SELECT dept_no, COUNT(*) AS emp_count FROM dept_emp GROUP BY dept_no ) tb_dervied ; 이처럼 레코드를 컬럼으로 변환하는 작업을 할 때는 목적이나 용도에 맞게 집합 함수를 사용하면 된다.\n이 예제는 부서 번호가 쿼리의 일부로 사용되기 때문에 변경되거나 추가되면 쿼리를 변경해야 하지만, 이런 부분은 동적 쿼리 등으로 보완할 수 있다.\n하나의 컬럼을 여러 컬럼으로 분리 SUM(CASE WHEN ...) 문장은 소그룹을 특정 조건으로 나눠 수를 구하는 용도로도 사용할 수 있다.\nORDER BY ORDER BY는 검색된 레코드를 어떤 순서로 정렬할 지 결정한다. ORDER BY 절이 사용되지 않으면 아래 규칙에 맞춰 정렬된다.\n인덱스를 사용한 SELECT 인덱스에 정렬된 순서대로 레코드를 가져온다. 풀 테이블 스캔을 실행하는 SELECT MyISAM: 테이블에 저장된 순서대로 가져온다. InnoDB: 프라이머리 키로 클러스터링 되어있기 때문에 프라이머리 키 순서대로 레코드를 가져온다. 임시 테이블로 처리되는 SELECT 조회되는 레코드의 순서를 예측하기 어렵다. ORDER BY절이 없는 SELECT 쿼리 결과의 순서는 처리 절차에 따라 달라질 수 있으며, 어떤 DBMS도 ORDER BY 절이 명시되지 않은 쿼리에 대해 정렬을 보장하지 않는다. 따라서 정렬이 필요한 곳에서는 ORDER BY 절을 사용해야한다.\n인덱스를 사용하지 못하는 ORDER BY ORDER BY에서 인덱스를 사용하지 못할 때는 추가 정렬 작업이 수행되며, 쿼리 실행 계획에 있는 Extra 컬럼에 \u0026ldquo;Using Filesort\u0026rdquo; 코멘트가 노출된다.\n\u0026ldquo;filesort\u0026quot;의 \u0026ldquo;file\u0026quot;은 디스크의 파일을 이용해 정렬을 수행한다는 의미가 아닌 MySQL 서버가 명시적으로 정렬 알고리즘을 수행했다는 의미이다. 정렬 대상이 많은 경우 여러 부분으로 나눠서 처리하는데, 정렬된 결과를 임시로 디스크나 메모리에 저장해 둔다.\n실제로 메모리만 이용해 정렬이 수행되었는지는 MySQL 서버 상태값을 확인해보면 알 수 있다. ORDER BY 사용법 및 주의사항 ORDER BY 절은 1개 이상의 컬럼으로 정렬을 수행할 수 있으며, 정렬 순서는 컬럼별로 다르게 명시할 수 있다.\n일반적으로 정렬할 대상은 컬럼명이나 표현식으로 명시하지만, SELECT되는 컬럼의 순번을 명시할 수도 있다.\n이러한 경우 숫자가 아닌 문자열을 사용할 경우 ORDER BY 절 자체가 무시된다. 컬럼 명이라고 하더라도 문자 리터럴이 사용된다면 무시된다. 여러 방향으로 동시 정렬 MySQL 8.0 버전부터는 오름차순과 내림차순을 혼용해서 인덱스를 생성할 수 있게 개선되었다.\n1 ALTER TABLE salaries ADD INDEX ix_salary_fromdate (salary DESC, from_date ASC); 응용 프로그램에서 예시 테이블을 내림차순으로만 조회하는 경우 2개의 인덱스 중 하나만 있어도 옵티마이저는 적절히 인덱스를 이용하여 정렬할 수 있게 최적화가 가능하다.\n1 2 3 4 5 SELECT * FROM salaries ORDER BY salary DESC LIMIT 10 ; 1 2 ALTER TABLE salaries ADD INDEX ix_salary_asc (salary ASC); ALTER TABLE salaries ADD INDEX ix_salary_desc (salary DESC); 하지만 쿼리가 내림차순으로만 레코드를 정렬해서 가져간다면 인덱스는 당연히 DESC를 생성하는 것이 좋다.\n함수나 표현식을 이용한 정렬 하나 이상의 컬럼 연산 결과를 이용한 정렬도 가능하다.\nMySQL 8.0 이전까지는 연산 결과를 기준으로 정렬하기 위해서 가상 컬럼(Virtual Column)을 추가하고 인덱스를 생성하는 방법을 사용해야 했지만, 함수 기반의 인덱스를 지원하기 시작했다.\n","date":"2023-07-26T12:08:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/4/2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/4/2/","title":"11.4 쿼리 작성 및 최적화 - SELECT (2)"},{"content":"스터디를 위해 Real MySQL 8.0를 주제로 학습을 진행중 11.4.7.3 OUTER JOIN의 성능과 주의사항 부분에서 \u0026quot;OUTER JOIN을 사용할 때 드리븐 테이블에 WHERE 조건 적용 시 INNER JOIN으로 변경되어 실행되니 주의해야 한다.\u0026quot; 라는 내용과 예시가 있었는데, 해당 부분이 이해가 잘 안되었습니다.\n관련 자료를 찾아보았으나, 실행 계획에서 OUTER JOIN이 INNER JOIN으로 변경되는 이유를 설명한 자료는 못 찾았고, 확인한 모든 글에서 WHERE, ON절 조건을 처리하는 순서가 또는 범위가 다르다. 라는 간략한 설명만 하고 있어 더 정확한 내용들을 알려드릴 수 있을까 싶어 제가 이해한 내용을 기록합니다.\n책의 예시 쿼리와 Real MySQL 8.0 학습 자료를 바탕으로 확인해보겠습니다.\n테이블 구조 예시로 사용되는 테이블의 구조는 아래와 같습니다.\nerDiagram EMPLOYEES ||--o{ DEPT_MANAGER: \"\" EMPLOYEES { emp_no int pk birth_date date first_name varchar last_name varchar gender enum hire_date date } DEPARTMENTS ||--o{ DEPT_MANAGER: \"\" DEPARTMENTS { dept_no char pk dept_name varchar emp_count int } DEPT_MANAGER { dept_no char pk, fk emp_no int pk, fk from_date date to_date date } 조인 처리 과정 조인 작업은 드라이빙 테이블을 스캔하고, 스캔한 드라이빙 테이블 레코드의 조인 컬럼으로 드리븐 테이블의 레코드에 대해 탐색 작업을 수행하게 됩니다.\n인덱스는 이미 정렬된 값을 가지기 때문에 빠른 속도를 보장하지만, 인덱스를 활용할 수 있는 경우라고 하더라도 여러 레코드를 가져오는 상황에서는 인덱스의 특정 범위를 가져오는 인덱스 스캔 작업 보다, 인덱스 값을 하나씩 비교해서 반복적으로 가져오는 탐색 작업이 상대적으로 부하가 높습니다.\n이러한 이유로 옵티마이저는 테이블의 인덱스 사용 가능 여부나 테이블 레코드 개수 같은 통계 정보를 활용하여 더 빠르게 처리될 수 있는 테이블을 드리븐 테이블로 선택하는 방식으로 최적화를 수행합니다.\n이너 조인과 아우터 조인의 차이 옵티마이저에서 이너 조인은 옵티마이저에 의해 드라이빙, 드리븐 테이블이 변경될 수 있는 반면 아우터 조인(LEFT JOIN, RIGHT JOIN)은 대상 테이블을 기준으로 조회될 레코드들이 결정되므로 테이블의 조인 순서를 변경할 수 없으므로(이미 결정된 순서이므로) 테이블 조인 순서에 의한 최적화는 발생하지 않게 됩니다.\n즉, 이너 조인은 여러 상황을 고려하여 더 효율적인 테이블을 드리븐 테이블로 결정하지만, 아우터 조인은 대상 테이블을 기준으로 레코드를 선택하기 때문에 드라이빙 테이블이 결정되므로 테이블 조인 순서에 의한 최적화를 진행할 수 없습니다.\n쿼리 분석 아우터 조인을 사용하는 쿼리에서 드리븐 테이블에 대한 조건을 WHERE 절에 명시하면 INNER JOIN으로 변경되어 처리된다고 하는데, EXPLAIN ANALYZE 명령으로 예시 쿼리가 어떤식으로 처리되는지 살펴보겠습니다.\nLEFT JOIN + WHERE 조건 1 2 3 4 5 6 7 8 9 10 11 12 13 EXPLAIN ANALYZE SELECT * FROM employees e LEFT JOIN dept_manager mgr ON mgr.emp_no = e.emp_no WHERE mgr.dept_no=\u0026#39;d001\u0026#39; ; /* -\u0026gt; Nested loop inner join (cost=2.89 rows=2) (actual time=0.196..0.207 rows=2 loops=1) -\u0026gt; Filter: (mgr.emp_no is not null) (cost=0.7 rows=2) (actual time=0.0439..0.0504 rows=2 loops=1) -\u0026gt; Index lookup on mgr using PRIMARY (dept_no=\u0026#39;d001\u0026#39;) (cost=0.7 rows=2) (actual time=0.0425..0.0488 rows=2 loops=1) -\u0026gt; Single-row index lookup on e using PRIMARY (emp_no=mgr.emp_no) (cost=1.04 rows=1) (actual time=0.077..0.077 rows=1 loops=2) */ WHERE 조건으로 처리한 쿼리가 Nested loop inner join으로 실행되었습니다. dept_manager 테이블을 드라이빙 테이블로 선택하였고, mgr.emp_no is not null 조건을 통해 필터링한 후, dept_no='d001' 조건을 통해 인덱스 스캔을 수행하였습니다.\n그리고 employees 테이블에 대해 조인 컬럼인 emp_no=mgr.emp_no 이용해 인덱스 탐색을 수행하여 조인을 완료하였습니다.\ndept_manager 테이블에 대한 mgr.emp_no is not null 필터 처리는 실행 계획 변경으로 발생할 수 있는 잘못된 데이터를 방지하는 차원에서 추가시키는 것이 아닌가 추측해봅니다.\n(참고로 드리븐 테이블의 모든 컬럼의 조건에서 같은 처리가 발생합니다.)\nINNER JOIN 책에서는 쿼리를 INNER JOIN으로 변경하여 실행한다고 언급했는데, 실제로 그럴까요? INNER JOIN으로 변경하여 실행 계획을 확인해보겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 EXPLAIN ANALYZE SELECT * FROM employees e INNER JOIN dept_manager mgr ON mgr.emp_no = e.emp_no WHERE mgr.dept_no=\u0026#39;d001\u0026#39; ; /* -\u0026gt; Nested loop inner join (cost=1.4 rows=2) (actual time=0.0411..0.0482 rows=2 loops=1) -\u0026gt; Index lookup on mgr using PRIMARY (dept_no=\u0026#39;d001\u0026#39;) (cost=0.7 rows=2) (actual time=0.025..0.0287 rows=2 loops=1) -\u0026gt; Single-row index lookup on e using PRIMARY (emp_no=mgr.emp_no) (cost=0.3 rows=1) (actual time=0.00877..0.00881 rows=1 loops=2) */ 거의 같은 방식으로 처리되지만, INNER JOIN으로 실행한 쿼리는 dept_manager 테이블을 mgr.emp_no is not null 조건으로 필터링하는 처리가 없다는 점이 다릅니다.\n그래서 이유는? 그림에서 보이듯 안티 조인이 아닌 아우터 조인(LEFT JOIN,OUTER JOIN)은 드라이빙 테이블의 레코드만 취하는 처리를 수행합니다. 따라서, 드라이빙 테이블의 조인되는 드리븐 테이블의 레코드가 존재하지 않을 경우 알 수 없다는 의미의 NULL을 채워 반환하게 됩니다.\n아우터 조인의 WHERE 절에서 드리븐 테이블의 컬럼을 조건으로 사용한다는 것을 \u0026ldquo;아우터 조인된 결과에서 특정 조건으로 필터링 처리를 한다\u0026rdquo; 라는 의미로 많이 해석되는 것 같은데 이는 사실 완벽하게 일치하는 표현은 아닙니다.\n결과적으로 조인 결과의 드리븐 테이블 컬럼에 대해 IS NULL을 제외한 모든 조건이 존재한다는 자체가 아우터 조인으로 만들어지는 조인 결과물의 해당 컬럼 값이 조인 결과가 있어야 한다는 것을 가정하고, 이는 드라이빙 테이블과 드리븐 테이블의 교집합을 의미하는 INNER JOIN 결과에 조건을 처리하는 것과 같은 결과를 만들게 됩니다.\n옵티마이저가 아우터 조인을 INNER JOIN으로 변경하는 것은 아우터 조인으로 처리되어 드라이빙 테이블을 모두 읽고 교집합이 아닌 드리븐 테이블 영역에 대해서 NULL을 채워주는 처리 후 WHERE 조건을 이용한 필터링으로 처리하는 방식보다 효율적이고, 조인 테이블 순서 변경에 의한 최적화도 가능하므로 합리적으로 보입니다.\n반대로 해당 컬럼이 NULL임을 확인해야하는 안티 조인은 아우터 조인 처리가 필요하므로 이너 조인으로 변경이 발생하지 않습니다.\n정리 아우터 조인의 목적은 드라이빙 테이블의 레코드를 조회하되, 드리븐 테이블 대상이 드라이빙 테이블에 없다면 NULL을 채우기 위함입니다. 예시 쿼리에서 ON을 사용하여 아우터 조인으로 처리한 경우 직원들을 보는데 특정 부서를 제외한 관리자 여부까지 확인하고 싶어(NULL이 아니면 관리자 의미) 아우터 조인의 드리븐 테이블 영역 컬럼에 IS NULL을 제외한 모든 WHERE 조건은 사실 조건에 해당하는 컬럼이 NULL이 아님을 의미합니다. 이는 이너 조인으로 처리되어도 동일한 결과를 만들고, 더 효율적 처리할 여지가 있기 때문에 옵티마이저가 아우터 조인을 이너 조인으로 변경합니다. 아우터 조인이 이너 조인으로 변경되게 되면 오작동을 예방하기 위한 처리로 성능에 손해가 있을 수 있고(추측), 잘못된 쿼리 해석 예방을 위해 드리븐 테이블의 WHERHE 조건을 사용하는 OUTER JOIN 쿼리는 모두 INNER JOIN로 사용해야 합니다. ","date":"2023-07-25T11:52:14+09:00","image":"https://codemario318.github.io/post/db/where-on/cover_hu3d445aeca10173b7df51d6f3cd6c9292_48066_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/db/where-on/","title":"OUTER JOIN의 WHERE, ON 처리에 대한 오해와 진실"},{"content":"웹 서비스와 같은 일반적인 온라인 트랜잭션 처리 환경의 데이터 베이스에서는 INSERT, UPDATE 같은 작업은 레코드 단위로 발생하기 때문에 성능 이슈가 발생하는 경우는 매우 적다.\n하지만 SELECT는 여러개의 테이블로 부터 데이터를 조합해서 빠르게 가져와야 하는 경우가 많아 여러개의 테이블을 어떻게 읽느냐에 따라 성능 이슈가 자주 발생하게 된다.\nSELECT 절의 처리 순서 1 2 3 4 5 6 7 8 9 SELECT s.emp_no, COUNT(DISTINCT e.first_name) AS cnt FROM salaries s INNER JOIN employees e ON e.emp_no=s.emp_no WHERE s.emp_no IN (100001, 100002) GROUP BY s.emp_no HAVING AVG(s.salary) \u0026gt; 1000 ORDER BY AVG(s.salary) LIMIT 10 ; SELECT 쿼리에서 어느 절이 먼저 실행될지 예측하지 못할 때가 있는데, 어느 절이 먼저 실행되는지 모른다면 처리 내용, 처리 결과를 예측할 수 없다.\nflowchart LR subgraph FIRST [\"WHERE 적용 및 조인 실행\"] direction TB A(드라이빙 테이블) -.-\u003e B(드리븐 테이블 1) -.-\u003e C(드리븐 테이블 2) end FIRST --\u003e D(\"GROUP BY\") --\u003e E(\"DISTINCT\") --\u003e F(\"HAVING 조건 적용\") --\u003e G(\"ORDER BY\") --\u003e H(\"LIMIT\") 대부분 위 순서대로 쿼리가 실행된다. 또한 SQL에는 ORDER BY나 GROUP BY 절이 있더라도 인덱스를 이용해 처리할 때는 그 단계 자체가 불필요하므로 생략된다.\nflowchart LR subgraph FIRST [\"WHERE 적용\"] A(드라이빙 테이블) end subgraph SECOND [\"조인 실행\"] direction TB B(드리븐 테이블 1) -.-\u003e C(드리븐 테이블 2) end FIRST --\u003e G(\"ORDER BY\") --\u003e SECOND --\u003e H(\"LIMIT\") GROUP BY 절이 없이 ORDER BY만 사용된 쿼리같은 경우 첫 번째 테이블만 읽어 정렬을 수행한 뒤 나머지 테이블을 읽는다.\n위의 실행 순서를 벗어나는 쿼리가 필요하다면 서브쿼리로 작성된 인라인 뷰(Inline View)를 사용해야 한다. 예를 들어 예시 쿼리에서 LIMIT를 먼저 적용하고 ORDER BY를 실행하기 위해 다음과 같이 쿼리를 작성할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 SELECT emp_no,cnt FROM ( SELECT s.emp_no ,COUNT(DISTINCT e.first_name) AS cnt ,MAX(s.salary) AS max_salary FROM salaries s INNER JOIN employees e ON e.emp_no=s.emp_no WHERE s.emp_no IN (100001, 100002) GROUP BY s.emp_no HAVING MAX(s.salary) \u0026gt; 1000 LIMIT 10 ) temp_view ORDER BY max_salary ; LIMIT를 GROUP BY전 실행하고자 할때도 서브쿼리로 인라인 뷰를 만들어, 뷰 안에서 LIMIT 를 정용하고 바깥 쿼리에서 GROUP BY, ORDER BY를 적용해야 한다.\n인라인 뷰가 사용되면 임시 테이블이 사용되기 때문에 주의가 필요하다.\n인덱스 사용을 위한 기본 규칙 인덱스는 B-Tree에 컬럼의 값을 아무런 변환 없이 정렬하여 저장하기 때문에 원본 값을 이용할 때만 인덱스를 사용하게 된다.\n따라서 인덱스를 사용하려면 기본적으로 인덱스된 컬럼의 값 자체를 변환하지 않고 그대로 사용한다는 조건을 만족해야 한다.\n1 2 3 4 5 /* 인덱스 사용 불가*/ SELECT * FROM salaries WHERE salary*10 \u0026gt; 150000; /* 인덱스 사용 가능 */ SELECT * FROM salaries WHERE salary \u0026gt; 150000/10; 복잡한 연산을 수행한다거나 MD5 함수와 같이 해시 값을 만들어 비교해야 하는 경우라면 미리 계산된 값을 저장하도록 MySQL 가상 컬럼(Virtual Column)을 추가하고 그 컬럼에 인덱스를 생성하거나 함수 기반의 인덱스를 사용하면 된다.\n저장하고자 하는 값의 타입에 맞춰 컬럼의 타입을 선정하고, 쿼리 작성시 데이터의 타입에 맞춰 비교 조건을 사용해야하며, 데이터 타입이 조금이라도 다른 경우 최적화되지 못하는 현상은 MySQL 서버의 버전이 업그레이드된다고 해서 해결될 수 있는 부분이 아니므로 주의가 필요하다.\nWHERE WHERE 조건이 인덱스를 사용하는 방법은 크게 작업 범위 결정 조건과 체크 조건 두 가지 방식으로 구분할 수 있다.\n그 중 작업 범위 결정 조건은 동등 비교 조건이나 IN으로 구성된 조건에 사용된 컬럼들이 인덱스의 컬럼 구성과 좌측에서부터 비교했을 때 얼마나 일치하는가에 따라 달라진다.\nflowchart TB subgraph SA [인덱스 컬럼 순서] direction LR saa(COL_1) ~~~ sab(COL_2) ~~~ sac(COL_3) ~~~ sad(COL_4) end subgraph SB [WHERE 조건절 컬럼 순서] direction LR sba(COL_2 = ?) ~~~ sbb(COL_4 = ?) ~~~ sbc(COL_3 \u003e ?) ~~~ sbd(COL_1 = ?) end SA ~~~ SB %% sba --\u003e sab %% sbb -.-\u003e sad %% sbc --\u003e sac %% sbd --\u003e saa WHERE절에서 조건절에 나열된 순서가 인덱스와 다르더라도 옵티마이저는 인덱스를 사용할 수 있는 조건들을 뽑아서 최적화를 수행하기 때문에 사용된 조건의 순서는 실제 인덱스 사용과 무관하다.\n동등 비교 조건: COL_1, COL_2 범위 비교 조건: COL_3 체크조건: COL_4 직전 컬럼인 COL_3가 동등 비교 조건이 아니라 범위 비교 조건으로 사용되었기 때문에 작업범위 결정 조건으로 사용하지 못한다. WHERE 절의 조건은 GROUP BY, ORDER BY와 달리 순서를 변경해도 결과의 차이가 없다.\n인덱스를 구성하는 컬럼에 대한 조건 존재 유무가 중요하다.\nWHERE 조건에서 OR 을 통해 조건을 사용할 경우 처리 방식이 바뀐다.\n1 2 3 4 5 SELECT * FROM employees WHERE first_name = \u0026#39;Kebin\u0026#39; OR last_name = \u0026#39;Poly\u0026#39; ; first_name컬럼에 인덱스가 있고, last_name 컬럼에 인덱스가 없는 경우 first_name의 인덱스를 활용할 경우 (풀 테이블 스캔) + (인덱스 레인지 스캔) 보다 (풀 테이블 스캔)이 작업량이 적으므로 옵티마이저는 풀 테이블 스캔을 선택한다. 조건에 사용된 모든 컬럼에 인덱스가 있는 경우 index_merge 접근 방법으로 쿼리가 실행될 수 있어 풀 테이블 스캔보다 빠르지만, 인덱스 하나를 레인지 스캔하는 것 보다는 느리다. WHERE 조건절에서 AND로 연결된다면 읽어와야 할 레코드 건수를 줄이지만, OR로 연결되면 비교해야 할 레코드가 늘어나기 때문에 사용에 주의가 필요하다.\nGROUP BY GROUP BY 절의 각 컬럼은 비교 연산자를 가지지 않으므로 작업 범위 결정 조건이나 체크 조건과 같이 구분해서 생각할 필요는 없다.\n쿼리에 GROUP BY가 사용될 때 명시된 컬럼의 순서가 인덱스를 구성하는 컬럼의 순서와 같으면 인덱스를 사용할 수 있다.\nflowchart TB subgraph SA [인덱스 컬럼 순서] direction LR saa(COL_1) --\u003e sab(COL_2) --\u003e sac(COL_3) --\u003e sad(COL_4) end subgraph SB [WHERE 조건절 컬럼 순서] direction LR sba(COL_1) --\u003e sbb(COL_2) --\u003e sbc(COL_3) end SA ~~~ SB %% sba --\u003e saa %% sbb -.-\u003e sab %% sbc --\u003e sac %% sbd --\u003e saa GROUP BY 절에 명시된 컬럼이 인덱스 컬럼의 순서와 위치가 같아야한다. 인덱스를 구성하는 컬럼 중에서 뒤쪽에 있는 컬럼은 명시되지 않아도 인덱스를 사용할 수 있지만 인덱스 앞쪽에 있는 컬럼이 명시되지 않으면 인덱스를 사용할 수 없다. GROUP BY 절에 명시도니 컬럼이 하나라도 익덱스에 없으면 전혀 인덱스를 사용하지 못한다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## 인덱스를 활용하지 못하는 GROUP BY 예시 # ix_col1234 (col_1, col_2, col_3, col_4) ## 1 ... GROUP BY COL_2, COL_1 ## 2 ... GROUP BY COL_1, COL_3, COL_2 ## 3 ... GROUP BY COL_1, COL_3 ## 4 ... GROUP BY COL_1, COL_2, COL_3, COL_4, COL_5 1,2: 인덱스를 구성하는 컬럼 순서와 불일치 3: COL_3 앞 COL_2 없음 4: 인덱스가 없는 COL_5 사용 1 2 3 4 5 6 7 8 ## 인덱스를 활용하는 GROUP BY 예시 # ix_col1234 (col_1, col_2, col_3, col_4) ... GROUP BY COL_1 ... GROUP BY COL_1, COL_2 ... GROUP BY COL_1, COL_2, COL_3 ... GROUP BY COL_1, COL_2, COL_3, COL_4 WHERE 조건절에 COL_1이나 COL_2같이 인덱스 순서가 앞인 컬럼에 대해 동등 비교 조건으로 사용된다면 상수로 비교하기 때문에 해당 컬럼을 명시하지 않아도 인덱스를 활용할 수 있다.\n1 2 3 4 5 6 ## GROUP BY에 컬럼을 명시하지 않아도 인덱스를 활용하는 예시 # ix_col1234 (col_1, col_2, col_3, col_4) ... WHERE COL_1 = \u0026#39;상수\u0026#39; ... GROUP BY COL_2, COL_3 ... WHERE COL_1 = \u0026#39;상수\u0026#39; AND COL_2 = \u0026#39;상수\u0026#39; ... GROUP BY COL_3, COL_4 ORDER BY MySQL에서 ORDER BY와 GROUP BY는 처리 방법이 상당히 비슷하여 인덱스 사용 여부 또한 거의 흡사하다.\n다른 점은 정렬되는 각 컬럼의 오름차순 및 내림차순 옵션이 인덱스와 같거나 정반대인 경우에만 사용할 수 있어 모든 컬럼이 오름차순이나 내림차순일 때만 인덱스를 사용할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## 인덱스를 활용하지 못하는 ORDER BY 예시 # ix_col1234 (col_1, col_2, col_3, col_4) ## 1 ... GROUP BY COL_2, COL_3 ## 2 ... GROUP BY COL_1, COL_3, COL_2 ## 3 ... GROUP BY COL_1, COL_2 DESC, COL_3 ## 4 ... GROUP BY COL_1, COL_3 ## 5 ... GROUP BY COL_1, COL_2, COL_3, COL_4, COL_5 COL_1 누락 컬럼 순서 불일치 COL_2 내림차순 COL_2 누락 인덱스에 존재하지 않는 COL_5 사용 WHERE + (ORDER BY or GROUP BY) 쿼리가 WHERE + (ORDER BY 또는 GROUP BY) 형식으로 구성되어 있는 경우 각각 다른 인덱스를 사용할 수 없기 때문에 3가지 방식 중 한가지 방법으로만 인덱스를 이용한다.\n동시에 같은 인덱스를 이용: 정렬 대상 컬럼이 모두 하나의 인덱스에 연속해서 포함되어 있는 경우 사용 나머지 방식보다 훨씬 빠르다. WHERE 절만 인덱스를 이용: ORDER BY 절은 인덱스를 이용한 정렬이 불가능하며, 인덱스를 통해 검색된 결과 레코드를 별도의 정렬 처리 과정을 거친다(Using Filesort) WHERE 절의 조건에 일치하는 레코드 건수가 많지 않을 때 효율적이다. ORDER BY 절만 인덱스를 이용 ORDER BY 절의 순서대로 인덱스를 읽으면서 레코드를 한 건씩 WHERE 절의 조건에 일치하는지 비교하여 아닌경우 버리는 형식으로 처리된다. 아주 많은 레코드를 조회해서 정렬해야 하는 경우 이러한 형태로 튜닝하기도 한다. 또한 WHERE 절에서 동등 비교 조건으로 비교된 컬럼과 ORDER BY 절에 명시된 컬럼이 순서대로 빠짐없이 인덱스 컬럼의 왼쪽부터 일치해야 한다.\n비교 조건에 사용된 컬럼과 ORDER BY 절의 컬럼의 중복은 상관 없다. 중간에 빠지는 컬럼이 있으면 모두 인덱스를 사용할 수 없어 주로 WHERE절만 인덱스를 활용한다. flowchart LR subgraph SAB [\" \"] direction TB SA ~~~ SB end subgraph SA [WHERE 절] direction TB saa(COL_1 = ?) end subgraph SB [ORDER BY 절] direction TB sba(COL_2) ~~~ sbb(COL_3) end subgraph SC [인덱스] direction TB sca(COL_1 ASC) ~~~ scb(COL_2 ASC) ~~~ scc(COL_3 ASC) ~~~ scd(COL_4 ASC) end subgraph SDE [\" \"] direction TB SD ~~~ SE end subgraph SD [WHERE 절] direction TB sda(COL_1 = ?) ~~~ sdb(COL_2 = ?) ~~~ sdc(COL_3 \u003e ?) end subgraph SE [ORDER BY 절] direction TB sea(COL_3 DESC) ~~~ seb(COL_4 DESC) end SA ~~~ SB SAB ~~~ SC ~~~ SDE 1 2 3 4 5 6 7 8 9 10 11 12 13 # 인덱스 사용 가능: 등등 비교로 사용된 컬럼 SELECT * FROM tb_test WHERE COL_1 = 10 ORDER BY COL_2, COL_3 ; # 위와 같은 방식으로 처리된다. SELECT * FROM tb_test WHERE COL_1 = 10 ORDER BY COL_1, COL_2, COL_3 ; 1 2 3 4 5 6 7 8 9 10 11 12 13 # 범위 조건으로 사용되었지만 모든 컬럼을 명시하여 인덱스 사용 가능 SELECT * FROM tb_test WHERE COL_1 \u0026gt; 10 ORDER BY COL_1, COL_2, COL_3 ; # 인덱스 사용 불가 SELECT * FROM tb_test WHERE COL_1 \u0026gt; 10 ORDER BY COL_2, COL_3 ; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 인덱스 사용 불가 SELECT * FROM tb_test WHERE COL_1 = 10 ORDER BY COL_3, COL_4 ; SELECT * FROM tb_test WHERE COL_1 \u0026gt; 10 ORDER BY COL_2, COL_3 ; SELECT * FROM tb_test WHERE COL_1 IN (1, 2, 3, 4) ORDER BY COL_2 ; GROUP BY + ORDER BY GROUP BY와 ORDER BY 동시에 사용된 쿼리에서 두 절이 모두 하나의 인덱스를 사용해 처리되려면 명시된 컬럼의 순서와 내용이 모두 일치해야 한다.\nMySQL 5.7 버전까지는 GROUP BY는 사용된 컬럼에 대한 정렬까지 함께 수행하는 방식이었으나 MySQL 8.0 부터는 컬럼의 정렬을 보장하지 않아 정렬이 필요한 GROUP BY라면 ORDER BY를 모두 명시해야 한다.\nWHERE + GROUP BY + ORDER BY 아래 질문을 기본으로 하여 흐름을 확인한다.\nWHERE 절이 인덱스를 사용할 수 있는가? GROUP BY 절이 인덱스를 사용할 수 있는가? GROUP BY 절과 ORDER BY 절이 동시에 인덱스를 사용할 수 있는가? flowchart LR A(START) --\u003e|Y| B{WHERE} B --\u003e|Y| C{GROUP BY} B --\u003e|N| D{GROUP BY} C --\u003e|Y| E{ORDER BY} C --\u003e|N| RB[Only WHERE] E --\u003e|Y| RA[WHERE, GROUP BY, ORDER BY] E --\u003e|N| RB D --\u003e|N| F{GROUP BY} F --\u003e|Y| G{ORDER BY} F --\u003e|N| RD[인덱스 사용 불가] G --\u003e|Y| RC[GROUP BY, ORDER BY] G --\u003e|N| RD WHERE 절 비교 조건 사용시 주의사항 WHERE절에 사용되는 비교 조건의 표현식은 상당히 중요하다. 쿼리가 최적으로 실행되려면 적합한 인덱스와 함께 비교 조건의 표현식을 적절하게 사용해야 한다.\nNULL 비교 다른 DBMS와 다르게 MySQL 에서는 NULL 값이 포함된 레코드도 인덱스로 관리한다.\n하지만 SQL 표준에서 NULL의 정의는 비교할 수 없는 값이며, 이에 따라 두 값이 모두 NULL을 가진다고 하더라도 이 두 값이 동등한지 비교하는 것은 불가능하다.\n쿼리에서 IS NULL 또는 \u0026lt;=\u0026gt;연산자를 사용하는 방법 외 컬럼의 값이 NULL인지 알 수 있는 방법은 없다.\n문자열이나 숫자 비교 문자열 컬럼이나 숫자 컬럼을 비교할 때는 반드시 그 타입에 맞는 상수값을 사용할 것을 권장한다.\n1 2 3 4 SELECT * FROM employees WHERE emp_no=1001; SELECT * FROM employees WHERE first_name=\u0026#39;smith\u0026#39;; SELECT * FROM employees WHERE emp_no=\u0026#39;1001\u0026#39;; SELECT * FROM employees WHERE first_name=1001; 1, 2번 쿼리는 컬럼의 타입과 비교하는 상수값이 동일한 타입으로 사용되어 인덱스를 적절히 이용할 수 있다. 3번 쿼리는 emp_no 컬럼이 숫자 타입이므로 문자열 상수값을 숫자로 타입 변환해서 사용하므로 특별한 성능 저하는 발생하지 않는다. 4번 쿼리는 first_name이 문자열 컬럼이지만 비교되는 상수값이 숫자 타입이므로 옵티마이저는 우선순위를 가지는 숫자 타입으로 비교를 수행하려고 실행 계획을 수립한다. first_name 컬럼의 문자열을 숫자로 변환하여 비교를 수행하게 되어 인덱스를 활용하지 못한다. 컬럼 타입에 맞게 상수 리터럴을 비교 조건에 사용하는 것이 중요하다.\n날짜 비교 DATE or DATETIME 문자열 비교\nDATE, DATETIME 타입 갑과 문자열을 비교할 때는 문자열 값을 자동으로 변환해서 비교를 수행하므로 명시적으로 변환하는 처리를 거치지 않아도 동일하게 처리된다. DATE, DATETIME 타입 컬럼을 변경하는 경우 인덱스를 효율적으로 이용할 수 없으므로 상수를 변형하는 형태로 처리하는 것이 좋다. 문자열로 변경 날짜 계산 처리 등 DATE 와 DATETIME 비교\n따로 변환하지 않고 비교해도 내부적으로 DATE 타입을 YYYY-MM-DD 00:00:00 형태로 변환하여 비교를 수행한다. 해당 타입 비교에서 타입 변환은 인덱스 사용여부에 영향을 미치지 않으므로 성능보다는 쿼리의 결과에 주의하여 사용한다. DATETIME 와 TIMESTAMP 비교\nDATE, DATETIME 값을 TIMESTAMP 값을 변환 없이 비교하면 문제없이 작동하고 실행 계획도 인덱스 레인지 스캔을 사용해서 처리하는 것 처럼 보이지만 그렇지 않다.\nTIMESTAMP값은 내부적으로 단순 숫자 값으로 원하는 결과를 얻지 못하기 때문에 비교 대상 컬럼 타입에 맞게 변환해서 사용해야한다.\nShort-Circuit Evalutation 1 2 3 4 5 boolean in_transaction; if ( in_transaction \u0026amp;\u0026amp; has_modified() ) { commit(); } 위 처럼 여러 개의 표현식이 AND, OR 논리 연산자로 연결된 경우 선행 표현식의 결과에 따라 후행 표현식을 평가할지 결정하는 최적화를 Short-circuit Evaluation 이라고 하며, MySQL 서버에서도 이러한 방식으로 쿼리의 성능 최적화를 수행한다.\nMySQL 서버는 쿼리의 WHERE 절에 나열된 순서대로 Short-circuit Evaluation 방식으로 평가하여 해당 레코드를 반환해야 할지 결정하지만, 나열된 조건 중 인덱스를 활용할 수 있는 조건이 있다면 해당 조건을 최우선으로 사용한다.\n그렇기 때문에 WHERE 조건절에서 나열된 순서가 인덱스의 사용 여부를 결정하지는 않게된다.\n인덱스 사용 불가\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # 0번: 2,844,047건 SELECT COUNT(*) FROM salaries; # 1번: 2,442,943건 SELECT COUNT(*) FROM salaries WHERE CONVERT_TZ(from_date, \u0026#39;+00:00\u0026#39;, \u0026#39;+09:00\u0026#39;) \u0026gt; \u0026#39;1991-01-01\u0026#39; ; # 2번: 0건 SELECT COUNT(*) FROM salaries WHERE to_date \u0026lt; \u0026#39;1985-01-01\u0026#39; ; # 3번(1 AND 2 조건): 0건 SELECT COUNT(*) FROM salaries WHERE CONVERT_TZ(from_date, \u0026#39;+00:00\u0026#39;, \u0026#39;+09:00\u0026#39;) \u0026gt; \u0026#39;1991-01-01\u0026#39; AND to_date \u0026lt; \u0026#39;1985-01-01\u0026#39; ; # 4번(2 AND 1 조건): 0건 SELECT COUNT(*) FROM salaries WHERE to_date \u0026lt; \u0026#39;1985-01-01\u0026#39; AND CONVERT_TZ(from_date, \u0026#39;+00:00\u0026#39;, \u0026#39;+09:00\u0026#39;) \u0026gt; \u0026#39;1991-01-01\u0026#39; ; 위 쿼리에서 사용된 두 개의 조건은 모두 인덱스를 사용하지 못하기 때문에 풀 테이블 스캔이 발생한다. 그리고 2번 조건의 결과 레코드 건수가 0 이므로 1번 조건과 2번 조건이 AND 연결된 3번, 4번 쿼리의 결과도 0건이다.\n1번 조건은 전체 레코드에 대해 CONVERT_TZ 함수를 실행 후 그 결과에 대해 비교 작업을 수행해야하지만, 2번 조건은 비교 작업만 전체 레코드에 대해 수행한다.\n이러한 상황에서 Short-circuit Evaluation 최적화를 수행하기 위해 조건의 결과가 0건인 2번 조건을 선행하면 1번 쿼리가 실행되지 않기 때문에, 3번 쿼리보다 더 좋은 성능을 보인다.\n인덱스 사용 가능\n1 2 3 4 5 6 7 # ix_firstname(first_name) SELECT * FROM employees WHERE last_name=\u0026#39;Aamodt\u0026#39; AND first_name=\u0026#39;Matt\u0026#39; AND MONTH(birth_date)=1 ; 위 쿼리는 인덱스를 사용 가능한 first_name을 먼저 평가하고, 이후 나열된 순서대로 조건들을 평가한다.\nDISTINCT 특정 컬럼의 유니크한 값을 조회하기 위해 SELECT 절에 DISTINCT를 사용하는데, 많은 사용자가 조인을 하는 경우 레코드 중복을 막기 위해 남발하는 경향이 있다.\n이러한 남용은 성능적인 문제도 있지만 쿼리의 결과도 의도한 바와 달라질 수 있으므로 주의해야한다.\n여러 테이블을 조인하는 쿼리에서는 조인 조건에 따라 레코드가 몇 배씩 불어나기도 하는데, 테이블간 업무적인 연결 조건을 이해하지 못하고 쿼리를 작성하는 경우 이러한 남용이 발생하게되므로 1:1. 1:N 조인인지 업무적인 특성을 잘 이해하는 것이 중요하다.\nLIMIT n LIMIT는 쿼리 결과에서 지정된 순서에 위치한 레코드만 가져오고자 할 때 사용한다.\n오라클과 같은 RDBMS와는 다르게 MySQL의 LIMIT는 WHERE 조건이 아니기 때문에 항상 쿼리의 가장 마지막에 실행된다.\n1 2 3 4 5 6 SELECT * FROM employees WHERE emp_no BETWEEN 10001 AND 10010 ORDER BY first_name LIMIT 0, 5 ; employees 테이블에서 WHERE 절의 검색 조건에 일치하는 레코드를 전부 읽어온다. 1번에서 읽어온 레코드를 first_name 컬럼 값에 따라 정렬한다. 정렬된 결과에서 상위 5건만 반환한다. MySQL에서 LIMIT는 쿼리에서 모든 레코드의 정렬이 완료되지 않았다고 하더라도 필요한 레코드 건수만 준비되면 즉시 쿼리를 종료한다.\n하지만 쿼리에 포함된 ORDER BY, GROUP BY 절에서 수행하는 정렬 작업이 인덱스를 적절히 이용하지 못하는 경우 처리가 완료된 후에 LIMIT가 적용되므로 성능 개선에 큰 의미가 없을 수 있다.\n풀 테이블 스캔 1 SELECT * FROM employees LIMIT 0, 10; 풀 테이블 스캔을 수행하지만 LIMIT 조건으로 인해 MySQL이 스토리지 엔진으로 부터 10개의 레코드를 읽어 들이는 순간 읽기 작업을 멈추게된다.\n정렬, 그루핑, DISTINCT가 없는 쿼리에서 LIMIT 조건을 사용하면 쿼리가 상당히 빨라질 수 있다.\nGROUP BY 1 SELECT * FROM employees GROUP BY first_name LIMIT 0, 10; 인덱스를 사용하지 못하는 GROUP BY는 그루핑과 정렬의 특성을 모두 가지고 있기 때문에 GROUP BY 작업이 완료되고 나서야 LIMIT 처리를 수행할 수 있다.\nLIMIT이 GROUP BY와 함께 사용되는 경우 실질적인 서버의 작업 내용을 크게 줄여주지는 못한다.\nDISTINCT 1 SELECT DISTINCT first_name FROM employees LIMIT 0, 10; 정렬이 필요없는 DISTINCT는 유니크한 그룹만 만들어 내면 된다.\nMySQL은 스토리지 엔진을 통해 풀 테이블 스캔으로 employees 테이블 레코드를 읽어 들임과 동시에 DISTINCT를 위한 중복 제거 작업(임시 테이블 사용)을 작업을 반복적으로 처리하다가 유니크한 레코드가 LIMIT 건수 만큼 채워졌다면 쿼리를 멈춘다.\n이렇게 DISTINCT 와 함께 사용된 LIMIT가 실질적인 중복 제거 작업 범위를 줄여주게 되므로 작업량도 줄이는 효과를 가진다.\nWHERE + ORDER BY 1 2 3 4 5 6 SELECT * FROM employees WHERE emp_no BETWEEN 10001 AND 11000 ORDER BY first_name LIMIT 0, 10 ; employees 테이블로부터 조건에 일치하는 레코드를 읽은 후 first_name 컬럼 값으로 정렬을 수행한다.\n정렬을 수행하면서 필요한 10건을 채우면 나머지 작업을 멈추고 결과를 사용자에게 반환한다.\n이 때 정렬을 수행하기 전에 WHERE 조건에 일치하는 모든 레코드를 읽어와야 하지만, 읽어온 결과가 전부 정렬돼야 쿼리가 완료되는 것이 아니라 필요한 만큼만 정렬하면 된다.\n하지만 이 쿼리도 GROUP BY 와 사용했을 때 처럼 크게 작업량을 줄여주지는 못한다.\nOFFSET 실제 쿼리의 성능은 사용자의 화면에 레코드가 몇 건 출력되느냐보다 MySQL 서버가 그 결과를 만들어내기 위해 어떠한 작업들을 했는지가 중요한데, LIMIT에 offset을 사용하여 페이징과 같은 처리를 할 때 큰 영향을 미칠 수 있다.\n1 2 3 4 5 SELECT * FROM salaries ORDER BY salary LIMIT 2_000_000, 10 ; 위 쿼리 처리는 salaries 테이블을 처음부터 읽으면서 2,000,010건의 레코드를 읽은 후 2,000,000건은 버리고 마지막 10건만 사용자에게 반환하는데, 사용자 화면에 보여주는 레코드는 10건이지만 실제 2,000,010건의 레코드를 읽어야 하므로 매우 느려질 수 있다.\n이처럼 LIMIT 조건의 페이징이 처음 몇 개 페이지 조회로 끝나지 않을 가능성이 높다면 WHERE 조건절로 읽어야 할 위치를 찾고 그 위치에서 10개만 읽는 형태의 쿼리를 사용하는 것이 좋다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 첫 페이지 SELECT * FROM salaries ORDER BY salary LIMIT 0, 10; # 두 번째 페이지(첫 페이지의 마지막 salary 값과 emp_no 값 이용) SELECT * FROM salaries WHERE salary \u0026gt;= 38864 AND NOT (salary = 38864 AND emp_no \u0026lt;= 274049) ORDER BY salary LIMIT 0, 10 ; # 마지막 페이지 SELECT * FROM salaries WHERE salary \u0026gt;= 154888 AND NOT (salary = 154888 AND emp_no \u0026lt;= 109334) ORDER BY salary LIMIT 0, 10 ; salary와 같이 중복이 발생할 수 있는 컬럼을 사용하면 조건에 따라서 데이터가 누락, 중복 등으로 의도하지 않은 데이터를 받아올 수 있으므로 주의해야 한다.\nCOUNT() COUNT() 한수는 결과 레코드의 건수를 반호나하는 함수이다. 컬럼이나 표현식을 인자로 받으며, 특별한 형태로 *을 받을 수 있다.\nCOUNT(*)처럼 함수 내에 사용된 *는 SELECT와 달리 레코드 자체를 의미하는 것으로 pk, 1 과 같은 값을 인자로 주지 않아도 동일한 성능을 보여준다. MyISAM 스토리지 엔진을 사용하는 테이블은 테이블의 메타 정보에 전체 레코드 건수를 관리하므로, 조건이 없는 경우 COUNT 처리는 매우 빠르게 처리되지만, 조건이 있는 경우 레코드를 읽지 않는 이상 알 수 없으므로 일반적인 DBMS와 같이 처리된다.\nInnoDB 스토리지 엔진을 사용하는 테이블에서는 조건이 없는 COUNT(*) 쿼리라고 하더라도 직접 데이터나 인덱스를 읽어야만 레코드 건수를 가져올 수 있기 때문에 큰 테이블에서 COUNT() 함수를 사용하는 작업에 주의해야 한다.\nCOUNT(*) 쿼리에서 가장 많이 하는 실수는 ORDER BY 구문이나 체크 조건을 가지지 않는 LEFT JOIN과 같은 레코드 건수를 가져오는 것과는 무관한 작업을 포함하는 것이다.\nCOUNT(*) 쿼리에서 ORDER BY 절은 어떠한 경우에서 필요하지 않으며, LEFT JOIN 또한 레코드 건수의 변화가 없거나 아우터 테이블에서 별도 체크를 하지 않아도 되는 경우 모두 제거해야한다.\n많은 사용자가 일반적으로 컬럼의 값을 SELECT 하는 쿼리보다 COUNT(*) 쿼리가 훨씬 빠르게 실행될 것으로 생각하지만, 인덱스를 제대로 사용하지 못한 COUNT쿼리는 페이징해서 데이터를 가져오는 쿼리보다 매우 느리게 실행 될 수 있다.\nCOUNT() 함수에 컬럼명이나 표현식이 인자로 사용되면 그 컬럼이나 표현식의 결과가 NULL이 아닌 레코드 건수만 반환한다. 따라서 NULL이 될 수 있는 컬럼에 COUNT 함수를 사용할 때는 위도대로 쿼리가 작동하는지 확인이 필요하다.\nJOIN JOIN의 순서와 인덱스 일반적으로 인덱스를 이용해서 쿼리하는 작업에서는 가져오는 레코드 건수가 소량이기 때문에 인덱스 스캔 작업은 부하가 작지만 특정 인덱스키를 찾는 인덱스 탐색 작업은 상대적으로 부하가 높은편이다.\n조인 작업에서 드라이빙 테이블을 읽을 때는 인덱스 탐색 작업을 단 한 번만 수행하고, 그 이후부터는 스캔만 실행한다.\n하지만 드리븐 테이블에서는 인덱스 탐색 작업과 스캔 작업을 드라이빙 테이블에서 읽은 레코드 건수만큼 반복하므로 드라이빙 테이블과 드리븐 테이블이 1:1로 조인되더라도 드리븐 테이블을 읽는 것이 훨씬 더 큰 부하를 차지한다.\n이에 따라 드라이빙 테이블과 드리븐 테이블이 1:1로 조인되더라도 드리븐 테이블을 읽는 것이 훨씬 더 큰 부하를 차지하므로 옵티마이저는 항상 드라이빙 테이블이 아니라 드리븐 테이블을 최적으로 읽을 수 있게 실행 계획을 수립한다.\n1 2 3 4 SELECT * FROM employees e, dept_emp de WHERE e.emp_no = de.emp_no ; 두 컬럼 모두 인덱스가 있는 경우 어느 테이블을 드라이빙으로 선택하든 인덱스를 이용해 드리븐 테이블의 검색 작업을 빠르게 처리할 수 있다. 이러한 경우 옵티마이저가 통계 정보를 이용해 적절히 드라이빙 테이블을 선택하게 된다. 레코드 건수 등 이러한 경우 옵티마이저가 선택하는 방법이 최적인 경우가 많다. e.emp_no에만 인덱스가 있는 경우 dept_emp 테이블이 드리븐 테이블로 선택되면 employees 테이블의 레코드 건수만큼 dept_emp 테이블을 풀 스캔 해야만 조건에 일치하는 레코드를 찾을 수 있다. 따라서 옵티마이저는 항상 dept_no 테이블을 드라이빙 테이블로 선택한다. e.emp_no=1001 같이 employees 테이블을 아주 효율적으로 접근할 수 있는 경우에도 같다. de.emp_no에만 인덱스가 있는 경우 employees 테이블이 반복된 풀 스캔을 피하기 위해 드라이빙 테이블로 선택된다. 두 컬럼 모두 인덱스가 없는 경우 어느 테이블을 드라이빙으로 선택하더라도 풀 스캔이 발생하므로 옵티마이저가 드라이빙 테이블을 적절히 선택한다. 레코드 건수가 적은 테이블을 드라이빙 테이블로 선택하는 것이 훨씬 효율적이다. 조인 조건을 빠르게 처리할 적절한 인덱스가 없는 경우 MySQL 8.0.18 이전 버전은 블록 네스티드 루프 조인, 이후 버전은 해시 조인을 사용한다. JOIN 컬럼의 데이터 타입 WHERE 절에 사용되는 조건에서 표현식의 데이터 타입을 동일하게 사용해야 하는 것과 마찬가지로 조인 조건에서도 동일하게 조인 컬럼 간의 비교에서 각 컬럼의 데이터 타입이 일치하지 않으면 인덱스를 효율적으로 사용할 수 없다.\n비교 조건에서 양쪽 항이 상수이든 테이블의 컬럼이든 관계없이 데이터 타입이 다르다면 대상 컬럼에 대한 타입 변환 후 비교를 수행하므로 인덱스를 활용할 수 없게된다.\n따라서 옵티마이저는 드리븐 테이블이 인덱스 레인지 스캔을 사용하지 못하고, 드리븐 테이블의 풀 테이블 스캔이 필요하게 된다는것을 미리 알기 때문에 조인 버퍼를 이용하여 작업을 수행한다.\n예외 상황\n인덱스 사용에 영향을 미치는 데이터 타입은 변환이 필요한 경우이며, 변환을 하지 않고도 비교할 수 있는 경우 인덱스를 활용할 수 있다.\nCHAR, VARCHAR INT, BIGINT, SMALLINT DATE, DATETIME 대표적으로 아래와 같은 패턴은 문제가 될 가능성이 높다.\nCHAR, INT 비교 같이 데이터 타입의 종류가 완전히 다른 경우 같은 CHAR 타입이라도 문자 집합이나 콜레이션이 다른 경우 같은 INT 타입이더라도 부호의 존재 여부가 다른 경우 이러한 상황을 예방하기 위해 컬럼의 문자 집합, 콜레이션을 통일하는 등 데이터베이스 모델에 대한 표준화 규칙을 수립하고, 규칙을 기반으로 설계를 진행해야 한다.\nOUTER JOIN 성능과 주의사항 INNER JOIN은 조인 대상 테이블에 모두 존재하는 레코드만 결과 집합으로 반환한다. 이러한 특성 때문에 아우터 조인으로만 조인을 실행하는 쿼리들도 자주 보이는데 일부 문제가 발생할 수 있는 여지가 있다.\nMySQL 옵티마이저는 절대로 아우터로 조인되는 테이블을 드라이빙 테이블로 선택하지 못하기 때문에 성능이 떨어지는 실행 계획을 수립할 수 있다. 이너 조인으로 사용할 수 있는 쿼리를 아우터 조인으로 작성하면 옵티마이저가 조인 순서를 변경하며 수행할 수 있는 최적화 기회를 빼앗는 결과를 만들 수 있다. 필요한 데이터가 조인되는 테이블 간의 관계를 명확히 파악해서 꼭 필요한 경우가 아니라면 이너 조인을 사용해야 한다. 아우터 조인으로 조인되는 테이블에 대한 조건을 WHERE 절에 함께 명시하는 것 이다. 아우터 조인 드리븐 테이블 컬럼에 IS NULL이외의 조건이 있는 경우 옵티마이저거 INNER JOIN으로 변경한다. JOIN과 외래키(FOREIGN KEY) 외래키를 생성하는 주목적은 데이터의 참조 무결성을 보장하기 위해서이며, 외래키는 조인 처리와 아무 관계가 없다.\nSQL로 테이블 간의 조인은 전혀 무관한 컬럼을 조인 조건으로 사용해도 문법적으로 문제가 되진 않는다. 하지만 데이터 모델링을 할 때는 각 테이블간의 관계를 표현하지만 외래키로 생성하지 않는 경우가 더 많다. 테이블 간의 조인을 사용하기 위해 외래키가 필요한 것은 아니다. 지연된 조인(Delayed Join) 지연된 조인이란 조인이 실행되기 이전에 GROUP BY, ORDER BY를 처리하는 방식을 의미한다.\n조인을 사용해서 데이터를 조회하는 쿼리에 GROUP BY, ORDER BY를 사용할 때 각 처리 방법에서 인덱스를 사용한다면 이미 최적으로 처리되고 있을 가능성이 높지만, 그렇지 않다면 MySQL 서버는 모든 조인을 실행한 후 GROUP BY, ORDER BY를 처리한다.\n조인은 대체로 실행되면 될수록 레코드 건수가 늘어나므로 GROUP BY, ORDER BY가 수행될 때는 조인 전 레코드에 수행할 때 보다 많은 레코드를 처리해야한다.\n이를 개선하기 위한 최적화이며, 주로 LIMIT와 사용될 때 큰 효과를 얻을 수 있다.\n지연된 조인 미적용\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 EXPLAIN SELECT e.* FROM salaries s, employees e WHERE e.emp_no = s.emp_no AND s.emp_no BETWEEN 10001 AND 13000 GROUP BY s.emp_no ORDER BY SUM(s.salary) DESC LIMIT 10 ; /* +----+-------------+-------+------------+-------+-------------------+---------+---------+--------------------+------+----------+----------------------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+-------------------+---------+---------+--------------------+------+----------+----------------------------------------------+ | 1 | SIMPLE | e | NULL | range | PRIMARY | PRIMARY | 4 | NULL | 3000 | 100.00 | Using where; Using temporary; Using filesort | | 1 | SIMPLE | s | NULL | ref | PRIMARY,ix_salary | PRIMARY | 4 | employees.e.emp_no | 9 | 100.00 | NULL | +----+-------------+-------+------------+-------+-------------------+---------+---------+--------------------+------+----------+----------------------------------------------+ */ employees 테이블을 드라이빙 테이블로 선택하여 조건에 만족하는 레코드를 읽고, salaries 테이블을 조인한다. 조인의 결과 12,000건의 레코드를 임시 테이블에 저장하고 GROUP BY 처리를 통해 3000건으로 줄인다. ORDER BY 처리 후 상위 10건을 반환한다. 지연된 조인 적용\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 EXPLAIN SELECT e.* FROM (SELECT s.emp_no FROM salaries s WHERE s.emp_no BETWEEN 10001 AND 13000 GROUP BY s.emp_no ORDER BY SUM(s.salary) DESC LIMIT 10) x, employees e WHERE e.emp_no = x.emp_no ; /* +----+-------------+------------+------------+--------+-------------------+---------+---------+----------+-------+----------+----------------------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+------------+------------+--------+-------------------+---------+---------+----------+-------+----------+----------------------------------------------+ | 1 | PRIMARY | \u0026lt;derived2\u0026gt; | NULL | ALL | NULL | NULL | NULL | NULL | 10 | 100.00 | NULL | | 1 | PRIMARY | e | NULL | eq_ref | PRIMARY | PRIMARY | 4 | x.emp_no | 1 | 100.00 | NULL | | 2 | DERIVED | s | NULL | range | PRIMARY,ix_salary | PRIMARY | 4 | NULL | 56844 | 100.00 | Using where; Using temporary; Using filesort | +----+-------------+------------+------------+--------+-------------------+---------+---------+----------+-------+----------+----------------------------------------------+ -\u0026gt; Nested loop inner join (cost=5 rows=0) -\u0026gt; Table scan on x (cost=2.5..2.5 rows=0) -\u0026gt; Materialize (cost=0..0 rows=0) -\u0026gt; Limit: 10 row(s) -\u0026gt; Sort: sum(s.salary) DESC, limit input to 10 row(s) per chunk -\u0026gt; Stream results (cost=17107 rows=56844) -\u0026gt; Group aggregate: sum(s.salary) (cost=17107 rows=56844) -\u0026gt; Filter: (s.emp_no between 10001 and 13000) (cost=11423 rows=56844) -\u0026gt; Index range scan on s using PRIMARY over (10001 \u0026lt;= emp_no \u0026lt;= 13000) (cost=11423 rows=56844) -\u0026gt; Single-row index lookup on e using PRIMARY (emp_no=x.emp_no) (cost=0.26 rows=1) */ 지연된 조인으로 개선된 쿼리는 임시 테이블을 한 번 더 사용하기 때문에 느라다고 예상할 수도 있지만, 임시 테이블에 저장할 레코드가 적으므로 메모리를 이용해 빠르게 처리된다. 실행 계획상으로 보면 지연된 조인으로 변경된 쿼리가 느릴 것 같지만 실제로는 3~4배 빠르다.\n지연된 쿼리의 원리를 정확히 이해하지 못한 상태로 작성하면 오히려 역효과가 날 수도 있지만 잘 튜닝된 지연된 쿼리는 원래 쿼리보다 훨씬 개선될 수 있다.\n하지만 모든 쿼리를 지연된 쿼리 형태로 개선할 수 있는것이 아니며 아래 조건을 만족해야한다.\nLEFT (OUTER) JOIN 드라이빙 테이블과 드리븐 테이블은 1:1, M:1 관계여야 한다. INNER JOIN 드라이빙 테이블과 드리븐 테이블은 1:1, M:1 관계여야 한다. 드라이빙 테입르에 있는 레코드는 드리븐 테이블에 모두 존재해야 한다. 두 번째와 세번째 조건은 드라이빙 테이블을 서브쿼리로 만들고 이 서브쿼리에 LIMIT를 추가해도 최종 결과의 건수가 변하지 않는다는 보증을 해주는 조건이기 때문에 반드시 정확히 확인한 후 적용해야 한다. 지연된 조인은 조인의 개수를 줄이는 것뿐만 아니라 GROUP BY, ORDER BY 처리가 필요한 레코드의 전체 크기를 줄이는 역할도 한다.\n래터럴 조인(Lateral Join) MySQL 버전부터는 래터럴 조인이라는 기능을 이용해 특정 그룹별로 서브쿼리를 실행해서 그 결과와 조인하는 것이 가능해졌다.\n1 2 3 4 5 6 7 8 9 10 SELECT * FROM employees e LEFT JOIN LATERAL ( SELECT * FROM salaries s WHERE s.emp_no = e.emp_no ORDER BY s.from_date DESC LIMIT 2 ) s2 ON s2.emp_no = e.emp_no WHERE e.frist_name = \u0026#39;Matt\u0026#39; ; 위 쿼리는 employees 테입르에서 이름이 Matt인 사원에 대해 사원별로 가장 최근 급여 변경 내역을 최대 2건씩만 반환한다.\n래터럴 조인에서 가장 중요한 부분은 FROM 절에 사용된 서브쿼리에서 외부 쿼리의 FROM 절에 정의된 테이블의 컬럼을 참조할 수 있다는 점이다. FROM 절에 사용하는 서브쿼리가 외부 쿼리의 컬럼을 참조하기 위해서는 LATERAL 키워드를 명시해야하며 누락된다면 오류를 발생시킨다.\nLATERAL 키워드를 가진 서브쿼리는 조인 순서상 후순위로 밀리고, 외부 쿼리의 결과 레코드 단위로 임시 테이블이 생성되기 때문에 꼭 필요한 경우에만 사용해야 한다.\n실행 계획으로 인한 정렬 흐트러짐 MySQL 8.0 이전 버전 까지는 알고리즘 특성상 테이블에서 읽은 순서가 유지되는 네스티드-루프 방식의 조인만 가능했지만, 정렬 순서가 바뀌는 해시 조인이 도입되면서 선택되는 정렬 알고리즘에 따라 순서가 보장되지 않을 수 있다.\n실행 계획은 옵티마이저에 의해 달라질 수 있으므로 정렬된 결과가 필요한 경우라면 드라이빙 테이블의 순서에 의존하지 말고 ORDER BY절을 사용하는 것이 좋다.\n","date":"2023-07-21T11:13:10+09:00","image":"https://codemario318.github.io/post/real-mysql/11/4/1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/11/4/1/","title":"11.4 쿼리 작성 및 최적화 - SELECT (1)"},{"content":"컬럼 이름과 달리 쿼리의 실행 계획에서 성능에 관련된 중요한 내용이 Extra 컬럼에 지주 표시된다.\n고정된 몇개의 문장이 표시되는데, 일반적으로 2~3개씩 함께 표시된다. 내부적인 처리 알고리즘에 대해 조금 더 깊이 있는 내용을 보여주는 경우가 많다. const row not found 쿼리의 실행 계획에서 const 접근 방법으로 테이블을 읽었지만 실제로 해당 테이블에 레코드가 1건도 존재하지 않을 경우 표시된다.\nDeleting all rows MyISAM 스토리지 엔진과 같이 스토리지 엔진의 핸들러 차원에서 테이블의 모든 레코드를 삭제하는 기능을 제공하는 스토리지 엔진의 경우 표시된다.\nWHERE 조건절이 없는 DELETE 문장의 실행 계획에서 자주 표시된다. 테이블의 모든 레코드를 삭제하는 핸들러 기능(API)을 한번 호출해서 처리되었다는 것을 의미한다. MySQL 8.0 버전부터 InnoDB, MyISAM 스토리지 엔진 모두 해당 최적하는 표시하지 않으며, 테이블의 모든 레코드를 삭제하고자 한다면 WHERE 조건절이 없는 DELETE 보다 TURNCATE TABLE 명령을 사용하는 것을 권장한다.\nDistinct 쿼리의 DISTINCT를 처리하기 위해 조인하지 않아도 되는 항목은 무시하고 곡 필요한 것만 조인했을 경우 표시된다.\nFirstMatch 세미 조인 최적화 중에서 FirstMatch 전략이 사용되면 표시된다.\nFull scan on NULL key col1 IN (SELECT col2 FROM ...)과 같은 조건을 가진 쿼리에서 col1의 값이 NULL이 가능할 경우 표시된다.\ncol1의 값이 NULL 일 경우 결과적으로 조건은 NULL IN (SELECT col2 FROM ...)로 처리되는데, 이 때 차선책으로 서브쿼리 테이블에 대해 풀 테이블 스캔을 사용할 것이라는 사실을 알려주는 키워드이다.\nIN, NOT IN 연산자 왼쪽 값이 NULL인 레코드가 있고 개별적으로 WHERE 조건이 지정되어 있다면 성능에 큰 문제가 발생할 수 있다.\ncol1이 NOT NULL로 정의된 컬럼이라면 표시되지 않는다. NULL 비교 조건을 무시해도 괜찮다면 IS NOT NULL 조건을 추가하면 표시되지 않는다. IN, NOT IN 연산자 왼쪽에 있는 값에 실제로 NULL이 없다면 풀 테이블 스캔은 발생하지 않는다. SQL 표준에서 NULL을 알 수 없는 값으로 정의하고 있으며 NULL 연산에 대한 규칙을 정의하고 있다.\n서브쿼리가 1건이라도 결과 레코드를 가진다면 최종 비교 결과는 NULL 서브쿼리가 1건도 결과 레코드를 가지지 않는다면 최종 비교 결과는 FALSE 이러한 비교 과정에서 col1이 NULL이면 서브쿼리에 사용된 테이블에 대해 풀 테이블 스캔을 해야만 결과를 알아낼 수 있다. Impossible HAVING 쿼리에 사용된 HAVING 절의 조건을 만족하는 레코드가 없을 때 표시된다.\n애플리케이션의 쿼리 중에서 해당 메시지가 출력된다면 쿼리가 제대로 작성되지 못한 경우가 대부분이므로 쿼리의 내용을 다시 점검하는 것이 좋다. Impossible WHERE WHERE 조건이 항상 FALSE가 될 수 밖에 없는 경우 표시된다.\nLooseScan 세미 조인 최적화 중에서 LooseScan 최적화 전략이 사용될 경우 표시된다.\nNo matching (min/max) row MIN(), MAX()와 같은 집합 한수가 있는 쿼리의 조건절에 일치하는 레코드가 한 건도 없을 경우 출력되며, 결과로 NULL이 반환된다.\nno matching row in const table 조인에서 사용된 테이블에서 const 접근 방법으로 접근할 때 일치하는 레코드가 없을경우 표시된다.\nNo matching rows after partition pruning 파티션된 테이블에 대한 UPDATE, DELETE 명령 실행시 해당 파티션에서 대상 레코드가 없을 경우 표시된다.\nNo tables used FROM 절이 없는 쿼리 문장이나 FROM DUAL 형태의 쿼리 실행 계획에서 출력된다.\nMySQL 서버는 FROM 절이 없는 쿼리도 허용된다.\nNot exists 아우터 조인을 이용해 안티 조인을 수행하는 쿼리에서 표시된다.\n옵티마이저가 테이블의 레코드를 이용해 테이블을 조인할 때 레코드가 존재하는지만 판단한다는 것을 의미한다. 조인 조건에 일치하는 레코드가 여러 건 있다고 하더라도 1건만 조회해보고 처리를 완료화는 최적화를 의미한다. 안티 조인(Anti-join)이란?\nA 테이블에서는 존재하지만 B 테이블에는 없는 값을 조회해야 하는 경우 NOT IN (subquery), NOT EXISTS 연산자를 사용하는데 이러한 형태의 조인을 안티-조인이라고 한다.\n똑같은 처리를 아우터 조인을 이용해서도 구현할 수 있는데 레코드 건수가 많을 경우 아우터 조인을 이용하면 더 빠른 성능을 낼 수 있다.\nPlan isn\u0026rsquo;t ready yet MySQL 8.0 버전에서는 다른 커넥션에서 실행 중인 쿼리의 실행 계획을 EXPLAIN FOR CONNECTION 명령으로 확인할 수 있는데, 이 때 해당 커넥션에서 아직 쿼리의 실행 계획을 수립하지 못한 경우 해당 메시지가 출력된다.\n이 경우 대상 커넥션의 쿼리가 실행 계획을 수립할 여유 시간을 주고 다시 명령을 실행하면 된다. Range checked for each record(index map:N) 1 2 3 4 5 6 EXPLAIN SELECT * FROM employees e1 , employees e2 WHERE e2.emp_no \u0026gt;= e1.emp_no ; 두 개의 테이블을 조인하는 경우 조인 조건에 상수가 없고 둘 다 변수인 경우 MySQL 옵티마이저는 e1의 레코드를 읽을 때마다 e1.emp_no 값이 계속 변화하므로 쿼리의 비용 계산을 위한 기준값이 바뀌므로 어떤 접근 방법으로 e2 테이블을 읽는 것이 좋을지 판단할 수 없다.\n조건에 만족하는 레코드를 조회하기 위해 e1 테이블을 처음부터 끝까지 스캔하며 e2 테이블에서 조건을 비교해야 한다.\n사번이 1 ~ 1억 번 까지 있다고 가정하면 e1.emp_no가 1인 경우 e2 테이블의 1억 건을 모두 읽어야 하고, e1.emp_no가 1억인 경우 e2 테이블을 한 건만 읽으면 된다.\n따라서 e1 테이블의 emp_no가 작을 때는 e2 테이블을 풀 테이블 스캔으로 접근하고, e1 테이블의 emp_no가 큰 값일 때는 e2 테이블을 레인지 스캔으로 접근하는 형태로 수행되는 것이 최적의 조인 방법이다.\n이렇게 레코드마다 인덱스 레인지 스캔 사용이 더 적합한지 체크하는 실행 계획이 활용될 경우 \u0026ldquo;Range checked for each record\u0026quot;가 노출되고, 출력 내용에서 표시되는 \u0026ldquo;(index map: N)\u0026ldquo;은 인덱스가 활용될 경우 사용될 후보 인덱스 순번의 16진수로 값으로 실제 확인 시 어떤 레코드가 사용될지는 알 수 없다.\nRecursive MySQL 8.0 버전부터 CTE(Common Table Expression)를 이용해 재귀 쿼리를 작성할 수 있게 되었으며, 재귀 쿼리의 실행 계획은 Recursive로 표시된다.\n1 2 3 4 5 6 7 8 WITH RECURSIVE cte (n) AS ( SELECT 1 UNION ALL SELECT n + 1 FROM cte WHERE n \u0026lt; 5 ) SELECT * FROM cte ; \u0026rsquo;n\u0026rsquo; 이라는 컬럼 하나를 가진 cte라는 이름의 내부 임시 테이블 생성 \u0026rsquo;n\u0026rsquo; 컬럼의 값이 1부터 5 까지 1씩 증가시켜 레코드를 5건 만들고 cte 내부 임시 테이블에 저장 WITH 구문을 이용한 CTE가 사용됐다고 반드시 \u0026ldquo;Recursive\u0026rdquo; 문구가 표시되는 것이 아닌, WITH 구분이 **재귀 CTE(Recursive CTE)**로 사용될 경우에만 메시지가 표시된다.\nRematerialize MySQL 버전부터 래터럴 조인(LATERAL JOIN) 기능이 추가되었는데, 래터럴로 조인되는 테이블은 선행 테이블의 레코드별로 서브 쿼리를 실행해서 그 결과를 임시 테이블에 저장한다. 이 과정을 \u0026ldquo;Rematerializing\u0026rdquo; 이라고 한다.\n1 2 3 4 5 6 7 8 9 10 11 SELECT * FROM employees e LEFT JOIN LATERAL ( SELECT * FROM salaries s WHERE s.emp_no=e.emp_no ORDER BY s.from_date DESC LIMIT 2 ) s2 ON s2.emp_no=e.emp_no WHERE e.first_name=\u0026#39;Matt\u0026#39; ; 래터럴 조인\nFROM절 서브쿼리에서 메인쿼리를 활용한 조건을 활용할 수 있도록 한다. 따라서 선행 테이블의 레코드 별로 서브 쿼리 실행이 필요하다.\n(WHERE s.emp_no=e.emp_no)\nSelect tables optimized away MIN(), MAX()만 SELECT 절에 사용되거나 GROUP BY로 MIN(), MAX()를 조회하는 쿼리가 인덱스를 오름차순 또는 내림차순으로 1건만 읽는 형태의 최적화가 적용될 경우 노출된다.\nMyISAM 테이블은 전체 레코드 건수를 별도로 관리하기 때문에GROUP BY 없이 COUNT(*)만 SELECT 하는 경우에도 이러한 형태의 최적화가 발생한다. 하지만 WHERE 조건절이 있는 쿼리는 이러한 최적화를 적용하지 못한다.\nStart temporary, End temporary 세미 조인 최적화 중 Duplicate Weed-out 최적화 전략이 사용되면 표시된다.\nDuplicate Weed-out 최적화 전략은 불필요한 중복 건을 제거하기 위해 내부 임시 테이블을 사용하는데, 이때 조인되어 내부 임시 테이블에 저장되는 테이블을 식별할 수 있도록 조인 첫 테이블에 \u0026ldquo;Start temporary\u0026rdquo;, 조인이 끝나는 부분에 \u0026ldquo;End temporary\u0026rdquo; 문구를 표시한다.\nUnique row not found 두 개의 테이블이 각각 유니크 컬럼으로 아우터 조인을 수행하는 쿼리에서 아우터 테이블과 일치하는 레코드가 존재하지 않을때 표시된다.\nUsing filesort ORDER BY를 처리하기 위해 인덱스를 사용하지 못할 때는 조회된 레코드를 다시 정렬하는데, 이처럼 인덱스를 사용하지 못하여 이미 조회된 레코드를 정렬용 메모리 버퍼에 복사하여 정렬을 수행하게 된 경우 표시된다.\nUsing filesort 문구가 출력되는 쿼리는 많은 부하를 일으키므로 쿼리를 튜닝하거나 인덱스를 생성하는 것이 좋다.\nUsing index(커버링 인덱스) 데이터 파일을 전혀 읽지 않고 인덱스만 읽어 쿼리를 모두 처리할 수 있을 때 표시된다.\n인덱스를 이용해 처리하는 쿼리에서 가장 큰 부하를 차지하는 부분은 인덱스 검색에서 일치하는 키 값들의 레코드를 읽기 위해 데이터 파일을 검색하는 작업으로, 최악의 경우 모든 레코드에 대해 디스크를 읽어야 할 수 있다.\n커버링 인덱스는 인덱스를 통해 모든 레코드를 처리할 수 있는 경우를 의미하며 매우 빠른 조회 성능을 보여준다.\n하지만 무조건 커버링 인덱스로 처리하려고 인덱스에 많은 컬럼을 추가하면 과도하게 인덱스 컬럼이 많아지면서 메모리 낭비가 심해지고, 레코드를 저장하거나 변경하는 작업이 매우 느려질 수 있다.\n접근 방법(실행 계획의 type 컬럼)이 eq_ref, ref, range, index_merge, index 등과 같이 인덱스를 사용하는 모든 실행 계획에 표시될 수 있으며, 인덱스 풀 스캔을 실행할 때도 커버링 인덱스로 처리되면 아닌 경우보다 훨씬 빠르게 처리된다.\nUsing index condition 옵티마이저가 인덱스 컨디션 푸시다운 최적화를 사용할 경우 표시된다.\nUsing index for group-by GROUP BY 처리가 인덱스를 활용해 추가 정렬 작업 없이 수행될 경우 표시된다.\nGROUP BY 처리를 위해 그루핑 컬럼을 이용해 정렬 작업을 수행하고 다시 정렬된 결과를 그루핑 하는 형태의 고부하 작업을 수행한다. 이러한 처리가 인덱스(B-Tree 인덱스에 한해)를 이용하면 별도의 추가 정렬 작업 없이 정렬된 인덱스 컬럼을 순서대로 읽으며 그루핑 작업만 수행하게 되어 효율적이고 빠르게 처리된다.\n타이트 인덱스 스캔을 통한 GROUP BY 처리 AVG(), SUM(), COUNT() 처럼 조회하려는 값이 모든 인덱스를 다 읽어야 할 경우 필요한 레코드만 읽을 수가 없는 경우 인덱스를 활용하지만, 루스 인덱스 스캔이라 하지 않으며, 이 에따라 실행 계획에는 표시되지 않는다.\n루스 인덱스 스캔을 통한 GROUP BY 처리 루스 인덱스 스캔은 필요한 부분만 활용하기 위해 인덱스를 듬성듬성 읽는 처리를 의미한다.\n단일 컬럼으로 구성된 인덱스에서는 그루핑 컬럼 말고는 아무것도 조회하지 않는 쿼리에서 루스 인덱스 스캔을 활용할 수 있다. 다중 컬럼으로 만들어진 인덱스에서는 \u0026lsquo;GROUP BY\u0026rsquo; 절이 인덱스를 사용할 수 있고, MIN(), MAX() 같이 첫 번째, 마지막 레코드만 읽어도 되는 경우 루스 인덱스 스캔을 사용할 수 있다. GROUP BY에서 인덱스를 사용하려면GROUP BY 조건에서 인덱스르 사용할 수 있는 조건이 갖춰져야 하지만 그 이전에 WHERE 절에서 사용하는 인덱스에서도 영향을 받는다.\nWHERE 조건절이 없는 경우 GROUP BY 절의 컬럼과 SELECT로 가져오는 컬럼이 루스 인덱스 스캔을 사용할 수 있는 조건을 갖추면 된다. 그렇지 못할 경우 타이트 인덱스 스캔이나 별도 정렬 과정을 통해 처리된다. WHERE 조건절이 있지만 검색을 위해 인덱스를 사용하지 못하는 경우 GROUP BY 절은 인덱스를 사용할 수 있지만 WHERE 조건이 인덱스를 사용하지 못하는 경우 GROUP BY를 위해 인덱스를 읽은 후 WHERE 조건의 비교를 위해 데이터 레코드를 읽어야 한다. 루스 인덱스 스캔을 활용할 수 없어 타이트 인덱스 스캔을 통해 처리된다. WHERE 절의 조건이 있고, 검색을 위해 인덱스를 사용하는 경우 하나의 단위 쿼리가 실행되는 경우에 index_merge 이외의 접근 방법에서는 단 하나의 인덱스만 사용할 수 있다. WHERE 절의 조건과 GROUP BY 처리가 똑같은 인덱스를 공통으로 사용할 수 있는 경우 루스 인ㄷ게스 스캔을 사용할 수 있다. 인덱스가 다른 경우 옵티마이저는 일반적으로 WHERE 조건 절이 작업 범위를 좁히는 못한다고 하더라도 인덱스를 사용하도록 실행 계획을 수립하는 경향을 보인다. 루스 인덱스 스캔은 주로 대량의 레코드를 GROUP BY하는 경우 성능 향상 효과가 있을 수 있다. 따라서 WHERE, GROUP BY 모두 인덱스를 사용할 수 있는 경우라도 WHERE 조건에 의해 검색된 레코드 건수가 적으면 루스 인덱스 스캔을 사용하지 않아도 충분히 빠르므로 사용하지 않는다.\n루스 인덱스를 이용한 처리보다 사용하지 않는 경우가 더 빠르다고 판단한다면 사용하지 않는다.\nUsing index for skip scan 인덱스 스킵 스캔 최적화를 사용될 경우 표시된다.\nUsing join buffer 테이블 조인시 조인 버퍼가 사용되는 쿼리의 실행 계획에 표시된다.\n일반적으로 빠른 쿼리 실행을 위해 조인되는 컬럼은 인덱스를 생성하는데, 실제 조인에 필요한 인덱스는 조인되는 양쪽 테이블 컬럼이 아니라 조인에서 뒤에 읽는 테이블의 컬럼이다. 드리븐 테이블이 검색 위주로 사용되기 때문에 인덱스가 없으면 성능에 미치는 영향이 커지므로, 조인되는 두 테이블에 있는 각 컬럼에서 인덱스를 조사하고, 인덱스가 없는 테이블을 먼저 읽어 조인을 실행한다. 드리븐 테이블에 검색을 위한 적절한 인덱스가 없다면 블록 네스티드 루프 조인이나 해시 조인을 사용하는데, 이때 조인 버퍼를 사용하게 되며 해당 문구가 노출된다. 카테시안 조인을 수행하는 쿼리는 항상 조인 버퍼를 이용한다. Block Nested Loop, Batched Key Access, Hash join 등 사용된 알고리즘이 같이 표시된다. Using MRR MMR(Multi Range Read) 최적화가 사용된 경우 노출된다.\nMMR? MySQL 엔진이 여러 개의 키 값을 한번에 스토리지 엔진으로 전달하고, 스토리지 엔진은 넘겨받은 키 값들을 정렬해서 최소한의 페이지 접근마능로 필요한 레코드를 읽는 최적화 방법이다.\nInnoDB를 포한한 스토리지 엔진 레벨에서는 쿼리 실행의 전체적인 부분을 알지 못하기 때문에 최적화가 한계가 있다. 아무리 많은 레코드를 읽는 과정이라 하더라도 스토리지 엔진은 MySQL 엔진이 넘겨주는 키 값을 기준으로 레코드를 한 건씩 읽어 반환하는 방식으로 작동한은 한계점이 있었다. 매번 읽어서 반환하는 레코드가 동일 페이지에 있다고 하더라도 레코드 단위로 API 호출이 필요하다. Using sort_union, Using union, Using intersect 쿼리가 index_merge 접근 방법으로 실행되는 경우 2개 이상의 인덱스가 동시에 사용될 수 있는데, 이 때 두 인덱스로부터 읽은 결과를 어떻게 병합했는지 조금 더 상세하게 설명하기 위해 3개 중 하나의 메시지를 선택적으로 출력한다.\nUsing intersect(\u0026hellip;) 각 인덱스를 사용할 수 있는 조건이 AND로 연결된 경우 각 처리 결과에서 교집합을 추출하는 작업을 수행한 경우 Using union(\u0026hellip;) 각 인덱스를 사용할 수 있는 조건이 OR로 연결된 경우 각 처리 결과에서 합집합을 추출해내는 작업 수행 Using sort_union(\u0026hellip;) Using union과 같은 작업을 수행하지만 Using union으로 처리될 수 없는 경우(ex: OR로 연결된 대량의 range 조건들) 이 방식으로 처리됨 프라이머리 키만 먼저 읽어 절렬하고 병합한 이후 레코드를 읽어서 반환할 수 있다. 실제로 레코드 건수에 관계 없이 WHERE 조건이 사용된 비교 조건이 모두 동등하다면 Using union, 아닐 경우 Using sort_union이 사용된다.\nUsing temporary 쿼리가 임시 테이블을 사용하여 처리된 경우 표시된다.\n사용된 임시 테이블이 메모리, 디스크 중 어디에 생성되었는지는 실행 계획으로 파악할 수 없다. Using temporary가 표시되지 않아도 내부적으로 임시 테이블을 사용할 때도 많다. FROM 절에 사용된 서브 쿼리는 무조건 임시 테이블을 생성한다.(파생 테이블) COUNT(DISTINCT col1)를 포함하는 쿼리도 인덱스를 사용할 수 없다면 임시 테이블을 만든다. UNION, UNION DISTINCT가 사용된 쿼리는 항상 임시 테이블을 사용해 결과를 병합한다. MySQL 8.0 부터 UNION ALL의 경우는 임시 테이블을 생성하지 않는다. 인덱스를 사용하지 못하는 정렬 작업은 임시 버퍼 공간을 사용하는데, 정렬해야 할 레코드가 많이지면 디스크를 사용하며, 디스크에 사용된 버퍼도 임시 테이블과 동일하다. 쿼리가 정렬을 수행할 때 실행 계획의 Extra 컬럼에 Using filesort가 표시된다. Using where MySQL 엔진은 내부적으로 크게 MySQL 엔진과 스토리지 엔진 2개의 레이어로 나눠볼 수 있다.\n스토리지 엔진은 디스크나 메모리상에서 필요한 레코드를 읽는 역할 수행 MySQL 엔진은 스토리지 엔진으로부터 받은 레코드를 가공 또는 연산하는 작업 수행. 이 때 MySQL 엔진 레이어에서 별도의 가공을 해서 필터링 작업을 처리한 경우 Using where가 표시된다. 실행 계획에서 Using where 문구가 노출된 이유를 이해할 수 없는 경우가 많아 성능상의 문제를 일으킬지 선별하는 능력이 필요한데, MySQL 8.0에서는 실행 계획에 filtered 컬럼이 표시되므로 성능상의 이슈가 있는지 알아낼 수 있다.\nZero limit MySQL 서버에서 데이터 값이 아닌 쿼리의 결과값의 메타데이터만 필요한 경우 LIMIT 0를 사용하는데, 이때 옵티마이저가 사용자의 의도를 알아채고 실제 테이블의 레코드는 전혀 읽지 않으며, 이러한 경우 해당 문구가 노출된다.\n","date":"2023-07-12T15:19:10+09:00","image":"https://codemario318.github.io/post/real-mysql/10/3/2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/10/3/2/","title":"10.3 실행 계획 - 실행 계획 분석 (2) Extra 컬럼"},{"content":"EXPLAIN 명령을 실행하면 쿼리 문장의 특성에 따라 표 형태로 1줄 이상의 결과가 표시된다.\n표의 각 라인(레코드)은 쿼리 문장에서 사용된 테이블(임시 테이블 포함)의 개수만큼 출력된다.\n실행 순서는 일반적으로 위에서 아래로 순서대로 표시되며 출력된 실행 계획에서 위쪽에 출력된 결과일수록 쿼리의 바깥 부분이거나 먼저 접근한 테이블이고, 아래쪽에 출력된 결과일수록 쿼리의 안쪽 부분 또는 나중에 접근한 테이블에 해당한다.\nid 컬럼 1 2 3 SELECT ... FROM (SELECT ... FROM tb_test1) tb1, tb_test2 tb2 WHERE tb1.id = tb2.id; 위 쿼리의 SELECT는 2개의 SELECT로 분리해서 생각해볼 수 있고, 이렇게 분리한 단위로 구분한 것을 단위 쿼리 나눌 수 있다.\n1 2 SELECT ... FROM tb_test1; SELECT ... FROM tb1, tb_test2 tb2 WHERE tb1.id = tb2.id; 실행 계획에서 가장 왼쪽에 표시되는 id 컬럼은 단위 SELECT 쿼리별로 부여되는 식별자 값이다. 하나의 테이블을 조인하면 조인되는 테이블의 개수만큼 실행 계획에 레코드가 출력되지만 같은 id 값이 부여된다. 1 2 EXPLAIN SELECT ((SELECT COUNT(*) FROM employees) + (SELECT COUNT(*) FROM departments)) AS total_count; 위 쿼리의 실행 계획에서는 쿼리 문장이 3개 단위의 SELECT 쿼리로 구성되어있으므로 실행 계획에 각 레코드가 각기 다른 id값을 가지게 된다.\n실행 계획의 id 컬럼이 테이블의 접근 순서를 의미하는 것은 아니다. select_type 컬럼 각 단위 SELECT 쿼리가 어떤 타입의ㅏ 쿼리인지 표시되는 컬럼이다.\nSIMPLE UNION, 서브쿼리를 사용하지 않는 단순한 SELECT 쿼리인 경우 해당 쿼리 문장의 select_type은 SIMPLE로 표시된다. (쿼리에 조인이 포함된 경우 까지)\n쿼리 문장이 아무리 복잡하더라도 실행 계획에서 select_type이 PRIMARY인 단위 SELECT 쿼리는 하나만 존재한다. 일반적으로 쿼리의 제일 바깥 SELECT 쿼리가 SIMPLE로 표시된다. PRIMARY UNION, 서브쿼리를 가지는 SELECT 쿼리의 실행 계획에서 가장 바깥쪽에 있는 단위 쿼리는 PRIMARY로 표시된다.\nSIMPLE과 마찬가지로 PRIMARY인 단위 쿼리는 하나만 존재한다. 쿼리의 제일 바깥쪽에 있는 단위 쿼리가 PRIMARY로 표시된다. UNION UNION으로 결합하는 단위 쿼리 가운데 첫 번째를 제외한 단위 쿼리의 select_type은 UNION으로 표시된다.\nUNION의 첫번째 단위 쿼리는 UNION되는 쿼리 결과들을 모아서 자장하는 임시테이블(DRIVED)이 select_type으로 표시된다. DEPENDENT UNION UNION, UNION ALL로 집합을 결함하는 쿼리에서 표시된다.\nDEPENDENT는 UNION, UNION ALL로 결함된 단위 퀄이가 외부 쿼리에 의해 영향을 받는 것을 의미한다. UNION RESULT UNION 결과를 담아두는 테이블을 의미한다.\nMySQL 8.0 이전 버전에서는 UNION, UNION ALL 쿼리는 모두 UNION의 결과를 임시 테이블로 생성했는데, UNION ALL은 임시 테이블을 사용하지 않도록 개선되어 임시 테이블 버퍼링이 필요한 UNION, UNION DISTINCT 쿼리 사용시 노출된다.\n실행 계획상에서 해당 임시 테이블을 가리키는 라인의 select_type이 UNION RESULT이다. UNION RESULT는 실제 쿼리에서 단위 쿼리가 아니기 때문에 id 값이 부여되지 않는다. SUBQUERY 일반적으로 서브쿼리라고 하면 여러 가지를 통틀에서 말하지만, select_type의 SUBQUERY는 FROM 절 이외에서 사용되는 서브쿼리를 의미한다.\nMySQL 서버의 실행 계획에서 FROM 절에 사용된 서브쿼리는 DERIVED로 표시된다.\n서브쿼리\n사용된 위치 중첩된 서브쿼리(Nested Query): SELECT 되는 컬럼에 사용된 경우 서브쿼리(Subquery): WHERE 절에서 사용된 경우 파생 테이블(Derived Table): FROM 절에 사용된 경우 다른 RDBMS에서는 일반적으로 인라인 뷰(Inline View), 서브 셀렉트(Sub Select)라고 한다. 반환하는 값의 특성 스칼라 서브쿼리(Scalar Subquery): 하나의 값만 (컬럼이 단 하나인 레코드 1건만) 반환. 로우 서브쿼리(Row Subquery): 컬럼의 개수와 관계없이 하나의 레코드만 반환하는 쿼리 DEPENDENT SUBQUERY 서브쿼리가 바깥쪽 SELECT 쿼리에서 정의된 컬럼을 사용하는 경우 DEPENDENT SUBQUERY라고 표시된다.\n안쪽의 서브쿼리 결과가 바깥쪽 SELECT 쿼리의 컬럼에 의존적이기 때문에 DEPENDENT라는 키워드가 붙는다. DEPENDENT UNION, DEPENDENT SUBQUERY 또한 외부 쿼리가 먼저 수행된 후 내부 쿼리가 실행되어야 하므로 일반 서브쿼리보다는 처리 속도가 느린 경우가 많다. DERIVED DERIVED는 단위 쿼리의 실행 결과로 메모리나 디스크에 임시 테이블을 생성하는 것을 의미한다.\nselect_type이 DERIVED인 경우에 생성되는 임시 테이블을 파생 테이블이라고 한다. MySQL 5.5 버전까지는 파생 테이블에 인덱스가 전혀 없으므로 다른 테이블과 조인할 때 성능상 불리할 때가 많다. FROM 절의 서브 쿼리를 제거하고 조인으로 처리할 수 있는 형태일 경우 다른 RDBMS에서는 쿼리를 재작성하는 형태의 최적화 기능을 제공한다. MySQL 8.0 이전 버전에서는 여전히 파생 테이블을 생성하기 때문에 실행 계획을 조인 형태로 변경하는 것이 좋다.\n또한, 옵티마이저가 처리할 수 있는 것은 한계가 있으므로 최적화된 쿼리를 작성하는 것이 중요하다.\nDEPENDENT DERIVED MySQL 8.0 버전부터 래터럴 조인(LATERAL JOIN) 기능 추가로 FROM 절의 서브쿼리에서도 외부 컬럼을 참조할 수 있게 되었다. 이로 인해 외부 영향을 받는 파생 테이블이 생성될 경우 표시된다.\nUNCACHEABLE SUBQUERY 하나의 쿼리 문장에 서브쿼리가 하나만 있더라도 실제 해당 서브쿼리가 한 번만 실행되는 것은 아니기 때문에 조건이 똑같은 서브쿼리가 실행될 경우 다시 실행하지 않고 이전 실행 결과를 그대로 사용할 수 있게 서브쿼리의 결과를 내부적인 캐시 공간에 담아둔다.\n특히 DEPENDENT SUBQUERY의 경우 서브쿼리의 결과가 외부 쿼리의 값 단위로 캐시가 만들어진다. 하지만 서브쿼리에 포함된 요소에 의해 캐시 자체가 불가능 할 수 있는데 이러한 경우 UNCACHEABLE SUBQUERY로 표시된다.\n캐시를 사용하지 못하는 경우\n사용자 변수가 서브쿼리에 사용 NOT-DETERMNISTIC 속성의 스토어드 루틴이 서브쿼리 내에 사용된 경우 UUID(), RAND() 같은 결괏값이 호출할 때마다 달라지는 함수가 서브쿼리에 사용된 경우 UNCACHEABLE UNION UNION과 UNCACHEABLE 속성이 혼합된 select_type\nMATERIALIZED FROM, IN (subquery) 형태의 쿼리에 사용된 서브쿼리가 내용을 임시 테이블로 구체화(Meterialization)한 후, 임시테이블과 대상 테이블이 조인되는 형태로 최적화가 적용될 경우 표시되는 select_type이다.\nMySQL 5.6 버전부터 도입되었다. DERIVED와 비슷하게 쿼리의 내용을 임시 테이블로 생성한다. table 컬럼 MySQL 서버의 실행 계획은 단위 SELECT 쿼리 기준이 아닌 테이블 기준으로 표시되는데, 테이블 이름에 별칭이 부여된 경우 별칭이 표시된다.\n\u0026lt;\u0026gt;로 둘러싸인 이름이 면시된 경우 임시 테이블을 의미한다. 안에 항상 표시되는 숫자는 SELECT 쿼리의 id 값을 지칭하게 된다. partitions 컬럼 MySQL 5.7 버전까지는 옵티마이저가 사용하는 파티션들의 목록은 EXPLAIN PARTITION 명령을 통해 확인 가능했지만, 8.0 버전부터 EXPLAIN 명령으로 파티션 관련 실행 계획까지 모두 확인할 수 있다.\n파티션을 사용할 경우 쿼리에서 조회하려는 데이터가 특정 파티션에 위치하게 되므로, 실행 계획 수립시 조건에 해당하지 않는 파티션에 대해 분석을 실행하지 않는다. (파티션 프루닝: Partition pruning)\n따라서 쿼리의 실행 계획을 통해 어느 파티션을 읽는지 확인할 수 있어야 쿼리 튜닝이 가능하며, 파티션을 참조하는 쿼리의 경우 옵티마이저가 쿼리 처리를 위해 필요한 파티션들의 목록만 모아 실행 계획의 partitions 컬럼에 표시해준다.\ntype 컬럼 MySQL 서버가 각 테이블의 레코드를 어떤 방식으로 읽었는지를 나타낸다. 일반적으로 쿼리를 튜닝할 때 인덱스를 효율적으로 사용하는지 확인하는 것이 중요하므로 실행 계획에서 type 컬럼은 반드시 체크해야한다.\nALL을 제외한 나머지는모두 인덱스를 사용하는 접근 방법이다. index_merge를 제외한 나머지 접근 방법은 하나의 인덱스만 사용한다. system 레코드가 1건만 존재하는 테이블 또는 한 건도 존재하지 않는 테이블을 참조하는 형태의 접근 방법이다.\nInnoDB 스토리지 엔진을 사용하는 테이블에서는 나타나지 않고, MyISAM, MEMORY 테이블에서만 사용되는 접근 방식이다. 테이블에 레코드가 1건 이하인 경우에만 사용할 수 있는 접근 방법이므로 실제 애플리케이션에서 사용되는 쿼리에서는 거의 보이지 않는다. const 테이블의 레코드 건수와 관계없이 쿼리가 프라이머리 키나 유니크 키 컬럼을 이용하는 WHERE 조건절을 가지고 있으며, 반드시 1건을 반환하는 쿼리의 처리방식이다.\n다중 컬럼으로 구성된 프라이머리 키나 유니크 키 중에서 인덱스의 일부 컬럼만 조건으로 사용할 때는 const 접근 방법을 사용할 수 없다. 실제로 레코드가 1건만 저장돼 있더라도 데이터를 읽어보지 않고서는 레코드가 1건이라는 것을 확신할 수 없기 때문 프라이머리 키의 일부만 조건으로 사용할 때는 ref로 표시된다. 프라이머리 키나 유니크 인덱스의 모든 컬럼을 동등 조건으로 명시하면 const 접근 방법을 사용한다. MySQL의 옵티마이저가 쿼리를 최적화하는 단계에서 쿼리를 먼저 실행화여 통째로 상수화 하기 때문에 컬럼의 값이 const로 표시된다.\neq_ref 여러 테이블이 조인되는 쿼리의 실행 계획에서만 표시된다. 조인에서 처음 읽은 테이블의 컬럼값을, 그다음 읽어야 할 테이블의 프라이머리 키나 유니크 키 컬럼의 검색 조건에 사용할 때를 가르켜 eq_ref라고 한다.\n조인에서 두 번째 이후에 읽는 테이블에서 반드시 1건만 존재한다는 보장이 있어야 사용할 수 있는 접근 방법이다.\n두 번째 이후에 읽는 테이블이 eq_ref가 표시된다. 두 번째 이후에 읽히는 테입르을 유니크 키로 검색할 때 해당 유니크 인덱스는 NOT NULL 이어야 한다. 다중 컬럼으로 만들어진 프라이머리 키나 유니크 인덱스라면 인덱스의 모든 컬럼이 비교 조건에 사용되어야 한다. ref 인덱스의 종류와 관계없이 동등 조건으로 검색할 때 사용되는 방법이다.\neq_ref와는 달리 조인의 순서와 관계없이 사용되며, 프라이머리 키나 유니크 키 등의 제약 조건도 없다.\nref 타입은 반환되는 레코드가 반드시 1건이라는 보장이 없으므로 const, eq_ref보다는 빠르지 않다. 하지만 동등 조건으로만 비교되므로 매우 빠른 레코드 조회 방법의 하나다. 동등 비교 연산자\n=, \u0026lt;=\u0026gt;를 의미한다.\nfulltext 전문 검색 인덱스를 사용해 레코드를 읽는 접근 방법을 의미한다.\n전문 검색 인덱스는 통계 정보가 관리되지 않아 성능이 일관적이지 않고, 전문 검색 인덱스를 사용하려면 전혀 다른 SQL 문법을 사용해야 한다.\n전문 검색 조건은 우선순위가 매우 높아, 쿼리에서 전문 인덱스를 사용하는 조건과 그 외 인덱스를 사용하는 조건을 함께 사용한다면 그외 조건이 const, eq_ref, ref가 아니라면 일반적으로 전문 검색 인덱스를 사용하는 조건으로 선택하여 처리된다.\n일반적으로 전문 검색 인덱스를 이용하는 fulltext보다 일반 인덱스를 이용하는 range 접근 방법이 더 빨리 처리되므로, 전문 검색 쿼리를 사용할 때는 조건별로 성능을 확인해 보는 것이 좋다.\nref_or_null ref 접근 방법에 NULL 비교가 추가된 방식이다. 실제 업무에서 많이 활용되지 않지만, 만약 사용된다면 나쁘지 않는 접근 방법이다.\nunique_subquery WHERE 조건절에서 사용될 수 있는 IN (subquery) 형태의 쿼리를 위한 접근 방법이다.\n서브쿼리에서 중복되지 않는 유니크한 값만 반활할 때 사용된다. MySQL 8.0 버전부터는 세미 조인을 최적화하는 형태로 처리되므로 실제로는 더 최적화된 다른 실행 계획이 보일 확률이 높다. index_subquery IN 연산자 특성으로 인해 괄호 안에 있는 값의 목록에서 중복된 값이 먼저 제거되어야 하는데, IN (subquery)에서 서브쿼리가 중복된 값을 반환할 수 있을때 이를 인덱스를 이용해서 중복을 제거할 수 있는 경우 index_subquery 접근 방법이 사용된다.\nrange 인덱스 레인지 스캔 형태의 접근 방법이다. range는 인덱스를 하나의 값이 아니라 범위로 검색하는 경우를 의미하며, 주로 \u0026lt;, \u0026gt;, IS NULL, BETWEEN, IN, LIKE 등의 연산자를 이용해 인덱스 검색할 때 사용된다.\n일반적으로 애플리케이션의 쿼리가 가장 많이 사용하는 접근 방법이다. 얼마나 많은 레코드를 필요로 하느냐에 따라 차이는 있지만 상당히 빠른 방법중 하나이다. index_merge 2개 이상의 인덱스르 이용해 각각의 검색 결과를 만들어낸 후, 그 결과를 병합해서 처리하는 방식이다.\n여러 인덱스를 읽어야 하므로 일반적으로 range 접근 방법보다 효율성이 떨어진다. 전문 검색 인덱스를 사용하는 쿼리에는 적용되지 않는다. 처리된 결과는 항상 2개 이상의 집합이 되기 때문에 그 두 집합의 교집합이나 합집합, 또는 중복 제거와 같은 부가적인 작업이 더 필요하다. index 인덱스를 처음부터 끝까지 읽는 인덱스 풀스캔을 의미한다.\n테이블을 처음부터 끝까지 읽는 풀 테이블 스캔 방식과 비교했을때 비교하는 레코드 건수는 같지만, 일반적으로 데이터 파일 전체보다 크기가 작으므로 풀 테이블 스캔보다 빠르게 처리된다. 쿼리의 내용에 따라 정렬된 인덱스의 장점을 이용할 수 있다. index 접근 방법은 다음 주건 가운데 (1 + 2) 또는 (1 + 3) 조건을 충족하는 쿼리에서 사용되는 읽기 방식이다.\nrange, const, ref 같은 접근 방법으로 인덱스를 사용하지 못하는 경우 인덱스에 포함된 컬럼만으로 처리할 수 있는 쿼리인 경우(데이터 파일을 읽지 않아도 되는 경우) 인덱스를 이용해 정렬이나 그루핑 작업이 가능한 경우(별도의 정렬 작업을 피할 수 있는 경우) ALL 풀 테이블 스캔을 의미하는 접근 방법이다. 테이블을 처음부터 끝까지 읽어 불필요한 레코드를 제거하고 반환하는 가장 비효율적인 방법이다.\n데이터 웨어하우스나 배치 프로그램처럼 대용량의 레코드를 처리하는 쿼리에서 잘못 튜닝된 쿼리(억지로 인덱스를 사용하게된 튜닝쿼리)보다 더 나은 접근방법이다. 일밙거으로 작업 범위를 제한하는 조건이 아니므로 빠른 응답을 사용자에게 보내야하는 웹 서비스에는 적합하지 않다. 테이블이 매우 작지 않다면 실제로 테이블에 데이터를 어느 ㅈ어도 저장한 상태에서 쿼리의 성능을 확인해보고 적용하는 것이 좋다. possible_keys 컬럼 옵티마이저가 최적의 실행 계획을 만들기 위해 후보로 선정했던 접근 방법에서 사용되는 인덱스의 목록이다.\n실제 실행 계획에서는 해당 테이블의 모든 인덱스가 목록에 포함되는 경우가 많아 쿼리 튜닝에 크게 도움되지는 않는다. 특별한 경우를 제외하고 무시해되 된다. 컬럼에 인ㄷ게스 이름이 나열됐다고 해서 그 인덱스를 사용하지는 않는다. key 컬럼 key 컬럼에 표시되는 인덱스는 최종 선택된 실행 계획에서 사용하는 인덱스를 의미한다.\n쿼리 튜닝시 의도했던 인덱스가 표시되는지 확인하는 것이 중요하다. 실행 계획의 type 컬럼이 index_merge가 아는 경우에는 반드시 테이블 하나당 하나의 인덱스만 이용할 수 있다. 인덱스를 사용하지 못하면 NULL로 표시된다. key_len 컬럼 실제 업무에서 사용하는 테이블은 단일 컬럼으로 만들어진 인덱스보다 다중 컬럼으로 만들어진 인덱스가 더 많은데, key_len 컬럼은 쿼리를 처리하기 위해 다중 컬럼으로 구성된 인덱스에서 몇 개의 컬럼까지 사용했는지 알려준다.(인덱스의 각 레코드에서 몇 바이트까지 사용했는지)\n다중 컬럼 인덱스뿐 아니라 단일 컬럼으로 만들어진 인덱스에서도 같은 지표를 제공한다. ref 컬럼 접근 방법이 ref라면 참조 조건(Equal 비교 조건)으로 어떤 값이 제공되었는지 보여준다.\n상숫값을 지정했다면 const 표시 다른 테이블의 컬럼 값이면 테이블명과 컬럼명 표시 콜레이션 변환이나 값 차체의 연산을 거쳐 참조했을 경우 func 표시 MySQL 서버가 내부적으로 값을 변환해야 하는 경우 func 표시 문자집합이 일치하지 않는 두 문자열 컬럼을 조인 숫자 타입의 컬럼과 문자열 타입의 컬럼 조인 등 rows 컬럼 rows 컬럼은 실행 계획의 효율성 판단을 위해 예측한 읽어야 할 총 레코드 건수를 의미한다.\nMySQL 옵티마이저는 각 조건에 대해 가능한 처리 방식을 나열하고, 각 처리방식의 비용을 비교해 최종적으로 하나의 실행 계획을 수립할 때 각 처리 방식이 얼마나 많은 레코드를 읽고 비교해야 하는지 예측해 비용을 산정한다.\n대상 테이블에 얼마나 많은 레코드가 포함되어 있는지 각 인덱스 값의 분포도가 어떤지 rows 값은 각 스토리지 엔질별로 가지고 있는 통계 정보를 참조해 산출해낸 예상값이므로 전확하지는 않다. 반환하는 레코드의 예측치가 아니라 쿼리를 처리하기 위해 얼마나 많은 레코드를 읽고 체크해야 하는지를 의미한다. filtered 컬럼 filtered 컬럼은 읽어야 할 총 레코드 건수 rows에서 필터링 되고 남는 레코드의 비율을 의미한다.\n1 2 3 4 5 6 EXPLAIN SELECT * FROM employees e WHERE e.first_name=\u0026#39;Matt\u0026#39; AND e.hire_date BETWEEN \u0026#39;1990-01-01\u0026#39; AND \u0026#39;1991-01-01\u0026#39; ; id select_type type key rows filtered 1 SIMPLE ref ix_firstname 233 16.03 위 실행계획에서 ix_firstname 인덱스를 활용하여 읽어야 할 범위를 제한하였고, 따라서 필수적으로 읽어야 하는 레코드 건수는 rows 값인 233이다.\n읽어야 하는 레코드 233건 중 인덱스를 활용할 수 없는 조건인 hire_date 조건을 통해 다시 값을 필터링하게 되는데, filtered 값은 읽어와야할 레코드 중 조건으로 인해 필터링 되고 남은 레코드의 비율을 의미한다.\n따라서 쿼리의 결과 예상 건수는 233 * 0.1603 으로 약 37건이다.\n대부분 쿼리에서 WHERE 절에서 사용되는 조건이 모두 인덱스를 사용할 수 있는 것은 아니며, 특히 조인이 사용되는 경우에는 WHERE 절에서 인덱스를 사용할 수 있는 조건도 중요하지만 인덱스를 사용하지 못하는 조건에 일치하는 레코드 건수를 파악하는 것도 매우 중요하다.\n","date":"2023-07-11T13:07:10+09:00","image":"https://codemario318.github.io/post/real-mysql/10/3/1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/10/3/1/","title":"10.3 실행 계획 - 실행 계획 분석 (1)"},{"content":"MySQL 실행 계획은 DESC 또는 EXPLAIN 명령으로 확인할 수 있다. 또한 MySQL 8.0 부터 EXPLAIN 명령에 사용할 수 있는 새로운 옵션이 추가되었다.\n실행 계획 출력 포맷 이전 버전에서는 EXPLAIN EXTENDED, EXPLAIN PARTITIONS 명령이 구분되어 있었지만 8.0 버전부터는 모든 내용이 통합되어 보이도록 개선되면서 옵션이 문법에서 제거되었다.\n또한 FORMAT 옵션을 사용해 실행 계획의 표시 방법을 JSON, TREE, 단순 테이블 형태로 선택할 수 있다.\n1 2 3 4 5 6 7 8 9 /* * 단순 테이블 */ EXPLAIN SELECT * FROM employees e INNER JOIN salaries s ON s.emp_no=e.emp_no WHERE first_name=\u0026#39;ABC\u0026#39; ; 1 2 3 4 5 6 7 8 9 /* * 트리 */ EXPLAIN FROMAT=TREE SELECT * FROM employees e INNER JOIN salaries s ON s.emp_no=e.emp_no WHERE first_name=\u0026#39;ABC\u0026#39; ; 1 2 3 4 5 6 7 8 9 /* * JSON */ EXPLAIN FORMAT=JSON SELECT * FROM employees e INNER JOIN salaries s ON s.emp_no=e.emp_no WHERE first_name=\u0026#39;ABC\u0026#39; ; 쿼리의 실행 시간 확인 MySQL 8.0.18 버전부터는 쿼리의 실행 계획과 단계별 소요된 시간 정보를 확인할 수 있는 EXPLAIN ANALYZE 기능이 추가되었다.\n기존 SHOW PROFILE 명령과 다르게 실행 계획의 단계별로 소요된 시간 정보를 보여주며, 항상 TREE 형식으로만 제공되어 FORMAT 옵션은 사용할 수 없다.\nEXPLAIN ANALYZE 명령의 결과에는 단계별로 실제 소요된 시간과 처리한 레코드 건수, 반복 획수가 표시된다.\nEXPAIN 명령과 달리 실행 계획만 추출하는 것이 아니라 실제 쿼리를 실행하고 사용된 실행 계획과 소요된 시간을 보여준다. 그래서 EXPLAIN ANALYZE 으로 실행하였을 때 실제로 쿼리가 모두 완료 되어야 확인할 수 있으므로, 실행 시간이 아주 많이 걸리는 쿼리라면 EXPLAIN 명령을 통해 확인한 실행 계획을 바탕으로 1차 튜닝 후에 EXPLAIN ANALYZE 명령을 실행하는 것이 좋다.\n","date":"2023-06-29T11:07:10+09:00","image":"https://codemario318.github.io/post/real_mysql/10/2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql/10/2/","title":"10.2 실행 계획 - 실행 계획 확인"},{"content":"MySQL 서버에서 보여주는 실행 계획을 읽고 이해하려면 MySQL 서버가 데이터를 처리하는 로직을 이해할 필요가 있다.\nMySQL 서버의 실행 계획에서 통계 정보는 실행 계획에 가장 큰 영향을 미친다.\nMySQL 5.7 버전까지는 테이블과 인덱스에 대한 개괄적인 정보를 가지고 실행 계획을 수립했으나, 이는 테이블 컬럼의 값들이 실제 어떻게 분포돼 있는지에 대한 정보가 없기 때문에 실행 계획의 정확도가 떨어지는 경우가 많았다.\n이에 따라 MySQL 8.0 버전부터는 인덱스되지 않은 컬럼들에 대해서도 데이터 분포도를 수집하여 저장하는 히스토그램(Histogram) 정보가 도입되었다.\n테이블 및 인덱스 통계 정보 히스토그램이 도입됐다고 해서 기존 테이블이나 인덱스의 통계 정보가 필요하지 않은 것은 아니다.\n비용 기반 최적화에서 가장 중요한 것은 통계 정보로 통계 정보가 정확하지 않다면 엉뚱한 방향으로 쿼리를 실행할 수 있다.\nMySQL 또한 다른 DBMS와 같이 비용 기반 최적화를 사용하지만, 다른 DBMS보다 통계 정보의 정확도가 높지 않고 통계 정보의 휘발성이 강했다. 이에 따라 쿼리의 실행 계획을 수립할 때 실제 테이블의 데이터를 일부 분석해서 통계 정보를 보완해서 사용했다.\nMySQL 서버의 통계 정보 MySQL 5.5 버전 까지는 각 테이블의 통계 정보가 메모리에만 관리되고, SHOW IDNEX 명령으로만 테이블의 인덱스 컬럼의 분포도를 볼 수 있어 서버가 재시작되면 수집된 통계 정보가 모두 휘발되었다.\n이에 따라 MySQL 5.6 버전부터는 각 테이블의 통계 정보를 mysql 데이터베이스의 innodb_index_stats 테이블과 innodb_table_stats 테이블을 통해 InnoDB 스토리지 엔진을 사용하는 테이블에 대한 통계 정보를 영구적으로(Persisdtent) 관리할 수 있게 개선되었다.\n1 SHOW TABLES LIKE \u0026#39;%_stats\u0026#39;; Tables_in_mysql (%_stats) innodb_index_stats innodb_table_stats MySQL 5.6에서 테이블을 생성할 때 STATS_PERSISTENT 옵션을 설정할 수 있는데, 이 설정값에 따라 테이블 단위로 영구적인 통계 정보를 보관할지를 결정할 수 있다.\n1 2 3 CREATE TABLE tab_test (fd1 INT, fd2 VARCHAR(20) PRIMARY KEY(fd1)) ENIGNE=InnoDB STATS_PERSISTENT={ DEFAULT | 0 | 1 } 0: 테이블 통계 정보를 MySQL 5.5 이전 방식대로 관리 1: 통계 정보를 mysql 데이터베이스의 innodb_index_stats, innodb_table_stats 테이블에 저장함 DEFAULT: innodb_stats_persistent 시스템 변수 값으로 결정. 기본적으로는 1 통계 정보의 각 컬럼은 다음과 같은 값을 저장하고 있다.\nstat_name={} n_diff_pfx% 인덱스가 가진 유니크한 값의 개수 n_leaf_pages 인덱스의 리프 노드 페이지 개수 size 인덱스 트리의 전체 페이지 개수 n_rows 테이블의 전체 레코드 건수 clusterd_index_size 프라이머리 키의 크기(InnoDB 페이지 개수 sum_of_other_index_sizes 프라이머리 키를 제외한 인덱스의 크기(InnoDB 페이지 개수) sum_of_other_index_sizes 컬럼은 테이블의 STATS_AUTO_RECALC 옵션에 따라 0으로 보일 수도 있는데, 그 경우 다음과 같이 테이블에 대해 ANALYZE TABLE 명령을 실행하면 통곗값이 저장된다.\nMySQL 5.5 버전 까지는 테이블의 통계 정보가 메모리에만 저장되었기 때문에 서버가 재시작되면 통계 정보가 초기화되어 다시 수집돼야 하고, 추가적으로 사용자나 관리자가 알지 못하는 순간에 다음과 같은 이벤트가 발생하면 자동으로 통계 정보가 갱신되었다.\n테이블이 새로 오픈되는 경우 테이블의 레코드가 대량으로 변경되는 경우 테이블의 전체 레코드 중에서 1/16 정도의 UPDATE, INSERT, DELETE가 실행되는 경우 ANALYZE TABLE 명령이 실행되는 경우 SHOW TABLE STATUS, SHOW INDEX FROM 명령이 실행되는 경우 InnoDB 모니터가 활성화되는 경우 innodb_stats_on_metadata 시스템 설정이 ON인 상태에서 SHOW TABLE STATUS 명령이 실행되는 경우 테이블의 통계 정보가 자주 갱신되면 응용 프로그램의 쿼리의 실행 계획을 정확히 처리하지 못할 수 있었으나 영구적인 통계 정보가 도입되면서 의도하지 않은 통계 정보 변경을 막을 수 있게 되었다. 또한 innodb_stats_auto_recalc 시스템 변수의 값을 OFF로 설정하여 통계 정보가 자동으로 갱신되는 것을 막을 수 있다.\nMySQL 5.5 버전에서는 테이블의 통계 정보를 수집할 때 몇 개의 InnoDB 테이블 블록을 샘플링할지 결정하는 옵션으로 innodb_stats_sample_pages 시스템 변수가 제공되었는데, 5.6 버전부터 없어지고 innodb_stats_transient_sample_pages, innodb_stats_persistent_sample_pages 시스템 변수로 분리되었다.\ninnodb_stats_transient_sample_pages 기본값: 8 자동으로 통계 정보 수집이 실행될 때 설정된 페이지 개수만 임의로 샘플링해서 분석하고 그 결과를 통계 정보로 활용 innodb_stats_persistent_sample_pages 기본값: 20 ANALYZE TABLE 명령이 실행되면 임의로 설정값 개수 만큼의 페이지만 샘플링해서 분석하고 그 결과를 영구적인 통계 정보 테이블에 저장하고 활용 영구적인 통계 정보를 사용한다면 MySQL 서버의 점검이나 사용량이 많지 않은 시간을 이용해 더 정확한 통계 정보를 수집할 수도 있다. 더 정확한 통계 정보를 수집하고자 한다면 innodb_stats_persistent_sample_pages 시스템 변수에 높을 값을 설정하면 되지만 이 값을 너무 높이면 통계 정보 수집 시간이 길어지므로 주의가 필요하다.\n히스토그램 MySQL 5.7 버전까지 활용되던 통계 정보는 인덱스된 컬럼의 유니크한 값의 개수 정도로 단순하여 옵티마이저가 최적의 실행 계획을 수립하기에는 많이 부족하여 옵티마이저는 실행 계획을 수립할 때 실제 인덱스의 일부 페이지를 랜덤으로 가져와 참조여 부족한 부분을 메웠다.\n8.0 버전으로 업그레이드 되면서 MySQL 서버도 컬름의 데이터 분포도를 참조할 수 있는 히스토그램 정보를 활용할 수 있게 되었다.\n히스토그램 정보 수집 MySQL 8.0 버전에서 히스토그램 정보는 컬럼 단위로 관리되는데, 이는 자동으로 수집되지 않고 ANALYZE TABLE ... UPDATE HISTOGRAM 명령을 통해 수동으로 수집 및 관리된다.\n수집된 히스토그램 정보는 시스템 딕셔너리에 함께 저장되고, MySQL 서버가 시작될 때 딕셔너리의 히스토그램 정보를 infomation_schema 데이터베이스의 column_statistics 테이블로 로드한다. 따라서 실제 히스토그램 정보를 조회하려면 column_statistics 테이블을 SELECT 해서 참조할 수 있다.\nMySQL 8.0 버전에서는 2종류의 히스토그램 타입이 지원된다.\nSingleton(싱글톤 히스토그램) 컬럼값 개별로 레코드 건수를 관리하는 히스토그램 Value-Based 히스토그램 또는 도수 분포라고도 불린다. 컬럼 가진 값 별로 버킷이 할당된다. 각 버킷이 컬럼의 값과 발생 빈도의 비율 2개 값을 가진다. Equi-Height(높이 균형 히스토그램) 컬럼값의 범위를 균등한 개수로 구분해서 관리하는 히스토그램 Height-Balances 히스토그램이라고도 불린다. 개수가 균등한 컬럼값의 범위별로 하나의 버킷이 할당된다 각 버킷이 범위 시작값 및 마지막 값, 발생 빈도율, 각 버킷에 포함된 유니크한 값의 개수등 4개 값을 가진다. 히스토그램은 버킷(Bucket) 단위로 구분되어 레코드 건수나 컬럼값의 범위가 괸리된다.\ninformation_schema.column_statistics 테이블의 HISTOGRAM 컬럼이 가진 나머지 필드들은 다음과 같은 의미를 가지고 있다.\nsampling-rate 히스토그램 정보를 수집하기 위해 스캔한 페이지의 비율 샘플링 비율이 0.35라면 전체 페이지의 35% 스캔을 의미 샘플림 비율이 높아질수록 더 정확한 히스토그램이 되겠지만, 테이블을 전부 스캔하는 것은 부하가 높으며 시스템의 자원을 많이 소모한다. histogram_generation_max_mem_size 시스템 변수에 설정된 메모리 크기에 맞게 적절히 샘플링한다. 기본값 20MB histogram-type 히스토그램의 종류 number-of-buckets-specified 히스토그램을 생성할 때 설정했던 버킷의 개수 별도로 개수를 지정하지 않았다면 100개의 버킷이 사용됨 최대 1024개로 설정할 수 있지만, 일반적으로 100개 버킷이면 충분한 것으로 알려져 있음 히스토그램 삭제 히스토그램의 삭제 작업은 테이블의 데이터를 참조하는 것이 아니라 딕셔너리의 내용만 삭제하기 때문에 다른 쿼리 처리 성능에 영향을 주지 않고 즉시 완료된다. 하지만 히스토그램이 사라지면 쿼리의 실행 계획이 달라질 수 있으므로 유의가 필요하다.\n1 2 ANALYZE TABLE employees.employees DROP HISTOGRAM ON gender, hire_date; 히스토그램을 삭제하지 않고 MySQL 옵티마이저가 히스토그램을 사용하지 않게 하려면 optimizer_switch 시스템 변수 값을 변경하면 된다. 시스템 변수의 값을 글로벌로 변경하면 MySQL 서버의 모든 쿼리가 히스토그램을 사용하지 않으며, condition_fanout_filter 옵션에 의해 영향받는 다른 최적화 기능들의 사용되지 않을 수 있으므로 주의가 필요하다.\n1 SET GLOBAL optimizer_switch=\u0026#39;condition_fanout_filter=off\u0026#39;; 히스토그램 정보가 없으면 옵티마이저는 데이터가 균등하게 분포돼 있을 것으로 예측하고, 히스토그램이 있으면 특정 범위의 데이터가 많고 적음을 식별하므로 실행 계획 수립이 달라지므로 쿼리 성능에 상당항 영향을 미칠 수 있다.\n히스토그램과 인덱스 히스토그램과 인덱스는 완전히 다른 객체이므로 서로 비교할 대상은 아니지만, MySQL 서버에서 인덱스는 부족한 통계 정보를 수집하기 위해 사용된다는 측면에서 어느 정도 공통점을 가진다.\nMySQL 서버에서는 쿼리의 실행 계획을 수립할 때 사용 가능한 인덱스들로부터 조건절에 일치하는 레코드 건수를 대략 파악하고 최종적으로 가장 나은 실행 계획을 선택한다.\n인덱스 다이브(Index Dive)\n조건절에 일치하는 레코드 건수를 예측하기 위해 실제 인덱스의 B-Tree를 샘플링해서 확인하는 작업\nMySQL 8.0 서버에서는 인덱스된 컬럼을 검색 조건으로 사용하는 경우 그 컬럼의 히스토그램을 사용하지 않고 실제 인덱스 다이브를 통해 직접 수집한 정보를 활용한다. 그래서 히스토그램은 주로 인덱스되지 않은 컬럼에 대한 데이터 분포도를 참조하는 용도로 사용된다.\n하지만 인덱스 다이브 작업은 어느 정도의 비용이 필요하며, 때로는 (IN 절에 값이 많이 명시된 경우) 실행 계획 수립만으로도 상당한 인덱스 다이브를 실행하고 비용도 커진다.\n조만간 실제 인덱스 다이브를 실행하기 보다 히스토그램을 활용하는 최적화 기능도 추가될 것으로 예상된다.\n코스트 모델(Cost Model) MySQL 서버가 쿼리를 처리하려면 다음과 같은 다양한 작업이 필요하다.\n디스크로부터 데이터 페이지 읽기 메모리(InnoDB 버퍼풀)로부터 페이지 읽기 인덱스 키 비교 레코드 평가 메모리 임시 테이블 작업 디스크 임시 테이블 작업 MySQL 서버는 사용자의 쿼리에 대해 이러한 다양한 작업이 얼마나 필요한지 예측하고 전체 작업비용을 계산한 결과를 바탕으로 최적 실행 계획을 찾는데, 이렇게 전체 쿼리의 비용을 계산하는데 필요한 단위 작업들의 비용을 코스트 모델이라고 한다.\nMySQL 5.7 이전 버전까지는 이런 코스트 모델을 소스 코드에 상수화 해서 새용했으나, 이러한 작업 비용은 서버가 사용하는 하드웨어에 따라 달라질 수 있기 때문에 최적 실행 계획 수립에 있어 항상 긍정적이지 않았다.\n이러한 부분을 보완하기 위해 MySQL 5.7 버전부터 소스 코드에 상수화 되어있던 각 단위 작업 비용을 DBMS 관리자가 조정할 수 있게 개선되었지만, 인덱스되지 않은 컬럼의 데이터 분포(히스토그램)나 메모리에 상주중인 페이지의 비율 등 비용 계산과 연관된 부분의 정보가 부족했다.\nMySQL 8.0 버전으로 업그레이드되면서 컬럼의 데이터 분포를 위한 히스토그램과 각 인덱스별 메모리에 적재된 페이지의 비율이 관리되고 옵티마이저의 실행 계획 수립에 사용되기 시작했다.\nMySQL 8.0 서버의 코스트모델은 다음 2개 테이블에 저장되어 있는 설정값을 사용하는데, 두 테이블 모두 mysql DB에 존재한다.\nserver_cost 인덱스를 찾고 레코드를 비교하고 임시 테이블 처리에 대한 비용 관리 engine_cost 레코드를 가진 데이터 페이지를 가져오는 데 필요한 비용 관리 각 테이블은 공통으로 5개 컬럼을 가진다.\ncost_name: 코스트 모델의 각 단위 작업 default_value: 각 단위 작업의 비용 기본값으로, MySQL 서버 소스코드에 설정된 값 cost_value: DBMS 관리자가 설정한 값 NULL이면 default_value 컬럼값 사용 last_update: 작업 비용이 변경된 시점 comment: 비용에 대한 추가 설명 engine_cost 테이블은 2개의 컬럼을 더 가진다.\nengine_name: 비용이 적용된 스토리지 엔진 스토리지 엔진별로 각 단위 작업 비용을 설정할 수 있다. 기본값은 default이며 특정 스토리지 엔진 비용이 설정되지 않았다면 해당 스토리지 엔진의 비용으로 값을 적용한다. device_type: 디스크 타입 MySQL 8.0에서는 아직 이 컬럼의 값을 활용하지 않는다. MySQL 8.0 버전의 코스트 모델에서 지원하는 단위 작업은 다음과 같이 8개이다.\ncost_name default_value 설명 engine_cost io_block_read_cost 1.00 디스크 데이터 페이지 읽기 memory_block_read_cost 0.25 메모리 데이터 페이지 읽기 sever_cost disk_temptable_create_cost 20.00 디스크 임시 테이블 생성 disk_temptable_row_cost 0.50 디스크 임시 테이블의 레코드 읽기 key_compare_cost 0.05 인덱스 키 비교 memory_temptable_create_cost 1.00 메모리 임시 테이블 생성 memory_temptable_row_cost 0.10 메모리 임시 테이블의 레코드 읽기 row_evaluate_cost 0.10 레코드 비교 각 단위 작업의 비용을 이용해 MySQL 서버의 실행 계획에 표시되는 비용을 직접 계산해보고 싶을 수 있지만, 역으로 이러한 계산을 직접 해보기는 쉽지 않다.\nB-Tree 깊이와 인덱스 키 검색을 위해 읽어야하는 페이지의 개수 디스크와 메모리(버퍼풀)에서 일어야하는 데이터 페이지의 개수 레코드 정렬 작업에서 사용되는 알고르즘별 키 값 비교 작업 횟수 등 이런 정보는 모두 사용자에게 표시되지 않기 때문에 직접 계산하는 것은 상당히 어렵다.\n코스트 모델에서 중요한 것은 각 단위 작업에 설정되는 비용 값이 커지면 어던 실행 계획들이 고비용으로 바뀌고 어떤 실행 계획들이 저비용으로 바뀌는지 파악하는 것이다. 대포적으로 각 단위 작업의 비용이 병견되면 예상할 수 있는 결과들은 다음과 같다.\nkey_compare_cost 비용을 높이면 MySQL 서버 옵티마이저가 가능하면 정렬을 수행하지 않는 방향의 실행 계획을 선택할 가능성이 높아진다. row_evaluate_cost 비용을 높이면 풀 스캔을 실행하는 쿼리들의 비용이 높아지고, MySQL 서버 옵티마이저는 가능하면 인덱스 레인지 스캔을 사용하는 실행 계획을 선택할 가능성이 높아진다. disk_temptable_create_cost와 dist_temptable_row_cost 비용을 높이면 MySQL 옵티마이저는 디스크에 임시 테이블을 만들지 않는 방향의 실행 계획을 선택할 가능성이 높아진다. memory_temptable_create_cost와 memory_temptable_row_cost 비용을 높이면 MySQL서버 옵티마이저는 메모리 임시 테이블을 만들지 않는 방향으로 실행 계획을 선택할 가능성이 높아진다. io_block_read_cost 비용이 높아지면 MySQL 서버 옵티마이저는 가능하면 InnoDB 버퍼풀에 데이터 페이지가 많이 적재되어있는 인덱스를 사용하는 실행 계획을 선택할 가능성이 높아진다. memory_block_read_cost 비용이 높아지만 InnoDB 버퍼풀에 적재된 데이터 페이지가 상대적으로 적다고 하더라도 그 인덱스를 사용할 가능성이 높아진다. 코스트 모델은 MySQL 서버가 사용하는 하드웨어와 MySQL 서버 내부적인 처리 방식에 대한 깊이 있는 지식을 필요로 한다. MySQL 서버에 적용된 기본 값으로도 20년이 넘는 시간동안 수많은 응용 프로그램에서 잘 사용되어 왔으므로, 이런 부분에 대해 전문적인 지식을 가지고 있지 않다면 서버스에 사용되는 MySQL 서버의 engine_cost 테이블과 server_cost 테이블의 기본값을 함부로 변경하지 않는 게 좋다.\n","date":"2023-06-27T11:42:10+09:00","image":"https://codemario318.github.io/post/real_mysql/10/1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql/10/1/","title":"10.1 실행 계획 - 통계 정보"},{"content":"MySQL 버전이 업그레이드되고 통계 정보나 옵티마이저의 최적화 방법들이 더 아양해지면서 쿼리의 실행 계획 최적화가 많이 성숙하고 있지만, 여전히 서비스 로직을 이하지는 못하기 때문에 개발자나 DBA보다 MySQL 서버가 부족한 실행 계획을 수립할 때가 있을 수 있다.\n이런 경우에는 옵티마이저에게 쿼리의 실행 계획을 어떻게 수립해야 할지 알려줄 수 있는 방법이 필요하다. 일반적인 DBMS는 이러한 목적으로 힌트가 제공되며, MySQL에서도 다양한 옵티마이저 힌트를 제공한다.\n인덱스 힌트 이전 부터 사용되어오던 USE INDEX 같은 힌트 옵티마이저 힌트 MySQL 5.6 버전부터 새롭게 시작한 힌트 및 STRAIGHT_JOIN 등 인덱스 힌트 STRAIGHT_JOIN, USE_INDEX 등 을 포함한 인덱스 힌트들은 SELECT, UPDATE 같은 SQL 문법에 맞게 사용해야 하기 때문에 사용 시 ANSI-SQL 표준 문법을 준수하지 못하게 된다. 따라서 주석 처럼 사용되고, 다른 RDBMS 에서 주석으로 해석하게 되는 옵티마이저 힌트를 사용할 것을 추천한다.\nSTRAIGHT_JOIN STRAIGHT_JOIN은 옵티마이저 힌트인 동시에 조인 키워드이기도 하다. STRAIGHT_JOIN은 SELECT, UPDATE, DELETE 쿼리에서 여러개의 테이블이 조인되는 경우 조인 순서를 고정하는 역할을 한다.\n1 2 3 4 5 6 EXPLAIN SELECT * FROM employees e, dept_emp de, departments d WHERE e.emp_no=de.emp_no AND d.dept_no ; id select_type table type key rows Extra 1 SIMPLE d index ix_deptname 9 Using index 1 SIMPLE de ref PRIMARY 41392 NULL 1 SIMPLE e eq_ref PRIMARY 1 NULL 위 쿼리는 3개의 테이블을 조인하지만 옵티마이저의 판단에 의해 조인 순서가 결정된다. 일반적으로 조인을 하기 위한 컬럼들의 인덱스 여부로 조인의 순서가 결정되며, 조인 컬럼의 인덱스에 아무런 문제가 없는 경우에는 (WHERE 조건이 있는 경우 조건을 만족하는) 레코드가 적은 테이블을 드라이빙 테이블로 선택한다.\n이러한 쿼리의 조인 순서를 변경하기 위해 STRAIGHT_JOIN 힌트를 사용할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 EXPLAIN SELECT STRAIGHT_JOIN e.first_name ,e.last_name ,d.dept_name FROM employees e, dept_emp de, departments d WHERE e.emp_no=de.emp_no AND d.dept_no ; EXPLAIN SELECT /*! STRAIGHT_JOIN */ e.first_name ,e.last_name ,d.dept_name FROM employees e, dept_emp de, departments d WHERE e.emp_no=de.emp_no AND d.dept_no ; id select_type table type key rows Extra 1 SIMPLE e ALL NULL 300473 NULL 1 SIMPLE de ref ix_empno_fromdate 1 Using index 1 SIMPLE d eq_ref PRIMARY 1 NULL STRAIGHT_JOIN 힌트는 옵티마이저가 FROM 절에 면시된 테이블의 순서대로 조인을 수행하도록 유도한다. 주로 다음 기준에 맞게 조인 순서가 결정되지 않느 ㄴ경우에만 STRAIGHT_JOIN 힌트로 조인 순서를 조정하는 것이 좋다.\n임시테이블과 일반 테이블의 조인 이러한 경우는 일반적으로 임시 테이블을 드라이빙 테이블로 선정하는 것이 좋다. 일반 테이블의 조인 컬럼에 인덱스가 없는 경우에는 레코드 건수가 작은 족을 먼저 읽도록 드라이빙으로 선택하는 것이 좋은데, 대부분 옵티마이저가 실행 계획을 제대로 수립하지 못해서 심각한 성능 저하가 있는 경우에는 힌트를 사용하면 된다. 임시 테이블끼리 조인 임시 테이블은 항상 인덱스가 없기 때문에 어느 테이블을 먼저 드라이빙으로 읽어도 무관하므로 크기가 작은 테이블을 드라이빙으로 선택해주는 것이 좋다. 일반 테이블끼리 조인 양쪽 테이블 모두 조인 컬럼에 인덱스가 있거나 양쪽 테이블 모두 조인 컬럼에 인덱스가 없는 경우에는 레코드 건수가 적은 테이블을 드라이빙으로 선택해주는 것이 좋다. 그 이외의 경우에는 조인 컬럼에 인덱스가 없는 테이블을 드라이빙으로 선택하는 것이 좋다. STRAIGHT_JOIN 힌트와 비슷한 역할을 하는 옵티마이저 힌트로는 다음과 같은 것들이 있다.\nJOIN_FIXED_ORDER JOIN_ORDER JOIN_PREFIX JOIN_SUFFIX STRAIGTH_JOIN와 JOIN_FIXED_ORDER 힌트는 한번 사용되면 FROM절의 모든 테이블에 대해 조인 순서가 결정되며, 나머지 3개는 일부 테이블의 조인 순서에 대해서만 제안하는 힌트이다.\nUSE_INDEX, FORCE IDNEX, IGNORE INDEX 조인의 순서를 변경하는 것 다음으로 자시 사용되는 것이 인덱스 힌트인데, STRAIGHT_JOIN 힌트와는 달리 인덱스 힌트는 사용하려는 인덱스를 가지는 테이블 뒤에 힌트를 명시한다.\nMySQL 옵티마이저는 어떤 인덱스를 사용해야 할지를 무난하게 잘 선택하는 편이지만, 3~4개 이상의 컬럼을 포함하는 비슷한 인덱스가 여러 개 존재하는 경우에는 실수를 하는데, 이런 경우에는 강제로 특정 인덱스를 사용하도록 힌트를 추가한다.\nUSE_INDEX 가장 자주 사용되는 힌트로, 옵티마이저에게 특정 테이블의 인덱스를 사용하도록 권장한다. 대부분 힌트를 채택하지만, 항상 사용하진 않는다. FORCE IDNEX USE_INDEX와 기능적으로는 같지만 조근 더 강한 우선도를 부여한다. USE_INDEX 힌트도 충분히 우선도가 높고, USE_INDEX를 사용했음에도 인덱스를 활용하지 않는다면 FORCE IDNEX를 사용해도 활용하지 않는 경우가 많다. IGNORE INDEX 특정 인덱스를 사용하지 못하기 하는 힌트이다. 옵티마이저가 풀 테이블 스캔을 사용하도록 유도하기 위해 사용할 수 있다. 대체로 GROUP BY, ORDER BY 작업에서 인덱스를 사용할 수 있다면 나은 성능을 보장하며, 옵티마이저가 대부분 최적으로 선택하기 때문에 크게 고려하지 않아도 괜찮다.\n1 2 3 4 5 6 SELECT * FROM employees WHERE emp_no=10001; SELECT * FROM employees FORCE INDEX(primary) WHERE emp_no=10001; SELECT * FROM employees USE INDEX(primary) WHERE emp_no=10001; SELECT * FROM employees IGNORE INDEX(primary) WHERE emp_no=10001; SELECT * FROM employees FORCE INDEX(ix_firstname) WHERE emp_no=10001; 1~3 쿼리는 인덱스 힌트가 주어지지 않아도 WHERE 조건을 통해 프라이머리 키를 이용하여 실행하는 것이 최적임을 옵티마이저가 판단하기 때문에 동일한 실행 계획으로 처리된다.\n옵티마이저느 ㄴ프라이머리 키나 전문 검색 인덱스와 같은 인덱스에 대해서 선택 시 가중치를 두고 실행 계획을 수립하기 때문에 전문 검색(Full Text search) 인덱스가 있는 경우에는 다른 일반 보조 인덱스를 사용할 수 있는 상황이라고 하더라도 전문 검색 인덱스를 선택하는 경우가 많다.\nMySQL의 옵티마이저가 쉬운 최적화는 매우 빠르게 처리하기 때문에 인덱스의 사용법이나 좋은 실행 계획이 어떤 것인지 판단하기 힘든 상황이라면 힌트를 사용해 강제로 옵티마이저의 실행 계획에 영향을 미치는 것은 피하는 것이 좋다.\n최적의 실행 계획은 데이터의 성격에 따라서 시시각각 변하므로, 현재 프라이머리 키를 사용하는 것이 좋은 계획이었다고 하더라도 이후 바뀔 수 있으므로 옵티마이저가 당시 통계 정보를 가지고 선택하게 하는 것이 좋다.\n가장 훌륭한 최적화는 그 쿼리를 서비스에서 없애버리거나 튜닝할 필요가 없게 데이터를 최소화하는 것이며, 그것이 어렵다면 데이터 모델의 단순화를 통해 쿼리를 간결하게 만들고 힌트가 필요하지 않게 만드는 것이다. 어떤 방법도 없다면 그 다음으로 힌트를 선택하는 것인데, 일반적으로 실무에서는 앞쪽의 작업들에 상당한 시간과 작업 능력이 필요하기 때문에 이런 힌트에 의존하는 경우가 많다.\nSQL_CALC_FOUND_ROWS MySQL의 LIMIT를 사용하는 경우, 조건을 만족하는 레코드가 명시된 수 보다 더 많다고 하더라도 지정한 수 만큼 찾으면 검색 작업을 멈추지만, 사용자에게는 LIMIT에 제한된 수만큼의 레코드를 반환됨에도 불구하고 끝까지 검색을 수행한다.\n1 2 3 4 5 SELECT SQL_CALC_FOUND_ROWS * FROM employees LIMIT 5 ; SELECT FOUND_ROWS() AS total_record_count; emp_no birth_date first_name last_name gender hire_date 10001 1953-09-02 Georgi Facello M 1986-06-26 total_record_count 300024 아마도 이 기능을 웹 프로그램의 페이징 기능에 적용하기 위해 검토했거나 이미 사용하고 있을지 모르지만 이 쿼리 힌트는 사용하지 않는 것이 좋다.\n1 2 3 4 5 6 7 SELECT SQL_CALC_FOUND_ROWS * FROM employees WHERE first_name=\u0026#39;Georgi\u0026#39; LIMIT 0, 20 ; SELECT FOUND_ROWS() AS total_record_count; 이 경우에는 한 번의 쿼리 실행으로 필요한 정보 2가지를 모두 가져오는 것 처럼 보이지만 FOUND_ROWS() 함수의 실행을 위해 또 한번의 쿼리가 필요하기 때문에 쿼리를 2번 실행해야 한다.\n위 쿼리는 first_name='Georgi' 조건을 처리하기 위해 employees 테이블의 ix_firstname 인덱스를 레인지 스캔으로 실제 값을 읽어온다. LIMIT를 통해 처음 20건만 가져오도록 했지만 SQL_CALC_FOUND_ROWS 힌트로 인해 실제 이 조건을 만족하는 레코드는 전체 253건을 전부 읽어야하며, 이로인해 ix_firstname 인덱스를 통해 실제 데이터 레코드를 찾아가는 작업으로 인해 랜덤 I/O가 253번 발생한다.\n1 2 3 4 5 6 7 8 9 10 SELECT COUNT(*) FROM employees WHERE first_name=\u0026#39;Georgi\u0026#39; ; SELECT SQL_CALC_FOUND_ROWS * FROM employees WHERE first_name=\u0026#39;Georgi\u0026#39; LIMIT 0, 20 ; 위 방법 또한 쿼리를 2번 실행해야 하지만, 실제 레코드 데이터가 필요한 것이 아니라 건수만 가져오면 되는 커버링 인덱스 쿼리 이므로 실제로 데이터 레코드를 찾기 위한 랜덤 I/O는 발생하지 않는다. 두번째 쿼리는 LIMIT 제한으로 인해 랜덤 I/O가 20번만 발생하게 된다.\n쿼리가 적절히 튜닝되지 않았거나 WHERE 조건에 대해서 적절한 인덱스가 준비되지 않았을 경우 SQL_CALC_FOUND_ROWS로 처리하는 것이 더 빠를 수 있지만 이러한 경우에는 쿼리나 인덱스를 튜닝하여 뒤에 설명한 방식으로 처리하는 것이 대체로 더 빠르다. 인덱스나 쿼리의 튜닝이 제대로 됐다면 뒤에 설명한 방식이 더 빠르게 실행될 것이므로 SQL_CALC_FOUND_ROWS 힌트는 사용하지 않는 것이 좋다.\n또한 SELECT 문장이 UNION, UNION DISTINT으로 연결 된 경우에는 SQL_CALC_FOUND_ROWS 힌트를 사용해도 FOUND_ROWS 함수로 정확한 레코드 건수를 가져올 수 없다.\n옵티마이저 힌트 MySQL 8.0에서 사용 가능한 힌트는 종류가 매우 다양하고 힌트가 미치는 영향 범위도 매우 다양하다.\n옵티마이저 힌트 종류 옵티마이저 힌트는 영향 범위에 따라 4개 그룹으로 나눌 수 있다.\n인덱스: 특정 인덱스의 이름을 사용할 수 있는 옵티마이저 힌트 테이블: 특정 테이블의 이름을 사용할 수 있는 옵티마이저 힌트 쿼리 블록: 특정 쿼리 블록에 사용할 수 있는 옵티마이저 힌트로서, 특정 쿼리 블록의 이름을 명시하는 것이 아닌 힌트가 명시된 쿼리 블록에 대해서만 영향을 미치는 옵티마이저 힌트 글로벌(쿼리 전체): 전체 쿼리에 영향을 미치는 힌트 이 구분으로 인해 힌트의 사용 위치가 달라지는 것은 아니며, 힌트에 인덱스 이름이 명시될 수 있는 경우 인덱스 수준의 힌트로 구분하고, 테이블 이름까지만 명시될 수 있는 경우를 테이블 수준의 힌트로 구분한다.\n또한 특정 힌트는 테이블과 인덱스의 이름을 모두 명시할 수도 있지만 인덱스의 이름을 명시하지 않고 테이블 이름만 명시하는 경우 인덱스와 테이블 수준의 힌트가 된다.\n힌트 이름 설명 영향 범위 MAX_EXECUTION_TIME 쿼리의 실행 시간 제한 글로벌 RESOUREC_GROUP 쿼리 실행의 리소스 그룹 설정 글로벌 SET_VAR 쿼리 실행을 위한 시스템 변수 제어 글로벌 SUBQUERY 서브쿼리의 세미 조인 최적화 전략 제어 쿼리 블록 BKA, NO_BKA BKA(Batched Key Access) 조인 사용 여부 제어 쿼리 블록, 테이블 BNL, NO_BNL 블록 네스티드 루프 조인 사용 여부 제어(MySQL 8.0.20 부터는 해시조인 사용 여부 제어) 쿼리 블록, 테이블 DERIVED_CONDITION_PUSHDOWN, NO_DERIVED_CONDITION_PUSHDOWN 외부 쿼리의 조건을 서브쿼리로 옮기는 최적화 사용 여부 제어 쿼리 블록, 테이블 HASH_JOIN, NO_HASH_JOIN 해시 조인 사용 여부 제어(MySQL 8.0.18 만 사용 가능) 쿼리 블록, 테이블 JOIN_FIXED_ORDER FROM 절에 명시된 테이블 순서대로 조인 실행 쿼리 블록 JOIN_ORDER 힌트에 명시된 테이블 순서대로 조인 실행 쿼리 블록 JOIN_PREFIX 힌트에 명시된 테이블을 조인의 드라이빙 테이블로 조인 실행 쿼리 블록 JOIN_SURFFIX 힌트에 명시된 테이블 드리븐 테이블로 조인 실행 쿼리 블록 QB_NAME 쿼리 블록의 이름 설정을 위한 힌트 쿼리 블록 SEMI_JOIN, NO_SEMIJOIN 서브쿼리의 세미 조인 최적화 전략 제어 쿼리 블록 MERGE, NO_MERGE FROM 절의 서브쿼리나 뷰를 외부 쿼리 블록으로 병합하는 최적화를 수행할지 여부 제어 테이블 INDEX_MERGE, NO_INDEX_MERGE FROM 절의 서브쿼리나 뷰를 외부 쿼리 블록으로 병합하는 최적화를 수행할지 여부 제어 테이블, 인덱스 MRR, NO_MRR MRR(Multi-Range Read) 사용 여부 제어 테이블, 인덱스 NO_ICP ICP(인덱스 컨디션 푸시다운) 최적화 전략 사용 여부 제어 테이블, 인덱스 NO_RANGE_OPTIMIZATION 인덱스 레인지 엑세스를 비활성화(특정 인덱스를 사용하지 못하도록 하거나 쿼리를 풀 테이블 스캔 방식으로 처리) 테이블, 인덱스 SKIP_SCAN, NO_SKIP_SCAN 인덱스 스킵 스캔 사용 여부 제어 테이블, 인덱스 INDEX, NO_INDEX GROUP_BY, ORDER_BY, WHERE 절 처리를 위한 인덱스 사용 여부 제어 인덱스 GROUP_INDEX, NO_GROUP_INDEX GROUP_BY 처리를 위한 인덱스 사용 여부 제어 인덱스 JOIN_INDEX, NO_JOIN_INDEX WHERE 처리를 위한 인덱스 사용 여부 제어 인덱스 ORDER_INDEX, NO_ORDER_INDEX ORDER_BY 처리를 위한 인덱스 사용 여부 제어 인덱스 ","date":"2023-06-09T11:42:10+09:00","image":"https://codemario318.github.io/post/real-mysql/9/4/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/9/4/","title":"9.4 옵티마이저와 힌트 - 쿼리 힌트"},{"content":"테이블의 개수가 많아지면 실행 계획을 찾는 것이 상당히 어려워지고, 하나의 쿼리에서 조인되는 테이블의 개수가 많아지면 실행 계획을 수립하는 데만 몇 분이 걸릴 수도 있다.\nMySQL에는 최적화된 조인 실행 계획 수립을 위한 2가지 알고리즘이 있으며, 4개의 테이블을 조인하는 쿼리가 조인 옵티마이저 알고리즘에 어떻게 처리되는지 살펴볼것이다.\n1 2 3 SELECT * FROM t1, t2, t3, t4 WHERE ... Exhaustive 검색 알고리즘 (완전 탐색) Exhausitve 검색 알고리즘은 MySQL 5.0 이하 버전에서 사용되던 조인 최적화 기법으로, FROM절에 명시된 모든 테이블의 조합에 대해 실행 계획의 비용을 계산해서 최적의 조합 1개를 찾는 방법이다.\n테이블이 20개라면 이 방법으로 처리했을 때 가능한 조인 조합은 모드 20!(3628800)개가 된다.\nGreedy 검색 알고리즘 Greedy 검색 알고리즘은 Exhaustive 검색 알고리즘의 시간 소모적인 문제점을 해결하기 위해 MySQL 5.0 부터 도입된 조인 최적화 기법이다.\n전체 N개의 테이블 중에서 optimizer_search_depth 시스템 변수에 정의된 개수의 테입르로 가능한 조인 조합 생성 1번에서 생성된 조인 조합 중 최소 비용의 실행 계획 하나를 선정 2번에서 선정된 실행 계획의 첫 번째 테이블을 \u0026ldquo;부분 실행 계획\u0026quot;의 첫 번째 테이블로 선정 전체 N-1개의 테이블 중 시스템 설정 변수에 정의된 개수의 테이블로 가능한 조인 조합을 생성 4번에서 생성된 조인 조합들을 하나씩 3번에서 생성된 \u0026ldquo;부분 실행 계획\u0026quot;에 대입해 실행 비용 계산 5번의 비용 계산 결과, 최적 실행 계획에서 두 번째 테이블을 3번에서 생성된 \u0026ldquo;부분 실행 계획\u0026quot;의 두 번째 테이블로 선정 남은 테이블이 모두 없어질 때까지 4~6 반복하며 \u0026ldquo;부분 실행 계획\u0026quot;에 테이블의 조인 순서를 기록 최종적으로 \u0026ldquo;부분 실행 계획\u0026quot;이 테이블의 조인 순서로 결정 Greedy 검색 알고리즘은 optimizer_search_depth 시스템 변수 값에 따라 최적화의 비용이 상당히 줄어들 수 있으며 기본값은 62이다.\n시스템 변수 optimizer_search_depth: 값에 따라 어떤 알고리즘을 선택할지 결정한다. 0: Exhaustive 검색 알고리즘 1+: Greedy 기본값은 62이나 많은 테이블이 조인되는 경우 상당히 부담될 수 있고 0일 경우 쿼리 성능에 심각항 영향을 미칠 수 있다.(일반적으로 4~5 가 적당하다.) optimizer_prune_level: MySQL 5.0 부터 추가된 Heuristic 검색이 작동하는 방식을 제어한다. 다양한 조인 순서 비용을 계산하는 도중 이미 계산했던 조인 순서보다 비용이 크다면 중간에 포기한다. 1로 설정시 Heuristic 알고리즘을 사용한다. 실제 Hueuristic 조인 최적화는 조인 대상 테이블이 몇개 되지 않터라도 상당한 성능 차이를 내므로 0으로 변경하지 않을 것을 권장한다. ","date":"2023-06-08T11:47:10+09:00","image":"https://codemario318.github.io/post/real-mysql/9/3/2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/9/3/2/","title":"9.3.2 옵티마이저와 힌트 - 고급 최적화(조인 최적화 알고리즘)"},{"content":"MySQL 서버의 옵티마이저가 실행 계획을 수립할 때 통계 정보와 옵티마이저 옵션을 결합하여 최적의 실행 계획을 수립하게 된다.\n조인 관련 옵티마이저 옵션 MySQL 초기 버전부터 제공되었고 많은 사람들이 신경쓰지 않지만, 조인이 많이 사용되는 서비스에서는 알아야한다. 옵티마이저 스위치 MySQL 5.5 버전부터 제공되어 고급 최적화 기능들을 활성화할지 제어하는 용도로 사용된다. 옵티마이저 스위치 옵션은 optimizer_switch시스템 변수를 이용해 제어하는데 여러 개의 옵션을 세트로 묶어서 설정하는 방식으로 사용한다.\n옵티마이저 스위치 이름 기본값 설명 batch_key_access off BKA 조인 알고리즘을 사용할지 여부 설정 block_nested_loop on Block Nested Loop 조인 알고리즘을 사용할지 여부 설정 engine_condition_pushdown on Engine Condition Pushdown 기능을 사용할지 여부 설정 index_condition_pushdown on Index Condition Pushdown 기능을 사용할지 여부 설정 use_index_extensions on Index Extension 최적화를 사용할지 여부 설정 index_merge on Index Merge 최적화를 사용할지 여부 설정 index_merge_intersection on Index Merge Intersection 최적화를 사용할지 여부 설정 index_merge_sort_union on Index Merge Sort Union 최적화를 사용할지 여부 설정 index_merge_union on Index Merge Union 최적화를 사용할지 여부 설정 mrr on MRR 최적화를 사용할지 여부 설정 mrr_cost_based on 비용 기반 MRR 최적화를 사용할지 여부 설정 semijoin on 세미 조인 최적화를 사용할지 여부 설정 firstmatch on FirstMatch 세미 조인 최적화를 사용할지 여부 설정 loosescan on LoooseScan 세미 조인 최적화를 사용할지 여부 설정 materialization on Materialization 최적화를 사용할지 여부 설정(서미 조인 최적화 포함) subquery_materialization_cost_based on 비용 기반의 Materialization 최적화를 사용할지 여부 설정 각각의 옵티마이저 스위치 옵션은 default, on, off 중에서 하나를 설정할 수 있다. 또한 글로벌과 세션별 모두 설정할 수 있는 시스템 변수이므로 MySQL 서버 전체적으로 또는 현재 커넥션에 대해서만 다음과 같이 설정할 수 있고 옵티마이저 힌트를 이용해 현재 쿼리에만 설정할 수 있다.\n1 2 3 4 5 6 7 8 9 10 // 글로벌 SET GLOBAL optimizer_switch=\u0026#39;index_merge=on,index_merge_union=on,...\u0026#39;; // 로컬 세션 SET SESSION optimizer_switch=\u0026#39;index_merge=on,index_merge_union=on,...\u0026#39;; // 옵티마이저 힌트 SELECT /*+ SET_VAR(optimizer_switch=\u0026#39;condition_fanout_filter=off`) */ ... FROM ... MRR과 배치 키 엑세스(mrr \u0026amp; batched_key_access) MRR이란?\nMRR은 \u0026ldquo;Multi-Range Read\u0026quot;를 줄여서 부르는 이름인데, 메뉴얼에서는 DS-MRR(Disk Sweep Multi-Range Read)이라고도 한다.\n네스티드 루프 조인\nMySQL 서버에서 지금까지 지원하던 조인 방식은 드라이빙 테이블(조인에서 가장 먼저 읽는 테이블)의 레코드를 한 건 읽어서 드리븐 테이블의 일치하는 레코드를 찾아서 조인을 수행하는 네스티드 루프 조인이다.\nMySQL 서버의 내부 구조상 조인 처리는 MySQL 엔진이 처리하지만, 실제 레코드를 검색하고 읽는 부분은 스토리지 엔진이 담당하기 때문에, 드라이빙 테이블의 레코드 건별로 테이블의 레코드를 찾으면 레코드를 찾고 읽는 스토리지 엔진에서는 아무런 최적화를 할 수 없다는 문제점이 있었다.\n이러한 점을 보완하기 위해 MySQL 서버는 드라이빙 테이블의 레코드를 읽어서 드리븐 테이블과의 조인을 즉시 실행하지 않고 조인 대상을 버퍼링 한다. 조인 버퍼에 레코드가 가득 차면 MySQL 엔진은 버퍼링된 레코드를 스토리지 엔진으로 한 번에 요청하게 된다. 이러한 처리로 인해 스토리지 엔진은 읽어야할 레코드들을 데이터 페이지에 정렬된 순서로 접근해서 디스크의 데이터 페이지 읽기를 최소화 한다.\n이러한 방식을 MRR이라고 하며, MRR을 응용해서 실행되는 조인 방식을 BKA(Batched Key Access) 조인이라고 한다. BKA 조인 최적화는 쿼리의 특성에 따라 BKA 조인이 큰 도움이 되는 경우도 있지만, 부가적인 정렬 작업이 필요해지면서 오히려 성능에 안좋은 영향을 미치는 경우가 있어 기본적으로 비활성화되어있다.\n블록 네스티드 루프 조인(block_nested_loop) MySQL 서버에서 사용되는 대부분의 조인은 네스티드 루프 조인이며, 조인의 연결 조건이 되는 컬럼에 모두 인덱스가 있는 경우 사용되는 조인 방식이다.\n1 2 3 4 5 6 7 8 EXPLAIN SELECT * FROM employees e INNER JOIN salaries s ON s.emp_no=e.emp_no AND s.from_date \u0026lt;= NOW() AND s.to_date \u0026gt;= NOW() WHERE e.first_name = \u0026#39;Amor\u0026#39; ; id select_type table type key rows Extra 1 SIMPLE e ref ix_firstname 1 NULL 1 SIMPLE s ref PRIMARY 10 Using where 위와 같은 형태의 조인은 프로그래밍 언어에서 마치 중첩된 반복 명령을 사용하는 것처럼 작동한다고 해서 네스티드 루프 조인이라고 한다.\n1 2 3 4 5 6 for(row1 IN employees) { for(row2 IN salaries) { if(condition_matched) return (row1, row2); } } 블록 네스티드 루프 조인은 조인 버퍼가 사용되는지 여부와 조인에서 드라이빙 테이블과 드리븐 테이블이 어떤 순서로 조인되느냐다.\n조인 알고리즘에서 \u0026ldquo;Block\u0026quot;이라는 단어가 사용되면 별도의 버퍼가 사용되었음을 의미이다.\n조인 쿼리의 실행 계획에서 Extra 컬럼에 \u0026ldquo;Using Join buffer\u0026quot;라는 문구가 표시되면 그 실행 계획은 조인 버퍼를 사용한다는 것을 의미한다.\n조인은 드라이빙 테이블에서 일치하는 레코드의 건수만큼 드리븐 테이블을 검색하면서 처리되어 드라이빙 테이블은 한 번에 쭉 읽지만, 드리븐 테이블은 여러 번 읽는 다는 것을 의미한다.\n예를 들어 드라이빙 테이블에서 일치하는 레코드가 1000건 이었는데, 드리븐 테이블의 조인 조건이 인덱스를 이용할 수 없었다면 드리븐 테이블에서 연결되는 레코드를 찾기 위해 1000번의 풀 테이블 스캔을 해야한다. 따라서 드리븐 테이블을 검색할 때 인덱스를 사용할 수 없는 쿼리는 상당히 느려지며, 옵티마이저는 최대한 드리븐 테이블의 검색이 인덱스를 사용할 수 있게 실행 계획을 수립한다.\n그런데 어떤 방식으로도 드리븐 테이블의 풀 테이블 스캔이나 인덱스 풀 스캔을 피할 수없다면 옵티마이저는 드라이빙 테이블에서 읽은 레코드를 메모리에 캐시한 후 드리븐 테이블과 이 메모리 캐시를 조인하는 형태로 처리한다. 이때 사용되는 메모리의 캐시를 조인 버퍼라고 한다. 조인 버퍼는 join_buffer_size 시스템 변수로 크기를 제한할 수 있으며, 조인이 완료되면 조인 버퍼는 바로 해제된다.\n1 2 3 4 5 SELECT * FROM dept_emp de, employees e WHERE de.from_date \u0026gt; \u0026#39;1995-01-01\u0026#39; AND e.emp_no \u0026lt; 109004 ; id select_type table type key rows Extra 1 SIMPLE de ref ix_fromdate 1 Using index condition 1 SIMPLE s ref PRIMARY 10 Using join buffer (block nested loop) 위 쿼리의 실행 계획을 살펴보면 다음과 같이 dept_emp테이블이 드라이빙 테이블이며, employees 테입르을 읽을 때는 조인 버퍼를 이용해 블록 네스티드 루프 조인을 한다는 것을 알 수 있다.\ndept_emp 테이블의 ix_fromdate인덱스를 이용해 조건을 만족하는 레코드를 검색한다. 조인에 필요한 나머지 컬럼을 모두 dept_emp 테이블로부터 읽어서 조인 버퍼에 저장한다. employees 테이블의 프라이머리 키를 이용해 조건을 만족하는 레코드를 검색한다. 3번에서 검색된 결과(employees)에 2번의 캐시된 조인 버퍼의 레코드를 결합해서 반환한다. 4번 단계가 employees 테이블의 결과를 기준으로 dept_emp 테이블의 결과를 병합하는 것 처럼 조인 버퍼가 사용되는 쿼리에서는 조인의 순서가 거꾸로인 것처럼 실행된다. 실제 이 쿼리의 실행계획 상으로는 dept_emp 테이블이 드라이빙 테이블, employees 테이블이 드리븐 테이블이지만, 실제 드라이빙 테이블의 결과를 조인 버퍼에 담아두고 드리븐 테이블을 읽어 조인 버퍼에서 일치하는 레코드를 찾는 방식으로 처리된다.\n일반적으로 조인이 수행된 후 가져오는 결과는 드라이빙 테이블의 순서에 의해 결정되지만, 조인 버퍼가 사용되는 조인에서는 결과의 정렬 순서가 흐트러 질 수 있다.\nMySQL 8.0.18 버전부터는 해시 조인 알고리즘이 도입돼었고, 8.0.20 버전부터는 블록 네스티드 루프 조인은 더이상 사용되지 않고 해시 조인 알고리즘이 대체되어 사용된다.\n인덱스 컨디션 푸시다운(index_condition_pushdown) 1 2 3 4 5 // 인덱스 추가 ALTER TABEL emplyees ADD INDEX ix_lastname_firstname (last_name, first_name); // index_condition_pushdown 비활성화 SET optimizer_swith=\u0026#39;index_condition_pushdown=off\u0026#39;; 1 2 3 4 5 SELECT * FROM employees WHERE last_name=\u0026#39;Action\u0026#39; AND first_name LIKE \u0026#39;%sal\u0026#39; ; id select_type table type key rows Extra 1 SIMPLE employees ref ix_lastname_firstname 66 Using where 인덱스 컨디션 푸시다운이 비활성화 된 후 실행된 조회 쿼리는 이 first_name LIKE %sal 조건으로 인해 인덱스 레인지 스캔으로는 검색 해야할 인덱스의 범위를 좁힐 수 없으므로 ix_lastname_firstname 인덱스에 함께 저장되어있는 fisrt_name 값을 활용하지 못한다. 따라서 last_name을 이용하여 데이터를 모두 읽은 후 first_name 조건을 사용자가 원하는 결과인지 하나씩 비교해보는 조건(체크조건, 필터링 조건)으로만 사용하게 되어 실행 계획의 Extra 컬럼에 Using where로 표시된다.\n따라서 해당 쿼리의 last_name='Action'을 만족하는 레코드가 100,000건이고, 그 중 first_name LIKE %sal 조건에 해당하는 레코드가 1건 뿐이라면 999,999건의 레코드 읽기 작업은 불필요한 작업이 되어버린다.\n인덱스를 비교하는 작업은 실제 InnoDB 스토리지 엔진에서 수행하지만 테이블의 레코드에서 조건을 비교하는 작업은 MySQL 엔진이 수행하게 된다. MySQL 5.5 버전까지는 인덱스를 범위 제한 조건으로 사용하지 못하기 때문에 MySQL 엔진이 스토리지 엔진으로 해당 조건을 전달하지 않았다.\n이는 MySQL 5.6 버전부터 개선되어 인덱스를 이용해 최대한 필터링까지 완료하여 꼭 필요한 레코드에 대해서만 테이블 읽기를 수행하게 되었다. 인덱스 컨디션 푸시다운을 활성화하고 실행 계획을 확인하면 아래와 같이 출력된다.\nid select_type table type key rows Extra 1 SIMPLE employees ref ix_lastname_firstname 66 Using index condition 인덱스 확장(use_index_extensions) 해당 옵션은 InnoDB 스토리지 엔진을 사용하는 테이블에서 세컨더리 인덱스에 자동으로 추가된 프라이머리 키를 활용할 수 있게 할지를 결정하는 옵션이다.\n1 2 3 4 5 6 7 8 CREATE TABLE dept_emp ( emp_no INT NOT NULL, dept_no CHAR(4) NOT NULL, from_date DATE NOT NULL, to_date DATE NOT NULL, PRIMARY KEY (dept_no, emp_no), KEY ix_fromdate (from_date), ) ENGINE=InnoDB; InnoDB 스토리지 엔진은 프라이머리 키를 클러스터링 키로 생성한다. 따라서 모든 세컨더리 인덱스는 리프 노드에 프라이머리 키 값을 가진다. dept_emp테이블에서 프라이머리 키는 (dept_no, emp_no)이며, 세컨더리 인덱스 ix_fromdate는 from_date 컬럼만 포함한다. 따라서 세컨더리 인덱스는 데이터 레코드를 찾아가기 위해 프라이머리 키인 (dept_no, emp_no) 컬럼을 순서대로 포함한다. 최종적으로 ix_fromdate인덱스는 (dept_no, emp_no) 조합으로 인덱스를 생성한 것과 흡사하게 작동할 수 있다.\n1 2 3 4 5 6 EXPLAIN SELECT COUNT(*) FROM dept_emp WHERE from_date=\u0026#39;1987-07-25\u0026#39; AND dept_no=\u0026#39;d001\u0026#39; ; id select_type table type key key_len ref 1 SIMPLE dept_emp ref ix_fromdate 19 const, const 예전 MySQL 버전에서는 위와 같은 쿼리가 세컨더리 인덱스의 마지막에 자동 추가되는 프라이머리 키를 제대로 활용하지 못했지만, 업그레이드 되면서 옵티마이저는 ix_fromdate인덱스의 마지막에 (dept_no, emp_no)가 있다는 것을 인지하고 실행 계획을 수립하도록 개선되었다.\n실행계획의 key_len 컬럼은 쿼리가 인덱스를 구성하는 컬럼 중에서 어느 부분까지 사용했는지를 바이트 수로 보여주는데, 위 예제에서는 from_date 컬럼(3바이트), dept_emp 컬럼(16바이트)까지 사용했다는 것을 알 수 있다.\n1 2 3 4 5 EXPLAIN SELECT COUNT(*) FROM dept_emp WHERE from_date=\u0026#39;1987-07-25\u0026#39; ; id select_type table type key key_len ref 1 SIMPLE dept_emp ref ix_fromdate 3 const dept_no='d001' 조건을 제거하면 from_date컬럼을 위한 3 바이트만 표시된다.\n1 2 3 4 5 6 EXPLAIN SELECT COUNT(*) FROM dept_emp WHERE from_date=\u0026#39;1987-07-25\u0026#39; ORDER BY dept_no ; id select_type table type key key_len Extra 1 SIMPLE dept_emp ref ix_fromdate 3 NULL InnoDB의 프라이머리 키가 세컨더리 인덱스에 포함돼 있으므로 위와 같은 정렬 작업도 인덱스로 처리되어 Extra 컬럼에 Using Filesort가 표시되지 않게 된다.\n인덱스 머지(index_merge) 인덱스를 이용해 쿼리를 실행하는 경우, 대부분 옵티마이저는 테이블별로 하나의 인덱스만 사용하다록 실행 계획을 수립하지만 인덱스 머지 실행 계획을 사용하면 하나의 테이블에 2개 이상의 인덱스를 이용해 쿼리를 처리한다.\n쿼리에서 한 테이블에 대한 WHERE 조건이 여러개 있더라도 하나의 인덱스만을 사용해서 작업 범위를 충분히 줄일 수 있는 경우라면 하나의 인덱스에 포함된 컬럼에 대한 조건만으로 인덱스를 검색하고 나머지 조건은 읽어온 레코드에 대해서 체크하는 형태로만 사용되는 것이 효율적이다. 하지만 쿼리에 사용된 각각의 조건이 서로 다른 인덱스를 사용할 수 있고 그 조건을 만족하는 레코드 건수가 많을 것으로 예상 될 때 MySQL 서버는 인덱스 머지 실행 계획을 선택한다.\n인덱스 머지 실행 계획은 3개의 실행 계획으로 나눌 수 있으며, 모두 여러개의 인덱스를 통해 결과를 가져온다는 것은 동일하지만 각각의 결과를 어떤 방식으로 병합할 지에 따라 구분된다.\n인덱스 머지 - 교집합(index_merge_intersection) 1 2 3 4 5 6 EXPLAIN SELECT ( FROM employees WHERE first_name=\u0026#39;Georgi\u0026#39; AND emp_no BETWEEN 10000 AND 20000 ; type key key_len Extra index_merge ix_firstname, PRIMARY 62, 4 Using intersect(ix_firstname, PRIMARY); Using where 위 쿼리는 2개의 WHERE조건을 가지고 있는데, employees 테이블의 first_name, emp_no 모두 인덱스를 가지고 있다. 즉 2개 중에서 어떤 조건을 사용하더라도 인덱스를 모두 사용할 수 있으므로 옵티마이저는 ix_firstname, PRIMARY 키를 모두 사용해서 쿼리를 처리하도록 결정했다.\n실행 계획의 Extra 컬럼에 Using intersect라고 표시된 것은 이 쿠러가 여러개의 인덱스를 각각 검색해서 그 결과의 교집합만 반환했다는 것을 의미한다.\n인덱스 머지 합집합(index_merge_union) 인덱스 머지의 Using union은 WHERE 절에 사용된 2개 이상의 조건이 각각의 인ㄷ게스를 사용하되 OR 연산자로 연결된 경우에 사용되는 최적화다.\n1 2 3 4 5 SELECT * FROM employees WHERE first_name=\u0026#39;Matt\u0026#39; OR hire=\u0026#39;1987-03-31\u0026#39; ; type key key_len Extra index_merge ix_firstname, ix_hiredate 58, 3 Using union(ix_firstname, ix_hiredate); employees 테이블에는 ix_firstname, ix_hiredate 인덱스가 준비되어 있으므로 모든 조건이 인덱스를 활용할 수 있다. 쿼리의 실행 계획의 Extra 컬럼에 Using union(\u0026hellip;, \u0026hellip;)라고 표시된다면 인덱스 머지 최적화가 각각의 인덱스 검색 결과를 Union 알고리즘으로 병합했다는 것을 의미한다.\nUnion 알고리즘\n인덱스의 결과가 이미 프라이머리 키로 정렬되어 있을 때, 각각의 인덱스로 조회한 결과를 하나씩 가져와 비교하면서 emp_no 컬럼의 값이 중복된 레코드들을 우선 순위 큐를 이용해 정렬 없이 걸러낼 수 있다.\n인덱스 머지 - 정렬 후 합집합(index_merge_sort_union) Union 알고리즘을 사용할 수 없는데 결과의 정렬이 필요한 경우 MySQL 서버는 인덱스 머지 최적화의 Sort union 알고리즘을 사용한다.\n1 2 3 4 5 SELECT * FROM employees WHERE first_name=\u0026#39;Matt\u0026#39; OR hire_date BETWEEN \u0026#39;1987-03-01\u0026#39; AND \u0026#39;1987-03-31\u0026#39; ; type key key_len Extra index_merge ix_firstname, ix_hiredate 58, 3 Using sort_union(ix_firstname, ix_hiredate); ix_hiredate 같은 경우 날짜 순서대로 프라이머리 키와 일치하지 않을 수 있다. 따라서 위 쿼리에서 중복 제거를 위해 우선순위 큐를 사용할 수 없게 된다. 따라서 MySQL 서버는 두 집합의 결과에서 중복을 제거하기 위해 각 집합을 emp_no컬럼으로 정렬한 다음 중복 제거를 수행한다.\n인덱스 머지 최적화에서 중복 제러를 위해 강제로 정렬을 수행해야 하는 경우에는 실행 계획의 Extra 컬럼에 \u0026ldquo;Using sort_union\u0026rdquo; 문구가 표시된다.\n세미 조인(semijoin) 다른 테이블과 실제 조인을 수행하지는 않고, 조건에 일치하는 레코드가 있는지 없는지만 체크하는 형태의 쿼리를 세미 조인(semi-join)이라고 한다.\n1 2 3 4 5 6 7 SELECT * FROM employees e WHERE e.emp_no IN ( SELECT de.emp_no FROM dept_emp de WHERE de.from_date=\u0026#39;1995-01-01\u0026#39; ) id select_type table type key rows 1 PRIMARY e ALL NULL 300363 2 SUBQUERY de ref ix_fromdate 57 일반적인 RDBMS는 dept_emp 테이블을 조회하는 서브쿼리 부분이 먼저 실행되고 그다음 employees 테이블에서 일치하는 레코드만 검색 하지만, MySQL 5.7 까지는 세미 조인 형태의 최적화가 부족하여 employees 테이블을 풀 스캔 하면서 한 건 한 건 서브쿼리의 조건에 일치하는지 비교한다. 이에따라 57건만 읽어도 될 쿼리를 30만 건 넘게 읽어 처리된다.\n세미조인 형태 쿼리와 안티 세미조인 형태의 쿼리는 최적화 방법이 차이가 있다.\n세미조인: = (subquery), IN (subquery) 세미 조인 최적화 IN-to-EXISTS 최적화 MATERIALIZATION 최적화 안티세미조인: \u0026lt;\u0026gt; (subquery), NOT IN (subquery) IN-to-EXISTS 최적화 MATERIALIZATION 최적화 MySQL 서버 8.0 버전 부터 세미 조인 쿼리의 성능을 개선하기 위해 다음과 같은 최적화 전략을 사용한다.\nTable Pull-out Duplicate Weed-out First Match Loose Scan Materialization Table Pull-out 최적화는 사용 가능하면 항상 세미조인보다는 좋은 성능을 내기 때문에 별도로 제어하는 옵티마이저 옵션을 제공하지 않는다.\n테이블 풀-아웃(Table Pull-out) Table Pull-out 최적화는 세미 조인의 서브쿼리에 사용된 테이블을 아우터 쿼리로 끄집어낸 후에 쿼리를 조인 쿼리로 재작성하는 형태의 최적화이다. 이 방법은 서브쿼리 최적화가 도입되기 이전에 수동으로 쿼리를 튜닝하던 대표적인 방법이었다.\n1 2 3 4 5 6 7 EXPLAIN SELECT * FROM employees e WHERE emp_no IN ( SELECT de.emp_no FROM dept_emp de WHERE de.dept_no=\u0026#39;d009\u0026#39; ); id select_type table type key rows Extra 1 SIMPLE de ref PRIMARY 46012 Using Index 1 SIMPLE e eq_ref PRIMARY 1 NULL MySQL 8.0 버전에서 이 쿼리의 실행 계획은 위와 같다.\ndept_emp 테이블과 employees 테이블이 순서대로 표시되어 있는데, 가장 중요한 부분은 id 컬럼 값이 모두 1이라는 것이다. 이 값이 동일한 값을 가진다는 것은 두 테이블이 서브쿼리 형태가 아니라 조인으로 처리되었음을 의미한다.\nTable pullout 최적화는 별도로 실행 계획의 Extra 컬럼에 특별한 문구를 표시하지 않는다. 그렇기 때문에 Table pullout 최적화가 사용됐는지는 실행 계획에서 해당 테이블들의 id 컬럼 값이 일치하는지 비교해보는 것이 가장 간단하다.\nTable pullout 최적화가 사용됐는지 더 정확하게 확인하는 방법은 EXPLAIN 명령을 실행한 직후 SHOW WARNINGS 명령으로 MySQL 옵티마이저가 재작성한 쿼리를 살펴보는 것이다.\nTable pullout 최적화는 모든 형태의 서브쿼리에서 사용될 수 있는 것은 아니다.\n세미 조인 서브쿼리에서만 사용 가능하다. 서브쿼리 부분이 UNIQUE 인덱스나 프라이머리 키 룩업으로 결과가 1건인 경우에만 사용 가능하다. Table pullout이 적용된다고 하더라도 기존 쿼리에서 가능했던 쵲거화 방법이 사용 불가능한 것은 아니므로 MySQL에서는 가능하다면 Table pullout 최적화를 최대한 적용한다. 서브쿼리의 테이블을 아우터 쿼리로 가져와서 조인으로 풀어쓰는 최적화를 수행하는데, 만약 서브쿼리의 모든 테이블이 아우터 쿼리로 끄집어 낼 수 있다면 서브쿼리 자체는 없어진다. MySQL에서는 \u0026ldquo;최대한 서브쿼리를 조인으로 풀어서 사용해라\u0026rdquo; 라는 튜닝 가이드가 많은데, Table pullout 최적화는 이 가이드를 그대로 실행하는 것이므로 서브쿼리를 조인으로 풀어서 사용할 필요가 없어졌다. 퍼스트 매치(firstmatch) First Match 최적화 전략은 IN (subquery) 형태의 세미 조인을 EXISTS (subquery) 형태로 튜닝한 것과 비슷한 방법으로 실행된다.\n1 2 3 4 5 6 7 8 9 EXPLAIN SELECT * FROM employees e WHERE e.first_name=\u0026#39;Matt\u0026#39; AND e.emp_no IN ( SELECT t.emp_no FROM titles t WHERE t.from_date BETWEEN \u0026#39;1995-01-01\u0026#39; AND \u0026#39;1995-01-30\u0026#39; ) ; id table type key rows Extra 1 e ref ix_firstname 233 NULL 1 t ref PRIMARY 1 Using where; Using index; FirstMatch(e) 실행 계획의 id 컬럼 값이 모두 1로 표시된 것으로 봐서 서브쿼리 패턴이 아닌 조인으로 처리되었음을 알 수 있다.\n\u0026ldquo;FirstMatch(e)\u0026rdquo; 문구는 employees 테이블의 레코드에 대해 titles 테이블에 일치하는 레코드 1건만 찾으면 더이상의 titles 테이블 검색을 하지 않는다는 것을 의미한다. 실제 의미론적으로 EXISTS (subquery)와 동일하게 처리된 것이다. 하지만 FirstMatch는 서브쿼라 아니라 조인으로 풀어서 실행하면서 일치하는 첫번째 레코드만 검색하는 최적화를 실행한 것이다.\nFirst Match 최적화는 MySQL 5.5에서 수행했던 최적화 방법은 IN-to-EXISTS 변환과 거의 비슷한 처리 로직을 수행하지만 First Match 최적화 전략은 다음과 같은 장점이 있다.\n여러 테이블이 조인되는 경우 원래 쿼리에는 없던 동등 조건을 옵티마이저가 자동으로 추가하는 형태의 최적화가 실행되기도 한다. 기존의 IN-to-EXISTS 최적화에서는 이러한 동등 조건 전파가 서브쿼리 내에서만 가능했지만 FirstMatch에서는 조인 형태로 처리되기 때문에 서브쿼리뿐만 아니라 아우터 쿼리의 테이블까지 전파될 수 있다. FirstMatch 최적화로 실행되면 더 많은 조건이 주어지는 것이므로 더 나은 실행계획을 수립할 수 있다. IN-to-EXISTS 변환 최적화 전략에서는 아무런 조건 없이 변환이 가능한 경우네는 무조건 그 최적화를 수행했지만 FirstMatch 최적화에서는 서브쿼리의 모든 테이블에 대해 최적화를 수행할지 일부 테이블에서만 수행할지 취사선택 할 수 있다는 것이 장점이다. FirstMatch 최적화 또한 특정 형태의 서브쿼리에서 자주 사용되는 최적화이다.\n서브쿼리에서 하나의 레코드만 검색되면 더이상의 검색을 멈추는 단축 실행 경로이기 때문에 서브쿼리는 그 서브쿼리가 참조하는 모든 아우터 에이블이 먼저 조회된 이후에 실행된다. 사용된다면 실행 계획의 Extra 컬럼에는 FirstMatch(table-N) 문구가 표시된다. 상관 서브쿼리(Correlated subquery)에서도 사용될 수 있다. GROUP BY나 집합 함수가 사용된 서브쿼리의 최적화에는 사용될 수 없다. 루스 스캔(loosescan) 세미 조인 서브쿼리 최적화의 LooseScan은 인덱스를 사용하는 GROUP BY 최적화 방법에서 살펴본 Using index for group-by의 루스 인덱스 스캔과 비슷한 읽기 방식을 사용한다.\n1 2 3 4 5 6 7 EXPLAIN SELECT * FROM departments d WHERE d.dept_no IN ( SELECT de.dept_no FROM dept_emp de ); id table type key rows Extra 1 de index PRIMARY 331143 Using index; LooseScan 1 d eq_ref PRIAMRY 1 NULL departments 테이블의 레코드 건수는 9건뿐이지만 dept_emp 테이블의 레코드 건수는 약 33만건 저장되어있다. 그런데 dept_emp 테이블에는 (dept_no + emp_no) 컬럼의 조합으로 프라이머리 키 인덱스가 만들어져 있다. 이 프라이머리 키는 전체 레코드 수는 33만 건 정도 있지만 dept_no 만으로 그루핑해서 보면 결국 9건 뿐이므로 dept_emp 테이블의 프라이머리 키를 루스 인덱스 스캔으로 유니카한 dept_no만 읽으면 효율적으로 서브쿼리 부분을 실행할 수 있다.\n서브쿼리에 사용된 dept_emp 테이블이 드라이빙 테이블로 실행되며, dept_emp 테이블의 프라이머리 키를 dept_no 부분에서 유니크하게 한 건씩만 읽고 있다는 것을 보여준다. 루스 인덱스 스캔의 Using index for group-by도 동일하게 작동한다.\nLooseScan 최적화는 다음과 같은 특성을 지닌다.\n루스 인덱스 스캔으로 서브쿼리 테이블을 읽고, 그다음으로 아우터 테이블을 드리븐으로 사용해서 조인을 수행한다. 그래서 서브쿼리 부분이 루스 인덱스 스캔을 사용할 수 있는 조건이 갖춰져야 사용할 수 있는 최적화다. 다음과 같은 형태의 서브쿼리들에서 사용할 수 있다. 1 2 SELECT .. FROM .. WHERE expr IN (SELECT keypart1 FROM tab WHERE ...) SELECT .. FROM .. WHERE expr IN (SELECT keypart2 FROM tab WHERE keypart1=\u0026#39;상수\u0026#39;) 구체화(Materialization) Materialization 최적화는 세미 조인에 사용된 서브쿼리를 통째로 구체화해서 쿼리를 최적화한다는 의미다. 구체화는 쉽게 표현하면 내부 임시 테이블을 생성한다는 것을 의미한다.\n1 2 3 4 5 6 7 8 EXPLAIN SELECT * FROM employees e WHERE e.emp_no IN ( SELECT de.emp_no FROM dept_emp de WHERE de.from_date=\u0026#39;1995-01-01\u0026#39; ); id select_type table type key ref 1 SIMPLE ALL NULL NULL 1 SIMPLE e eq_ref PRIMARY .emp_no 2 MATERIALIZED de ref ix_fromdate const 위 쿼리를 FirstMatch 최적화를 사용하면 employees 테이블에 대한 조건이 서브쿼리 이외에는 아무것도 없기 때문에 employees 테이블을 풀 스캔해야 하므로 성능 향상에 도움이 되지 않는다.\nMySQL 서버 옵티마이저는 이런 형태의 쿼리를 위해 서브쿼리 구체화(Subquery Materialization)라는 최적화를 도입했다. 실행 계획 마지막 라인의 select_type 컬럼에 \u0026ldquo;MATERIALIZED\u0026quot;라고 출력된다.\n이 쿼리에서 사용하는 테이블은 2개이지만 임시 테이블()이 생성되어 3개 라인이 출력된다. dept_emp 테이블을 읽는 서브퀄가 먼저 실행되어 그결과로 임시 테이블을 만들고, 서브쿼라 구체화된 임시 테이블과 employees 테이블을 조인해서 결과를 반환한다.\nMaterialization 최적화는 다른 서브쿼리와 달리, 서브 쿼리 내에 GROUP BY절이 있어도 사용할 수 있다.\n1 2 3 4 5 6 7 8 9 EXPLAIN SELECT * FROM employees e WHERE e.emp_no IN ( SELECT de.emp_no FROM dept_emp de WHERE de.from_date=\u0026#39;1995-01\u0026#39;01\u0026#39; GROUP BY de.dept_no ); Materialization 최적화가 사용될 수 있는 형태의 쿼리에도 역시 몇 가지 제한사항과 특성이 있다.\nIN (subpuery)에서 서브쿼리는 상관 서브쿼리가 아니어야 한다. 서브쿼리는 GROUP BY나 집합 함수들이 사용되어도 구체화를 사용할 수 있다. 구체화가 사용된 경우에는 내부 임시 테이블이 사용된다. 세미조인이 아닌 서브쿼리의 최적화에서도 구최화를 이용한 최적화가 사용될 수 있다. 그러나 해당 옵션이 비활성화된다면 세미 조인이 아닌 서브쿼리 최적화에서도 구체화를 이용한 최적화는 사용되지 못한다.\n중복 제거(Duplicated Weed-out) Duplicated Weed-out은 세미 조인 서브쿼리를 일반적인 INNER JOIN 쿼리로 바꿔 실행하고 마지막에 중복된 레코드를 제거하는 방법으로 처리되는 최적화 알고리즘이다.\n1 2 3 4 5 6 7 8 EXPLAIN SELECT * FROM employees e WHERE e.emp_no IN ( SELECT s.emp_no FROM salaries s WHERE s.salary \u0026gt; 150000 ); salaries 테이블의 프라이머리 키가 (emp_no + from_date)이므로 salary가 150000 이상인 레코드를 salaries 테이블에서 조회하면 그 결과에는 중복된 emp_no가 발생할 수 있다.\n1 2 3 4 5 6 7 SELECT e.* FROM employees e salaries s WHERE e.emp_no=s.emp_no AND s.salary \u0026gt; 150000 GROUP BY e.emp_no ; 위와 같은 방식으로 GROUP BY를 활용하면 세미 조인 서브쿼리와 동일한 결과를 얻을 수 있다. 실제로 Duplicate Weedout 최적화 알고리즘은 원본 쿼리를 위와 같이 INNER JOIN + GROUP BY 절로 바꿔 실행하는 것과 동일한 작업으로 쿼리를 처리한다.\nsqlaries 테이블의 ix_salary 인덱스를 스캔해서 salary가 150000보다 큰 사원을 검색해 employees 테이블을 조인 조인된 결과를 임시 테이블에 저장 임시 테이블에 저장된 결과에서 emp_no 기준으로 중복 제거 중복을 제거하고 남은 레코드를 최종적으로 반환 id select_type table type key Extra 1 SIMPLE s range ix_salary Using where; Using index; Start temporary 1 SIMPLE e eq_ref PRIMARY End temporary 최적화를 이용한 예제 쿼리의 실행 계획은 다음과 같으며, 이 실행 계획에서는 \u0026ldquo;Duplicate Weedout\u0026quot;이라는 문구가 별도로 표시되지 않는다. 하지만 Extra 컬럼에 \u0026ldquo;Start temporary\u0026rdquo;, \u0026ldquo;End temporary\u0026rdquo; 문구가 별도로 표시된다.\nDuplicate Weedout 최적화는 다음과 같은 장점과 제약 사항이 있다.\n서브쿼리가 상관 서부쿼리라고 하더라도 사용할 수 있는 최적화다. 서브쿼리가 GROUP BY나 집합 함수가 사용된 경우에는 사용될 수 없다. 서브쿼리의 테이블을 조인으로 처리하기 때문에 최적화할 수 있는 방법이 많다. 컨디션 팬아웃(condition_fanout_filter) 조인을 실행할 때 드라이빙 테이블의 레코드를 기준으로 드리븐 테이블을 읽기 때문에 테이블의 순서는 쿼리의 성능에 매우 큰 영향을 미친다. 그래서 MySQL 옵티마이저는 여러 테이블이 조인되는 경우 가능하면 일치하는 레코드 건수가 적은 순서대로 조인을 실행한다.\n1 2 3 4 5 6 SELECT * FROM employees e INNER JOIN salaries s ON s.emp_no=e.emp_no WHERE e.first_name=\u0026#39;Matt\u0026#39; AND e.hire_date BETWEEN \u0026#39;1985-11-21\u0026#39; AND \u0026#39;1986-11-21\u0026#39; ; Condition_fanout_filter 비활성화 실행 계획\nid table type key rows filtered Extra 1 e ref ix_firstname 233 100.00 Using where 1 s ref PRIMARY 10 100.00 NULL employees 테이블에서 ix_firstname 인덱스를 이용해 first_name='Matt' 조건에 일치하는 233건의 레코드를 검색 검색된 레코드 중에서 hire_date가 \u0026lsquo;1985-11-21\u0026rsquo; 부터 \u0026lsquo;1986-11-21\u0026rsquo;일 사이인 레코드만 걸러냄. 이 실행계획에서는 filtered 컬럼의 컬럼 값이 100인 것은 옵티마이저가 233건 모두 조건을 만족할 것으로 예측했다는 의미 employees 테이블을 읽은 결과에 대해 salaries 테이블의 프라이머리 키를 이용해 salaries 테이블의 레코드를 읽는다. MySQL 옵티마이저는 employees 테이블의 레코드 한 건당 salaries 테이블의 레코드 10건이 일치할 것으로 예상함. Condition_fanout_filter 비활성화 실행 계획\nid table type key rows filtered Extra 1 e ref ix_firstname 233 23.20 Using where 1 s ref PRIMARY 10 100.00 NULL rows 컬럼의 값은 233으로 동일하지만 filtered 컬럼의 값이 23.20로 바뀌었다. MySQL 옵티마이저는 인덱스를 사용할 수 있는 first_name 컬럼 조건 이외의 나머지 조건(hire_date)에 대해서도 얼마나 조건을 충족할지를 고려했다는 의미이다.\n즉 condition_fanout_filter 최적화가 비활성화 된 경우에는 employees 테이블에서 모든 조건을 충족하는 레코드가 233건일 것으로 예측했지만, 활성화시 54건만 조건을 충족할 것이라고 예측한것이다. MySQL 옵티마이저가 조건을 만족하는 레코드 건수를 정확하게 예측할 수 있다면 더 빠른 실행 계획을 만들어 낼 수 있다.\n해당 최적화가 활성화되면 다음과 같은 조건을 만족하는 컬럼의 조건들에 대해 조건을 만족하는 레코드의 비율을 계산할 수 있다.\nWHERE 조건절에 사용된 컬럼에 대해 인덱스가 있는 경우 WHERE 조건절에 사용된 컬럼에 대해 히스토그램이 존재하는 경우 예제 쿼리가 실제 실행되는 경우에는 ix_firstname 인덱스만 사용한다. 하지만 실행 계획을 수립하는 경우 해당 인덱스를 통해 조건이 일치하는 레코드를 233건 정도라는 것을 알아내고, hire_date 컬럼의 조건을 만족하는 레코드의 비율을 대략 23.2%일 것으로 예측한다. hire_date 컬럼의 인덱스가 없다면 MySQL 옵티마이저는 first_name 컬럼의 인덱스를 이용해 hire_date 컬럼의 분포도를 살펴보고 filterd컬럼의 값을 예측한다.\nMySQL 옵티마이저가 실행 계획을 수립할 때 테이블이나 인덱스의 통계 정보만 사용하는 것이 아니라 다음의 순서대로 사용 가능한 방식을 선택한다.\n옵티마이저(Range optimizer)를 이용한 예측 히스토그램을 이용한 예측 인덱스 통계를 이용한 예측 추측에 기반한 예측(Guesstimates) 레인지 옵티마이저는 실제 인덱스의 데이터를 살펴보고 레코드 건수를 예측하는 방식인데, 실제 쿼리 실행 전 실행 계획 수립 단계에서 빠르게 소량의 데이터를 읽어본다. (인덱스를 이용해서 쿼리가 실행될 수 있을때만)\ncondition_fanout_filter 최적화 기능을 활성화하면 MySQL 옵티마이저는 더 정교한 계산을 거쳐 실행 계획을 수립한다.\n그에 따라 쿼리의 실행 계획 수립에 더 많은 시간과 컴퓨팅 자원을 사용하게 되므로, MySQL 8.0 이전 버전에서도 쿼리 실행 계획이 잘못된 선택을 한 적이 별로 없다면 성능 향상에 크게 도움이 되지 않을수도 있고, 실행 계획 수립의 오버헤드가 더 크게 보일 수 있으므로 업그레이드 전 확인이 필요하다.\n파생 테이블 머지(derived_merge) 1 2 3 4 5 6 7 8 9 EXPLAIN SELECT * FROM ( SELECT * FROM employees WHERE first_name=\u0026#39;Matt\u0026#39; ) derived_table WHERE derived_table.hire_date=\u0026#39;1986-04-03\u0026#39; ; 이전 버전의 MySQL 서버 처리 방식\nid select_type table type key 1 PRIMARY ref \u0026lt;auto_key0\u0026gt; 2 DERIVED employees ref ix_firstname employees 테이블을 읽는 라인의 select_type 컬럼 값이 DREIVED라고 표시되어 있는데, 이는 employees 테이블에서 first_name 컬럼 값이 \u0026lsquo;Matt\u0026rsquo;인 레코드들만 읽어 임시 테이블을 생성하고, 이 임시 테이블을 다시 읽어 hire_date 컬럼 값이 \u0026lsquo;1986-04-03\u0026rsquo;인 레코드만 걸러내어 봔환한 것이다. MySQL 서버에서는 FROM 절에 사용된 서브쿼리를 파생 테이블이라고 부른다.\n이 실행 계획은 경우 MySQL 서버는 내부적으로 임시 테이블을 생성하고 first_name='Matt 인 레코드를 employees 테이블에서 읽어 임시 테이블로 INSERT 한다. 그 후 다시 임시 테이블을 읽으므로 MySQL 서버는 레코드를 복사하고 읽는 오버헤드가 더 추가된다.\n내부적으로 생성되는 임시 테이블은 처음에는 메모리에 생성되지만, 임시 테이블에 저장될 레코드 건수가 많아지면 결국 디스크로 다시 기록되므로 임시 테이블이 메모리에 상주할 만큼 크기가 작다면 성능에 큰 영향을 미치지 않지만 레코드가 많아진다면 임시 테이블로 레코드를 복사하고 읽는 오버헤드로 인해 쿼리의 성능은 많이 느려진다.\nMySQL 5.7 이후 처리 방식\nMySQL 5.7 버전부터 파생 테이블로 만들어지는 서브쿼리를 외부 쿼리와 병합해서 서브쿼리 부분을 제거하는 최적화가 도입되었다.\nid select_type table type key 1 SIMPLE employees index_merge ix_hiredate, ix_firstname 이 실행 계획에서는 select_type 컬럼이 DERIVED였던 라인이 없어지고, 서브쿼리 없이 employees 테이블을 조회하단 형태의 단순 실행 계획으로 바뀌었다. SHOW WARNINGS 명령으로 옵티마이저가 새로 작성한 쿼리를 살펴보면 서브쿼리 부분이 어떻게 외부 쿼리로 병합되었는지 확인할 수 있다.\n1 2 3 4 5 6 SELECT employees.employees.emp_no , ... FROM employees.employees WHERE employees.employees.hire_date = DATE\u0026#39;1986-04-03\u0026#39; AND employees.employees.first_name = \u0026#39;Matt\u0026#39; \u0026#39; 예전 버전의 MySQL 서버에서는 서브쿼리로 작성된 쿼리를 외부 쿼리로 병합하는 작업을 DBA가 수작업으로 많이 처리했으나, 옵티마이저가 처리할 수 있어 굳이 쿼리를 새로 작성할 필요는 없어졌다.\n하지만 모든 쿼리에 대해 옵티마이저가 서브쿼리를 외부 쿼리로 병합할 수 있는것은 아니며, 다음과 같은 조건에서는 옵티마이저가 자동으로 서브쿼리를 외부 쿼리로 병합할 수 없게 된다. 따라서 가능하다면 서브쿼리는 외부 쿼리로 수동으로 병합해서 작성하는 것이 쿼리의 성능 향상에 도움이 된다.\nSUM(), MIN(), MAX() 같은 집계 함수와 윈도우 함수(Window Function)가 사용된 서브쿼리 DISTINCT가 사용된 서브쿼리 GROUP BY, HAVING이 사용된 서브 쿼리 LIMIT이 사용된 서브쿼리 UNION, UNION ALL을 포함하는 서브쿼리 SELECT 절에 사용된 서브쿼리 값이 변경되는 사용자 변수가 사용된 서브쿼리 인비저블 인덱스(use_invisivble_indexes) MySQL 8.0 이전 버전까지는 인덱스가 존재하면 항상 옵티마이저가 실행 계획을 수립할 때 해당 인덱스를 검토하고 사용했으나, 인덱스를 삭제하지 않고, 해당 인덱스를 사용하지 못하게 제어하는 기능을 제공한다.\nALTER TABLE ... ALTER INDEX ... [ VISIBLE | INVISIBLE ] 명령으로 인덱스의 가용 상태를 변경할 수 있다.\n해당 옵션을 이용하면 INVISIBLE로 설정된 인덱스라 하더라도 옵티마이저가 사용하게 제어할 수 있다.\n1 SET optimizer_switch=\u0026#39;use_invisible_indexes=on\u0026#39;; 스킵 스캔(skip_scan) 인덱스의 핵심은 값이 정렬되어 있다는 것이며, 이로 인해 인덱스를 구성하는 컬럼의 순서가 매우 종요하다. (A, B, C) 컬럼으로 구성된 인덱스가 있을 때의 WHERE 절에 B와 C 컬럼에 대한 조건이 있다면 쿼리는 인덱스를 사용할 수 없다. 인덱스 스킵 스캔은 제한적이긴 하지만 인덱스의 이런 제약 사항을 뛰어넘을 수 있는 최적화 기법이다.\n1 2 ALTER TABLE employees ADD INDEX ix_gender_birthdate (gender, birth_date); 위와 같은 테이블이 있다면 WHERE 조건절에 gender 컬럼에 대한 비교 조건이 필수적이다.\n1 2 3 4 5 6 7 8 9 10 11 12 // 인덱스를 사용하지 못하는 쿼리 SELECT * FROM employees WHERE birth_date \u0026gt;= \u0026#39;1965-02-01\u0026#39; ; // 인덱스를 사용할 수 있는 쿼리 SELECT * FROM employees WHERE gender=\u0026#39;M\u0026#39; AND birth_date \u0026gt;= \u0026#39;1965-02-01\u0026#39; ; 위 쿼리에서 처음 쿼리는 인덱스를 사용할 수 없어 birth_date 컬럼부터 시작하는 인덱스를 새로 생성해야했으나, MySQL 8.0 버전부터는 인덱스 스킵 스캔 최적화가 도입되어 인덱스의 선행 컬럼이 조건절에 사용되지 않더라도 후행 컬럼만의 조건으로도 인덱스를 이용한 쿼리 성능 개선이 가능하다.\n첫 번째 쿼리를 실행할 때 옵티마이저는 테이블에 존재하는 모든 gender 컬럼의 값을 가져와 두 번째 쿼리와 같이 gender 컬럼의 조건이 있는 것처럼 쿼리를 최적화한다. 따라서 인덱스의 선행 컬럼이 매우 다양한 값을 가지는 경우 인덱스 스킵 스캔 최적화가 비효율적일 수 있게된다. 따라서 인덱스의 선행 컬럼이 소수의 유니크한 값을 가질때만 인덱스 스킵 스캔 최적화를 사용한다.\n해시 조인 MySQL 8.0.18 버전부터는 해시 조인이 추가로 지원되기 시작했다. 해시 조인 기능은 기존 네스티드 루프 조인보다 해시 조인이 빠르다고 생각하지만 항상 참은 아니다.\n해시 조인은 첫 번째 레코드를 찾는 데는 시간이 많이 걸리지만 최종 레코드를 찾는 데 까지는 시간이 많이 걸리지 않는다. 네스티드 루프 조인은 마지막 레코드를 찾는 데 까지는 시간이 많이 걸리지만 첫 번째 레코드를 찾는것은 상대적으로 빠르다.\n즉, 해시 조인 쿼리는 최고 스루풋(Best Throughput) 전략에 적합하고, 네스티드 루프 조인은 최고 응답 속도(Best Response-time) 전략에 적합하다.\n일반적인 웹 서비스는 온라인 트랜잭션 서비스이기 때문에 스루풋도 중요하지만 응답 속도가 더 중요하다. 분석과 같은 서비스는 사용자의 응답 시간보다는 전체적으로 처리 소요시간이 중요하기 때문에 응답속도보다는 전체 스루풋이 중요하다. MySQL 서버는 온라인 트랜잭션 처리를 위한 범용 RDBMS이므로 서버의 응답 속도가 더 중요할 수 있다. 이러한 이유로 MySQL 서버는 주로 조인 조건의 컬럼이 인덱스가 없다거나 조인 대상 테이블 중 일부의 레코드 건수가 매우 적은 경우에 대해서만 해시 조인 알고리즘을 사용하도록 설계되어있다.\n해시 조인 최적화는 네스티드 루프 조인의 차선책 같은 기능이므로 옵티마이저 힌트를 이용해서 강제로 쿼리 실행 계획을 해시 조인으로 유도하는 것은 좋지 않다.\n해시 조인 기능이 없었을때는 조인 조건이 인덱스를 제대로 활용할 수 없는 경우 블록 네스티드 루프 조인이라는 조인 알고리즘을 사용했다. 따라서 인덱스가 잘 설계된 데이터베이스에서는 블록 네스티드 루프 조인 실행 계획은 거의 볼 수 없었다. 블록 기반의 네스티드 루프 조인에서 블록은 join_buffer_size 시스템 변수로 크기를 조정할 수 있는 메모리 공간을 의하는데, 조인 버퍼를 무한정 크게 설정할 수 없고, 조인 대상 테이블의 레코드 크기가 조인 버퍼보다 큰 경우 드라이빙 테이블을 여러 번 반복해서 스캔해야 하는 문제가 있었다.\n그래서 MySQL 8.0.18 ~ 8.0.19 버전에서는 동등 조인(Equi-Join)을 위해서는 해시 조인이 사용됐지만 안티 조인이나 세미 조인을 위해서는 블록 네스티드 루프 조인이 사용됐다. MySQL 8.0.20 버번부터는 block_nested_loop 같은 optimizer_switch 또는 BNL과 NO_BNL과 같은 힌트들도 블록 네스티드 루프가 아닌 해시 조인을 유도하는 목적으로 사용된다.\n1 2 3 4 5 6 EXPLAIN SELECT * FROM employees e IGNORE INDEX(PRIMARY, ix_hiredate) INNER JOIN dept_emp de IGNORE INDEX(ix_empno_fromdate, ix_fromdate) ON de.emp_no=e.emp_no AND de.from_date=e.hire_Date ; id select_type table type Extra 1 SIMPLE de ALL NULL 1 SIMPLE e ALL Using where; Using join buffer (hash join) IGNORE로 인해 통해 인덱스를 활용하지 못하여 옵티마이저는 적절한 인덱스를 찾을수 없어 해시 조인을 활용하게 된다.\n일반적으로 해시 조인은 빌드 단계(Build-phase) 와 프로브 단계(Probe-phase)로 나뉘어 처리된다.\n빌드단계 조인 대상 테이블 중에서 레코드 건수가 적어 해시 테이블로 만들기에 용이한 테이블을 골라 메모리에 해시 테이블을 생성(빌드)하는 작업을 수행한다. 해시 테이블을 만들 때 사용되는 원본 테이블을 빌드 테이블이라고도 한다. 프로브 단계 나머지 해시 테이블의 레코드를 읽어 해시 테이블의 일치 레코드를 찾는 과정을 의미한다. 읽는 나머지 테이블을 프로브 테이블이라고도 한다. 위의 실행계획에서는 어느 테이블이 빌드 테이블이고 어느 테이블이 프로브 테이블인지 식별하기 어렵다. 이러한 경우에는 EXPLAIN FORMAT=TREE 명령 또는 EXPLAIN ANALYZE 명령을 사용하면 조금 더 쉽게 구분할 수 있다.\n1 2 3 4 5 6 EXPLAIN FORMAT=TREE SELECT * FROM employees e IGNORE INDEX(PRIMARY, ix_hiredate) INNER JOIN dept_epq de IGNORE INDEX(ix_empno_fromdate, ix_fromdate) ON de.emp_no=e.emp_no AND de.from_date=e.hire_date \\G 1 2 3 4 5 -\u0026gt; Inner hash join (e.hire_date = de.from_date), (e.emp_no = de.emp_no) (cost=9942694661.05 rows=331143) -\u0026gt; Table scan on e (cost=0.08 rows=300252) -\u0026gt; Hash -\u0026gt; Table scan on de (cost=33979.30 rows=331143) Tree 포맷 실행 계획에서 최하단 제일 안쪽의 dept_emp 테이블이 빌드 테이블로 선정되었다. MySQL 옵티마이저는 해시 조인을 위해 빌드 테이블인 dept_emp 테이블의 레코드를 읽어서 메모리에 해시 테이블을 생성했다. 그리고 프로브 테이블로 선택된 employees 테이블을 스캔하면서 메모리에 생성된 해시 테이블에서 레코드를 찾아 결과를 사용자에게 반환한다.\n해시 테입르을 메모리에 저장할 때 MySQL 서버는 join_buffer_size 시스템 변수로 크기를 제어할 수 있는 조인 버퍼를 사용한다. 해시 테이블의 레코드 건수가 많아 공간이 부족할 경우 MySQL 서버는 빌드 테입르과 프로브 테이블을 적당한 크기의 청크로 분리 후 청크별로 동일 방식으로 해시 조인을 처리한다.\n만들어질 해시 테이블이 설정된 메모리 크기보다 큰지 알 수 없기 때문에 해시 조인의 처리 방법이 복잡해진다. MySQL 서버는 dept_emp 테이블을 읽으며 메모리의 해시 테이블을 준비하다가 지정된 메모리 크기를 넘어서면 dept_emp 테이블의 나머지 레코드를 디스크에 청크로 구분하여 저장하고 employees 테이블의 emp_no 값을 이용해 메모리의 해시 테이블을 검색하여 1차 조인 결과를 생성한다.\n동시에 employees 테이블에서 읽은 레코드를 디스크에 청크로 구분해 저장한다. 이때 \u0026ldquo;빌드 테이블 청크\u0026quot;는 dept_emp 테이블의 레코드들을 저장해둔 공간이고, \u0026ldquo;프로브 테이블 청크\u0026quot;는 employees 테이블의 레코드들을 저장해둔 공간이다.\n1차 조인이 완료되면 MySQL 서버는 디스크에 저장된 \u0026ldquo;빌드 테이블 청크\u0026quot;에서 첫 번째 청크를 읽어 다시 \u0026ldquo;메모리 해시 테이블\u0026quot;을 구축한다. 그리고 \u0026ldquo;프로브 테이블 청크\u0026quot;에서 첫 번째 청크를 읽으면서 새로 구축된 \u0026ldquo;메모리 해시 테이블\u0026quot;과 조인을 수행해 2차 결과를 가져온다.\n이후 디스크에 저장된 청크 개수만큼 반복 처리해서 완성된 조인 결과를 만들어내고, 청크 단위로 조인을 수행하기 위해 2차 해시 함수를 이용해 \u0026ldquo;빌드 테이블\u0026quot;과 \u0026ldquo;프로브 테이블\u0026quot;을 동일 개수의 청크로 쪼개 디스크에 저장한다.\nMySQL 옵티마이저는 빌드 테이블의 크기에 따라 메모리에서 모두 처리 가능한 해시 조인의 경우 클래식 해시 조인 알고리즘을 사용하고, 해시 테이블이 조인 버퍼 메모리보다 큰 경우 그레이스 해시 조인 알고리즘을 하이브리드하게 활용하도록 구현되어있다.\n인덱스 정렬 선호(prefer_ordering_index) MySQL 옵티마이저는 ORDER BY, GROUP BY 처리시 인덱스를 사용 가능한 경우 쿼리의 실행 계획에서 인덱스의 가중치를 높게 설정해 실행된다.\n1 2 3 4 5 6 EXPLAIN SELECT * FROM employees WHERE hiring_date BETWEEN \u0026#39;1985-01-01\u0026#39; AND \u0026#39;1985-02-01\u0026#39; ORDER BY emp_no ; id table key rows Extra 1 employees PRIMARY 300252 Using where 선택될 수 있는 대표적인 실행계획\nix_hiredate 인덱스를 이용해 조건에 일치하는 레코드를 찾은 다음 emp_no로 정렬해서 결과를 반환 employees 테이블의 프라이머리 키가 emp_no이므로 프라이머리 키를 정순으로 읽으며 hire_date 컬럼의 조건에 일치하는지 비교 후 결과를 반환 상황에 따라 다르지만 일반적으로 hire_date 컬럼의 조건에 부합되는 레코드 건수가 많지 않다면 1번으로 실행될 것 같지만, 실행 계획에서는 체므해야 하는 레코드 건수가 상당히 많음에도 불구하고 PRIMARY 키를 풀 스캔하면서 hire_date 컬럼을 필터링하도록 쿼리를 처리하고 있다. 이는 정렬된 인덱스를 활용하도록 하기 위해 잘못된 실행계획을 선택했을 확률이 높다.\nMySQL 8.0.20 버전까지는 이러한 옵티마이저의 실수가 자주 발생하면 IGNORE INDEX 힌트를 사용하여 ORDER BY를 위한 특정 인덱스를 사용하지 못하도록 했으나, MySQL 8.0.21 버전부터는 옵티마이저가 ORDER BY를 위한 인덱스에 너무 가중치를 부여하지 않도록 prefer_ordering_index 옵티마이저 옵션이 추가되었다.\n기본값은 ON 이지만 비효율적인 선택을 자주 한다면 OFF로 변경할 수 있다.\n","date":"2023-06-06T12:30:10+09:00","image":"https://codemario318.github.io/post/real-mysql/9/3/1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/9/3/1/","title":"9.3.1 옵티마이저와 힌트 - 고급 최적화(옵티마이저 스위치 옵션)"},{"content":"GROUP BY 처리 GROUP BY 또한 ORDER BY와 같이 쿼리가 스트리밍된 처리를 할 수 없게 하는 처리 중 하나다. GROUP BY절 사용시 결과에 대한 필터링 처리를 할 수 있는 HAVING절을 사용할 수 있느데, GROUP BY 에서 사용된 조건은 인덱스를 사용해서 처리될 수 없으므로 HAVING절을 튜닝하려고 인덱스를 생성하거나 다른 방법을 고민할 필요는 없다.\nGROUP BY 작업도 인덱스를 사용하는 경우과 그렇지 못한 경우로 나눌 수 있다.\n인덱스를 이용할 때 인덱스 스캔: 인덱스를 차례대로 읽는다. 루스 인덱스 스캔: 인덱스를 건너뛰면서 읽는다. 인덱스를 이용하지 않을 때 임시 테이블을 사용한다. (타이트) 인덱스 스캔을 사용하는 GROUP BY ORDER BY의 경우와 마찬가지로 조인의 드라이빙 테이블에 속한 컬럼만을 이용해 그루핑 할 때 GROUP BY 컬럼으로 이미 인덱스가 있다면 그 인덱스를 차례대로 읽으며 그루핑 작업을 수행하고 그 결과로 조인을 처리한다.\nGROUP BY가 인덱스를 통해 처리된다고 하더라도 그룹 함수 등의 그룹값을 처리로 인해 임시 테이블이 필요할 때도 있다. GROUP BY가 인덱스를 통해 처리되는 쿼리는 이미 정렬된 인덱스를 읽는 것이므로 쿼리 실행 시점에 추가적인 정렬 작업이나 내부 임시 테이블은 필요하지 않다. 이러한 그루핑 방식을 사용하는 쿼리의 실행 계획에서는 Extra 컬럼에 별도로 \u0026ldquo;Using index for group-by\u0026rdquo;, \u0026ldquo;Using temporary, Using filesort\u0026quot;가 표시되지 않는다. 루스 인덱스 스캔을 사용하는 GROUP BY 루스 인덱스 스캔 방식은 인덱스의 레코드를 건너 뛰면서 필요한 부분만 읽어서 가져오는 것을 의미하는데, 옵티마이저가 루스 인덱스 스캔을 사용할 때는 실행 계획의 Extra 컬럼에 \u0026ldquo;Using index for group-by\u0026rdquo; 코멘트가 표시된다.\n1 2 3 4 5 6 EXPLAIN SELECT emp_no FROM salaries WHERE from_date = \u0026#39;1985-03-01\u0026#39; GROUP BY emp_no ; id table type key Extra 1 salaries range PRIMARY Using where; Using index for group-by salaries 테이블의 인덱스는 (emp_no, from_date)로 생성돼 있으므로 위의 쿼리 문장에서 WHERE 조건은 인덱스 레인지 스캔 접근 방식으로 이용할 수 없는 쿼리지만, 인덱스 레인지 스캔을 이용했으며, GROUP BY 처리까지 인덱스르 사용했다.\n(emp_no, from_date) 인덱스를 차례대로 스캔하면서 emp_no의 첫 번째 유일한 값(그룹 키)를 찾아낸다. (emp_on, from_date) 인덱스에서 emp_no가 \u0026lsquo;10001\u0026rsquo;인 것 중에서 from_date 값이 \u0026lsquo;1985-03-01\u0026rsquo;인 레코드만 가져온다. 인덱스에서 emp_no의 그다음 유니크한(그룹 키) 값을 가져온다. 3번 단계에서 결과가 더 없으면 처리를 종료하고, 결과가 있다면 2번 과정으로 돌아가서 반복 수행한다. MySQL 루스 인덱스 스캔 방식은 단일 테이블에 대해 수행되는 GROUP BY 처리에만 사용할 수 있다. 프리픽스 인덱스(Prefix index, 컬럼의 앞쪽 일부만으로 생성된 인덱스)는 루스 인덱스 스킨을 사용할 수 없다. 인덱스 레인지 스캔에서는 유니크한 값의 수가 많을수록 성능이 향상되는 반면 루스 인덱스 스캔에서는 인덱스의 유니크한 값의 수가 적을수록 성능이 향상된다. 루스 인덱스 스캔은 분포도가 좋지 않은 인덱스일수록 더 빠른 결과를 만들어낸다. 루스 인덱스 스캔으로 처리되는 쿼리에서는 별도의 임시 테이블이 필요하지 않다. 루스 인덱스 스캔이 사용될 수 있을지 없을지 판단하는 것은 WHERE절의 조건이나 ORDER BY 절이 인덱스를 사용할 수 있을지 없을지 판단하는 것보다는 더 어렵다.\nMIN, MAX 의외의 집합 함수가 사용된 경우 루스 인덱스 스캔 사용 불가 GROUP BY에 사용된 컬럼이 인덱스 구성 컬럼의 왼쪽부터 일치하지 않는 경우 사용 불가 SELECT 절의 컬럼이 GROUP BY와 일치하지 않는 경우 사용 불가 MySQL 8 버전부터는 루스 인덱스 스캔과 동일한 방식으로 작동하는 인덱스 스킵 스캔 최적화도 도입됐다. 이에 따라 옵티마이저가 쿼리에서 필요로 하는 레코드를 검색하는 부분까지 루스 인덱스 스캔 방식으로 최적화가 가능해졌다. 인덱스 스킵 스캔 또한 루스 인덱스 스캔과 마찬가지로 조건이 누락된 인덱스의 선행 컬럼이 유니크한 값을 많이 가질수록 쿼리 처리 성능이 떨어지게 되어 선행 컬럼의 유니크한 값의 개수가 많으면 인덱스 스킵 최적화를 사용하지 않게 된다.\n임시 테이블을 사용하는 GROUP BY 1 2 3 4 5 6 7 EXPLAIN SELECT e.last_name, AVG(s.salary) FROM employees e, salaries s WHERE s.emp_no = e.emp_no GROUP BY e.last_name ; id table type key rows Extra 1 e ALL NULL 300584 Using temporary 1 s ref PRIMARY 10 NULL 위 쿼리의 실행 계획에서는 Extra 컬럼에 \u0026ldquo;Using temporary\u0026rdquo; 메시지가 표시되었는데, 이는 employees를 풀 스캔하기 때문이 아니라 인덱스를 전혀 사용할 수 없는 GROUP BY이기 때문이다.\nExtra 컬럼에 \u0026ldquo;Using filesort\u0026quot;가 표시되지 않은 이유는 기존에는 GROUP BY가 사용된 쿼리는 그루핑 되는 컬럼을 기준으로 묵시적인 정렬까지 실행되었으나 MySQL 8 버전으로 업그레이드 되며 묵시적인 정렬을 처리하지 않기 때문이다.\nMySQL 8에서는 GROUP BY가 필요한 경우 내부적으로 GROUP BY 절의 컬럼들로 구성된 유니크 인덱스를 가진 임시 테이블을 만들어 중복 제거와 집합 함수 연산을 수행한다.\n1 2 3 4 5 CREATE TEMPORARY TABLE... ( last_name VARCHAR(16), salary INT, UNIQUE INDEX ux_lastname(last_name) ); 그리고 조인의 결과를 한 건씩 가져와 임시 테이블에서 중복 체크를 하면서 INSERT, UPDATE를 실행하여, 별도의 정렬 작업 없이 GROUP BY가 처리된다.\n하지만 MySQL 8.0에서도 ORDER BY가 같이 사용되면 명시적으로 정렬 작업을 실행한다.\nDISTINCT 처리 특정 컬럼의 유니크한 값만 조회하려면 SELECT 쿼리에 DISTINT를 사용한다. DISTINT 같은 경우 집합 함수와 함께 사용되는 경우 키워드가 영향을 미치는 범위가 다르다. 또한 집합 함수와 같이 사용될 때 인덱스를 사용하지 못할 때는 항상 임시 테이블이 필요하지만 Extra 컬럼에 \u0026ldquo;Using temporary\u0026rdquo; 메시지가 출력되지 않는다.\nSELECT DISTINCT ... 단순히 조회되는 레코드 중에서 유니크한 레코드만 가져오고자 하면 SELECT DISTINCT 형태의 쿼리 문장을 사용한다. 이 경우에는 GROUP BY와 동일한 방식으로 처리되며 특히 MySQL 8.0 버전부터는 GROUP BY를 수행하는 쿼리에 ORDER BY절이 없으면 정렬을 사용하지 않기 때문에 아래 쿼리는 같은 방식으로 처리된다.\n1 2 3 4 5 6 SELECT DISTINCT emp_no FROM salaries; SELECT emp_no FROM salaries GROUP BY emp_no; DISTINT는 SELECT하는 레코드(튜플)을 유니크하게 조회하는 것이지, 특정 컬럼만 유니크하게 조회하는 것이 아니다. 1 SELECT DISTINCT first_name, last_name FROM employees; DISTINT는 함수처럼 동작하지 않는다. 1 SELECT DISTINCT(first_name), last_name FROM employees; 실제 처리시 의미 없는 괄호로 판단하여 제거된다. SELECT 절에 사용된 DISTINCT 키워드는 조회되는 모든 컬럼에 영향을 미치지만, 집합 함수와 함께 사용된 경우 조금 다르게 처리된다.\n집합 함수와 함께 사용된 DISTINCT COUNT, MIN, MAX 같은 집합 함수 내에서 DISTINCT 키워드가 사용될 수 있는데, 이 경우에는 일반적으로 다른 형태로 해석된다. 집합 함수가 없는 조회 쿼리에서는 조회하는 모든 컬럼의 조합이 유니크한 것들만 가져오지만, 집합 함수 내에서 사용될 경우 집합 함수의 인자로 전달된 컬럼 값이 유니크한 것들을 가져온다.\n1 2 3 4 5 6 7 EXPLAIN SELECT COUNT(DISTINCT s.salary) FROM employees e, salaries s WHERE e.emp_no = s.emp_no AND e.emp_no BETWEEN 100001 AND 100100 ; id table type key rows Extra 1 e range PRIMARY 100 Using where; Using index 1 s ref PRIMARY 10 NULL 위 쿼리는 내부적으로 COUNT(DISTINCT s.salary)를 처리하기 위해 임시 테이블을 사용하지만, 실행 계획에서 \u0026ldquo;Using temporary\u0026quot;를 표시하지 않는다.\nemployees 테이블과 salaries 테이블을 조인한 결과에서 salary 컬럼의 값만 저장하기 위한 임시 테이블을 만드렁 사용한다. 이때 임시 테이블의 salary컬럼에는 유니크 인덱스가 생성되기 때문에 레코드 건수가 많아진다면 상당히 느려질 수 있다.\n1 2 3 4 5 6 7 SELECT COUNT(DISTINCT emp_no) FROM employees; SELECT COUNT(DISTINCT emp_no) FROM dept_emp GROUP BY dept_no ; id table type key rows Extra 1 dept_emp index PRIMARY 331143 Using index 인덱스된 컬럼에 대해 DISTINCT 처리를 수행할 때는 인덱스를 풀 스캔하거나 레인지 스캔하면서 임시 테이블 없이 최적화된 처리를 수행할 수 있다.\n내부 임시 테이블 활용 MySQL 엔진이 스토리지 엔진으로부터 받아온 레코드를 정렬하거나 그루핑할 때는 내부적인 임시 테이블(Internal temporary table)을 사용한다. 일반적으로 MySQL 엔진이 사용하는 임시 테이블은 처음에는 메모리에 생성됐다가 테이블의 크기가 커지면 디스크로 옮겨진다. 물론 특정 예외 케이스에는 메모리를 거치지 않고 바로 디스크에 임시 테이블이 만들어지기도 한다.\nMySQL 엔진이 내부적인 가공을 위해 생성하는 임시 테이블은 다른 세션이나 다른 쿼리에서는 볼 수 없으며 사용하는 것도 불가능하다. 사용자가 생성한 임시 테이블과는 달리 내부적인 임시 테이블은 쿼리의 처리가 완료되면 자동으로 삭제된다.\n메모리 임시 테이블과 디스크 임시 테이블 MySQL 8.0 이전 버전까지는 원본 테이블의 스토리지 엔진과 관계없이 임시 테이블이 메모리를 사용할 때는 MEMORY 스토리지 엔진을 사용하며, 디스크에 저장될 때는 MyISAM 스토리지 엔진을 이용했다.\nMEMORY 스토리지 엔진 VARBINARY, VARCHAR 같은 가변 길이 타일을 지원하지 못하기 때문에 임시 테이블이 메모리에 만들어지게 되면 가변 길이 타입의 경우 최대 길이만큼 메모리를 할당하여 메모리 낭비가 심해지는 문제가 있었다. MyISAM 스토리지 엔진 트랜잭션을 지원하지 못한다. 이러한 문제로 인해 MySQL 8.0 버전 부터는 TempTable 이라는 스토리지 엔진을 사용하고, 디스크에 저장되는 임시 테이블은 InnoDB 스토리지 엔진을 사용하도록 개선되었다.\ninternal_tmp_mem_storage_engine: 시스템 변수를 이용해 메모리용 임시 테이블을 MEMORY와 TempTable 중에서 선택할 수 있으며 기본값은 TempTable이다. temptable_max_ram: TempTable이 최대한 사용 가능한 메모리 공간의 크기를 제어할 수 있으며 기본값은 1GB이다. temptable_usemmap: 임시 테이블의 크기가 1GB보다 커지는 경우 MySQL 서버는 메모리의 임시 테이블을 디스크로 기록하는데, 2가지 방법 중 하나를 선택하며 기본값은 ON이며 OFF시 InnoDB 테이블로 기록한다. MMAP 파일로 디스크에 기록: InnoDB 테이블로 전환하는 것 보다 오버헤드가 적다. InnoDB 테이블로 기록 internal_tmp_disk_storage_engine: 내부 임시 테이블이 메모리에 생성되지 않고 처음부터 디스크 테이블로 생성되는 경우 해당 시스템 변수를 따라 생성되며 기본값은 InnoDB이다. 임시 테이블이 필요한 쿼리 다음과 같은 패턴의 쿼리는 MySQL엔진에서 별도의 데이터 가공 작업을 필요로 하므로 대표적으로 내부 임시 테이블을 생성하는 케이스다.\nORDER BY와 GROUP BY에 명시된 컬럼이 다른 쿼리 ORDER BY, GROUP BY에 면시된 컬럼이 조인의 순서상 첫 번째 테이블이 아닌 쿼리 DISTINCT와 ORDER BY가 동시에 쿼리에 존재하는 경우 또는 DISTINCT가 인덱스로 처리되지 못하는 쿼리 UNION이나 UNION DISTINCT가 사용된 쿼리(select_type 컬럼이 UNION RESULT인 경우) 쿼리 실행 계획에서 select_type이 DERIVED인 쿼리 쿼리의 실행 계획에서 임시 테이블을 사용 여부는 Extra 컬럼에서 \u0026ldquo;Using temporary\u0026quot;라는 메시지가 표시되는지 확인한다. 하지만 메시지가 표시되지 않는 경우에도 임시 테이블을 사용할 수 있으며, 마지막 3개 패턴이 그러한 경우다.\n첫 번째부터 네 번째까지의 쿼리 패턴은 유니크 인덱스를 가지는 내부 임시 테이블이 만들어지고, 마지막 쿼리 패턴은 유니크 인덱스가 없는 내부 임시 테이블이 생성된다.\n일반적으로 유니크 인덱스가 있는 내부 임시 테이블은 그렇지 않은 쿼리보다 처리 성능이 상당히 느리다.\n임시 테이블이 디스크에 생성되는 경우 내부 임시 테이블은 기본적으로는 메모리상에 만들어지지만 다음과 같은 조건을 만족하면 메모리 임시 테이블을 사용할 수 없게 되어 디스크 기반의 임시 테이블을 사용한다.\nUNION이나 UNION ALL에서 SELECT되는 컬럼 중 길이가 512바이트 이상인 크기의 컬럼이 있는 경우 GROUP BY나 DISTINCT 컬럼에서 512 바이트 이상인 컬럼이 있는 경우 메모리 임시 테이블의 크기가 시스템 변수의 크기보다 큰 경우(tmp_table_size, max_heap_table_size, temptable_max_ram) 임시 테이블 관련 상태 변수 실행 계획상에서 \u0026ldquo;Using temporary\u0026quot;가 표시되면 임시 테이블을 사용햇다는 사실을 알 수 있지만 임시 테이블이 메모리에서 처리됐는지 디스크에서 처리됐는지는 알 수 없으며, 몇개의 임시 테이블이 사용됐는지도 알 수 없다.\n임시 테이블의 생성 방식을 확인하려면 MySQL 서버의 상태 변수를 확인해보면 된다.\n1 2 3 4 5 6 7 FLUSH STATUS; SELECT first_name, last_name FROM employees GROUP BY first_name, last_name; SHOW SESSION STATUS LIKE \u0026#39;Created_tmp%\u0026#39;; Created_tmp_tables: 쿼리의 처리를 위해 만들어진 내부 임시 테이블의 개수를 누적하는 상태값이다. 내부 임시 테입르이 메모리에 만들어 졌는지를 구분하지 않고 모두 누적한다. Created_tmp_disk_tables: 디스크에 내부 임시 테이블이 만들어진 개수만 누적해서 가지고 있는 상태값이다. ","date":"2023-05-23T15:30:10+09:00","image":"https://codemario318.github.io/post/real-mysql/9/2/2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/9/2/2/","title":"9.2 기본 데이터 처리(2)"},{"content":"레코드 1~2건을 가져오는 쿼리를 제외하면 대부분의 SELECT쿼리에서 정렬은 필수적으로 사용된다. 정렬을 처리하는 방법은 인덱스를 이용하는 방법과 쿼리가 실행될 때 Filesort라는 별도의 처리를 이용하는 방법으로 나눌 수 있다.\n장점 단점 인덱스 이용 INSERT, UPDATE, DELETE 쿼리가 실행될 때 이미 인덱스가 정렬 돼 있어서 순서대로 읽기만 하면 되므로 매우 빠르다. INSERT, UPDATE, DELETE 작업 시 부가적인 인덱스 추가/삭제 작업으로 인해 디스크 공간이 더 많이 필요하다. 인덱스의 개수가 늘어날수록 InnoDB 버퍼풀을 위한 메모리가 많이 필요하다. Filesort 이용 인덱스를 생성하지 않아도 되므로 인덱스를 이용할 때의 단점이 장점으로 바뀐다. 정렬해야 할 레코드가 많지 않으면 메모리에서 Filesort가 처리되므로 충분히 빠르다. 정렬 작업이 쿼리 실행 시 처리되므로 레코드 대상 건수가 많아질수록 쿼리의 응답 속도가 느리다. 레코드를 정렬하기 위해 항상 Filesort 정렬 작업을 거쳐야 하는 것은 아니나, 모든 정렬을 인덱스를 이용하도록 튜닝하는 것은 거의 불가능하다.\n정렬 기준이 너무 많아 요건별로 모두 인덱스를 생성하는 것이 불가능한 경우 GROUP BY의 결과 또는 DISTINT같은 처리의 겨로가를 정렬해야 하는 경우 UNION의 결과와 같이 임시 테이블의 결과를 다시 정렬해야 하는 경우 랜덤하게 결과 레코드를 가져와야 하는 경우 MySQL 서버에서 인덱스를 이용하지 않고 별도의 정렬 처리를 수행했는지는 실행 계획의 Extra컬럼에 Using filesort 메시지가 표시되는지 여부로 판단할 수 있다.\nMySQL의 정렬 특성을 이해하면 쿼리를 튜닝할 때 어떠헥 하면 조금이라도 더 빠른 쿼리가 될지 쉽게 판단할 수 있을것이다.\n소트 버퍼 MySQL은 정렬을 수행하기 위해 별도의 메모리 공간을 할당받아 사용하는데, 이 메모리 공간을 소트 버퍼라고 한다.\n소트 버퍼는 정렬이 필요한 경우에만 할당되며, 버퍼의 크기는 정렬해야 할 레코드의 크기에 따라 가변적으로 증가하지만 최대 사용 가능한 소트 버퍼의 공간은 sort_buffer_size 시스템 변수로 설정할 수 있다. 소트 버퍼를 위한 메모리 공간은 쿼리의 실행이 완료되면 즉시 시스템으로 반납된다.\n정렬해야할 레코드가 적어 메모리에 할당된 소트 버퍼만으로 정렬할 수 있다면 빠르게 정렬이 처리되지만, 레코드 건수가 소트 버퍼로 할당된 공간보다 크면 정렬해야 할 레코드를 여러 조각으로 나누어 처리하게된다.\n이 과정에서 메모리의 소트 버퍼에서 정렬을 수행하고 그 결과를 임시로 디스크에 기록하는 과정을 반복한다. 이처럼 각 버퍼 크기만큼 정렬된 레코드를 다시 병합하면서 정렬을 수행해야 하며(멀티 머지, Multi-merge) 수행된 멸티 머지 횟수는 Sort_merge_passes라는 상태 변수에 누적해서 집계된다.\n이러한 작업들이 모두 디스크의 쓰기와 읽기를 유발하게 되어 레코드 건수가 많을수록 반복 작업 횟수도 많아진다.\n소트 버퍼를 크게 설정하면 디스크를 사용하지 않아 더 빨라질 것으로 생각할 수 있지만, 실제 벤치마크 결과로는 큰 차이를 보이지 않는다. 그리고 리눅스 계열의 운영체제에서는 너무 큰 소트 버퍼 크기를 사용하는 경우, 큰 메모리 공간 할당 때문에 성능이 훨씬 떨어질 수 있다.\n또한 소트 버퍼는 세션 메모리 영역을 사용하여 여러 클라이언트가 공유해서 사용할 수 없다. 커넥션이 많을수록, 정렬 작업이 많을수록 소트 버퍼로 소비되는 메모리 공간이 커져 운영체제에서 메모리 부족 현상이 발생할 수 있다.\n소트 버퍼를 설정해서 빠른 성능을 얻을 수는 없지만 디스크의 읽기와 쓰기 사용량은 줄일 수 있다. 따라서 MySQL 서버의 데이터가 많거나 디스크 I/O 성능이 낮은 장비라면 소트 버퍼의 크기를 더 크게 설정하는 것이 도움이 될 수도 있다.\n정렬 알고리즘 레코드를 정렬할 때 레코드 전체를 소트 버퍼에 담을지 또는 정렬 기준 컬럼만 소트 버퍼에 담을지에 따라 싱글 패스, Single-pass와 투 패스, Two-pass 2가지 정렬 모드로 나눌 수 있다.\n정렬을 수행하는 쿼리가 어떤 정렬 모드를 사용하는지는 다음과 같이 옵티마이저 트레이스 기능으로 확인할 수 있다.\n1 2 3 4 5 6 SET OPTIMIZER_TRACE=\u0026#34;enabled=on\u0026#34;, END_MARKERS_IN_JSON=on; SET OPTIMIZER_TRACE_MAX_MEM_SIZE=1000000; SELECT * FROM employees ORDER BY last_name LIMIT 100000, 1; SELECT * FROM INFORMATION_SCHEMA.OPTIMIZER_TRACE \\G 마지막 쿼리 실행 후 출력된 내용에서 \u0026ldquo;filesort_summary\u0026rdquo; 섹션의 \u0026ldquo;sort_algorithm\u0026rdquo; 필드에 정렬 알고리즘이 표시되고, \u0026ldquo;sort_mode\u0026rdquo; 필드에는 3가지 정렬 방식중 하나가 표시된다.\n\u0026lt;sort_key, rowid\u0026gt;: 정렬 키와 레코드의 로우 아이드만 가져와서 정렬하는 방식 \u0026lt;sort_key, additional_fields\u0026gt;: 정렬 키와 레코드 전체를 가져와서 정렬하는 방식으로, 레코드의 컬럼들은 고정 사이즈로 메모리에 저장 \u0026lt;sort_key, packed_additional_fields\u0026gt;: 정렬 키와 레코드 전체를 가져와서 정렬하는 방식으로, 레코드의 컬럼들은 가변 사이즈로 메모리 저장 여기서는 첫 번째 방식을 투 패스정렬 방식이라 명명하고, 나머지 방식을 싱글 패스정렬 방식이라고 명명하겠다. 세 번째 방식은 MySQL 5.7 버전부터 도입됐는데, 이는 정렬을 위한 메모리 공간의 효율적인 사용을 위해 추가 도입된 방식이다.\n싱글 패스 정렬 방식 소트 버퍼에 정렬 기준 컬럼을 포함해 SELECT 대상이 되는 컬럼 전부를 담아 정렬을 수행하는 방식이다.\n1 2 3 SELECT emp_no, first_name, last_name FROM employees ORDER BY first_name; 위 쿼리와 같이 first_name으로 정렬해서 emp_no, first_name, last_name을 SELECT하는 쿼리를 싱글 패스 정렬 방식으로 처리하게 되면, 처음 employees 테이블을 읽을 때 정렬에 필요하지 않는 last_name 컬럼까지 전부 읽어 소트 버퍼에 담고 정렬을 수행한다. 그리고 정렬이 완료되면 정렬 버퍼의 내용을 그대로 클라이언트로 넘겨준다.\n투 패스 정렬 방식 정렬 대상 컬럼과 프라이머리 키 값만 소트 버퍼에 담아서 정렬을 수행하고, 정렬된 순서대로 다시 프라이머리 키로 테이블을 읽어서 SELECT할 컬럼을 가져오는 정렬 방식으로, 싱글 패스 정렬 방식이 도입되기 이전부터 사용하던 방식이다.\n처음 employees 테이블을 읽을 때는 정렬에 필요한 first_name컬럼과 프라이머리 키인 emp_no만 읽어서 정렬을 수행한다. 이 정렬이 완료되면 그 결과 순서대로 employees 테이블을 한번 더 읽어서 last_name을 가져오고, 최종적으로 그 결과를 클라이언트 쪽으로 넘기는 과정을 확인할 수 있다.\n정리 MySQL 예전 정렬 방식인 투 패스 방식은 테이블을 두 번 읽어야 하기 때문에 상당히 불합히하지만, 새로운 정렬 방식인 싱글 패스는 이러한 불합리가 없다. 하지만 싱글 패스 정렬 방식은 더 많은 소트 버퍼 공간이 필요하다.\n최신 버전에서는 일반적으로 싱글 패스 정렬 방식을 주로 사용하지만 경우에 따라 투 패스 정렬 방식을 이용할 수 있다.\n레코드의 크기가 max_length_for_sort_data 시스템 변수에 설정된 값보다 클 때 BLOB이나 TEXT 타입의 컬럼이 SELECT 대상에 포함할 때 싱글 패스 방식은 정렬 대상 레코드의 크기나 건수가 작은 경우 빠른 성능을 보이며, 투 패스 방식은 정렬 대상 레코드의 크기나 건수가 상당히 많은 경우 효율적이라고 볼 수 있다.\nSELECT 쿼리에서 꼭 필요한 컬럼만 조회하지 않고, 모든 컬럼을 가져오도록 개발하는 경우가 많은데, 이는 정렬 버퍼를 몇 배에서 몇십 배 까지 비효율 적으로 사용할 가능성이 크므로 정렬이 필요한 SELECT는 불필요한 컬럼을 포함하지 않게 쿼리를 작성하는 것이 효율적이다.\n정렬 처리 방법 쿼리에 ORDER BY가 사용되면 반드시 다음 3가지 처리 방법 중 하나로 정렬이 처리된다. 일반적으로 아래쪽에 있는 정렬 방법으로 갈수록 처리 속도는 떨어진다.\n정렬 처리 방법 실행 계획 Extra 컬럼 내용 인덱스를 사용한 정렬 별도 표기 없음 조인에서 드라이빙 테이블만 정렬 \u0026ldquo;Using filesort\u0026rdquo; 조인에서 조인 결과를 임시 테이블로 저장 후 정렬 \u0026ldquo;Using temporary; Using filesort\u0026rdquo; 옵티마이저는 정렬 처리를 위해 인덱스를 이용할 수 있을지 검토하는데, 사용할 수 있다면 \u0026ldquo;filesort\u0026rdquo; 과정 없이 인덱스를 순서대로 읽어 반환한다. 하지만 인덱스를 사용할 수 없다면 WHERE 조건에 일치하는 레코드를 검색해 정렬 버퍼에 저장하면서 정렬을 처리한다.\n이때 MySQL 옵티마이저는 정렬 대상 레코드를 최소화 하기 위해 다음 2가지 방법 중 하나를 선택한다.\n조인의 드라이빙 테이블만 정렬한 다음 조인을 수행 조인이 끝나고 일치하는 레코드를 모두 가져온 후 정렬을 수행 일반적으로 조인이 수행되면서 레코드 건수와 레코드의 크기는 거의 배수로 불어나기 때문에 가능하다면 드라이빙 테이블만 정렬한 다음 조인을 수행하는 방법이 효율적이다. 그래서 두 번째 방법보다는 첫 번째 방법이 더 효율적으로 처리된다.\n인덱스를 이용한 정렬 인덱스를 이용한 정렬을 위해서는 반드시 ORDER BY에 명시된 컬럼이 제일 먼저 읽는 테이블(조인이 사용된 경우 드라이빙 테이블)에 속하고, ORDER BY의 순서대로 생성된 인덱스가 있어야 한다. WHERE절에 첫 번째로 읽는 테이블의 컬럼에 대한 조건이 있다면 그 조건과 ORDER BY는 같은 인덱스를 사용할 수 있어야 한다. B-Tree 계열의 인덱스가 아닌 해시 인덱스나 전문 검색 인덱스 등에서는 인덱스를 이용한 정렬을 사용할 수 없다. (R-Tree 인덱스 불가) 여러 테이블이 조인되는 경우에는 네스티드-루프(Nested-loop) 방식의 조인에서만 사용할 수 있다. 인덱스를 이용해 정렬이 처리되는 경우에는 실제 인덱스의 값이 정렬돼 있기 때문에 인덱스의 순서대로 읽기만 하면 되어 MySQL 엔진에서 별도의 정렬을 위한 추가 작업을 수행하지는 않는다.\n하지만 조인이 사용된 쿼리의 실행 계획에 조인 버퍼가 사용되면 순서가 흐트러 질 수 있으므로 주의해야 한다.\nORDER BY 절을 넣지 않아도 자동으로 정렬되므로 ORDER BY 자체를 쿼리에서 제거하기도 하지만, MySQL 서버는 정렬을 인덱스로 처리할 수 있는 경우 부가적으로 불필요한 정렬 작업을 수행하지 않기 때문에 ORDER BY가 쿼리에 명시된다고 해서 작업량이 늘지는 않는다. 그리고 알 수 없는 이유로 실행 계획이 조금 변경된다면, 기대했던 순서대로 결과를 가져오지 못해 버그로 연결될 수 있으므로 ORDER BY 절을 명시하는 것이 좋다.\n조인의 드라이빙 테이블만 정렬 일반적으로 조인이 수행되면 결과 레코드의 건수가 몇 배로 불어나고, 레코드 하나의 크기도 늘어나기 때문에 조인을 실행하기 전 첫 번째 테이블의 레코드를 먼저 정렬한 다음 조인을 실행하는 것이 정렬의 차선책이다. 이러한 방법으로 정렬이 처리되려면 조인에서 첫 번째로 읽히는 테이블(드라이빙 테이블)의 컬럼만으로 ORDER BY 절을 작성해야 한다.\n1 2 3 4 5 6 SELECT * FROM employees e, salaries s WHERE s.emp_no = e.emp_no AND e.emp_no BETWEEN 100002 AND 100010 ORDER BY e.last_name ; WHERE 절의 조건으로 인해 옵티마이저는 employees 테이블을 드라이빙 테이블로 선택한다.\nWHERE 절의 검색조건 emp_no는 employees 테이블의 파리이머리 키를 이용해 검색하면 작업량을 줄일 수 있다. 드리븐 테이블의 조인 컬럼인 emp_no 컬럼에 인덱스가 있다. 검색은 인덱스 레인지 스캔으로 처리할 수 있지만 ORDER BY 절에 명시된 컬럼은 employees 테이블의 프라이머리 키와 전혀 연관이 없으므로 인덱스를 이용한 정렬은 불가능하지만, 정렬 기준 컬럼이 드라이빙 테이블에 포함된 컬럼이므로 옵티마이저는 드라이빙 테이블만 검색해서 정렬을 먼저 수행하고, 그 결과와 salaries 테이블을 조인하는 방식을 선택한다.\n인덱스를 이용해 BETWEEN 조건을 만족하는 데이터 검색 검색 결과를 last_name 컬럼으로 정렬을 수행(Filesort) 정렬된 결과를 순서대로 읽으며 sqlaries 테이블과 조인을 수행해 최종 결과를 가져옴 임시 테이블을 이용한 정렬 쿼리가 여러 테이블을 조인하지 않고, 하나의 테이블로 부터 SELECT해서 정렬하는 경우라면 임시 테이블이 필요하지 않지만, 2개 이상의 테이블을 조인해서 그 결과를 정렬해야 한다면 임시 테이블이 필요할 수 있다.\n\u0026ldquo;조인 드라이빙 테이블만 정렬\u0026quot;하는 경우라면 2개 이상의 테이블이 조인되면서 정렬이 실행되지만 임시 테이블을 사용하지 않는다. 하지만 그 외 패턴의 쿼리에서는 항상 조인의 결과를 임시 테이블에 저장하고, 그 결과를 다시 정렬하는 과정을 거친다. 따라서 이 방법은 정렬 3가지 방식 중 정렬해야 할 레코드 건수가 가장 많기 때문에 가장 느리다.\n1 2 3 4 5 6 SELECT * FROM employees e, salaries s WHERE s.emp_no = e.emp_no AND e.emp_no BETWEEN 100002 AND 100010 ORDER BY s.salary ; id table type key Extra 1 e range PRIMARY Using where; Using temporary; Using filesort 1 s ref PRIMARY NULL 쿼리 실행 계획 \u0026ldquo;Extra\u0026quot;컬럼에 \u0026ldquo;Using where; Using temporary; Using filesort\u0026quot;코멘트가 표시되는데, 이는 조인의 결과를 임시 테이블에 저장하고, 그 결과를 다시 정렬 처리했음을 의미한다.\n정렬 처리 방법의 성능 비교 주로 웹 서비스용 쿼리에서는 ORDER BY와 함께 LIMIT이 거의 필수로 사용되는 경우가 많은데, 일반적으로 LIMIT은 테이블이나 처리 결과의 일부만 가져오기 때문에 MySQL 서버가 처리해야 할 작업량을 줄이는 역할을 한다.\n하지만 ORDER BY, GROUP BY 같은 작업은 WHERE 족너을 만족하는 레코드를 LIMIT 건수 만큼만 가져와서는 처리할 수 없고, 우선 조건을 만족하는 레코드를 모두 가져와서 정렬을 수행하거나 그루핑 작업을 실행해야만 처리할 수 있다. 이로 인해 WHERE 조건이 아무리 인덱스를 잘 활용하도록 튜닝해도 잘못된 ORDER BY, GROUP BY 때문에 쿼리가 느려지는 경우가 많이 발생한다.\n스트리밍 방식 서버 쪽에서 처리할 데이터가 얼마인지에 관계 없이 조건에 일치하는 레코드가 검색될 때마다 바로바로 클라이언트로 전성해주는 방식으로, 클라이언트는 쿼리를 요청하고 곧바로 원했던 첫 번째 레코드를 전달받는다. (마지막 레코드는 언제 받을지 알 수 없다.)\n쿼리가 스트리빙 방식으로 처리될 수 있다면 클라이언트는 MySQL 서버가 일치하는 레코드를 찾는 즉시 전달받기 때문에 동시에 데이터의 가공 작업을 시작할 수 있다. 웹서비스 같은 OLTP 환경에서는 쿼리의 요청에서 첫 번째 레코드를 전달받게 되기까지의 응답 시간이 중요한 경우가 많은데, 스트리빙 방식으로 처리되는 쿼리는 쿼리가 얼마나 많은 레코드를 조회하느냐에 상관없이 빠른 응답 시간을 보장해준다.\n또한 스트리밍 방식으로 처리되는 쿼리에서 LIMIT 처럼 결과 건수를 제한하는 조건들은 풀 테이블 스캔의 결과가 아무런 버퍼링 처리나 필터링 과정 없이 바로 클라이언트로 스트리밍 되기 때문에 쿼리의 전체 실행 시간을 상당히 줄여줄 수 있다.\n버퍼링 방식 ORDER BY, GROUP BY 같은 처리는 WHERE 조건에 일치하는 모든 레코드를 가져온 후, 정렬하거나 그루핑해서 차례대로 보내야 하므로 쿼리의 결과가 스트리밍되는 것이 불가능하다.\n또한 버퍼링 방식으로 처리되는 쿼리는 먼저 결과를 모아서 MySQL 서버에서 일괄 가옹해야 하므로 모든 결과를 스토리지 엔진으로부터 가져올 때가지 기다려야 한다. 따라서 LIMIT 처럼 결과 건수를 제한하는 조건이 있어도 성능 향상에 별로 도움이 되지 않는다.\n정렬 처리 방버에서 소개한 ORDER BY의 3가지 처리 방법 가운데 인덱스를 사용한 정렬 방식만 스트리밍 형태의 처리이며, 나머지는 모두 버퍼링된 후에 정렬된다. 즉 인덱스를 사용한 정렬 방식은 LIMIT로 제한된 건수만큼만 읽으면서 바로 클라이언트로 결과를 전송해 줄 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 /* tb_test1의 레코드는 100건 tb_test2 테이블의 레코드가 1000건 (tb_test1의 레코드 1건당 tb_test2의 레코드가 10건씩 존재함) */ SELECT * FROM tb_test1 t1, tb_test2 t2 WHERE t1.col1=t2.col1 ORDER BY t1.col2 LIMIT 10 ; tb_test1가 드라이빙되는 경우\n정렬 방법 읽어야 할 건수 조인 횟수 정렬해야 할 대상 건수 인덱스 사용 tb_test1: 1건, tb_test2: 10건 1번 0건 조인의 드라이빙 테이블만 정렬 tb_test1: 100건, tb_test2: 10건 1번 100건(tb_test1 테이블의 레코드 건수 만큼 정렬 필요 임시 테이블 사용 후 정렬 tb_test1: 100건, tb_test2: 1000건 100번 1000건(조인된 결과 레코드 건수를 모두 정렬해야함) tb_test2가 드라이빙되는 경우\n정렬 방법 읽어야 할 건수 조인 횟수 정렬해야 할 대상 건수 인덱스 사용 tb_test2: 10건, tb_test1: 10건 10번 0건 조인의 드라이빙 테이블만 정렬 tb_test2: 1000건, tb_test1: 1건 10번 1000건(tb_test2 테이블의 레코드 건수 만큼 정렬 필요 임시 테이블 사용 후 정렬 tb_test2: 1000건, tb_test2: 100건 1000번(tb_test2 테이블의 레코드 건수만큼 조인 발생) 1000건(조인된 결과 레코드 건수를 모두 정렬해야함) 어느 테이블이 먼저 조인되는지도 중요하지만 어떤 정렬 방식으로 처리되는지는 더 큰 성능 차이를 만든다. 가능하다면 인덱스를 사용한 정렬로 유도하고, 그렇지 못하다면 최소한 드라이빙 테이블만 정렬해도 되는 수준으로 유도하는 것도 좋은 튜닝 방법이라고 할 수 있다.\n정렬 관련 상태 변수 MySQL 서버는 처리하는 주요 작업에 대해서는 해당 작업의 실행 횟수를 상태 변수로 저장한다. 정려로가 관련해서도 지금까지 몇 건의 레코드나 정렬 처리를 수행했는지, 소트 버퍼 간의 병합 작업(멀티 머지)은 몇 번이나 발생했는지 등을 다음과 같은 명령으로 확인해 볼 수 있다.\n1 2 3 FLUSH STATUS; SHOW STATUS LIKE \u0026#39;Sort%\u0026#39; ; Sort_merge_passes: 멀티 머지 처리 횟수 Sort_range: 인덱스 레인지 스캔을 통해 검색된 결과에 대한 정렬 작업 횟수(누적) Sort_rows: 풀 테이블 스캔을 통해 검색된 결과에 대한 정렬 작업 횟수(누적) Sort_rows: 지금까지 정렬한 전체 레코드 건수 ","date":"2023-05-23T12:40:10+09:00","image":"https://codemario318.github.io/post/real-mysql/9/2/1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/9/2/1/","title":"9.2 기본 데이터 처리(1) - ORDER BY 처리 (Using filesort)"},{"content":"MySQL 서버로 요청된 쿼리는 결과는 동일하지만 내부적으로 그 결과를 만들어내는 방법은 매우 다양하며, 어떤 방법이 최적이고 최소의 비용이 소모될지 결정해야 한다. MySQL 옵티마이저는 쿼리를 최적으로 실행하기 위해 각 테이블의 데이터가 어떤 분포로 저장돼 있는지 통계 정보를 참조하여 기본 데이터를 비교해 최적의 실행 계획을 수립한다.\n개요 어떤 DBMS든지 쿼리의 실행 계획을 수립하는 옵티마이저는 가장 복잡한 부분으로 알려져 있으며, 옵티마이저가 만들어 내는 실행 계획을 이해하는 것 또한 상당히 어려운 부분이다. 하지만 실행 계획을 이해할 수 있어야만 실행 계획의 불합리한 부분을 찾아내고, 더 최적화된 방법으로 실행 계획을 수립하도록 유도할 수 있다.\n실행 절차 MySQL 서버에서 쿼리가 실행되는 과정은 크게 세 단계로 나눌 수 있다.\nSQL 파싱: 사용자로부터 요청된 SQL 문장을 잘게 쪼개어 MySQL 서버가 이해할 수 있는 수준으로 분리(파스 트리)한다. SQL 파서라는 모듈로 처리한다. 이때 문법적으로 잘못된다면 걸러진다. 이 단계에서 SQL 파스 트리가 만들어지고, MySQL 서버는 SQL 문장 자체가 아닌 파스 트리를 이용해 쿼리를 실행한다. 최적화 및 실행 계획 수립: MySQL 옵티마이저에 의해 SQL의 파싱 정보(파스 트리)를 확인하며 어떤 테이블 부터 읽고 어떤 인덱스를 이용해 테이블을 읽을지 선택한다. 불필요한 조건 제거 및 복잡한 연산 단순화 여러 테이블의 조인이 있는 경우 어떤 순서로 테이블을 읽을지 결정 각 테이블에 사용된 조건과 인덱스 통계 정보를 이용해 사용할 인덱스를 결정 가져온 레코드들을 임시 테이블에 넣고 다시 한번 가공해야 하는지 결정 두 번째 단계에서 결정된 테이블의 일기 순서나 선택된 인덱스를 이용해 스토리지 엔진으로부터 데이터를 가져온다. 두 번째 단계가 완료되면 쿼리의 실행 계획이 만들어지며, 세 번째 단계에서 수립된 실행 계획대로 스토리지 엔진에 레코드를 읽어오도록 요청하고, MySQL 엔진에서는 스토리지 엔진으로부터 받은 레코드를 조인하거나 정렬하는 작업을 수행한다.\n옵티마이저의 종류 옵티마이저는 데이터베이스 서버에서 두뇌와 같은 역할을 담당한다. 옵티마이저는 현재 대부분의 DBMS가 선택하고 있는 비용 기반 최적화(Cost-based optimizer, CBO) 방법과 예전 초기 버전의 오라클 DMBS에서 많이 사용했던 규칙 기반 최적화 방법(Rule-based optimizer, RBO)로 나눌 수 있다.\nRBO: 기본적으로 대상 테이블의 레코드 건수나 선택도 등을 고려하지 않고 옵티마이저에 내장된 우선순위에 따라 실행 계획을 수립하는 방식을 의미한다. 통계 정보(테이블의 레코드 건수나 컬럼 값의 분포도)를 조사하지 않고 실행 계획이 수립되기 때문에 같은 쿼리에 대해서는 거의 항상 같은 실행 방법을 만들어 낸다. 사용자의 데이터는 분포도가 매우 다양하기 때문에 규칙 기반의 최적화는 이미 오래전부터 사용되지 않는다. CBO: 쿼리를 처리하기 위한 여러 가지 가능한 방법을 만들고, 각 단위 작업의 비용(부하) 정보와 대상 테이블의 예측된 통계 정보를 이용해 실행 계획별 비용을 산출한다. 산출된 실행 방법별로 비용이 최소로 소요되는 처리 방식을 선택해 최종적으로 쿼리를 실행한다. 규칙 기반 최적화는 각 테이블이나 인덱스의 통계 정보가 거의 없고 상대적으로 느린 CPU 연산 탓에 비용 계산 과정이 부담스럽다는 이유로 사용되는 최적화 방법이다. 현재는 MySQL을 포함한 대부분의 RDBMS가 비용 기반의 옵티마이저를 채택하고 있다.\nMySQL 서버를 포함한 모든 RDBMS는 데이터를 정렬하거나 그루핑하는 등 기본 데이터 가공 기능을 가지고 있다. 하지만 결과물은 동일하더라도 RDBMS 별로 그 결과를 만들어 내는 과정은 천차만별이다.\n풀 테이블 스캔과 풀 인덱스 스캔 풀 테이블 스캔: 인덱스를 사용하지 않고 테이블의 데이터를 처음부터 끝까지 읽어 요청된 작업을 처리하는 작업을 의미한다. 테이블의 레코드 건수가 너무 작아서 인덱스를 통해 읽는 것 보다 풀 테이블 스캔을 하는 편이 더 빠른경우(테이블이 페이지 1개로 구성된 경우) WHERE 절이나 ON 절에 인덱스를 이용할 수 있는 적절한 조건이 없는 경우 인덱스 레인지 스캔을 사용할 수 있는 쿼리라고 하더라도 옵티마이저가 판단한 조건 일치 레코드 건수가 너무 많은 경우(인덱스 B-Tree를 샘플링해서 조사한 통계 정보 기준) 일반적으로 테이블의 전체 크기는 인덱스보다 훨씬 크기 때문에 테이블을 처음부터 끝까지 읽는 작업은 상당히 많은 디스크 읽기가 필요하다. 그래서 대부분 DBMS는 풀 테이블 스캔을 실행할 때 한꺼번에 여러개의 블록이나 페이지를 읽어오는 기능을 내장하고 있다.\n하지만 MySQL에는 풀 테이블 스캔을 실행할 때 한꺼번에 몇 개씩 테이블을 읽어올지 설정하는 시스템 변수는 없다. 그래서 많은 사람들이 MySQL은 풀 테이블 스캔을 실행할 때 디스크로부터 페이지를 하나씩 읽어 오는 것으로 생각하지만 InnoDB 스토리지 엔진은 특정 테이블의 연속된 데이터 페이지가 읽히면 백그라운드 스레드에 의해 리드 어헤드(Read ahead) 작업이 자동으로 시작되며, 이를 통해 요청이 오기 전에 미리 디스크에서 필요할 데이터를 예측하여 읽어 버퍼풀에 가저다둔다.\n즉, 풀 테이블 스캔이 실행되면 처음 몇개의 데이터 페이지는 포그라운드 스레드(Foreground thread, 클라이언트 스레드)가 페이지 읽기를 실행하지만 특정 시점부터는 읽기 작업을 백그라운드 스레드로 넘긴다.\n백그라운드 스레드가 읽기를 넘겨받는 시점부터는 한번에 4개 또는 8개씩 페이지를 읽으면서 계속 그 수를 증가시키고, 한 번에 최대 64개의 데이터 페이지까지 읽어 버퍼풀에 저장해둔다. 포그라운드 스레드는 미리 버퍼풀에 준비된 데이터를 가져다 사용하기만 하면 되므로 쿼리가 빠르게 처리된다.\nMySQL 서버에서는 innodb_read_ahead_threshold 시스템 변수를 이용해 InnoDB 스토리지 엔진이 언제 리드 어헤드를 시작할지 임계값을 설정할 수 있다. 포그라운드 스레드에 의해 시스템 변수에 설정된 개수만큼의 연속된 데이터 페이지가 읽히면 InnoDB 스토리지 엔진은 백그라운드 스레드를 이용해 대량으로 그 다음 페이지들을 읽어서 버퍼풀로 적재한다.\n일반적으로 기본 설정으로 충분하지만 데이터 웨어하우스용으로 MySQL을 사용한다면 이 옵션을 더 낮은 값으로 설정하여 더 빨리 리드 어헤드가 시작되게 유도하는 것도 성능을 향상시킬 수 있다.\n리드 어헤드는 풀 인덱스 스캔에서도 동일하게 사용된다.\n병렬 처리 MySQL 8.0 버전부터는 용도가 한정돼 있긴 하지만 쿼리 병렬 처리가 가능해졌다.\n여기서 말하는 병렬 처리는 하나의 쿼리를 여러 스레드가 나누어 동시에 처리한다는 것을 의미한다.\ninnodb_parallel_read_threads 시스템 변수를 이용해 하나의 쿼리를 최대 몇개의 스레드를 이용해서 처리할지를 변경할 수 있다. 아직 MySQL 서버에서는 쿼리를 여러 개의 스레드를 이용해 병렬로 처리하게 하는 힌트나 옵션은 없으며, 아무런 WHERE조건 없이 단순히 테이블의 전체 건수를 가져오는 쿼리만 병렬로 처리할 수 있다.\n1 2 SET SESSION innodb_parallel_read_threads={n}; SELECT COUNT(*) FROM salaries; 병렬 처리용 스레드 개수가 늘어날수록 쿼리 처리에 걸리는 시간이 대체로 줄어들지만, 서버에 장착된 CPU의 코어 개수를 넘어서는 경우에는 오히려 성능이 떨어질 수도 있으니 주의가 필요하다.\n","date":"2023-05-22T16:09:10+09:00","image":"https://codemario318.github.io/post/real-mysql/9/1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/9/1/","title":"9. 옵티마이저와 힌트 (1)"},{"content":"함수 기반 인덱스 일반적인 인덱스는 컬럼의 값 일부(걸럽의 값 앞부분) 또는 전체에 대해서만 인덱스 생성이 허용되지만, 컬럼의 값을 변형해서 만들어진 값에 대해 인덱스를 구축해야 하는 경우도 있는데, 이러한 경우 함수 기반 인덱스를 활용할 수 있다.\nMySQL 서버의 함수 기반 인덱스는 인덱싱할 값을 계산하는 과정의 차이만 있을 뿐, 실제 인덱스의 내부적인 구조 및 유지 관리 방법은 B-Tree 인덱스와 동일하다.\n가상 컬럼을 이용한 인덱스 1 2 3 4 5 6 7 CREATE TABLE user ( user_id BIGINT, first_name VARCHAR(10), last_name VARCHAR(10), PRIMARY KEY (user_id) ); 이전 버전의 MySQL은 해당 테이블에서 first_name과 last_name을 합쳐 검색해야 하는 요건이 생겼다면 full_name이라는 컬럼을 추가하고 모든 레코드에 대해 해당 컬럼을 업데이트 하는 작업을 거처야 인덱스를 생성 할 수 있었다.\n하지만 MySQL 8.0 버전 부터는 가상 컬럼을 추가하고 해당 가상 컬럼에 인덱스를 생성할 수 있게 됐다.\n1 2 3 ALTER TABLE user ADD full_name VARCHAR(30) AS (CONCAT(first_name, \u0026#39; \u0026#39;, last_name)) VIRTUAL, ADD INDEX ix_fullname (full_name); 가상 컬럼이 VIRTUAL, STORED 옵션 중 어떤 옵션으로 생성됐든 관계 없이 해당 가상 컬럼에 인덱스를 생성할 수 있다. 가상 컬럼은 테이블에 새로운 컬럼을 추가하는 것과 같은 효과를 내기 때문에 실제 테이블의 구조가 변경된다는 단점이 있다.\n함수를 이용한 인덱스 가상 컬럼은 MySQL 5.7 버전에서도 사용할 수 있었지만 함수를 직접 인덱스 생성 구문에 사용할 수는 없었다. 하지만 MySQL 8.0 버전 부터는 테이블 구조를 변경하지 않고, 함수를 직접 사용하는 인덱스를 생성할 수 있게 됐다.\n1 2 3 4 5 6 7 8 CREATE TABLE user ( user_id BIGINT, first_name VARCHAR(10), last_name VARCHAR(10), PRIMARY KEY (user_id), INDEX ix_fullname ((CONCAT(first_name, \u0026#39; \u0026#39;, last_name))) ); 함수를 직접 사용하는 인덱스는 테이블의 구조는 변경하지 않고, 계산된 결괏값의 검색을 빠르게 만들어준다. 함수 기반 인덱스를 제대로 활용하려면 반드시 조건절에 함수 기반 인덱스에 명시된 표현식이 그대로 사용돼야 한다. 함수 생성 시 명시된 표현식과 쿼리의 WHERE 조건절에 사용된 표현식이 다르다면 결과가 같다고 하더라도 MySQL 옵티마이저는 다른 표현식으로 간주해서 함수 기반 인덱스를 사용하지 못한다.\n가상 컬럼과 함수를 직접 이용하는 인덱스는 사용법과 문법에서 조금 차이가 있지만, 내부적으로 동일한 구현 방법을 사용하므로 어떤 방법을 사용하더라도 둘의 성능 차이는 발생하지 않는다.\n멀티 밸류 인덱스 전문 검색 인덱스를 제외한 모든 인덱스는 레코드 1건이 1개의 인덱스 키 값을 가지는 1:1 관계이다. 하지만 멀티 밸류 인덱스는 하나의 데이터 레코드가 여러 개의 키 값을 가질 수 있는 형태의 인덱스다. 일반적인 RDBMS를 기준으로 생각하면 이러한 인덱스는 정규화에 위배되는 형태이지만, 최근 RDBMS들이 JSON 데이터 타입을 지원하기 시작하면서 JSON의 배열 타입의 필드에 저장된 원소들에 대한 인덱스 요건이 발생한 것이다.\nJSON 포맷으로 데이터를 저장하는 MongoDB는 처음부터 이런 형태의 인덱스를 지원하고 있었지만 MySQL 서버는 멀티 밸류 인덱스에 대한 지원 없이 JSON 타입의 컬럼만 지원했다. 하지만 배열 형태에 대한 인덱스 생성이 되지 않아 MongoDB의 기능과 많이 비교되곤 했다.\nMySQL 8.0 버전으로 업그레이드되면서 JSON 관리 기능은 MongoDB에 비해서도 부족함이 없는 상태로 발전했다.\n1 2 3 4 5 6 7 8 9 10 CREATE TABLE user ( user_id BIGINT AUTO_INCREMENT PRIMARY KEY, first_name VARCHAR(10), last_name VARCHAR(10), credit_info JSON, INDEX imx_creditscores ((CAST(credit_info-\u0026gt;\u0026#39;$.credit_scores\u0026#39; AS UNSIGNED ARRAY))) ); INSERT INTO user VALUES (1, \u0026#39;mario\u0026#39;, \u0026#39;lee\u0026#39;, \u0026#39;{\u0026#34;credit_scores\u0026#34;:[360, 353, 351]}\u0026#39;); 멀티 밸류 인덱스를 활용하기 위해서는 일반적인 조건 방식을 사용하면 안 되고, 반드시 다음 함수들을 이용해서 검색해야 옵티마이저가 인덱스를 활용한 실행 계획을 수립한다.\nMEMBER OF() JSON_CONTAINS() JSON_OVERLAPS() 1 SELECT * FROM user WHERE 360 MEMBER OF(credit_info-\u0026gt;\u0026#39;$.credit_scores\u0026#39;); MySQL 서버의 Worklog에는 DECIMAL, INTEGER, DATETIME, VARCHAR/CHAR 타입에 대해 멀티 밸류 인덱스를 지원한다고 명시돼 있지만 MySQL 8.0.21 버전에서는 VARCHAR/CHAR 타입에 대해서는 지원하지 않는다. 하지만 곧 VARCHAR/CHAR타입의 배열 형태 CAST와 멀티 밸류 인덱스가 지원될 것으로 예상된다.\n클러스터링 인덱스 MySQL 서버에서 클러스터링은 테이블의 레코드를 비슷한 것(프라이머리 키를 기준으로)들끼리 묶어서 저장하는 형태로 구현되는데, 이는 주로 비슷한 값들을 동시에 조회하는 경우가 많다는 점에서 착안되었다. MySQL에서 클러스터링 인덱스는 InnoDB 스토리지 엔진에서만 지원한다.\n클러스터링 인덱스 클러스터링 인덱스는 테이블의 프라이머리 키에 대해서만 적용되는 내용이다. 즉 프라이머리 키 값이 비슷한 레코드끼리 묶어서 저장하는 것을 클러스터링 인덱스라고 표현한다.\n따라서 프라이머리 키 값에 의해 레코드에 저장 위치가 결정되며, 프라이머리 키 값이 변경된다면 그 레코드의 물리적인 저장 위치가 바뀌어야 한다는 것을 의미한다.\n프라이머리 키 값으로 클러스터링 된 테이블은 프라이머리 키 값 자체에 대한 의존도가 상당히 크기 때문에 신중히 프라이머리 키를 결정해야한다.\n클러스터링 인덱스는 프라이커리 키 값에 의해 레코드의 저장 위치가 결정되므로 인덱스 알고리즘이라기 보다는 테이블 레코드의 저장 방식이라고 볼 수 있다, 그래서 클러스터링 인덱스와 클러스터링 테이블은 동의어로 사용되기도 한다. 일반적으로 InnoDB와 같이 항상 클러스터링 인덱스로 저장되는 테이블은 프라이머리 키 기반 검색이 매우 빠르며, 대신 레코드의 저장이나 프라이머리 키의 변경이 상대적으로 느리다.\n클러스터링 인덱스 구조를 보면 클러스터링 테이블의 구조 자체는 일반 B-Tree와 비슷하다. 하지만 세컨더리 인덱스를 위한 B-Tree의 리프 노드와는 달리 클러스터링 인겟스의 리프 노드에는 레코드의 모든 컬럼이 같이 저장돼 있음을 알 수 있다.\n1 UPDATE tb_test SET emp_no=10002 WHERE emp_no=10007; 프라이머리 키가 없는 InnoDB 테이블에서는 InnoDB 스토리지 엔진이 다음 우선순위대로 프라이머리 키를 대체할 컬럼을 선택한다.\n프라이머리 키가 있다면 기본적으로 프라이머리 키를 클러스터링 키로 선택 NOT NULL옵션의 유니크 인덱스 중에서 첫 번째 인덱스를 클러스터링 키로 선택 자동으로 유니크한 값을 가지도록 증가되는 컬럼을 내부적으로 추가한 후, 클러스터링 키로 선택 InnoDB 스토리지 엔진이 적절한 클러스터링 키 후보를 찾지 못하는 경우 InnoDB 스토리지 엔진이 내부적으로 레코드의 일련번호 컬럼을 생성하는데, 자동으로 추가된 프라이머리 키는 사용자에게 노출되지 않으며, 쿼리 문장에 명시적으로 사용할 수 없다. 즉 프라이머리 키나 유니크 인덱스가 전혀 없는 InnoDB 테이블에서는 아무 의미 없는 숫자 값으로 클러스터링되어 사용자에게 아무런 혜택도 주지 않는다.\n따라서 InnoDB 테이블에서 클러스터링 인덱스는 테이블당 단 하나만 가질수 있는 엄청난 혜택이므로 가능하다면 프라이머리 키를 명시적으로 생성하는 것이 좋다.\n세컨더리 인덱스에 미치는 영향 MyISAM이나 MEMORY 테이블 같은 클러스터링되지 않은 테이블은 INSERT될 때 처음 저장된 공간에서 절대 이동하지 않는다. 데이터 레코드가 저장된 주소는 내부적인 레코드 아이디(ROWID) 역할을 하며, 프라이머리 키나 세컨더리 인덱스의 각 키는 그 주소를 이용해 실제 데이터를 찾아오기 때문에 프라이머리 키와 세컨더리 인덱스는 구조적으로 아무런 차이가 없다.\nInnoDB는 클러스터링 키 값이 변경될 때마다 데이터 레코드의 주소가 변경되고 그때마다 해당 테이블의 모든 인덱스에 저장된 주솟값을 변경해야 한다. 이런 오버헤드를 제거하기 위해 InnoDB 테이블(클러스터링 테이블)의 모든 세컨더리 인덱스는 해당 레코드가 저장된 주소가 아닌 프라이머리 키 값을 저장하도록 구현돼 있다.\nInnoDB가 MyISAM보다 조금 더 복잡하게 처리되지만, InnoDB테이블에서 프라이머리 키는 더 큰 장점을 제공하기 때문에 성능 저하에 대해 걱정하지 않아도 된다.\n클러스터링 인덱스의 장점과 단점 장점\n프라이머리 키로 검색할 때 처리 성능이 매우 빠름(특히, 프라이머리 키를 범위 검색하는 경우 매우 빠름) 테이블의 모든 세컨더리 인덱스가 프라이머리 키를 가지고 있기 때문에 인덱스만으로 처리될 수 잇는 경우가 많음(커버링 인덱스) 단점\n테이블의 모든 세컨더리 인덱스가 클러스터링 키를 갖기 때문에 클러시터링 키 값의 크기가 클 경우 전체적으로 인덱스의 크기가 커짐 세컨더리 인덱스를 통해 검색할 때 프라이머리 키로 다시 한번 검색해야 하므로 처리 성능이 느림 INSERT할 때 프라이머리 키에 의해 레코드의 저장 위치가 결정되기 때문에 처리 성능이 느림 프라이머리 키를 변경할 때 레코드를 DELETE하고 INSERT 하는 작업이 필요하기 때문에 처리 성능이 느림 일반적으로 웹 서비스와 같은 온라인 트랜잭션 환경에서는 쓰기와 읽기 비율이 2:8, 1:9 정도이기 때문에 조금 느린 쓰기를 감수하고 읽기를 빠르게 유지하는 것은 매우 중요하다.\n클러스터링 테이블 사용 시 주의사항 클러스터링 인덱스 키의 크기 클러스터링 테이블의 경우 모든 세컨더리 인덱스가 프라이머리 키(클러스터링 키) 값을 포함한다. 그래서 프라이머리 키의 크기가 커지면 세컨더리 인덱스도 자동으로 크기가 커진다. 하지만 일반적으로 테이블에 세컨더리 인덱스가 4~5개 정도 생성된다는 것을 고려하며 ㄴ세컨더리 인덱스 크기는 급격히 증가한다.\n프라이머리 키의 크기 레코드당 증하가하는 인덱스 크기 100만 건 저장 시 증가하는 인덱스 크기 10바이트 10 바이트 * 5 = 50 바이트 50바이트 * 1,000,000 = 47MB 50바이트 50 바이트 * 5 = 250 바이트 250바이트 * 1,000,000 = 238MB 레코드 한 건을 생각하면 커 보이지 않을 수 있으나 레코드 건수가 100만 건만 돼도 인덱스의 크기가 거의 190MB나 증가한다. 또한 인덱스가 커질수록 같은 성능을 내기 위해 그만큼의 메모리가 더 필요해지므로 InnoDB 테이블의 프라이머리 키는 신중하게 선택해야 한다.\n프라이머리 키는 AUTO-INCREMENT 보다는 업무적인 컬럼으로 생성(가능하면) InnoDB의 프라이머리 키는 클러스터링 키로 사용되며, 이 값에 의해 레코드의 위치가 결정된다. 즉 프라이머리 키로 검색하는 경우 클러스터링 되지않은 테이블에 비해 매우 빠르게 처리될 수 있음을 의미한다. 프라이머리 키는 의미만큼이나 중요한 역할을 하기 때문에 대부분 검색에서 상당히 빈번하게 사용되는 것이 일반적이다. 그러므로 설령 컬럼의 크기가 크더라도 업무적으로 해당 레코드를 대표할 수 있다면 그 컬럼을 프라이머리 키로 설정하는 것이 좋다.\n프라이머리 키는 반드시 명시할 것 가능하면 AUTO_INCREMENT컬럼을 이용해서라도 프라이머리 키는 생성하는 것을 권장한다. InnoDB에서 프라이머리 키를 정의하지 않으면 스토리지 엔진이 내부적으로 일련번호 컬럼을 추가하지만 사용자는 전혀 접근할 수 없으므로AUTO_INCREMENT 컬럼을 생성하고 프라이머리 키로 설정하여 사용자가 활용할 수 있는 값으로 사용하는 것이 좋다. 또한 ROW 기반 복제나 InnoDB Cluster에서는 모든 테이블이 프라이머리 키를 가져야만 정상적인 복제 성능을 보장하기도 하므로 프라이머리 키는 꼭 생성해야 한다.\nAUTO-INCREMENT 컬럼을 인조 식별자로 사용할 경우 여러 개의 컬럼이 복합으로 프라이머리 키가 만들어지는 경우 프라이머리 키의 크기가 길어질 때가 가끔 있는데, 프라이머리 키의 크기가 길어도 세컨더리 인덱스가 필요하지 않다면 그대로 프라이머리 키를 사용하는 것이 좋다. 세컨더리 인덱스도 필요하고 프라이머리 키의 크기도 길다면 AUTO_INCREMENT컬럼을 추가하고 이를 프라이머리 키로 설정한다.\n이렇게 프라이머리 키를 대체하기 위해 인위적으로 추가된 프라이머리 키를 인조 식별자(Surrogate key)라고 한다. 그리고 로그 테입르과 같이 조회보다는 INSERT 위주의 테이블들은 AUTO_INCREMENT를 이용한 인조 식별자를 프라이머리 키로 설정하는 것이 성능 향상에 도움이 된다.\n유니크 인덱스 유니크는 인덱스라기보다는 제약 조건에 가깝다. 테이블이나 인덱스에 같은 값이 2개 이상 저장될 수 없음을 의미하는데, MySQL에서는 인덱스 없이 유니크 제약만 설정할 방법이 없다.\n유니크 인덱스에서 NULL도 저장될 수 있는데, NULL은 특정 값이 아니므로 2개 이상 저장될 수 있다. MySQL에서 프라이머리 키는 기본적으로 NULL을 허용하지 않는 유니크 속성이 자동으로 부여된다.\n유니크 인덱스와 일반 세컨더리 인덱스의 비교 유니크 인덱스와 유니크하지 않은 일반 세컨더리 인덱스는 인덱스 구조상 아무런 차이점이 없다. 하지만 읽기와 쓰기에 대해 성능적으로 다른 부분이 있다.\n인덱스 읽기 많은 사람들이 유니크 인덱스가 빠르다고 생각하지만, 유니크하지 않은 세컨더리 인덱스에서 한 번 더 해야 하는 작업은 디스크 읽기가 아니라 CPU에서 컬럼 값을 비교하는 작업이기 때문에 성능상 영향이 거의 없다.\n유니크하지 않은 세컨더리 인덱스는 중복된 값이 허용되므로 읽어야 할 레코드가 많아서 느린것이지, 인덱스 자체의 특성 때문에 느린 것이 아니다.\n하나의 값을 검색하는 경우, 유니크 인덱스와 일반 세컨더리 인덱스는 사용되는 실행 계획이 다르다. 하지만 이는 인덱스의 성격이 유니크한지 아닌지에 따른 차이일 뿐 큰 차이는 없다.\n인덱스 쓰기 새로운 레코드가 INSERT 되거나 인덱스 컬럼의 값이 변경되는 경우에는 인덱스 쓰기 작업이 필요하다. 그런데 유니크 인덱스의 키 값을 쓸 때는 중복된 값이 있는지 없는지 체크하는 과정이 한 단계 더 필요하다. 그래서 유니크하지 않은 세컨더리 인덱스의 쓰기보다 느리다.\nMySQL에서는 유니크 인덱스에서 중복된 값을 체크할 때는 읽기 잠금을 사용하고, 쓰기를 할 때는 쓰기 잠금을 사용하는데, 이 과정에서 데드락이 아주 빈번히 발생한다.\n또한 InnoDB 스토리지 엔진에는 인덱스 키의 저장을 버퍼링 하기 위해 체인지 버퍼(Change Buffer)를 사용하여 인덱스의 저장이나 변경 작업이 상당히 빨리 처리되지만, 유니크 인덱스는 반드시 중복 체크를 해야하므로 작업 자체를 버퍼링 하지 못한다. 이 때문에 유니크 인덱스는 일반 세컨더리 인덱스보다 변경 작업이 더 느리다.\n유니크 인덱스 사용시 주의사항 꼭 필요한 경우라면 유니크 인덱스를 생성하는 것은 당연하나, 성능이 좋아질 것으로 생각하고 불필요하게 유니크 인덱스를 생성하지는 않는 것이 좋다. 하나의 테이블에서 같은 컬럼에 유니크 인덱스와 일반 인덱스를 각각 중복해서 생성해 둔 경우가 가끔 있는데, 같은 역할을 하므로 중복해서 인ㄷ게스를 생성할 필요는 없다. 똑같은 컬럼에 대해 프라이머리 키와 유니크 인덱스를 동일하게 생성하는 경우도 있는데 이 또한 중복이다. 결론적으로 유일성이 꼭 보장되어야 하는 컬럼에 대해서는 유니크 인덱스를 생성하되, 꼭 필요하지 않다면 유니크 인덱스 보다는 유니크하지 않은 세컨더리 인덱스를 생성하는 방법도 한 번씩 고려해보자.\n외래키 MySQL에서 외래키는 InnoDB 스토리지 엔진에서만 생성할 수 있으며, 외래키 제약이 설정되면 자동으로 연관되는 테이블의 컬럼에 인덱스까지 생성된다. 외래키가 제거되지 않은 상태에서는 자동으로 생성된 인덱스를 삭제할 수 없다.\nInnoDB의 외래키 관리에는 중요한 두 가지 특징이 있다.\n테이블 변경(쓰기 잠금)이 발생하는 경우에만 잠금 경합(잠금 대기)이 발생한다. 외래키와 연관되지 않은 컬럼의 변경은 최대한 잠금 경함(잠금 대기)를 발생시키지 않는다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 CREATE TABLE tb_parant ( id INT NOT NULL , fd VARCHAR(100) NOT NULL, PRIMARY KEY (id) ) ENGINE=InnoDB; CREATE TABLE tb_child ( id INT NOT NULL, pid INT DEFAULT NULL, fd VARCHAR(100) DEFAULT NULL, PRIMARY KEY (id), KEY ix_parentid (pid), CONSTRAINT child_ibfk_1 FOREIGN KEY (pid) REFERENCES tb_parant (id) ON DELETE CASCADE ) ENGINE=InnoDB; INSERT INTO tb_parant VALUES (1, \u0026#39;p1\u0026#39;), (2, \u0026#39;p2\u0026#39;); INSERT INTO tb_child VALUES (100, 1, \u0026#39;c100\u0026#39;); 자식 테이블의 변경이 대기하는 경우 작업 번호 커넥션-1 커넥션-2 1 BEGIN; 2 UPDATE tb_parent SET fb=\u0026lsquo;changed-2\u0026rsquo; WHERE id=2; 3 BEGIN; 4 UPDATE tb_child SET pid=2 WHERE id=100; 5 ROLLBACK; 6 Query OK, 1 row affected 2번 커넥션 는 부모 테이블의 변경 작업이 완료될 때까지 대기한다. 1번 커넥션에서 트랜잭션이 종료하면 2번 커넥션의 대기중이던 작업이 즉시 처리되는 것을 확인할 수 있다. 즉 자식 테이블의 외래 키 컬럼의 변경은 부모 테이블의 확인이 필요한데, 이 상태에서 부모 테이블의 해당 레코드가 쓰기 잠금이 걸려있으면 해당 쓰기 잠금이 해제될 때까지 기다리게 된다.\n자식 테이블의 외래키가 아닌 컬럼의 변경은 외래키로 인한 잠금 확장이 발생하지 않는다.\n부모 테이블의 변경 작업이 대기하는 경우 작업 번호 커넥션-1 커넥션-2 1 BEGIN; 2 UPDATE tb_child SET fb=\u0026lsquo;changed-100\u0026rsquo; WHERE id=100; 3 BEGIN; 4 DELETE FROM tb_parent WHERE id=1; 5 ROLLBACK; 6 Query OK, 1 row affected 1번 커넥션에서 부모 키를 참조하는 자식 테이블의 레코드를 변경하면 tb_child 테이블의 레코드에 대해 쓰기 잠금을 획득한다. 이 상태에서 2번 커넥션이 부모 테이블에서 해당 부모 레코드를 삭제하는 경우 이 쿼리는 자식 테이블의 레코드에 대한 쓰기 잠금이 해제될 때까지 기대려야 한다. 이는 자식 테이블이 생성될 때 적용된 외래키의 특성(ON DELETE CASCADE) 때문에 부모 레코드가 삭제되면 자식 레코드도 동시에 삭제되는 식으로 작동하기 때문이다.\n데이터베이스에서 외래 키를 물리적으로 생성하려면 이러한 현상으로 인한 잠금 경합까지 고려해 모델링을 진행하는 것이 좋다.\n물리적으로 외래키를 생성하면 자식 테이블에 레코드가 추가되는 경우 해당 참조키가 부모 테이블에 있는지 확인하는데, 물리적인 외래키의 고려 사항은 이러한 체크 작업이 아니라 이러한 체크를 위해 연관 테이블에 읽기 잠금을 걸어야 한다는 점이다.\n이렇게 잠금이 다른 테이블로 확장되면 그만큼 전체적으로 쿼리의 동시 처리에 영향을 미친다.\n","date":"2023-05-14T19:24:10+09:00","image":"https://codemario318.github.io/post/real-mysql/8/4/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/8/4/","title":"8. 인덱스 (4)"},{"content":"문서의 내용 전체를 인덱스화해서 특정 키워드가 포함된 문서를 검색하는 전문(Full Text) 검색에는 InnoDB나 MyISAM 스토리지 엔진에서 제공하는 일반적인 용도의 B-Tree 인덱스를 사용할 수 없다.\n문서 전체에 대한 분석과 검색을 위한 인덱싱 알고리즘을 전문 검색(Full Text Search) 인덱스라고 하는데, 전문 검색 인덱스는 일반화된 기능의 명칭으로 알고리즘의 이름을 지칭하는 것은 아니다.\n인덱스 알고리즘 전문 검색에서는 문서 본문의 내용에서 사용자가 검색하게 될 키워드를 분석하고, 빠른 검색용으로 사용할 수 있게 이러한 키워드로 인덱스를 구축한다. 키워드의 분석 및 인덱스 구축에는 여러 가지 방법이 있을 수 있다.\n전문 검색 인덱스는 문서의 키워드를 인덱싱하는 기법에 따라 구분될 수 있다.\n어근 분석 알고리즘 n-gram 알고리즘 어근 분석 알고리즘 MySQL 서버의 전문 검색 인덱스는 다음과 같은 두 가지 중요한 과정을 거쳐서 색인 작업이 수행된다.\n불용어(Stop Word)처리 검색에서 별 가치가 없는 단어를 모두 필터링해서 제거하는 작업을 의미 불용어의 개수가 많지 않기 때문에 알고리즘을 구현한 코드에 상수로 정의해 사용하는 경우가 많다. 유연성을 위해 불용어 자체를 데이터베이스화해서 사용자가 추가하거나 삭제할 수 있게 구현하는 경우도 있다. 어근 분석(Stemming) 검색어로 선정된 단어의 뿌리인 원형을 찾는 작업이다. MySQL 서버는 오픈소스 형태소 분석 라이브러리인 MeCab을 플러그인 형태로 사용할 수 있게 지원한다. 한글이나 일본어의 경우 영어와 같이 단어의 변형 자체는 거의 없기 때문에 어근분석보다는 문장의 형태소를 분석해서 명사와 조사를 구분하는 기능이 더 중요한 편이다. n-gram 알고리즘 MeCab을 위한 형태소 분석은 매우 전문적인 전문 검색 알고리즘이어서 만족할 만한 결과를 내기 위해서는 많은 노력과 시간을 필요로 한다. 전문적인 검색 엔진을 고려하는 것이 아니라면 범용적으로 적용하기는 쉽지 않기 때문에 이러한 단점을 보완하기 위한 방법으로 n-gram 알고리즘이 도입되었다.\nn-gram이란 본문을 무조건 몇 글자씩 잘라서 인덱싱하는 방법이다. 형태소 분석보다는 알고리즘이 단순하고 국가별 언어에 대한 이해와 준비 작업이 필요 없는 반면, 만들어진 인덱스의 크기는 상당히 큰 편이다. n은 인덱싱할 키워드의 최소 글자 수를 의미하는데, 일반적으로는 2글자 단위로 키워드를 쪼개서 인덱싱하는 2-gram(Bi-gram) 방식이 많이 사용된다.\n1 To be or not to be. That is the question 각 단어는 띄어쓰기와 마침표를 기준으로 10개의 단어로 구분되고, 2글자씩 중첩해서 토큰으로 분리된다.\n단어 bi-gram 토큰 To To be be or or not no ot to to be be that Th ha at is is the th he question qu ue es st ti io on 각 글자가 중첩되어 토큰화 되기 때문에 Bi-gram 알고리즘에서는 글자수 - 1개의 토큰으로 구분된다. 이렇게 구분된 토큰을 인덱스에 저장한다. 이때 중복된 토큰은 하나의 인덱스 엔트리로 병합되어 저장한다.\nMySQL 서버는 이렇게 생성된 토큰들에 대해서 불용어를 걸러내는 작업을 수행하는데, 이때 불용어와 동일하거나 불용어를 포함하는 경우 걸러서 버린다. 기본적으로 MySQL 서버에 내장된 불용어는 information_schema.innodb_ft_default_stopword 테이블을 통해 확인 가능하다.\n입력 불용어 일치 불용어 포함 출력(최종 인덱스 등록) at O be O be O es et ha O he he he io O is O no no on O or O ot ot qu qu st st Th Th th th ti O To O to O ue ue 전문 검색을 더 빠르게 하기 위해 2단계 인덱싱(프론트엔드와 백엔드 인덱스)과 같은 방법도 있지만 MySQL 서버는 구분된 토큰을 단순한 B-Tree 인덱스에 저장한다.\n불용어 변경 및 삭제 n-gram의 토큰 파싱 및 불용어 처리 예시 결과를 보면 \u0026ldquo;ti\u0026rdquo;, \u0026ldquo;at\u0026rdquo;, \u0026ldquo;ha\u0026rdquo; 같은 토큰들은 \u0026ldquo;a\u0026rdquo;, \u0026ldquo;i\u0026rdquo; 철자가 불용어로 등록돼 있기 때문에 모두 걸러진다. 실제로 이 같은 불용어 처리는 사용자에게 도움이 되기보다는 사용자를 더 혼란스럽게 하는 기능일 수도 있다. 그래서 불용어 처리 자체를 완전히 무시하거나 MySQL 서버에 내장된 불용어 대신 사용자가 직접 불용어를 등록하는 방법을 권장한다.\n전문 검색 인덱스의 불용어 처리 무시\n스토리지 엔진과 관계 없이 MySQL 서버의 모든 전문 검색 인덱스에 대해 불용어를 완전히 제거한다. MySQL 서버의 설정 파일(my.cnf)의 ft_storpword_file 시스템 변수에 빈 문자열을 설정한다. 해당 시스템 변수는 서버가 재시작될 때만 인지하기 때문에 설정 변경시 서버를 재시작해야 반영된다. 사용자 정의 불용어를 적용할 때도 해당 파일 경로를 적용하여 반영할 수 있다. InnoDB 스토리지 엔진을 사용하는 테이블의 전문 검색 인덱스에 대해서만 불용어 처리 무시 innodb_ft_enable_stopword 시스템 변수를 OFF로 설정한다. 해당 시스템 변수는 동적인 시스템 변수이므로 서버가 실행 중인 상태에서도 변경할 수 있다. 사용자 정의 불용어 사용\n불용어 목록을 파일로 저장하고, MySQL 서버 설정파일에서 파일의 경로를 ft_stopword_file 설정에 등록한다. InnoDB 스토리지 엔진을 사용하는 테이블의 전문 검색 엔진에만 사용할 수 있는데, innodb_ft_server_stopword_table 시스템 변수에 불용어 테이블을 설정한다. 이때 불용어 목록을 변경한 이후 전문 검색 인덱스가 생성돼야만 변경된 불용어가 적용된다. 1 2 3 4 5 6 CREATE TABLE my_stopword(value VARCHAR(30)) ENGINE = INNODB; INSERT INTO my_stopword(value) VALUES (\u0026#39;MySQL\u0026#39;); SET GLOBAL innodb_ft_server_stopword_table=\u0026#39;mydb/my_stopword\u0026#39;; ALTER TABLE tb_bi_gram ADD FULLTEXT INDEX fx_title_body(title, body) WITH PARSER ngram; innodb_ft_user_stopword_table 시스템 변수를 이용하는 방법도 있으며, innodb_ft_server_stopword_table와 사용법이 동일하다. 여러 전문 검색 인덱스가 서로 다른 불용어를 사용해야 하는 경우에 활용할 수 있다.\n전문 검색 엔진 인덱스의 가용성 전문 검색 인덱스를 사용하려면 반드시 2가지 조건을 만족해야 한다.\n쿼리 문장이 전문 검색을 위한 문법(MATCH ... AGAINST)을 사용 테이블이 전문 검색 대상 컬럼에 대해서 전문 인덱스 보유 1 2 3 4 5 6 7 CREATE TABLE tb_test ( doc_id INT, doc_body TEXT, PRIMARY KEY (doc_id), FULLTEXT KEY fx_docbody (doc_body) WITH PARSER ngram ) ENGINE=InnoDB; 1 2 3 4 5 6 /* 풀 테이블 스캔으로 처리되는 쿼리 */ SELECT * FROM tb_test WHERE doc_body LIKE \u0026#39;%애플%\u0026#39;; 1 2 3 4 5 6 /* 전문 검색 인덱스로 처리되는 쿼리 */ SELECT * FROM tb_test WHERE MATCH(doc_body) AGAINST (\u0026#39;애플\u0026#39; IN BOOLEAN MODE); 전문 검색 인덱스를 사용하려면 반드시 MATCH ... AGAINST ...구문으로 검색 쿼리를 작성해야 하며, 인덱스를 구성하는 컬럼들은 MATCH절의 괄호 안에 모두 명시되어야 한다.\n","date":"2023-05-14T17:24:10+09:00","image":"https://codemario318.github.io/post/real-mysql/8/3/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/8/3/","title":"8. 인덱스 (3) - 전문 검색 인덱스"},{"content":"B-Tree는 데이터베이스의 인덱싱 알고리즘 가운데 가장 일반적으로 사용되고, 가장 먼저 도입되었지만 아직까지도 가장 범용적으로 사용되는 인덱스 알고리즘이다.\nB-Tree에는 여러 변형된 형대의 알고리즘이 있는데, 일반적으로 DMBS에서는 B+-Tree, B*-Tree가 사용된다.\nB-Tree는 컬럼의 원래 값을 변형시키지 않고 인덱스 구조체 내에서는 항상 정렬된 상태로 유지한다. 전문 검색같은 특수한 요건이 아닌 경우, 대부분 인덱스는 B-Tree를 사용한다.\n구조 및 특성 B-Tree는 트리 구조의 최상위에 하나의 루트 노드가 존재하고 그 하위에 자식 노드가 붙어있는 형태이다. 트리 구조의 가장 하위에 있는 노드를 리프 노드라 하고, 트리 구조에서 루트 노드도 아니고 리프 노드도 아닌 중간 노드를 브랜치 노드라고 한다.\n데이터베이스에서 인덱스와 실제 데이터가 저장된 데이터는 따로 관리되는데, 인덱스의 리프 노드는 항상 실제 데이터 레코드를 찾아가기 위한 주솟값을 가진다.\n인덱스의 키 값은 모두 정렬돼 있지만, 데이터 파일릐 레코드는 정렬돼 있지 않고 임의의 순서로 저장돼 있다. 레코드가 삭제되어 빈 공간이 생기면 그다음 INSERT는 가능한 한 삭제된 공간을 재활용 하도록 DBMS가 설계되기 때문에, 항상 INSERT순서대로 저장되는 것은 아니다.\n인덱스는 테이블의 키 컬럼만 가지고 있으므로 나머지 컬럼을 읽으려면 데이터 파일에서 해당 레코드를 찾아야 한다. 이를 위해 인덱스의 리프 노드는 데이터 파일에 저장된 레코드의 주소를 가진다.\nMyISAM 레코드 주소는 MyISAM 테이블 생성 옵션에 따라 레코드가 테이블에 INSERT된 순번이거나 데이터 파일 내의 위치다. (ROWID) 세컨더리 인덱스가 물리적인 주소를 가진다. InnoDB 프라이머리 키가 ROWID 역할을 한다 프라이머리 키를 주소처럼 사용하기 때문에 논리적인 주소를 가진다고 볼 수 있다. 따라서 인덱스를 통해 레코드를 읽을때는 데이터 파일을 바로 찾아가지 못하고, 인덱스에 저장되어있는 프라이머리 키 인덱스의 리프 페이지에 저장돼 있는 레코드를 읽기 위해서는 반드시 프라이머리 키를 저장하고 있는 B-Tree를 다시 한번 검색해야 한다. 이러한 특징으로 인해 InnoDB 스토리지 엔진을 사용하는 테이블의 성능이 떨어질 것 처럼 보이지만 각각 장단점을 가지고 있다.\nB-Tree 인덱스 키 추가 및 삭제 테이블의 레코드를 저장하거나 변경하는 경우 인덱스 키 추가나 삭제 작업이 발생한다. 이에따라 주의해야 할 사항이 있다.\n인덱스 키 추가 새로운 키 값이 B-Tree에 저장될 때는 저장될 키 값을 이용해 B-Tree상의 적절한 위치를 검색해야 한다. 저장될 위치가 결정되면 레코드의 키 값과 대상 레코드의 주소 정보를 B-Tree의 리프 노드에 저장한다.\n리프 노드가 꽉 차서 더는 저장할 수 없을 때는 리프 노드가 분리돼야 하는데, 이는 상위 브랜치 노드까지 처리 범위가 넓어진다. 이러한 작업 탓이 B-Tree는 상대적으로 쓰기 작업(새로운 키를 추가)에 비용이 많이 드는 것으로 알려졌다.\n인덱스 추가로 인해 INSERT, UPDATE 문장이 어떤 영향을 받을지 예상해보려면 테이블의 컬럼 수, 컬럼 크기, 인덱스 컬럼의 특성 등을 확인해야 한다. 대략적으로 테이블의 레코드를 추가하는 작업 비용을 1이라고 가정하면, 해당 테이블의 인덱스에 키를 추가하는 작업 비용을 1.5 정도로 예측한다. 테이블에 인덱스가 3개가 있다면 5.5(1.5 * 3 + 1) 정도로 예측한다. 중요한 것은 이 비용의 대부분이 디스크로부터 인덱스 페이지를 읽고 쓰기를 해야해서 걸리는 시간이다.\nMyISAM이나 MEMORY 스토리지 엔진을 사용하는 테이블에서는 INSERT 문장이 실행되면 즉시 새로운 키 값을 B-Tree 인덱스에 변경하지만, InnoDB 스토리지 엔진은 필요하다면 인덱스 키 추가 작업을 지연시켜 나중에 처리할 수 있다. 하지만 프라이머리 키나 유니크 인덱스의 경우 중복 체크가 필요하기 때문에 즉시 B-Tree에 추가하거나 삭제한다.\n인덱스 키 삭제 B-Tree의 키 값이 삭제되는 경우는 상당히 간단하다. 해당 키 값이 저장된 B-Tree의 리프 노드를 찾아서 그냥 삭제 마크만 하면 작업이 완료된다. 삭제 마킹된 인덱스 키 공간은 계속 그대로 방치하거나 재사용할 수 있다.\n인덱스 키 삭제로 인한 마킹 작업 또한 디스크 쓰기가 필요하며, MySQL 5.5 이상 버전의 InnoDB 스토리지 엔진에서는 버퍼링 되어 지연 처리될 수 있다. 처리가 지연된 인덱스 키 삭제 또한 사용자에게는 특별한 악영향 없이 MySQL 서버가 내부적으로 처리하므로 특별히 걱정할 것은 없다.\nMyISAM이나 MEMORY 스토리지 엔진의 테이블에서는 체인지 버퍼와 같은 기능이 없으므로 인덱스 키 삭제가 완료된 후 쿼리 실행이 완료된다.\n인덱스 키 변경 인덱스의 키 값은 값에 따라 저장될 리프 노드의 위치가 결정되므로 B-Tree의 키 값이 변경되는 경우에는 단순히 인덱스상의 키 값만 변경하는 것은 불가능하다. 따라서 B-Tree의 키 값 변경 작업은 먼저 키 값을 삭제한 후, 다시 새로운 키 값을 추가하는 형태로 처리된다.\n결국 인덱스 키 값을 변경하는 작업은 기존 인덱스 키 값을 삭제한 후 새로운 인덱스 키 값을 추가하는 작업으로 처리되고 InnoDB 스토리지 엔진을 사용하는 테이블에 대해서는 이 작업 모두 체인지 버퍼를 활용해 지연 처리 될 수 있다.\n인덱스 키 검색 인덱스를 검색하는 작업은 B-Tree의 루트 노드부터 시작해 브랜치 노드를 거쳐 최종 리프 노드까지 이동하면서 비교 작업을 수행하는데, 이 과정을 트리 탐색이라고 한다.\n인덱스 트리 탐색은 SELECT뿐만 아니라 UPDATE, DELETE를 처리하기 위해 해당 레코드를 검색해야 할 때도 사용된다.\nB-Tree 인덱스를 이용한 검색은 100% 일치 또는 값의 앞부분만 일치하는 경우에 사용할 수 있다.\n부등호 비교 조건에서도 인덱스를 활용할 수 있지만, 인덱스를 구성하는 키 값의 뒷부분만 검색하는 용도로는 사용할 수 없다. 인덱스를 키 값에 변형이 가해진 후 비교되는 경우 빠른 검색 기능을 활용할 수 없다.(변형된 값은 인덱스에 존재하는 값이 아니므로) 함수나 연산을 수행한 결과로 정렬 또는 검색 InnoDB 테이블에서 지원하는 레코드 잠금이나 넥스트 키락이 검색을 수행한 인덱스를 잠근 후 테이블의 레코드를 잠그는 방식으로 구현돼있어, UPDATE, DELETE 문장이 실행될 때 테이블에 적절히 사용할 수 있는 인덱스가 없으면 불필요하게 많은 레코드를 잠그게 된다.\nB-Tree 인덱스 사용에 영향을 미치는 요소 컬럼 크기 레코드의 건수 유니크한 인덱스 키 값의 개수 인덱스 키 값의 크기 InnoDB 스토리지 엔진은 디스크에 데이터를 저장하는 가장 기본 단위를 페이지 또는 블록이라고 하며, 디스크의 모든 읽기 및 쓰기 작업의 최소 작업 단위가 된다. 또한 페이지는 InnoDB 스토리지 엔진의 버퍼풀에서 데이터를 버퍼링하는 기본 단위이기도 하다. 인덱스도 결국 페이지 단위로 관리되며, 루트와 브랜치 리프 노드를 구분한 기준이 페이지 단위이다.\n일반적으로 DBMS의 B-Tree는 자식 노드의 개수가 가변적인 구조다. 인덱스의 페이지 크기와 키 값의 크기에 따라 자식 노드의 최대 개수가 결정된다. MySQL 5.7 버전부터는 InnoDB 스토리지 엔진의 페이지 크기를 innodb_page_size 시스템 변수를 이용해 4KB ~ 64KB 사이의 값을 결정 할 수 있으며, 기본값은 16KB 이다.\n인덱스 페이지는 인덱스 키 값과 해당하는 주솟값을 인덱스 페이지에 지정하게 되며, 페이지 크기가 변하지 않았을 경우 인덱스 키 값이 커지면 하나의 페이지에 저장되는 레코드의 개수는 줄어들게 된다. 키 크기가 커서 한 페이지에 300개의 레코드가 인덱스 페이지에 저장된다고 가정하면 SELECT 쿼리를 통해 500개의 데이터를 조회해야 한다고 했을때 최소한 2번 읽기 작업이 발생하게 된다.\n따라서, 인덱스를 구성하는 키 값의 크기가 커지면 디스크로 부터 읽어와야 하는 횟수가 늘어날 수 있고, 그만큼 느려질 수 있다. 또한 인덱스 키 값의 길이가 길어진다는 것은 전체적인 인덱스의 크기가 커진다는 것을 의미하기 때문에, 무한정 인덱스를 캐시해둘 수 없는 InnoDB의 버퍼풀, MyISAM의 키 캐시에 저장할 수 있는 공간이 부족해져 메모리 효율이 떨어질 수 있다.\nB-Tree 깊이 인덱스의 깊이는 상당히 중요하지만 제어할 방법은 없다.\nB-Tree의 깊이는 MySQL에서 값을 검색할 때 몇 번이나 랜덤하게 디스크를 읽어야 하는지와 직결되는 문제다. 인덱스 키 값의 크기가 커질수록 하나의 인덱스 페이지가 담을 수 있는 인덱스 키 값의 개수가 적어지고, 이에 따라 같은 개수의 인덱스 키값을 저장하게 되면 더 깊어지게된다. 따라서 디스크 읽기가 더 많이 필요하게 된다.\n인ㄷ게스 키 값의 크기는 가능한 작게 만드는 것이 좋지만, 실제로 아무리 대용량 데이터베이스라도 B-Tree의 깊이가 5단계 이상까지 깊어지는 경우는 흔하지 않다.\n선택도(기수성) 인덱스에서 선택도(Selectivity) 또는 기수성(Cardinality)은 거의 같은 의미로 사용되며, 모든 인덱스 키 값 가운데 유니크한 값의 수를 의미한다.\n인덱스 키 값 가운데 중복된 값이 많아질수록 기수성은 낮아지고 동시에 선택도 또한 떨어지게 된다. 인덱스는 선택도가 높을수록 검색 대상이 줄어들기 때문에 그만큼 빠르게 처리된다.\n선택도가 좋지 않다고 하더라도 정렬이나 그루핑과 같은 작업을 위해 인덱스를 만드는 것이 훨씬 나은 경우도 많다. 인덱스가 항상 검색에만 사용되는 것은 아니므로 여러 가지 용도를 고려해 적절히 인덱스를 설계해야한다.\n선택도가 낮은 인덱스를 처리하면 MySQL 서버는 불필요한 데이터를 더 많이 읽어오게 된다.\n읽어야 하는 레코드의 건수 인덱스를 통해 테이블의 레코드를 읽는 것은 인덱스를 거치지 않고 바로 테이블의 레코드를 읽는 것 보다 높은 비용이 드는 작업이다. 따라서 인덱스를 이용한 읽기의 손익 분기점이 얼마인지를 판단할 필요가 있다.\n일반적인 DBMS의 옵티마이저에서는 인덱스를 통해 레코드 1건을 읽는 것이 테이블에서 직접 레코드 1건을 읽는 것보다 4~5배 정도 비용이 더 많이 드는 작업인 것으로 예측한다. 즉 인덱스를 통해 읽어야 할 레코드의 건수가 전체 테이블 레코드의 20%~25%를 넘어서면 인덱스를 이용하지 않고 테이블을 모두 직접 읽어서 필요한 레코드만 가려내는 방식으로 처리하는 것이 효율적이다.\n많은 레코드를 읽을 때는 강제로 인덱스를 사용하도록 힌트를 추가해도 성능상 얻을 수 있는 이점이 없다.\nB-Tree 인덱스를 통한 데이터 읽기 어떤 경우에 인덱스를 사용하게 유도할지, 또는 사용하지 못하게 할지 판단하려면 MySQL이 어떻게 인덱스를 이용해서 레코드를 읽어 내는지 알아야 한다.\n인덱스 레인지 스캔 1 2 3 SELECT * FROM employees WHERE first_name BETWEEN \u0026#39;Ebbe\u0026#39; AND \u0026#39;Gad\u0026#39;; 인덱스 레인지 스캔은 검색해야 할 인덱스의 범위가 결정됐을 대 사용하는 방식이다. 검색하려는 값의 수나 검색 결과 레코드 건수와 관계 없이 레인지 스캔이라고 표현한다.\n루트 노드에서부터 비교를 시작해 브랜치 노드를 거치고 최종적으로 리프 노드까지 찾아 들어가야만 비로소 필요한 레코드의 시작 지점을 찾을 수 있다. 이처럼 차례대로 죽 읽는 것을 스캔이라고 표현하며, 스캔하다가 리프 노드의 끝까지 읽으면 리프 노드의 구간은 실제 스캔하는 범위를 표현한다.\nB-Tree 인덱스에서 루트와 브랜치 노드를 이용해 스캔 시작 위치를 검색하고, 그 지점부터 필요한 방향으로 인덱스를 읽어 나간다. 인덱스 자체 정렬 특성으로 인해 어떤 방식으로 스캔하든 관계없이, 해당 인덱스를 구성하는 컬럼의 정순 또는 역순으로 정렬된 상태로 레코드를 가져온다.\n인덱스의 리프 노드에서 검색 조건에 일치하는 건들은 데이터 파일에서 레코드를 읽어오는 과정이 필요하다. 이때 리프노드에 저장된 레코드 주소로 데이터 파일의 레코드를 읽어오는데, 레코드 한 건 단위로 랜덤 I/O가 한 번 씩 일어난다. 이러한 이유 때문에 인덱스를 통해 데이터 레코드를 읽는 작업은 비용이 많이 드는 작업으로 분류된다.\n인덱스에서 조건을 만족하는 값이 저장된 위치를 찾는다. 1번에서 탐색된 위치부터 필요한 만큼 인덱스를 차례대로 쭉 읽는다.(인덱스 스캔) 읽어 들인 인덱스 키와 레코드 주소를 이용해 레코드가 저장된 페이지를 가져오고, 최종 레코드를 읽어온다. 쿼리가 필요로 하는 데이터에 따라 3번 과정은 필요하지 않을 수도 있는데, 이를 커버링 인덱스라고 한다. 커버링 인덱스로 처리되는 쿼리는 디스크의 레코드를 읽지 않아도 되기 때문에 랜덤 읽기가 상당히 줄어들고 그만큼 빨라진다.\n1 SHOW STATUS LIKE \u0026#39;Handler_%\u0026#39;; 위 쿼리를 통해 읽은 레코드 건수를 조회할 수 있으나, 실제 인덱스만 읽었는지 인덱스를 통해 테이블의 레코드를 읽었는지(3번)은 구분할 수 없다.\nHandler_read_key: 1번 단계가 실행된 횟수 Handler_read_next: 인덱스 정순으로 읽은 레코드 건수 Handler_read_prev: 인덱스 역순으로 읽은 레코드 건수 Handler_read_first: 첫 번째 레코드를 읽은 횟수 (MIN) Handler_read_last: 마지막 레코드를 읽은 횟수 (MAX) 인덱스 풀 스캔 인덱스 레인지 스캔과는 달리 인덱스의 처음부터 끝까지 모두 읽는 방식이다. 대표적으로 쿼리의 조건절에 사용된 컬럼이 인덱스의 첫 번째 컬럼이 아닌 경우 인덱스 풀 스캔 방식이 사용된다.\n인덱스 리프 노드의 제일 앞 또는 제일 뒤로 이동한다. 인덱스의 리프 노드를 연결하는 링크드 리스트를 따라서 처음부터 끝까지 스캔한다. 인덱스 풀 스캔은 인덱스에 포함된 컬럼만으로 쿼리를 처리할 수 있는 경우 테이블의 레코드를 읽을 필요가 없어 테이블 풀 스캔보다는 효율적이다. 인덱스의 전체 크기는 테이블 자체의 크기보다는 훨씬 작으므로 더 적은 디스크 I/O로 처리할 수 있다.\n루스 인덱스 스캔 오라클 DBMS의 인덱스 스킵 스캔비슷하게 처리되는 방법으로 MySQL 5.7 버전 까지는 기능이 많이 제한적이었지만, MySQL 8.0 버전부터는 다른 상용 DBMS에서 지원하는 인덱스 스킵 스캔과 같은 최적화를 조금씩 지원하기 시작했다.\n인덱스 레인지 스캔과 인덱스 풀 스캔은 루스 인덱스 스캔과는 상반된 의미로 타이트 인덱스 스캔으로 분류한다.\n루스 인덱스 스캔이란 말 그대로 느슨하게 또는 듬성듬성하게 인덱스를 읽는 것을 의미한다. 인덱스 레인지 스캔과 비슷하게 작동하지만 중간에 필요치 않은 인덱스 키 값은 무시하고 다음으로 넘어가는 형태로 처리한다. 일반적으로 GROUP BY 또는 집합 함수 중 MIN(), MAX() 함수에 대해 최적화를 하는 경우 사용된다.\n1 2 3 4 5 6 SELECT dept_no ,MIN(emp_no) FROM dept_emp WHERE dep_no BETWEEN \u0026#39;d002\u0026#39; AND \u0026#39;d004\u0026#39; GROUP BY dept_no ; 인덱스에서 WHERE조건을 만족하는 범위 전체를 다 스캔할 필요가 없다는 것을 옵티마이저는 알고 있기 때문에 조건에 만족하지 않는 레코드는 무시하고 다음 레코드로 이동한다. 루스 인덱스 스캔을 사용하려면 여러가지 조건을 만족해야 한다.\n인덱스 스킵 스캔 데이터베이스 서버에서 인덱스의 핵심은 값이 정렬돼 있다는 것이며, 이로 인해 인덱스를 구성하는 컬럼의 순서가 매우 중요하다.\n1 2 3 ALTER TABLE employees ADD INDEX ix_gender_birthdate (gender, birth_date) ; 인덱스를 사용하려면 WHERE조건절에 gender 컬럼에 대한 비교 조건이 필수이다.\n1 2 3 4 5 6 7 8 9 /* 인덱스 X */ SELECT * FROM employees WHERE birth_date \u0026gt;= \u0026#39;1965-02-01\u0026#39;; /* 인덱스 O */ SELECT * FROM employees WHERE gender = \u0026#39;M\u0026#39; AND birth_date \u0026gt;= \u0026#39;1965-02-01\u0026#39; ; 따라서 위 두 쿼리중 gender 컬럼과 birth_date 컬럼의 조건을 모두 가진 두 번째 쿼리는 인덱스를 효율적으로 사용할 수 있지만, gender 컬럼에 대한 비교 조건이 없는 첫 번째 쿼리는 인덱스를 사용할 수 없어 birth_date 컬럼부터 시작하는 인덱스를 생성해야만 했다.\nMySQL 8.0 버전부터는 옵티마이저가 gender 컬럼을 건너 뛰어서 birth_date 컬럼만으로도 인덱스 검색이 가능하게 해주는 인덱스 스킵 스캔 최적화 기능이 도입됐다.\n1 2 3 4 5 6 SET optimizer_switch=\u0026#39;skip_scan=on\u0026#39;; SELECT * FROM employees WHERE birth_date \u0026gt;= \u0026#39;1965-02-01\u0026#39; ; 1 2 3 4 5 6 7 8 9 10 11 SELECT * FROM employees WHERE gender = \u0026#39;M\u0026#39; AND birth_date \u0026gt;= \u0026#39;1965-02-01\u0026#39; ; SELECT * FROM employees WHERE gender = \u0026#39;F\u0026#39; AND birth_date \u0026gt;= \u0026#39;1965-02-01\u0026#39; ; 위의 쿼리는 아래의 쿼리로 나눠 실행한 것과 비슷한 형태로 최적화를 실행한다.\n인덱스 스킵 스캔은 새롭게 도입된 기능이어서 아직 다음과 같은 단점이 있다.\nWHERE 조건절에 조건이 없는 인덱스의 선행 컬럼의 유니크한 값의 개수가 적어야함 쿼리가 인덱스에 존재하는 컬럼만으로 처리 가능해야함(커버링 인덱스) 다중 컬럼 인덱스 실제 서비스용 데이터베이스에서는 2개 이상의 컬럼을 포함하는 인덱스가 더 많이 사용된다. 두개 이상의 컬럼으로 구성된 인덱스를 다중 컬럼 인덱스(복합 컬럼 인덱스, Concatenated Index)라고 한다.\n인덱스의 두 번째 컬럼은 첫 번째 컬럼에 의존해서 정렬돼있다. 즉 두 번째 컬럼의 정렬은 첫 번째 컬럼이 똑같은 레코드에서만 의미가 있다. 위의 예제에서 emp_no 값의 정렬 순서가 빠르다고 하더라도 dept_no 컬럼의 정렬 순서가 늦다면 인덱스의 두쪽에 위치한다. 따라서 다중 컬럼 인덱스에서는 인덱스 내에서 각 컬럼의 위치(순서)가 상당히 중요하여 신중히 결정해야 한다.\nB-Tree 인덱스의 정렬 및 스캔 방향 인덱스를 생성할 때 설정한 정렬 규칙에 따라 인덱스의 키 값은 항상 오름차순이거나 내림차순으로 정렬되어 저장된다. 하지만 읽을때는 반대로도 가능하며, 인덱스를 어느 방향으로 읽을지는 쿼리에 따라 옵티마이저가 실시간으로 만들어 내는 실행 계획에 따라 결정된다.\n인덱스의 정렬 MySQL 5.7 버전까지는 컬럼 단위로 정렬 순서를 혼합해서 인덱스를 생성할 수 없었지만, 8.0 버전부터는 순서를 혼합한 인덱스도 생성할 수 있게 되었다.\n1 CREATE INDEX ix_teamname_userscore ON employees (team_name ASC, user_score DESC); 인덱스 스캔 방향 인덱스는 항상 오름차순으로만 정렬돼 있지만 인덱스를 최솟값부터 읽으면 오름차순으로 값을 가져올 수 있고, 최댓값부터 거꾸로 읽으면 내림차순으로 값을 가져올 수 있다는 것을 MySQL 옵티마이저는 이미 알고 있다. 즉 인덱스 생성 시점에 오름차순 또는 내림차순으로 정렬이 결정되지만 쿼리가 그 인덱스를 사용하는 시점에 인덱스를 읽는 방향에 따라 오름차순 또는 내림차순 정렬 효과를 얻을 수 있다.\nORDER BY 처리나 MIN, MAX 함수등의 최적화가 필요한 경우에도 인덱스의 읽기 방향을 전환해서 사용하도록 실행 계획을 만들어낸다.\n내림차순 인덱스 MySQL 서버는 실제 내림차순인지 오름차순인지 관계없이 인덱스를 읽는 순서만 변경해서 해결할 수 있지만, 2개 이상의 컬럼으로 구성된 복합 인덱스에서 각각의 컬럼이 내림차순과 오름차순이 혼합된 경우는 내림차순 인덱스로만 해결될 수 있다.\n실제 역순 정려 쿼리가 정순 정렬 쿼리보다 더 시간이 많이 걸린다. MySQL 서버의 InnoDB 스토리지 엔진에서 정순 스캔과 역순 스캔은 페이지 간의 양방향 연결 고리를 통해 전진하느냐 후진하느냐의 차이만 있지만, 실제 내부적으로는 InnoDB에서 인덱스 역순 스캔이 더 느린 이유는 2가지가 있다.\n페이지 잠금이 인덱스 정순 스캔에 적합한 구조 페이지 내에서 인덱스 레코드가 단방향으로만 연결된 구조 쿼리가 많은 레코드를 조회하면서 빈번하게 실행된다면 오름차순 인덱스보다는 내림차순 인덱스가 더 효율적이다. 또한 많은 쿼리가 인덱스의 한쪽만을 집중적으로 읽어 특정 페이지 잠금이 병목이 될 것으로 예상된다면 쿼리에서 자주 사용되는 정렬 순서대로 인덱스를 생성하는 것이 잠금 병목 현상은 완화하는 데 도움이 될 수 있다.\nB-Tree 인덱스의 가용성과 효율성 쿼리의 WHERE 조건이나 GROUP BY, ORDER BY 절이 어떤 경우에 인덱스를 사용할 수 있고 어떤 방식으로 사용할 수 있는지 식별할 수 있어야 인덱스를 최적으로 생성할 수 있다.\n비교 조건의 종류와 효율성 다중 컬럼 인덱스에서 각 컬럼의 순서와 그 컬럼에 사용된 조건이 동등 비교인지 아니면 범위 조건인지에 따라 인덱스 컬럼 활용 형태가 달라지며, 효율 또한 다르다.\n1 2 3 4 5 SELECT * FROM dept_emp WHERE dept_no=\u0026#39;d002\u0026#39; AND emp_no \u0026gt;= 10114 ; A: INDEX (dept_no, emp_no) dept_no='d002' AND emp_no \u0026gt;=10114 인 레코드들을 찾고 dept_no가 \u0026lsquo;d002\u0026rsquo;가 아닐 때까지 인덱스를 읽는다. 조건을 만족하는 레코드를 찾는데 필요한 비교 작업만 수행하므로 효율적으로 인덱스를 활용했다. B: INDEX (emp_no, dept_no) emp_no \u0026gt;=10114 AND dept_no='d002' 인 레코드들을 찾고 dept_no가 모든 레코드가 \u0026lsquo;d002\u0026rsquo;인지 비교한다. 인덱스를 통해 읽은 레코드가 나머지 조건에 맞는지 비교하면서 취사선택하는 작업을 필터링이라고 하며, 케이스 B 인덱스에서는 다중 컬럼 인덱스의 정렬 방식으로 인해 최종적인 조건을 만족하는 레코드를 찾기 위해 더 큰 범위의 데이터를 가져와 비교했다.\n공식적인 명칭은 아니지만 케이스 A 인덱스에서의 도 조건과 같이 작업의 범위를 결정하는 조건을 작업 범위 결정 조건이라 하고 케이스 B 인덱스같이 비교 작업의 범위를 줄이지 못하고 거름종이 역할만 하는 조건을 필터링 조건, 체크 조건이라고 표현한다.\n작업 범위를 결정하는 조건은 많으면 많을수록 쿼리의 처리 성능을 높이지만 체크 조건은 많다고 해서 쿼리의 처리 성능을 높히지는 못한다.\n인덱스의 가용성 B-Tree 인덱스의 특징은 왼쪽 값에 기준해서 오른쪽 값이 정렬된다. 하나의 컬럼 내에서 뿐만 아니라 다우 컬럼 인덱스의 컬럼에 대해서도 함께 적용된다.\nA: INDEX (first_name) B: INDEX (dept_no, emp_no) 인덱스 키 값의 이런 정렬 특성은 빠른 검색의 전제 조건이다. 즉 하나의 컬럼으로 검색해도 값의 왼쪽 부분이 없으면 인덱스 레인지 스캔 방식의 검색이 불가능하다. 또한 다중 컬럼 인덱스에서도 왼쪽 컬럼의 값을 모르면 인덱스 레인지 스캔을 사용할 수 없다.\n1 2 3 4 SELECT * FROM employees WHERE first_name LIKE \u0026#39;%mer\u0026#39; ; first_name 컬럼에 저장된 값의 왼쪽부터 한 글자씩 비교해 가면서 일치하는 레코드를 찾아야 하는데, LIKE 조건으로 왼쪽 부분이 고정되지 않았기 때문에 인덱스 레인지 스캔 방식으로 인덱스를 사용할 수 없다.\n1 2 3 4 SELECT * FROM dept_emp WHERE emp_no \u0026gt;= 10144 ; 인덱스가 dept_no 기준으로 생성되었기 때문에 dept_no 조건 없이 검색하면 인덱스를 효율적으로 사용할 수 없다.\n가용성과 효율성 판단 기본적으로 B-Tree 인덱스의 특성상 다음 조건에서는 사용할 수 없다. (작업 범위 결정 조거능로 사용할 수 없다.)\nNOT-EQUAL로 비교된 경우(\u0026lt;\u0026gt;,NOT IN, NOT BETWEEN, IS NOT NULL) LIKE '%??' 형태로 문자열 패턴이 비교된 경우 스토어드 함수나 다른 연산자로 인덱스 컬럼이변형된 후 비교된 경우 WHERE SUBSTRING(column, 1, 1) = 'X WHERE DAYOFMONTH(column) = 1 NOT-DETERMINISTIC 속성의 스토어드 함수가 비교 조건에 사용된 경우 WHERE column = deterministic_function() 데이터 타입이 서로 다른 비교(인덱스 컬럼의 타입을 변환해야 비교가 가능한 경우) 문자열 데이터 타입의 콜레이션이 다른 경우 다른 일반적은 DBMS에서는 NULL 값이 인덱스에 저장되지 않지만 MySQL 에서는 NULL 값도 인덱스에 저장된다. 다음과 같은 WHERE 조건도 작업 범위 결정 조건으로 인덱스를 사용한다. (WHERE column IS NULL)\n다중 컬럼 인덱스 1 INDEX ix_test ( column_1, .. column_n ) 작업 범위 결정 조건으로 인덱스를 사용하지 못하는 경우 column_1 컬럼에 대한 조건이 없는 경우 column_1 컬럼의 비교 조건이 위의 인덱스 사용 불가 조건 중 하나인 경우 작업 범위 결정 조건으로 사용하는 경우 column_1 ~ column_(i-1)컬럼까지 동등 비교 형태(=, IN) column_i 컬럼에 대해 다음 연산자 중 하나로 비교 동등 비교 크다 작다 형태 LIKE로 좌특 일치 패턴 적업 범위 결정 조건으로 인덱스를 사용하는 쿼리 패턴은 이 밖에도 상당히 많지만, 대표적인 것을 기억해 두면 좀 더 효율적인 쿼리를 쉽게 작성할 수 있다.\n","date":"2023-05-07T17:24:10+09:00","image":"https://codemario318.github.io/post/real-mysql/8/2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/8/2/","title":"8. 인덱스 (2) - B-Tree 인덱스"},{"content":"인덱스는 데이터베이스 쿼리의 성능을 언급하면서 뻬놓을 수 없는 부분이다. 각 인덱스의 특성과 차이는 상당히 중요하며, 물리 수준의 모델링을 할 때도 중요한 요소가 된다.\nMySQL 8.0 버전까지 업그레이드되어 오면서 다른 상용 RDBMS에서 제공하는 많은 기능을 지원하게 됐으며, 기존의 MyISAM 스토리지 엔진에서만 제공하던 전문 검색이나 위치 기반 검색 기능도 모두 InnoDB 스토리지 엔진에서 사용할 수 있게 개선되었다.\n하지만 아무리 MySQL 서버의 옵티마이저가 발전하고 성능이 개선됐다고 해도 여전히 관리자의 역할은 매우 중요하며, 인덱스에 대한 기본 지식은 지금도 앞으로도 개발자나 관리자에게 매우 중요하고 쿼리 튜닝의 기본이 될 것이다.\n디스크 읽기 방식 컴퓨터의 CPU나 메모리처럼 전기적 특성을 띤 장치의 성능은 짧은 시간동안 매우 빠른 속도로 발전했으나 디스크 같은 기계식 장치의 성능은 상당히 제한적으로 발전했다. 최근에는 자기 디스크 원판에 의존하는 하드 디스크보다 SSD 드라이브가 많이 활용되고 있지만, 여전히 데이터 저장 매체는 컴퓨터에서 가장 느린 부분이라는 사실에는 변함이 없다.\n데이터베이스나 쿼리 튜닝에 어느정도 지식을 사용자가 절감하고 있듯이 데이터베이스의 성능 튜닝은 어떻게 디스크 I/O를 줄이느냐가 관권일 때가 많다.\nHDD와 SSD 데이터베이스 서버에서는 항상 디스크 장치가 병목이 된다. SSD는 기존 하드 디스크 드라이브에서 데이터 저장용 원판을 제거하는 대신 플래시 메모리를 장착하고 있다. 따라서 원판을 물리적으로 회전시킬 필요가 없으므로 빠르게 데이터를 읽고 쓸 수 있다.\n플래시 메모리를 사용하는 SSD는 하드 디스크 드라이브보다 용량은 적지만 1000배 가량 빠른 성능을 보여주기 때문에 요즘은 DBMS용 서버는 SSD를 채택하고 있다.\n디스크의 헤더를 움직이지 않고 한 번에 많은 데이터를 읽는 순차 I/O 에서는 SSD가 압도적으로 빠르진 않지만 데이터베이스 서버에서는 대부분 랜덤 I/O가 발생하여 대부분의 상황에서 SSD 성능이 우수하다.\n랜덤 I/O와 순차 I/O 하드디스크 기준으로 랜덤 I/O는 그만큼 디스크 헤드를 자주 움직여야 한다는 뜻이며, SSD도 랜덤 I/O가 순차 I/O에 비해 스루풋이 떨어진다. 데이터베이스 대부분의 작업은 이러한 작은 데이터를 빈번히 읽고 쓰기 때문에 MySQL 서버에는 그룹 커밋이나 바이너리 로그 버퍼 도는 InnoDB 로그 버퍼 등의 기능으로 대응하고 있다.\n쿼리를 튜닝해서 랜덤 I/O를 순차 I/O로 바꿔 실행할 방법은 그다지 많지 않고, 일반적으로 쿼리 튜닝은 랜덤 I/O 자체를 줄여주는 것이 목적이다.\n랜덤 I/O를 줄인다는 것은 쿼리를 처리하는 데 꼭 필요한 데이터만 읽도록 쿼리를 개선하는 것을 의미\n인덱스 레인지 스캔은 데이터를 읽기 위해 주로 랜덤 I/O 사용하고 풀 테이블 스캔은 순차 I/O 사용한다. 따라서 큰 테이블의 레코드 대부분을 읽는 작업에서는 인덱스를 사용하지 않고 풀 테이블 스캔을 사용하도록 유도할 때도 있다. 이런 형태는 온라인 서비스에서는 거의 활용되지 않으며 주로 데이터웨어하우스나 통계 작업에서 많이 활용된다.\n인덱스란? 책의 맨 끝에 있는 색인으로 설명된다. 색인으로 찾는 페이지 번호는 데이터 파일에 저장된 레코드의 주소에 비유될 수 있다.\nDBMS도 데이터베이스 테이블의 모든 데이터를 검색해서 원하는 결과를 가져오려면 시간이 오래 걸리기 때문에, 컬럼의 값과 해당 레코드가 저장된 주소를 키와 값의 쌍으로 삼아 인덱스를 만들고, 빠른 검색을 위해 칼럼의 값을 기준으로 정렬해놓는다.\n자료구조로 비유하면, SortedList, ArrayList를 예로 들 수 있다.\nSortedList: DBMS의 인덱스와 같은 자료 구조. 저장된 값을 항상 정렬된 상태로 유지한다. 데이터가 저장될 때마다 항상 값을 정렬해야 하므로 저장하는 과정이 복잡하고 느리다. 이미 정렬돼 있어 원하는 값을 아주 빨리 찾아올 수 있다. INSERT, UPDATE는 느려지지만 SELECT는 매우 빠르다. ArrayList: 데이터 파일과 같은 자료구조. 저장된 순서대로 별도 정렬 없이 그대로 저장한다. 인덱스 추가시 주의사항 결론적으로 DBMS에서 인덱스는 데이터의 저장 성능을 희생하고 그 대신 데이터의 읽기 속도를 높이는 기능이다. 따라서 다음을 고려하여 인덱스 추가를 결정한다.\n데이터의 저장 속도를 어느정도까지 희생할 수 있는지 읽기 속도를 얼마나 더 빠르게 만들어야 하는지 SELECT쿼리 문장의 WHERE조건절에 사용되는 컬럼이라고 해서 전부 인덱스로 생성하면 데이터 저장 성능이 떨어지고 인덱스의 크기가 비대해져 오히려 역효과만 불러올 수 있다.\n인덱스 분류 인덱스는 데이터를 관리하는 방식(알고리즘)과 중복 값의 허용 여부 등에 따라 여러가지로 나눠볼 수 있다.\n역할 별 분류\n인덱스를 역할 별로 구분해 본다면 프라이머리키와 보조키(Secondary Index, Secondary Key)로 구분할 수 있다.\n프라이머리 키 레코드를 대표하는 컬럼의 값으로 만들어진 인덱스. 테이블에서 해당 레코드를 식별할 수 있는 기준값이 되기 때문에 이를 식별자라고도 부른다. NULL값과 중복값을 허용하지 않는다. 세컨더리 인덱스 프라이머리 키를 제외한 나머지 모든 인덱스. 유니크 인덱스는 프라이머리 키와 성격이 비슷하고 프라이머리 키를 대체해서 사용할 수 있어 대체키로도 불린다. 데이터 저장 방식(알고리즘) 분류\n대표적으로 B-Tree 인덱스와 Hash 인덱스로 구분할 수 있다. 최근에는 Fractal-Tree 인덱스나 로그 기반의 Merge-Tree 인덱스와 같은 알고리즘을 사용하는 DBMS도 개발되고 있다.\nB-Tree 알고리즘 가장 일반적으로 사용되는 인덱스 알고리즘으로, 상당히 오래전에 도입된 만큼 성숙해진 상태이다. 컬럼의 값을 변형하지 않고 원래의 값을 이용해 인덱싱하는 알고리즘이다. Hash 인덱스 알고리즘 컬럼의 값으로 해시값을 계산해서 인덱싱하는 알고리즘으로, 매우 빠른 검색을 지원한다. 값을 변형해서 인덱싱하므로 전방(Prefix)일치와 같이 값의 일부만 검색하거나 범위 검색할 때에는 해시 인덱스를 사용할 수 없다. 메모리 기반의 데이터베이스에서 많이 사용된다. 데이터 중복 허용 여부로 분류\n데이터의 중복 허용 여부로 분류하면 유니크 인덱스와 유니크하지 않은 인덱스로 구분할 수 있다.\n인덱스가 유니크한지 아닌지는 단순히 같은 값이 1개만 존재하는지 1개 이상 존재할 수 있는지를 의미하지만, 실제 DBMS의 쿼리를 실행할 때 유니크 인덱스에 대해 동등 조건으로 검색한다는 것은 1건의 레코드만 찾으면 더 찾지 않아도 된다는 것을 옵티마이저에게 알려주는 효과를 내기 때문에 옵티마이저에게는 상당히 중요하다.\n","date":"2023-05-07T16:24:10+09:00","image":"https://codemario318.github.io/post/real-mysql/8/1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/8/1/","title":"8. 인덱스 (1)"},{"content":"데이터 암호화는 MySQL 5.7 버전부터 지원되기 시작했으며, 처음에는 데이터 파일(테이블스페이스)에서만 암호화 기능이 제공 되었으나 MySQL 8.0으로 업그레이드 리두 로그나 언두 로그, 복제를 위한 바이너리 로그 등도 모두 암호화 기능을 지원하기 시작했다.\n데이터 암호화 여부는 보안 감사에서 필수적으로 언급되는 부분이며, 핀테크 서비스처럼 중요한 정보를 저장하는 서비스에서는 응용 프로그램에서 암호화한 데이터를 데이터베이스 서버에서 다시 암호화하는 이중 암호화 방법을 선택하기도 한다.\n응용 프로그램의 암호화는 주로 중요 정보를 가진 칼럼 단위로 암호화를 수행하며, 데이터베이스 수준에서는 테입르 단위로 암호화를 적용한다.\nMySQL 서버의 데이터 암호화 MySQL 서버의 암호화 기능은 데이터베이스 서버와 디스크 사이의 데이터 읽고 쓰기 지점에서 암호화 또는 복호화를 수행한다. 즉 MySQL 서버(InnoDB 스토리지 엔진)의 I/O 레이어에서만 데이터의 암호화 및 복호화 과정이 실행되므로디스크 입출력 이외의 부분에서는 암호화 처리가 전혀 필요치 않다.\nMySQL 서버가 사용자의 쿼리를 처리하는 과정에서 테이블의 데이터가 암호화돼 있는지 여부를 식별할 필요가 없으며, 암호화된 테이블도 그렇지 않은 테이블과 동일한 처리 과정을 거친다.\n데이터 암호화 기능이 활성화돼 있다고 하더라도 MySQL 내부와 사용자의 입장에서는 아무런 차이가 없기 때문에 이러한 암호화 방식을 가리켜 TDE(Transparent Data Encryption)이라고 한다.\n2단계 키 관리 MySQL 서버의 TDE에서 암호화 키는 키링(KeyRing) 플러그인에 의해 관리되며, MySQL 8.0 버전에서 지왼되는 키링 플러그인은 다음과 같다.\nkeyring_file File-Based 플러그인 keyring_encrypted_file Keyring 플러그인 keyring_okv KMIP 플러그인 keyring_aws Amazon Web Services keyring 플러그인 MySQL커뮤니티 에디션에서는 keyring_file 플러그인만 사용 가능하고, 나머지 플러그인은 모두 엔터프라이즈 에디션에서만 사용 가능하다.\n다양한 플러그인이 제공되지만 마스터 키를 관리하는 방법만 다를 뿐 MySQL 서버 내부적으로 작동하는 방식은 모두 동일하다. MySQL 서버의 키링 플러그인은 2단계(2-Tier) 키 관리 방식을 사용한다.\nMySQL 서버의 데이터 암호화는 마스터 키(master key)와 테이블스페이스 키(tablespace key)라는 두 가지 종류의 키를 가지고 있는데, 테이블스페이스 키는 프라이빗 키(private key)라고도 한다.\nHasicorp Vault 같은 외부 키 관리 솔루션(KMS, Key Management Service) 또는 디스크의 파일(Keyring_file 또는 keyring_encrypted_file 플러그인 사용시)에서 마스터 키를 가져오고, 암호화된 테이블이 생성될 때마다 해당 테이블을 위한 임의의 테이블스페이스 키를 발급한다. 마스터 키를 이용해 테이블 스페이스키를 암호화해서 각 테이블의 데이터 파일 헤더에 저장한다. 이렇게 생성된 테이블스프에스 키는 테이블이 삭제되지 않는 이상 절대 변경되지 않지만, 테이블 스페이스키는 절대 MySQL 서버 외부로 노출되지 않기 때문에 테이블스페이스 키를 주기적으로 변경하지 않아도 보안상 취약점이 되지는 않는다.\n하지만 마스터 키는 외부의 파일을 이용하기 때문에 노출될 가능성이 있어 주기적으로 변경해야 한다.\n1 ALTER INSTANCE ROTATE INNODB MASTER KEY; 마스터키를 변경하면 MySQL서버는 기존의 마스터 키를 이용해 각 테이블의 테이블스페이스 키를 복호화한 다음 새로운 마스터 키로 다시 암호화화한다. 마스터 키가 변경되는 동안 MySQL 서버의 테이블스페이스 키 자체와 데이터 파일의 데이터는 전혀 변경되지 않는다.\nMySQL 서버에서 이렇게 2단계 암호화 방식을 사용하는 이유는 암호화 키 변경으로 인한 과도한 시스템 부하를 피하기 위해서다.\n테이블스페이스 키가 변경된다면 MySQL 서버는 데이터 파일의 모든 데이터를 다시 복호화했다가 다시 암호화해야 하므로, 키를 변경할 때마다 매우 큰 작업을 수행해야 하며, 이에따라 사용자 쿼리를 처리하는 데도 상당한 영향을 미치게 된다.\nMySQL 서버의 TDE에서 지원되는 암호화 알고리즘은 AES 256bit이며, 이외의 알고리즘은 지원되지 않는다.\n테이블스페이스 키는 AES-256(Electronic CodeBook) 알고리즘을 이용해 암호화 되고, 실제 데이터 파일은 AES-256 CBC(Cipher Block Chaining) 알고리즘을 이용해 암호화 된다. 암호화 성능 MySQL 서버의 암호화는 TDE 방식이기 때문에 디스크로부터 한 번 읽은 데이터 페이지는 복호화되어 InnoDB 버퍼풀에 적재된다. 따라서 데이터 페이지가 한 번 메모리에 적재되면 암호화되지 않은 테이블과 동일한 성능을 보인다.\n쿼리가 InnoDB 버퍼풀에 존재하지 않는 데이터 페이지를 읽어야 하는 경우에는 복호화 과정을 거치기 때문에 복호화 시간동안 쿼리 처리가 지연될 수 있다. 암호화된 테이블이 변경되면 다시 디스크로 동기화될 때 암호화돼야 하기 때문에 디스크에 저장할 때도 추가로 시간이 더 걸린다. 데이터 페이지 저장은 사용자의 쿼리를 처리하는 스레드가 아는 MySQL 서버으 백그라운드 스레드가 수행하기 때문에 실제 사용자 쿼리가 지연되는 것은 아니다. UPDATE, DELETE 명령 또한 변경하고자 하는 레코드를 InnoDB 버퍼풀로 읽어와야 하기 대문에 새롭게 디스크에서 읽어야 하는 데이터 페이지의 개수에 따라서 복호화 지연이 발생할 수 있다. AES 암호화 알고리즘은 암호화하고자 하는 평문의 길이가 짧은 경우 암호화 키의 크기에 따라 암호화된 결과의 용량이 더 커질수도 있지만, 이미 데이터 페이지는 암호화 키보다 훨씬 크기 때문에 암호화 결과가 평문의 결과와 동일한 크기의 암호문을 반환한다. 따라서 TDE를 적용한다고 해도 데이터 파일의 크기는 암호화되지 않은 테입르과 동일한 크기를 가진다. 즉 암호화한다고 해서 InnoDB 버퍼풀의 효율이 달라지거나 메모리 사용 효율이 떨어지는 현상은 발생하지 않는다.\n같은 테이블에 대해 암호화와 압축이 동시에 적용되면 MySQL 서버는 압축을 먼저 실행하고 암호화를 적용한다.\n일반적으로 암호화된 결과문은 아주 랜덤한 바이트의 배열을 가지게 되는데, 이는 암축률을 상당히 떨어뜨린다. 암호화된 테이블의 데이터 페이지는 복호화된 상태로 InnoDB 버퍼풀에 저장되지만, 압축된 데이터 페이지는 압축 또는 압축 해제의 모든 상태로 InnoDB 버퍼풀에 전재할 수 있다. 암호화와 복제 MySQL 서버의 복제에서 레플리카 서버는 소스 서버의 모든 사용자 데이터를 동기화할 때 TDE를 이용한 암호화 사용 시 마스터 키와 테이블스페이스 키는 제외된다.\nMySQL 서버에서 기본적으로 모든 노드는 각자의 마스터 키를 할당해야 한다. 데이터베이스 서버의 로컬 디렉터리에 마스터 키를 관리하는 경우에는 소스 서버와 레플리카 서버는 서로 다른 마스터 키를 갖도록 설정해야 한다. 마스터 키 자체가 레플리카로 복제되지 않기 때문에 테이블스페이스 키 또한 레플리카로 복제되지 않는다.\n결국 소스 서버와 레플리카 서버는 서로 각자의 마스터 키와 테이블 스페이스 키를 관리하기 때문에 복제 멤버들의 데이터 파일은 암호화 되기 전의 값이 동일하더라도 실제 암호화된 데이터가 저장된 데이터 파일의 내용은 완전히 달라진다.\n복제 소스 서버의 마스터 키를 변경할 때는 ALERT INSTANCE ROTATE INNODB MASTER KEY 명령을 실행하는데, 이때 명령 자체는 레플리카 서버로 복제되지만 실제 소스 서버의 마스터 키 자체가 레플리카 서버로 전다로디는 것은 아니다. 그래서 마스터 키 로테이션을 실행하면 소스 서버와 레플리카 서버가 각각 서로 다른 마스터 키를 새로 발급받는다.\nMySQL 서버의 백업에서 TDE의 키링(Keyr Ring)파일을 백업하지 않는 경우가 있는데, 이 경우 키링 파일을 찾지 못하면 데이터를 복구할 수 없게 된다. 키링 파일을 데이터 백업과 별도로 백업한다면 마스터 키 로테이션 명령으로 TDE의 마스터 키가 엊네 변경됐는지까지 기억하고 있어야 한다.\nKeyring_file 플러그인 설치 MySQL 서버의 데이터 암호화 기능인 TDE의 암호화 키 관리는 플러그인 방식을 제공된다.\nKeyring_file플러그인은 테이블스페이스 키를 암호화하기 위한 마스터 키를 디스크의 파일로 관리하는데, 이때 마스터 키는 평문으로 디스크에 저장된다. 즉 마스터키가 저장된 파일이 외부에 노출된다면 데이터 암호화는 무용지물이 된다.\nkeyring_file플러그인은 마스터 키를 암호화하지 않은 상태의 평문으로 로컬 디스크에 저장하기 때문에 보안 요건을 충족시켜주지 않을 수 있다. 그럼에도 keyring_file 플러그인을 사용하고자 한다면 MySQL 서버가 시작될 때만 키링 파일을 다른 서버로부터 다운로드해서 로컬 디스크에 저장한 후 MySQL 서버를 시작하는 방법을 고려할 수 있다. MySQL 서버가 시작되면 마스터 키를 메모리에 캐시하기 때문에 로컬 디스크의 키링 파일을 삭제해도 문제는 전혀 없다. Percona Server는 HashiCorp Vault를 연동하는 키 관리 플러그인을 오픈소스로 제공하므로 함께 검토해보는 것을 권장한다.\nTDE 플러그인의 경우 MySQL 서버가 시작되는 단계에서도 가장 빨리 초기화돼야 한다.\n1 2 early-plugin-load = keyring_file.so keyring_file_data = /very/secure/directory/tde_master.key 그래서 다음과 같이 MySQL 서버의 설정 파일(my.cnf)에서 early-plugin-load 시스템 변수에 keyring_file 플러그인을 위한 라이브러리를 명시하면 된다. 그리고 keyring_file 플러그인이 마스터 키를 저장할 키링 파일의 경로를 keyring_file_data 설정에 명시하면 된다.\n설정 파일이 준비되면 MySQL 서버 시작시 자동으로 keyring_file플러그인이 초기화된다.\n1 SHOW PLUGINS; 초기화와 동시에 지정한 경로에 빈 파일을 생성한다. 데이터 암호화 기능을 사용하는 테이블을 생성하거나 마스터 로테이션을 실행하면 키링 파일의 마스터 키가 초기화된다.\n테이블 암호화 키링 플러그인은 마스터 키를 생성하고 관리하는 부분까지만 담당하기 때문에 어떤 키링 플러그인을 사용하든 관계 없이 암호화된 테이블을 생성하고 활용하는 방법은 모두 동일하다.\n테이블 생성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 CREATE TABLE tab_encrypted ( id INT, data VARCHAR(100), PRIMARY KEY (id) ) ENCRYPTION=\u0026#39;Y\u0026#39;; INSERT INTO tab_encrypted VALUES (1, \u0026#39;test\u0026#39;); SELECT * FROM tab_encrypted; /** +--+----+ |id|data| +--+----+ | 1|test| +--+----+ */ MySQL 서버에서 암호화된 테이블만 검색할 때는 information_schema의 TABLES 뷰를 이용한다.\n1 2 3 4 5 SELECT table_schema ,table_name ,create_options FROM information_schema.tables WHERE table_name=\u0026#39;tab_encrypted\u0026#39;; 테이블을 생성할 때마다 옵션을 설정하면 실수로 암호화 적용을 잊어버릴 수도 있으므로 MySQL 서버의 모든 테이블에 암호화를 적용하고자 한다면 default_table_encryption 시스템 변수를 ON으로 설정하면 ENCRYPTION 옵션을 별도로 설정하지 않아도 암호화된 테이블로 생성된다.\n응용 프로그램 암호화와의 비교 응용 프로그램에서 직접 암호화해서 MySQL 서버에 저장하는 경우도 있는데, 이 경우 저장되는 칼럼의 값이 이미 암호화된 것인지 여부를 MySQL 서버는 인지하지 못한다. 그래서 응용 프로그램에서 암호화된 컬럼은 인덱스를 생성하더라도 인덱스의 기능을 100% 활용할 수 없다.\n응용 프로그램에서 직접 암호화하지 않고 MySQL 서버의 암호화 기능(TDE)을 사용한다면 MySQL 서버는 인덱스 관련된 작업을 모두 처리한 후 최종 디스크에 데이터 페이지를 저장할 때만 암호화 하기 때문에 제약이 줄어든다.\n응용 프로그램에서의 암호화 기능은 서비스의 요건과 성능을 고려해서 선택해야 하고, MySQL 서버의 암호화 기능과 혼합해서 사용한다면 더 안전한 서비스를 구축할 수 있을 것이다.\n테이블스페이스 이동 MySQL 서버의 데이터베이스 관리자라면 테이블스페이스만 이동하는 기능을 자주 사용하게 되는데, 테이블을 다른 서버로 복사해야 하는 경우 또는 특정 테이블의 데이터 파일만 백업했다가 복구하는 경우라면 테이블스페이스 이동(Export \u0026amp; Import) 기능이 레코드르 덤프했다가 복구하는 방식보다 훨씬 효율적이고 빠르다.\n그런데 TDE가 적용되어 암호화된 테이블의 원본 MySQL 서버와 목적지 MySQL 서버의 암호화 키(마스터 키)가 다르기 때문에 FLUSH TABLES 명령으로 테입르스페이스를 익스포트 할 수 있다.\n1 FLUSH TABLES souce_table FOR EXPORT; MySQL 서버는 source_table의 저장되지 않은 변경을 모드 디스크로 기록하고, 더이상 접근할 수 없게 잠금을 건다. 그와 동시에 source_table의 구조를 source_table.cfg 파일로 기록한다. 암호화된 테이블의 테이블스페이스 키를 기존 마스터 키로 복호화한 후, 임시로 발급한 마스터 키를 이용해 다시 암호화해서 데이터 파일의 헤더 부분에 저장한다. 따라서 암호화된 테이블의 경우 테이블스페이스 이동 기능을 사용할 때는 반드시 데이터 파일과 임시 마스터 키가 저장된 *.cfp 파일을 함께 복사해야 한다. *.cfg파일은 단순히 테이블의 구조만 가지고 있기 때문에 파일이 없어져도 경고만 발생하지만, *.cfp 파일이 없어지면 복구가 불가능해진다.\n언두 로그 및 리두 로그 암호화 테이블의 암호화를 적용하더라도 디스크로 저장되는 데이터만 암호화되고 MySQL 서버의 메모리에 존재하는 데이터는 복호화된 평문으로 관리되며, 이 평문 데이터가 테이블의 데이터 파일 이외의 디스크 파일로 기록되는 경우에는 여전히 평문으로 저장된다.\n그래서 테이블 암호화를 적용해도 리두 로그나 언두 로그, 그리고 복제를 위한 바이너리 로그에는 평문으로 저장되는 것이다. MySQL 8.0.16 버전 부터는 innodb_undo_log_encrypt, innodb_redo_log_encrypt 시스템 변수를 이용해 언두 로그와 리두 로그를를 암호화 된 상태로 저장할 수 있게 개선되었다.\nMySQL 서버는 리두 로그나 언두 로그를 평문으로 저장하다가 암호화가 활성화되면 그때부터 생성되는 리두 로그와 언두 로그만 암호화해서 저장한다. 반대로 리두 로그와 언두 로그가 암호화되는 상태에서 암호화를 비활성화하면 그때부터 저장되는 로그만 평문으로 저장한다.\n따라서 리두 로그와 언두 로그는 암호화를 활성화 했다가 비활성화 한다고 해서 즉시 암호화에 사용된 키가 불필요해지는 것이 아니다. 특히 언두 로그의 경우 암호화를 비활성화 한다고 하더라도 새로 생성되는 언두 로그는 평문으로 저장되겠지만 기존 언두 로그는 여전히 암호화된 상태로 남아있어 상황에 따라 계속해서 암호화키가 필요할 수 있다.\n바이너리 로그 암호화 테이블 암호화가 적용돼도 바이너리 로그와 릴레이 로그 파일 또한 리두 로그나 언두 로그처럼 평문을 저장한다. 일반적으로 언두 로그와 리두 로그는 기맂 않은 시간동안의 데이터만 가지기 때문에 보안에 민감하지 않을 수 있지만 바이너리 로그 파일의 암호화는 상황에 따라 중요도가 높아질 수 있다.\n바이너리 로그는 의도적으로 상당히 긴 시간동안 보관할 수도 있다. 증분 백업(Incremental Backup)을 위해 바이너리 로그를 보관하기도 한다. 바이너리 로그와 릴레이 로그 파일 암호화 기능은 디스크에 저장된 로그 파일에 대한 암호화만 담당하고, MySQL 서버의 메모리 내부 또는 소스 서버와 레플리카 서버 간의 네트워크 구간에서 로그 데이터를 암호화하지는 않는다. 복제 멤버 간의 네트워크 구간에서도 바이너리 로그를 암호화하고자 한다면 MySQL 복제를 위한 계정이 SSL을 사용하도록 설정한다.\n바이너리 로그 암호화 키 관리 바이너리 로그와 릴레이 로그 파일 데이터의 암호화를 위해서도 MySQL 서버는 2단계 키 관리 방식을 사용한다.\n바이너리 로그와릴레이 로그 파일의 데이터는 파일 키(File Key)로 암호홰해서 디스크로 저장하고, 파일 키는 \u0026ldquo;바이너리 로그 암호화 키\u0026quot;로 암호화 해서 각 바이너리 로그와 릴레이 로그 파일의 헤더에 저장된다.\n즉 \u0026ldquo;바이너리 로그 암호화 키\u0026quot;는 테이블 암호화의 마스터 키와 동일한 역할을 하며, 파일 키는 바이너리 로그와 릴레이 로그 파일 단위로 자동으로 생성되어 해당 로그 파일의 데이터 암호화에만 사용된다.\n바이너리 로그 암호화 키 변경 1 ALTER INSTANCE ROTATE BINLOG MASTER KEY; 바이너리 로그 암호화 키가 변경되면 다음 과정을 거친다.\n증가된 시퀀스 번호와 함께 새로운 바이너리 로그 암호화 키 발금 후 키링 파일에 저장 바이너리 로그 파일과 릴레이 로그 파일 스위치(새로운 로그 파일로 로테이션) 새로 생성되는 바이너리 로그와 릴레이 로그 파일의 암호화를 위해 파일 키를 생성하고, 파일 키는 바이너리 로그 파일 키로 암호화해서 각 로그 파일에 저장 기조 ㄴ바이너리 로그와 릴레이 로그 파일의 파일 키를 읽어서 새로운 바이너리 로그 파일 키로 암호화해서 다시 저장(암호화되지 않은 로그 파일은 무시) 모든 바이너리 로그와 릴레이 로그 파일리 새로운 바이너리 로그 암호화 키로 다시 암호화됐다면 기조 ㄴ바이너리 로그 암호화 키를 키링 파일에서 제거 4번 과정은 상당히 시간이 걸릴 수 있는데, 이를 위해 키링 파일에서 \u0026ldquo;바이너리 로그 암호화 키\u0026quot;는 내부적으로 버전 관리가 이뤄진다.\nmysqlbinlog 도구 활용 MySQL 서버에서는 트랜잭션의 내용을 추적하거나 백업 복구를 위해 암호화된 바이너리 로그를 평문으로 복호화할 일이 자주 발생한다. 하지만 한 번 바이너리 로그 파일이 암호화되면 바이너리 로그 암호화 키가 없으면 복호화할 수 없다.\n그런데 바이너리 로그 암호화 키는 MySQL 서버만 가지고 있어서 복호화가 불가능하다. mysqlbinlog 도구를 이용해 암호화된 바이너리 로그 파일을 직접 열어볼 수 없다는 에러 메시지를 출력하게 된다.\n바이너리 로그 암호화 키는 그 바이너리 로그나 릴레이 로그 파일을 생성한 MySQL 서버만 가지고 있기 때문에 MySQL 서버와 관게없이 mysqlbinlog 도구만으로는 복호화할 방법이 없다. 그래서 예전처럼 다른 서버로 복사하거나 바이너리 로그 파일을 백업하는 것은 소용없어졌다.\n바이너리 로그 파일의 내용을 확인할 방법은 MySQL 서버를 통해 가져오는 방법이 유일하다.\n1 mysqlbinlog --read-from-remote-server -uroot -p -vvv mysql-bin.000011 ","date":"2023-05-01T16:24:10+09:00","image":"https://codemario318.github.io/post/real-mysql/7/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/7/","title":"7. 데이터 암호화"},{"content":"MySQL 서버에서 디스크에 저장된 데이터 파일의 크기는 일반저긍로 쿼리의 처리 성능과도 직결되지만 백업 및 복구 시간과도 밀접하게 연결된다.\n디스크의 데이터 파일이 크면 클수록 쿼리를 처리하기 위해서 더 많은 데이터 페이지를 InnoDB 버퍼풀로 읽어야 할 수 있다. 새로운 페이지가 버퍼풀로 적재되기 때문에 그만큼 더티 페이지가 더 자주 디스크로 기록돼야 한다. 데이터 파일이 크면 클수록 백업 시간이 오래 걸리며, 복구하는 데도 그만큼의 시간이 걸린다. 그만큼 저장 공간이 필요하기 때문에 비용 문제도 있을 수 있다. 많은 DBMS가 이러한 문제점을 해결하기 위해 데이터 압축 알고리즘을 제공하면, MySQL 서버에서 사용 가능한 압축 방식은 크게 테이블 압축과 페이지 압축의 두 가지 종류로 구분할 수 있다.\n페이지 압축 페이지 압축은 MySQL 서버가 디스크에 저장하는 시점에 데이터 페이지가 압축되어 저장되고, 반대로 MySQL 서버가 디스크에서 데이터 페이지를 읽어올 때 압축이 해제되어, 버퍼풀에 데이터 페이지가 적재되면 InnoDB 스토리지 엔진은 압축이 해제된 상태로만 데이터 페이지를 관리한다. 이에 따라 서버의 내부 코드에서는 압축 여부와 관계없이 투명(Transparent)하게 작동하여 Transparent Page Compression로 불리기도 한다.\n16KB 데이터 페이지를 압축한 결과가 용량이 얼마나 될지 예측이 불가능한데 적어도 하나의 테이블은 동일한 크기의 페이지(블록)로 통일돼야 한다. 따라서 페이지 압축 기능은 운영체제별로 특정 버전의 파일 시스템에서만 지원되는 펀치홀(Punch hole)이라는 방식을 사용한다.\n펀치홀이란?\n운영체제에서 제공하는 파일 시스템 인터페이스 일부로, 파일 내용에서 일부 데이터 블록을 삭제하여 디스크 공간을 확보하는 기능이다.\n이전에는 파일 전체를 복사하고 일부분을 수정하는 드으이 방법으로 파일을 수정해야 했으나 시간과 디스크 공간을 많이 소모하게 되어 펀치홀이 개발되었다.\n펀치홀은 일부 운영체제에서만 지원되는 기능이며(리눅스), 대용량 파일 시스템에 사용되어 성능을 향상시키는데 도움을 준다.\n16KB 페이지를 압축(압축 결과를 7KB로 가정) MySQL 서버는 디스크에 압축된 결과 7KB를 기록(이때 MySQL 서버는 압축 데이터 7KB에 9KB 빈 데이터를 기록) 디스크에 데이터를 기록한 후, 7KB 이후의 공간 9KB에 대해 펀치 홀(Punch-hole) 생성 파일 시스템은 7KB만 남기고 나머지 디스크의 9KB 공간은 다시 운영체제로 반납 운영체제(파일 시스템)의 블록 사이즈가 512바이트인 경우, MySQL 서버는 특정 테이블에 대해 16KB 크기의 페이지를 유지하면서도 압축된 다양한 크기의 데이터 페이지를 디스크에 저장하고 압축된 만큼의 공간을 절약할 수 있다.\n문제점\n펀치홀 기능은 운영체제뿐만 아니라 하드웨어 자체에서도 지원을 해야 사용 가능하다. 아직 파일 시스템 관련 명령어(유틸리티)가 펀치홀을 지원하지 못한다. MySQL 서버의 데이터 파일은 해당 서버에만 머무는 것이 아니라 백업했다가 복구하는 과정에서 데이터 파일 복사 과정이 실행되고, 그 외에도 많은 파일 관련 유틸리티들을 활용한다. 이러한 이유로 실제 페이지 압축은 많이 사용되지 않는 상태이다.\n1 2 3 4 5 6 /* 테이블 생성시 */ CREATE TABLE t1 (c1 INT) COMPRESSION=\u0026#34;zlib\u0026#34;; /* 테이블 변경 시 */ ALTER TABLE t1 COMPRESSION=\u0026#34;zlib\u0026#34;; OPTIMAIZE TABLE t1; 테이블 압축 테이블 압축은 운영체제나 하드웨어에 대한 제약 없이 사용할 수 있기 때문에 일반적으로 더 활용도가 높은편이다. 테이블 압축은 우선 디스크의 데이터 파일의 크기를 줄일 수 있기 때문에 그만큼의 이득은 있지만, 내부적인 처리 과정과 버퍼풀에서 처리 방식으로 인해 몇가지 단점이 존재한다.\n버퍼풀 공간 활용률이 낮음 쿼리 처리 성능이 낮음 빈번한 데이터 변경시 압축률 떨어짐 압축 테이블 생성 테이블 압축을 사용하기 위해 압축을 사용하려는 테이블이 별도의 테이블 스페이스를 사용해야 한다.\n이를 위해서는 innodb_file_per_table 시스템 변수가 ON으로 설정된 상태에서 테이블이 생성돼야 한다. 테이블 압축을 사용하는 테이블들은 테이블을 생성할 때 ROW_FORMAT=COMPRESSED 옵션을 명시해야 한다. KEY_BLOCK_SIZE 옵션을 이용해 압축된 페이지의 타깃 크기(목표 크기, 2n(n \u0026gt;= 2))를 명시해야 한다. InnoDB 스토리지 엔진의 페이지 크기가(innodb_page_size)가 16KB 라면 KEY_BLOCK_SIZE는 4KB 또는 8KB만 설정할 수 있다. 페이지 크기(innodb_page_size)가 32KB or 64KB인 경우에는 테이블 압축을 적용할 수 없다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SET GLOBAL innodb_file_per_table=ON; /** ROW_FORMAT 옵션과 KEY_BLOCK_SIZE 옵션을 모두 명시 **/ CREATE TABLE compressed_table ( c1 INT PRIMARY KEY ) ROW_FORMAT=COMPRESSED KEY_BLOCK=8; /** KEY_BLOCK_SIZE 옵션을 모두 명시 ROW_FORMAT 옵션이 생략되면 자동으로 ROW_FORMAT=COMPRESSED 옵션이 추가되어 생성 **/ CREATE TABLE compressed_table ( c1 INT PRIMARY KEY ) ROW_FORMAT=COMPRESSED KEY_BLOCK=8; innodb_file_per_table 시스템 변수가 0 인 상태에서 제너럴 테이블스페이스(General Tablespace)에 생성되는 테이블도 테이블 압축을 사용할 수 있으나, 제너럴 테이블스페이스의 FILE_BLOCK_SIZE에 의해 압축을 사용하지 못할 수 있다.\n테이블 압축 동작 방식 현재 InnoDB스토리지 엔진의 데이터 페이지 크기가 16KB, KEY_BLOCk_SIZE가 8로 설정되었다면,\n16KB의 데이터 페이지를 압축 압축된 결과가 8KB 이하이면 그대로 디스크에 저장 압축된 결과가 8KB를 초과하면 원본 페이지를 스플릿(split)해서 2개의 페이지에 8KB씩 저장 나뉜 페이지 각각에 대해 \u0026ldquo;1\u0026rdquo; 단계를 반복 실행 테이블 압축 방식에서 가장 중요한 것은 원본 데이터 페이지의 압축 결과가 목표 크기(KEY_BLOCK_SIZE)보다 작거나 같을 때까지 반복해서 페이지를 스플릿하는 것이다.\n따라서 목표 크기가 잘못 설정되면 MySQL 서버의 처리 성능이 급격히 떨어질 수 있다.\nKEY_BLOCK_SIZE 결정 테이블 압축에서 가장 중요한 부분은 압축된 결과가 어느 정도가 될지 예측해서 KEY_BLOCK_SIZE를 결정하는 것이다.\n따라서 테이블 압축을 적용하기 전에 먼저 KEY_BLOCK_SIZE를 4KB 또는 8KB로 테이블을 생성하여 샘플 데이터를 저장해 보고 적절한지 판단하는 것이 좋다.\n이때 샘플 데이터는 많으면 많을수록 더 정확한 테스트가 가능한데, 최소한 테이블 데이터 페이지가 10개 정도는 생성되도록 테스트 데이터를 INSERT해보는 것이 좋다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 USE employees; CREATE TABLE employees_comp4k ( emp_no int NOT NULL, /** ... */ first_name varchar(14) NOT NULL, hire_date date NOT NULL, PRIMARY KEY (emp_no), KEY ix_firstname (first_name), KEY ix_hiredate (hire_date) ) ROW_FORMAT=COMRESSED KEY_BLOCK_SIZE=4; /** 테스트 실행 전 innodb_cmp_per_index_enabled 시스템 변수를 ON으로 변경해야 인덱습려로 압축 실행 횟수와 성공 횟수가 기록된다. */ SET GLOBAL innodb_cmp_per_index_enabled=ON; INSERT INTO employees_comp4k SELECT * FROM employees; SELECT table_name ,index_name ,compress_ops ,compress_ops_ok (compress_ops - compress_ops_ok) / compress_ops * 100 AS compression_failure_pct FROM information_schema.INNODB_CMP_PER_INDEX; 결과\n\u0026hellip; index_name \u0026hellip; compression_failure_pct PRIMARY 27.6737 ix_firstname 8.0168 ix_hiredate 13.4561 실패율이 높다는 의미는 압축 결과가 4KB를 초과하여 데이터 페이지를 스플릿해서 다시 압축을 많이 했다는 뜻이다. 일반적으로 압축 실패율은 3~5%미만으로 유지하는 것이 좋으며 이에 맞추어 KEY_BLOCk_SIZE를 적절히 조절해야 한다.\nKEY_BLOCk_SIZE를 크게 설정하였는데도 압출 실패율이 높게 나타난다면, InnoDB 버퍼풀에서 디스크로 기록되기 전에 압축하는 과정에 꽤 오랜 시간이 걸릴 것으로 예측 가능하다. 성능에 민감한 서비스라면 압축을 적용하지 않는 것이 좋을 수 있다.\n압축 실패율이 높다고 해서 무조건 압축을 사용하지 말아야 한다는 것을 의미하지는 않는다.\nINSERT만 되는 로그 테이블은 실패율이 높다고 하더라도 데이터 파일의 크기가 큰 폭으로 줄어든다면 큰 손해는 아닐 수 있다. 반대로 실패율이 낮다고 해서 무조건 압축을 적용하는 것도 좋지 않을 수 있다.\n알고리즘 실행에 적지 않은 CPU 자원을 소비하므로, 테이블의 데이터가 빈번하게 조회되고 변경되는 경우 압축을 적용하지 않는 것이 좋을 수 있다. 압축된 페이지의 버퍼풀 적재 및 사용 InnoDB 스토리지 엔진은 압축된 테이블의 데이터 페이지를 버퍼풀에 적재하면 압축된 상태와 압축이 해제된 생태 2개 버전을 관리한다. 따라서 InnoDB 스토리지 엔진은 디스크에서 읽은 상태 그대로의 데이터 페이지 목록을 관리하는 LRU 리스트와 압축된 페이지들의 압축 해제 버전인 Unzip_LRU 리스트를 별도로 관리하게 된다.\n결국 InnoDB 스토리지 엔진은 압축된 테이블에 대해서는 버퍼풀의 공간을 이주응로 사용함으로써 메모리를 낭비하게 된다. 압축된 페이지에서 데이터를 읽거나 변경하기 위해서 압축을 해제해야하므로 CPU를 상대적으로 많이 소모한다. 이러한 단점을 보완하기 위해 Unzip_LRU 리스트를 별도로 관리하고 있다가 MySQL 서버로 유입되는 요청 패턴에 따라 적절히 다음과 같은 처리를 수행한다.\nInnoDB 버퍼풀 공간이 필요한 경우에는 LRU 리스트에서 원본 데이터 페이지(압축)는 유지하고, Unzip_LRU 리스트에서 압축 해제된 버전은 제거해서 버퍼풀의 공간을 확보한다. 압축된 데이터 페이지가 자주 사용되는 경우에는 Unzip_LRU 리스트에 압축 해제된 페이지를 계속 유지하면서 압축 및 압축 해제 작업을 최소화한다. 압축된 데이터 페이지가 사용되지 않아서 LRU 리스트에서 제거되는 경우에는 Unzip_LRU 리스트에서도 함께 제거된다. InnoDB 스토리지 엔진은 버퍼풀에서 압축 해제된 버전의 데이터 페이지를 적절한 수준으로 유지하기 위해 다음과 같은 어댑티브 알고리즘을 사용한다.\nCPU 사용량이 높은 서버에서는 가능하면 압축과 압축 헤제를 피하기 위해 Unzip_LRU의 비율을 높에서 유지한다. Disk IO 사용량이 높은 서버에서는 가능하면 Unzip_LRU 리스트의 비율을 낮춰 InnoDB 버퍼풀 공간을 더 확보하도록 작동한다. 테이블 압축 관련 설정 테입르 압축을 사용할 때 연관된 시스템 변수가 몇가지 있는데, 모두 페이지의 압축 실패율을 낮추기 위해 필요한 튜닝 포인트를 제공한다.\ninnodb_cmp_index_enable\n테이블 압축이 사용된 테이블의 모든 인덱스별로 압축 성공 및 압축 실행 횟수를 수집하도록 설정한다. 비활성화시 테이블 단위의 압축 성공 및 실행 횟수만 수집 테이블 단위 수집된 정보: infoamtion_schema.INNODB_CMP테이블에 기록 인덱스 단위 수집된 정보: infoamtion_schema.INNODB_CMP_PER_INDEX테이블에 기록 innodb_compression_level\nInnoDB의 데이터 압축은 zlib알고리즘만 지원하는데, 시스템 변수를 이용해 압축률을 설정할 수 있다(0~9). 값이 커질수록 느려지고, 작아진다. 기본값은 6 innodb_compression_failure_threshold_pct\n테이블 단위로 압축 실패율이 시스템 설정값 보다 커지면 압축을 실행하기 전 원본 데이터 페이지 끝에 의도적으로 일정 크기의 빈 공간을 추가한다. 추가된 빈 공간은 압축률을 높여서 압축 결과가 KEY_BLOCK_SIZE보다 작아지게 만드는 효과를 낸다. 추가하는 빈 공간을 패딩이라고 하며, 패딩 공간은 실패율이 높아질수록 계속 증가된 크기를 가진다. innodb_log_compressed_pages\nMySQL 서버가 비정상적으로 종료됐다가 다시 시작되는 경우 압축 알고리즘의 버전 차이가 있더라도 복구 과정이 실패하지 않도록 InnoDB 스토리지 엔진은 압축된 데이터 페이지를 그대로 리두 로그에 기록한다. 압축 알고리즘을 덥그레이드 할 대 도움이 되지만, 데이터 페이지를 통째로 로그에 저장하는 것은 리두 로그의 증가량에 상당한 영향을 미칠수도 있다. 압축을 적용한 후 리두 로그 용량이 매우 빠르게 증가한다건아 버퍼풀로부터 더티 페이지가 한꺼번에 많이 기록되는 패턴으로 바뀌었다면, 해당 변수를 OFF로 설정하여 모니터링 해보는 것이 좋다. 기본값은 ON, 가능하면 기본값 상태를 유지하자. ","date":"2023-05-01T14:12:10+09:00","image":"https://codemario318.github.io/post/real-mysql/6/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/6/","title":"6. 데이터 압축"},{"content":"데이터베이스의 데이터를 영속화하기 위해 볼륨 마운트를 사용할 수 있으며, 볼륨 마운트는 어플리케이션의 데이터를 영구적으로 저장해야 할 때 좋은 선택이 될 수 있다.\n바인드 마운트는 호스트 파일 시스템에서 디렉토리를 컨테이너로 공유하는 다른 유형의 마운트이다. 어플리케이션을 개발할 때 바인드 마운트를 사용하여 소스 코드를 컨테이너로 마운트 할 수 있다. 파일 시스템 변경 사항을 감시하고 그에 따라 대응하는 프로세스를 컨테이너에서 실행할 수 있기 때문에, 컨테이너는 파일을 저장할 때마다 즉시 코드 변경 사항을 인식한다.\n따라서 바인드 마운트와 파일 변경 상항을 감지하고 이에 따라 원하는 동작을 수행할 수 있으며, 이번 장에서는 자동으로 애플리케이션을 다시 시작하는 도구인 nodemon을 활용해본다.\nQuick volume type comparisons Named volumes Bind mounts 호스트 위치 도커가 선택함 사용자가 결정 마운트 예시(\u0026ndash;mount) type=volume,src=my-volume,target=/usr/local/data type=bind,src=/path/to/data,target=/usr/local/data Populates new volume with container contents Yes No Supports Volume Drivers Yes No ","date":"2023-04-24T16:46:25+09:00","image":"https://codemario318.github.io/post/docker/5/docker_cover_hue12353db563619e41ee3a11307d3cf25_62602_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/docker/5/","title":"5. Use bind mounts"},{"content":"컨테이너의 파일 시스템 컨테이너가 실행되면, 파일시스템은 여러 레이어들을 사용한다. 각각의 컨테이너는 파일을 생성, 업데이트, 삭제하기 위한 자신만의 \u0026ldquo;스크래치 공간\u0026quot;을 가지는데, 다른 컨테이너에서 같은 이미지를 사용한다고 해도 어떤 변경 사항에 대해서도 다른 컨테이너에서 활용할 수 없다.\nScratch image?\n빈 베이스 이미지로, 최소한의 컨테이너 이미지를 만들기 위해 사용된다. scratch를 기본 이미지로 사용하면 컨테이너에는 파일 시스템 내용이 없으므로 앱이 필요로 하는 모든 파일을 이미지의 일부로 포함해야 한다.\nscratch space는 사용 중인 이미지에 기반하여 각 컨테이너마다 생성되는 파일 시스템이다.\n컨테이너 볼륨 컨테이너는 파일을 생성, 업데이트 및 삭제할 수 있지만 컨테이너를 제거하면 해당 변경 사항이 모두 손실되며 Docker는 해당 컨테이너의 모든 변경 사항을 격리한다. 이때, 볼륨을 사용하면 모든 변경 사항을 반영할 수 있다.\n볼륨은 컨테이너의 특정 파일 시스템 경로를 호스트 머신과 연결하는 기능을 제공한다. 컨테이너에서 디렉토리를 마운트하면 해당 디렉토리의 변경 사항을 호스트 머신에서도 볼 수 있다. 컨테이너 재시작에서 동일한 디렉토리를 마운트하면 동일한 파일을 확인할 수 있다.\n예제 Todo 앱 데이터 유지하기 todo앱은 컨테이너 파일 시스템에 위치한 /etc/todos/todo.db 파일에 데이터를 저장한다(SQLite).\n데이터베이스 관련 파일을 호스트에 지속적으로 유지하고 다음 컨테이너에서 사용할 수 있도록 만들 수 있어야한다. 볼륨을 생성하고 데이터가 저장된 디렉토리에 연결(마운트)하여 컨테이너가 todo.db 파일에 쓰기를 수행하면 데이터는 볼륨에서 호스트로 유지된다.\n볼륨 마운트는 일종의 불투명한 데이터 버킷이며, Docker는 디스크 상의 저장 위치를 포함하여 볼륨 전체를 관리한다.\n볼륨 생성 1 2 3 docker volume create todo-db # todo-db 기존 컨테이너 중지 및 삭제: 볼륨을 사용하지 않은 상태에서 실행된 컨테이너 이므로\u0026hellip; 1 docker rm -f \u0026lt;id\u0026gt; --mount 옵션을 추가하여 볼륨 마운트를 지정하여 앱 컨테이너 실행 1 2 3 docker run -dp 3000:3000 --mount type=volume,src=todo-db,target=/etc/todos getting-started # ba3740f92525240162cff2cd3dac9420c048bff2087482cc30bd8411ce9ba198 앱을 열고 데이터 todo 리스트 추가 컨테이너 삭제 후 3번 명령어 실행 후 데이터 확인 Dive into the volume docker volume inspect 명령을 사용하면 Docker가 볼륨을 사용할 때 데이터를 어디에 저장하고 있는지 등 여러 정보들을 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 docker volume inspect todo-db [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2023-04-24T07:37:34Z\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: {}, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/todo-db/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;todo-db\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] Mountpoint가 디스크상의 실제 위치를 의미하며, 대부분의 컴퓨터에서 호스트가 이 디렉토리에 접근하기 위해 root 권한이 필요하다.\n","date":"2023-04-24T14:45:25+09:00","image":"https://codemario318.github.io/post/docker/4/docker_cover_hue12353db563619e41ee3a11307d3cf25_62602_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/docker/4/","title":"4. Persist the DB"},{"content":"만들어진 Docker 이미지는 Docker 레지스트리를 활용하여 공유할 수 있다. 기본 레지스트리는 Docker Hub이다.\nDocker ID\nDocker ID는 Docker Hub에 접근할 수 있는 권한을 제공한다. Docker Hub는 세계에서 가장 큰 라이브러리, 커뮤니티로 무료로 Docker ID를 만들어 사용할 수 있다.\nrepo 만들기 Docker Hub에 이미지를 push하려면 먼저 Docker Hub에 Repository를 생성해야 한다.\nDocker Hub 회원가입 및 로그인 \u0026ldquo;Create Repository\u0026rdquo; 버튼 클릭 repo 이름 설정 및 public 설정 선택 Create 버튼 클릭 이미지 푸시하기 1 2 3 4 5 docker push codemario318/getting-started # Using default tag: latest # The push refers to repository [docker.io/codemario318/getting-started] # An image does not exist locally with the tag: codemario318/getting-started 이미지를 docker push 명령으로 로컬에 존재하는 이미지를 푸시하기 위해서 tag이 필요하다. tag가 존재하지 않는다면 명령을 실행해도 이미지를 찾을 수 없기 때문에 오류가 발생하며, 빌드된 이미지에 다른 이름을 지정해여 태그를 붙여야 한다.\n1 docker login -u YOUR-USER-NAME 1 2 3 4 5 docker image ls # REPOSITORY TAG IMAGE ID CREATED SIZE # codemario318/getting-started latest 8b52580feb5e 2 days ago 262MB # getting-started latest 8b52580feb5e 2 days ago 262MB 1 2 3 4 5 6 7 8 9 10 11 12 docker push codemario318/getting-started # Using default tag: latest # The push refers to repository [docker.io/codemario318/getting-started] # 90b7c400bc47: Pushed # 0cf2f64e7aa4: Pushed # 9e6afbbf5565: Pushed # d9f41532a73b: Mounted from library/node # 6c0a2592426a: Mounted from library/node # 1984e605c08a: Mounted from library/node # 26cbea5cba74: Mounted from library/node # latest: digest: sha256:c11adf2600f6c2da46bfb1a8f74664a766bb69f11badcd3ed16a2de75a464c1a size: 1788 Docker Hub에서 값을 복사하고 있다면, 이미지 이름 부분에서 tagname을 추가하지 않아도 된다. 태그를 지정하지 않으면 Docker는 latest라는 이름의 태그를 사용한다.\n새 인스턴스에서 이미지 실행하기 이미지가 빌드되고 레지스트리에 푸시되었으므로, 컨테이너 이미지를 보유하지 않은 새 인스턴스에서 앱을 실행해도 같은 결과가 나온다.\nPlay with Docker 사용시 주의사항\nPlay with Docker는 amd64 플랫폼을 사용하므로, Apple silicon 기반의 ARM 기반 Mac을 사용하는 경우 이미지를 Play with Docker와 호환되도록 다시 빌드하고 새 이미지를 레포지토리에 푸시해야 한다.\n1 2 3 4 5 6 7 8 9 10 docker build --platform linux/amd64 -t YOUR-USER-NAME/getting-started . docker image ls # REPOSITORY TAG IMAGE ID CREATED SIZE # codemario318/getting-started latest 769987ace2c3 About a minute ago 265MB # getting-started latest 8b52580feb5e 2 days ago 262MB # codemario318/getting-started \u0026lt;none\u0026gt; 8b52580feb5e 2 days ago 262MB docker push codemario/getting-started Play with Docker에서 새 인스턴스를 추가한 후 명령을 실행한다.\n정상적으로 동작하는 것을 확인할 수 있다.\n","date":"2023-04-24T09:50:25+09:00","image":"https://codemario318.github.io/post/docker/3/docker_cover_hue12353db563619e41ee3a11307d3cf25_62602_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/docker/3/","title":"3. Share the application"},{"content":"애플리케이션과 컨테이너 이미지를 업데이트 및 삭제할 수 있다.\n소스 코드 업데이트 하기 1 2 3 4 ... - \u0026lt;p className=\u0026#34;text-center\u0026#34;\u0026gt;No items yet! Add one above!\u0026lt;/p\u0026gt; + \u0026lt;p className=\u0026#34;text-center\u0026#34;\u0026gt;You have no todo items yet! Add one above!\u0026lt;/p\u0026gt; ... 1 docker build -t getting-started . 1 2 3 4 docker run -dp 3000:3000 getting-started docker: Error response from daemon: driver failed programming external connectivity on endpoint laughing_burnell (2b5285b3ebfa65b51a44116c620e56b96b787874c71b25c8643b6a9ee137cb49): Bind for 0.0.0.0:3000 failed: port is already allocated. src/static/js/app.js의 56 라인의 소스 코드를 수정한다.\n이전과 마찬가지로 build 후 run 명령을 실행하면 기존 컨테이너가 이미 호스트의 포트 3000을 사용하고 있어 오류가 발생한다. 이 문제를 해결하려면 이전 컨테이너를 제거해야 한다.\n이전 컨테이너 지우기 컨테이너를 제거하려면 먼저 중지되어야 한다. 그 후 CLI 또는 Docker Desktop을 사용하여 제거할 수 있다.\nCLI를 통해 컨테이너를 중지하고, 삭제하기 위해서 컨테이너 ID가 필요한데 docker ps 명령을 통해 확인할 수 있다.\n1 2 3 4 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2b5285b3ebfa getting-started \u0026#34;docker-entrypoint.s…\u0026#34; 35 minutes ago Up 35 minutes 0.0.0.0:3000-\u0026gt;3000/tcp youthful_yonath 1 docker stop \u0026lt;the-container-id\u0026gt; 1 docker rm \u0026lt;the-container-id\u0026gt; force 플래그를 docker rm 명령에 추가하여 컨테이너를 중지하고 제거할 수도 있다.\n1 docker rm -f \u0026lt;the-container-id\u0026gt; 업데이트된 앱 컨테이너 실행하기 1 2 docker run -dp 3000:3000 getting-started 9732df7c2474fea5223bcd682bbd1a3b7f18cc1f965330bd666478d10a7a73cb 이전과 같은 docker run 명령으로 업데이트된 애플리케이션이 실행된 것을 확인할 수 있다.\n","date":"2023-04-24T09:19:25+09:00","image":"https://codemario318.github.io/post/docker/2/docker_cover_hue12353db563619e41ee3a11307d3cf25_62602_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/docker/2/","title":"2. Update the application"},{"content":"트랜잭션의 격리 수준(isolation level)이란 여러 트랜잭션이 동시에 처리될 때 특정 트랜잭션이 다른 트랜잭션에서 변경하거나 조회하는 데이터를 볼 수 있게 허용할지 말지를 결정한다.\nREAD UNCOMMITTED\nDIRTY READ라고도 하며 일반적인 데이터베이스에서는 거의 사용하지 않는다. READ COMMITTED REPEATABLE READ SERIALIZABLE\n동시성이 중요한 데이터베이스에서는 거의 사용되지 않는다. 4개의 격리 수준에서 뒤로 갈수록 각 트랜잭션 간의 데이터 고립 정도가 높아진다. 격리 수준이 높아질수록 MySQL 서버의 처리 성능이 많이 떨어질 것으로 생각하지만, SERIALIZABLE 격리 수준이 아니라면 크게 성능의 개선이나 저하는 발생하지 않는다.\nDIRTY READ NON-REPEATABLE READ PHANTOM READ READ UNCOMMITTED 발생 발생 발생 READ COMMITTED 없음 발생 발생 REPEATABLE READ 없음 없음 발생(InnoDB X) SERIALIZABLE 없음 없음 없음 SQL-92, SQL-99 표준에 따르면 REPEATABLE READ 격리 수준에서는 PHANTOM READ가 발생할 수 있지만, InnoDB에서는 독특한 특성으로 인해 발생하지 않는다.\n일반적인 온라인 서비스 용도의 데이터베이스는 READ COMMITED와 REPEATABLE READ 중 하나를 사용한다.\n오라클: READ COMMITED MySQL: REPEATABLE READ READ UNCOMMITTED READ UNCOMMITTED 격리 수준에서는 각 트랜잭션에서 변경 내용이 COMMIT, ROLLBACK 여부에 상관 없이 다른 트랜잭션에서 보인다.\nA가 emp_no가 500000이고 first_name이 Lara인 새로운 사원을 INSERT 하는데, B가 변경된 내용을 커밋하기도 전에 emp_no=500000인 사원을 검색해도 조회 가능하다.\n이에 따라 A의 INSERT가 ROLLBACK 되어도, B는 조회한 내용이 정상적인 사원이라고 생각하여 처리하게 된다.\n이처럼 어떤 트랜잭션에서 처리한 작업이 완료되지 않았는데도 다른 트랜잭션에서 볼 수 있는 현상을 더티 리드라고 하며, 더티 리드가 허용되는 격리 수준이 READ UNCOMMITTED이다.\n더티 리드 현상은 데이터가 나타났다 사라지는 현상을 초래하므로 개발자와 사용자를 혼란스럽게 만들 수 있다. 더티 리드를 유발하는 READ UNCOMMITTED는 RDBMS 표준에서는 트랜잭션의 격리 수준으로 인정하지 않을 정도로 정합성에 문제가 많으므로 READ COMMITTED 이상의 격리 수준을 사용할 것을 권장한다.\nREAD COMMITTED 오라클 DBMS에서 기본으로 사용되는 격리 수준이며, 온라인 서비스에서 가장 많이 선택되는 격리 수준이다. 어떤 트랜잭션에서 데이터를 변경했더라도 COMMIT이 완료된 데이터만 다른 트랜잭션에서 조회할 수 있기 때문에 더티 리드 현상은 발생하지 않는다.\nA가 emp_no=500000인 사원의 first_name을 \u0026ldquo;Lara\u0026quot;에서 \u0026ldquo;Toto\u0026quot;로 변경했는데, 이때 새로운 값인 \u0026ldquo;Toto\u0026quot;는 employees 테이블에 즉시 기록되고 이전 값인 \u0026ldquo;Lara\u0026quot;는 언두 영역으로 백업된다.\nCOMMIT이 완료 되기 전 B가 emp_no=500000인 사원의 first_name을 조회하면, 언두 영역에 백업된 레코드에서 가져온 \u0026ldquo;Lara\u0026quot;를 조회하게 된다.\nNON-REPEATABLE READ READ COMMITTED 격리 수준에서도 NON-REPEATABLE READ라는 부정합 문제가 있다.\nB가 BEGIN 명령으로 트랜잭션을 시작하고 first_name이 \u0026ldquo;Toto\u0026quot;인 사용자를 검색했는데 일치하는 결과가 없었다. 하지만 A가 사원 번호가 500000인 사원의 이름을 \u0026ldquo;Toto\u0026quot;로 변경하고 커밋을 실행한 후, B가 똑같은 쿼리로 다시 조회하면 1건이 조회된다.\n중요한 포인트는 사용자가 동일 트랜잭션 내에서 같은 쿼리를 활용하여 조회했는데도 불구하고 다른 결과를 조회해 온 것이며, 이는 하나의 트랜잭션 내에서 똑같은 SELECT 쿼리를 실행했을 때는 항상 같은 결과를 가져와야 한다는 REPEATABLE READ 정합성에 어긋나는 것이다.\n이러한 부정합 현상은 일반적인 웹 프로그램에서는 크게 문제되지 않을 수 있지만 하나의 트랜잭션에서 동일 데이터를 여러 번 읽고 변경하는 작업이 금전적인 처리와 연결되면 문제가 될 수도 있다.\n트랜잭션 내에서 실행되는 SELECT 문장과 트랜잭션 없이 실행되는 SELECT 문장의 차이\nREAD COMMITTED 격리 수준에서는 차이가 별로 없으나 REPEATABLE READ 격리 수준에서는 SELECT 쿼리도 트랜잭션 범위 내에서만 작동한다.\n즉 트랜잭션을 시작한 상태에서 SELECT 쿼리를 실행한다면, 다른 트랜잭션에서 데이터를 변경하고 COMMIT 되었다고 하더라도 동일한 결과를 반환한다.\nREPEATABLE READ MySQL의 InnoDB 스토리지 엔진에서 기본으로 사용되는 격리 수준이다. 바이너리 로그를 가진 MySQL 서버에서는 최소 REPEATABLE READ 격리 수준 이상을 사용해야 한다.\nInnoDB 스토리지 엔진의 REPEATABLE READ 격리수준에서는 트랜잭션이 ROLLBACK될 가능성에 대비해 변경되기 전 레코드를 언두 공간에 백업해두고 실제 레코드 값을 변경하고(MVCC) 백업된 이전 데이터를 이용해 동일 트랜잭션 내에서는 동일한 결과를 보여줄 수 있도록 보장하므로 NON-REPEATABLE READ 부정합이 발생하지 않는다.\nREPEATABLE READ와 READ COMMITTED의 차이는 언두 영역에 백업된 레코드의 여러 버전 가운데 몇 번째 이전 버전까지 찾아 들어가야 하느냐에 있다.\nInnoDB의 모든 트랜잭션은 고유한 트랜잭션 번호(순차적으로 증가하는 값)를 가지며, 언두 영역에 백업된 모든 레코드에는 변경을 발생시킨 트랜잭션 번호가 포함되어 있다. 언두 영역에 백업된 데이터는 InnoDB 스토리지 엔진이 불필요하다고 판단하는 시점에 주기적으로 삭제한다.\nREPEATABLE READ 격리 수준에서는 MVCC를 보장하기 위해 실행중인 트랜잭션 가운데 가장 오래된 트랜잭션 번호보다 트랜잭션 번호가 앞선 언두 영역의 데이터는 삭제할 수가 없다. 그렇다고 가장 오래된 트랜잭션 번호 이전의 트랜잭션에 의해 변경된 모든 언두 데이터가 필요한 것은 아니며, 특정 트랜잭션 번호의 구간 내에서 백업된 언두 데이터가 보존 돼야 한다.\n12번 트랜잭션에서 사원 이름을 \u0026ldquo;Toto\u0026quot;로 변경하고 커밋을 수행했다. 하지만 10번 트랜잭션에서 12번 트랜잭션 커밋 전 후로 해당 사원을 조회해도 자신의 트랜잭션 번호보다 작은 트랜잭션에서 변경한 것만 보게 되므로, 이전 값인 \u0026ldquo;Lara\u0026quot;를 받아온다.\n트랜잭션을 시작하고 장시간 종료하지 않으면 언두 영역이 백업된 데이터로 무한정 커질 수도 있으며, 이로 인해 백업된 레코드가 많아지면 MySQL 서버의 처리 성능이 떨어질 수 있다.\nPHANTOM READ REPEATABLE READ 격리 수준에서도 부정합이 발생 가능하다.\nA가 employees 테이블에 INSERT를 실행하는 도중 B가 SELECT ... FOR UPDATE 쿼리로 테이블을 2번 조회하는데 다른 결과를 반환하고 있다.\nSELECT ... FOR UPDATE 쿼리는 SELECT하는 레코드에 쓰기 잠금을 걸어야 하는데, 언두 레코드에는 잠금을 걸 수 없다. 따라서 SELECT ... FOR UPDATE나 SELECT ... LOCK IN SHARE MODE로 조회되는 레코드는 언두 영역의 변경 전 데이터를 가져오는 것이 아니라 현재 레코드의 값을 가져오게 된다.\n이렇게 다른 트랜잭션에서 수행한 변경 작업에 의해 레코드가 보였다 안 보였다 하는 현상을 PHANTOM READ(PHANTOM ROW)라고 한다.\nSERIALIZABLE 가장 단순한 격리 수준이면서 동시에 가장 엄격한 격리 수준이다. 그만큼 동시 처리 성능도 다른 트랜잭션 격리 수준보다 떨어진다.\nInnoDB 테이블에서 기본적으로 순수한 SELECT 작업은 아무런 레코드 잠금도 설정하지 않고 실행(Non-locking consistent read: 잠금이 필요없는 일관된 읽기)되지만 트랜잭션의 격리 수준이 SERIALIZABLE로 설정되면 읽기 작업도 공유 잠금(읽기 잠금)을 획득해야만 하며, 동시에 다른 트랜잭션은 레코드를 변경하지 못하게 된다. 따라서 SERIALIZABLE 격리 수준에서는 일반적인 DBMS에서 일어나는 PHANTOM READ 문제가 발생하지 않는다.\n하지만 InnoDB 스토리지 엔진에서는 갭 락과 넥스트 키 락 덕분에 REPEATABLE READ 격리 수준에서도 PHANTOM READ가 발생하지 않아 굳이 사용할 필요성은 없다.\n엄밀하게는 SELECT ... FOR UPDATE, SELECT ... FOR SHARE 쿼리의 경우 REPEATABLE READ 격리수준에서 PHANTOM READ 현상이 발생할 수 있다.\n하지만 레코드의 변경 이력(언두 레코드)에 잠금을 걸 수는 없기 때문에, 이러한 잠금을 동반한 SELECT 쿼리는 예외적인 상황으로 볼 수 있다.\n","date":"2023-04-22T15:31:10+09:00","image":"https://codemario318.github.io/post/real-mysql/5/3/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/5/3/","title":"5.3 MySQL의 격리 수준"},{"content":"잠금은 동시성을 제어하기 위한 기능으로 하나의 데이터를 여러 커넥션에서 동시에 동일한 자원(레코드나 테이블)을 요청할 경우 순서대로 한 시점에는 하나의 커넥션만 변경할 수 있게 해주는 역할을 한다.\nMySQL 엔진 레벨\n모든 스토리지에 영향을 미친다.\n스토리지 엔진 레벨\n스토리지 엔진 간 상호 영향을 미치지는 않는다.\nMySQL 엔진의 잠금 글로벌 락 글로벌 락은 FLUSH TABLES WITH READ LOCK 명령으로 획득할 수 있으며, MySQL에서 제공하는 잠금 가운데 가장 범위가 크다. 한 세션에서 글로벌 락을 획득하면 다른 세션에서 SELECT를 제외한 대부분의 DDL 문장이나 DML 문장을 실행하는 경우 락이 해제될 때까지 대기 상태로 남는다.\n글로벌 락이 영향을 미치는 범위는 MySQL 서버 전체이며, 작업 대상 테이블이나 데이터베이스가 다르더라도 동일하게 영향을 미친다. 여러 데이터베이스에 존재하는 MyISAM이나 MEMORY 테이블에 대해 mysqldump로 일관된 백업을 받아야 할 때는 글로벌 락을 사용해야 한다.\n글로벌 락을 거는 FLUSH TABLES WITH READ LOCK 명령은 실행과 동시에 서버에 존재하는 모든 테이블을 닫고 잠금을 거는데, 읽기 잠금을 걸기 전에 먼저 테이블을 플러시 해야 하기 때문에 실행 중인 모든 종류의 쿼리가 완료돼야 한다. 명령이 실행되기 전에 테이블이나 레코드에 쓰기 잠금을 거는 SQL이 실행되었다면 해당 테이블의 읽기 잠금을 걸기 위해 먼저 실행된 SQL과 그 트랜잭션이 완료될 때 까지 기다려야 한다.\n장시간 실행되는 쿼리와 FLUSH TABLES WITH READ LOCK 명령이 최악의 케이스로 실행되면 서버의 모든 쿼리가 오랜 시간 실행되지 못하고 대기할 수 있다.\n백업락 MySQL 8부터 InnoDB가 기본 스토리지 엔진으로 채택되었고, InnoDB 스토리지 엔진은 트랜잭션을 지원하기 때문에 일관된 데이터 상태를 위해 모든 데이터 변경 작업을 멈출 필요는 없기 때문에 조금 더 가벼운 글로벌 락의 필요성이 생겼고, 백업 락이 도입되었다.\n1 2 LOCK INSTANCE FOR BACKUP; UNLOCK INSTANCE; 특정 세션에서 백업 락을 획득하면 모든 세션에서 다음과 같이 테이블의 스키마나 사용자의 인증 관련 정보를 변경할 수 없게 되지만, 일반적인 테이블의 데이터 변경은 허용된다.\n데이터베이스 및 테이블 등 모든 객체 생성 및 변경, 삭제 REPAIR TABLE과 OPTIMIZE TABLE 명령 사용자 관리 및 비밀번호 변경 테이블 락 테이블락은 개별 테이블 단위로 설정되는 잠금이며, 명시적 또는 묵시적으로 특정 테이블의 락을 획들할 수 있다. MyISAM뿐 아니라 InnoDB 스토리지 엔진을 사용하는 테이블도 동일하게 설정 가능하다.\n명시적: LOCK TABLES table_name [ READ | WRITE ] 명령\nUNLOCK TABLES 명령으로 해제 특별한 상황이 아니면 애플리케이션에서 사용할 필요가 거의 없다. 묵시적: 테이블에 데이터를 변경하는 쿼리를 실행하면 발생\nMySQL 서버가 데이터가 변경되는 테이블에 잠금을 설정하고, 변경한 후 즉시 잠금을 해제한다. InnoDB 테이블의 경우 스토리지 엔진 차원에서 레코드 기반 잠금을 제공하기 때문에 단순 데이터 변경 쿼리로 인해 묵시적인 테이블 락이 설정되지는 않고, DDL의 경우에만 영향을 미친다. 네임드 락 네임드 락은 GET_LOCK() 함수를 이용해 임의의 문자열에 대해 잠금을 설정할 수 있다. 잠금 대상이 테이블이나 레코드 또는 AUTO_INCREMENT와 같은 데이터베이스 객체가 아니라 사용자가 지정한 문자열(String)에 대해 획득하고 반납하는 잠금이다.\n네임드 락은 자주 사용되지는 않지만, 여러 클라이언트가 상호 동기화를 처리해야 할 때 네임드 락을 이용하면 쉽게 해결할 수 있다.\n많은 레코드에 대해서 복잡한 요건으로 레코드를 변경하는 트랜잭션에 유용하게 사용할 수 있다. 배치 프로그램처럼 한꺼번에 많은 레코드를 변경하는 쿼리는 데드락의 원인이 되곤 하는데, 동일 데이터를 변경하거나 참조하는 프로그램끼리 분류해서 네임드 락을 걸고 쿼리를 실행하면 아주 간단히 해결할 수 있다.\n1 2 3 4 5 6 7 8 -- \u0026#34;mylock\u0026#34; 이라는 문자열에 대해 잠금을 획득 SELECT GET_LOCK(\u0026#39;mylock\u0026#39;, 2); -- \u0026#34;mylock\u0026#34; 이라는 문자열에 대해 잠금 설정 확인 SELECT IS_FREE_LOCK(\u0026#39;mylock\u0026#39;); -- \u0026#34;mylock\u0026#34; 이라는 문자열에 대해 잠금을 반납(해제) SELECT RELEASE_LOCK(\u0026#39;mylock\u0026#39;); MySQL 8 버전부터는 네임드 락을 중첩해서 사용할 수 있게 되었으며, 현재 세션에서 획득한 네임드 락을 한번에 모두 해제하는 기능도 추가되었다.\n1 2 3 4 5 6 7 SELECT GET_LOCK(\u0026#39;mylock_1\u0026#39;, 10); SELECT GET_LOCK(\u0026#39;mylock_2\u0026#39;, 10); SELECT RELEASE_LOCK(\u0026#39;mylock_1\u0026#39;); SELECT RELEASE_LOCK(\u0026#39;mylock_2\u0026#39;); SELECT RELEASE_ALL_LOCKS(); 메타데이터 락 메타데이터 락은 데이터베이스 객체(테이블이나 뷰 등)의 이름이나 구조를 변경하는 경우 획득하는 잠금이다. 명시적으로 획득하거나 해제할 수 없고, 테이블의 이름을 변경하는 경우 자동으로 획득한다.\nREAD TABLE 명령의 경우 원본 이름과 변경될 이름 두 개 모두 한꺼번에 잠금을 설정한다.\n1 RENAME TABLE rank TO rank_backup , rank_new TO rank; 위와 같이 하나의 RENAME TABLE 명령문에 두 개의 RENAME 작업을 한번에 실행하면 실제 애플리케이션에서는 \u0026ldquo;Table not found 'rank'\u0026ldquo;같은 상황을 발생시키지 않고 적용하는 것이 가능하다.\n하지만 이 문장을 나누어 실행하면 아주 짧은 시간이지만 rank 테이블이 존재하지 않는 순간이 생기며, 그 순간에 실행되는 쿼리는 오류를 발생시킨다.\n1 2 RENAME TABLE rank TO rank_backup; RENAME TABLE rank_new TO rank; 메타데이터 잠금과 트랜잭션 동시 활용 때로는 메타데이터 잠금과 InnoDB의 트랜잭션을 동시에 사용해야 하는 경우도 있다.\n1 2 3 4 5 CREATE TABLE access_log ( id BIGINT NOT NULL AUTO_INCREMENT, ... PRIMARY KEY(id) ); 위와 같이 INSERT만 실행되는 로그 테이블이 있을때, 이 테이블의 구조를 변경해야 한다면 Online DDL을 이용하여 변경할 수 도 있지만, 시간이 너무 오래 걸리는 경우 언두 로그의 증가와 Online DDL이 실행되는 동안 누적된 Online DDL 버퍼의 크기 등 고민해야 할 문제가 많다. 더 큰 문제는 MySQL서버의 DDL은 단일 스레드로 작동하기 대문에 상당히 많은 시간이 소모된다.\n이때는 새로운 구조의 테이블을 생성하고 먼저 최근 데이터까지는 프라이머리 키인 id 값을 범위별로 나눠서 여러 개의 스레드로 빠르게 복사하고, 나머지 데이터는 트랜잭션과 테이블 잠금, RENAME TABLE 명령으로 응용 프로그램의 중단 없이 실행할 수 있다.\n이때 남은 데이터를 복사 하는 시간 동안은 테이블의 잠금으로 인해 INSERT를 할 수 없게 되어 가능하면 미리 아주 최근 데이터까지 복사해 둬야 잠금 시간을 최소화하여 서비스에 미치는 영향을 줄일 수 있다.\n1 2 3 4 5 6 7 8 9 10 CREATE TABLE access_log_new ( id BIGINT NOT NULL AUTO_INCREMENT, ... PRIMARY KEY(id) ) KEY_BLOCK_SIZE=4; INSERT INTO access_log_new SELECT * FROM access_log WHERE id \u0026gt;= 0 AND id \u0026lt; 10000; INSERT INTO access_log_new SELECT * FROM access_log WHERE id \u0026gt;= 10000 AND id \u0026lt; 20000; INSERT INTO access_log_new SELECT * FROM access_log WHERE id \u0026gt;= 20000 AND id \u0026lt; 30000; INSERT INTO access_log_new SELECT * FROM access_log WHERE id \u0026gt;= 30000 AND id \u0026lt; 40000; 1 2 3 4 5 6 7 8 9 10 11 12 SET autocommit = 0; LOCK TABLES access_log WRITE, access_log_new WRITE; SELECT MAX(id) as @MAX_ID FROM access_log_new; INSERT INTO access_log_new SELECT * FROM access_log WHERE pk \u0026gt; @MAX_ID; COMMIT; RENAME TABLE access_log TO access_log_old, access_log_new TO access_log; UNLOCK TABLES; DROP TABLE access_log_old; InnoDB 스토리지 엔진 잠금 InnoDB 스토리지 엔진은 MySQL에서 제공하는 잠금과는 별개로 스토리지 엔진 내부에서 레코드 기반의 잠금 방식을 탑재하고 있다. 이러한 레코드 기반 잠금 방식 덕뿐에 MyISAM보다는 훨씬 뛰어난 동시성 처리를 제공할 수 있다.\n하지만 이원화된 잠금 처리로 인해 InnoDB 스토리지 엔진에서 사용되는 잠금에 대한 정보는 MySQL 명령을 이용해 접근하기 까다롭고, 내용도 어셈블리 코드를 보는 것 같아서 이해하기 어려웠다.\nlock_monitor innodb_lock_monitor라는 이름의 InnoDB 테이블을 생성하여 잠금 정보를 덤프하는 방법 SHOW ENGINE INNODB STATUAS 최근 버전에서 InnoDB의 트랜잭션과 잠금, 잠금 대기중인 트랜잭션의 목록을 조회할 수 있는 방법이 도입되었다.\nMySQL 서버의 information_schema 데이터베이스 INNODB_TRX, INNODB_LOCKS, INNODB_LOCK_WAITS 테이블 활용 Performance Schema InnoDB 스토리지 엔진의 내부 잠금(세마포어)에 대한 모니터링 InnoDB 스토리지 엔진의 잠금 InnoDB 스토리지 엔진은 레코드 기반의 잠금 기능을 제공하며, 잠금 정보가 상당히 작은 공간으로 관리되기 때문에 레코드 락이 페이지 락, 테이블 락으로 락 에스컬레이션 되는 경우는 없다.\n레코드 락 뿐만 아니라 레코드와 레코드 사이의 간격을 잠그는 갭 락도 존재한다.\n레코드락 레코드 자체만 잠그는 것을 레코드 락(Record lock, Record only lock)이라고 하며, 다른 상용 DBMS의 레코드 락과 동일한 역할을 한다. 중요한 차이는 InnoDB 스토리지 엔진은 레코드 자체가 아니라 인덱스의 레코드를 잠근다.\n인덱스가 하나도 없는 테이블이더라도 내부적으로 자동 생성된 클러스터 인덱스를 이용해 잠금을 설정한다. 따라서 InnoDB에서는 대부분 보조 인덱스를 이용한 변경 작업은 이어서 설명할 넥스트 키 락(Next key lock) 또는 갭 락(Gap lock)을 사용하지만 프라이머리 키 또는 유니크 인덱스에 의한 변경 작업에서는 갭에 대해서는 잠그지 않고 레코드 자체에 대해서만 락을 건다.\n갭 락 갭 락은 레코드 자체가 아니라 레코드와 바로 인접한 레코드 사이의 간격만을 잠그는 것을 의미한다. 레코드와 레코드 사이의 간격에 새로운 레코드가 생성(INSERT)되는 것을 제어한다.\n갭 락은 그 자체보다는 넥스트 키 락의 일부로 자주 사용된다. 넥스트 키 락 레코드 락과 갭 락을 합쳐 놓은 형태의 잠금을 넥스트 키 락(Next key lock)이라고 한다.\nSTATEMENT 포맷의 바이너리 로그를 사용하는 MySQL 서버에서는 REPEATABLE READ 격리 수준을 사용해야 한다. 또한 innodb_locks_unsafe_for_binlog 시스템 변수가 비활성화되면 변경을 위해 검색하는 레코드에는 넥스트 키 락 방식으로 잠금이 걸린다.\nInnoDB의 갭 락이나 넥스트 키 락은 바이너리 로그에 기록되는 쿼리가 레플리카 서버에서 실행될 때 소스 서버에서 만들어 낸 결과와 동일한 결과를 만들어 내도록 보장하는 것이 주목적이다.\n넥스트 키 락과 갭 락으로 인해 데드락이 발생하거나 다른 트랜잭션을 기다리게 만드는 일이 자주 발생하므로, 바이너리 로그 포맷을 ROW 형태로 바꿔서 넥스트 키 락이나 갭 락을 줄이는 것이 좋다.\n자동 증가 락 MySQL에서는 자동 증가하는 숫자 값을 추출하기 위해 AUTO_INCREMENT라는 컬럼 속성을 제공한다. AUTO_INCREMENT 컬럼이 사용된 테이블에 동시에 여러 레코드가 INSERT 되는 경우, 저장되는 각 레코드는 중복되지 않고 저장된 순서대로 증가하는 일련번호 값을 가져야 한다. 이를 위해 내부적으로 AUTO_INCREMENT 락이라고 하는 테이블 수준의 잠금을 사용한다.\nAUTO_INCREMENT락은 INSERT, REPLACE 쿼리 문장과 같이 새로운 레코드를 저장하는 쿼리에서만 필요하다. InnoDB의 다른 잠금과 달리 트랜잭션과 관계 없이 INSERT나 REPLACE 문장에서 AUTO_INCREMENT 값을 가져오는 순간만 락이 걸렸다가 즉시 해제된다. AUTO_INCREMENT 락은 테이블에 단 하나만 존재하기 대문에 두 개의 INSERT 쿼리가 동시에 실행되는 경우 하나의 쿼리가 락을 걸면 나머지 쿼리는 락을 기다려야 한다. AUTO_INCREMENT 컬럼에 명시적으로 값을 설정하더라도 자동 증가 락을 걸게 된다. innodb_autoinc_lock_mode\nMySQL 5.0 이하 버전에서는 AUTO_INCREMENT 락을 명시적으로 획득하고 해제하는 방법은 없다. 하지만 아주 짧은 시간동안 걸렸다가 해제되는 잠금이라서 대부분의 경우 문제가 되지 않는다.\nMySQL 5.1 이상 부터는 innodb_autoinc_lock_mode 시스템 변수를 이용해 자동 증가 락의 작동 방식을 변경할 수 있다.\ninnodb_autoinc_lock_mode=0\n모든 INSERT 문장이 자동 증가 락을 사용한다.\ninnodb_autoinc_lock_mode=1\nINSERT하는 쿼리 중에서 MySQL 서버가 INSERT되는 레코드의 건수를 정확히 예측할 수 있을 때는 자동 증가 락을 사용하지 않고, 훨씬 가볍고 빠른 래치(뮤텍스)를 이용해 처리한다.\n개선된 래치는 자동 증가 락과 달리 아주 짧은 시간 동안만 잠금을 걸고 필요한 자동 증가 값을 가져오면 즉시 잠금이 해제된다. 건수를 예측할 수 없을때는 이전 같이 자동 증가 락을 사용한다. 한번에 할당 받은 자동 증가 값이 남아서 사용되지 못하면 폐기하므로 레코드 자동 증가 값은 연속되지 않고 누락된 값이 발생할 수 있다. 하나의 INSERT 문장으로 INSERT 되는 레코드는 연속된 자동 증가 값을 가지게 된다. innodb_autoinc_lock_mode=2\n절대로 자동 증가 락을 걸지 않고 경량화된 래치(뮤텍스)를 사용한다.\n설정에서는 하나의 INSERT 문장으로 INSERT 되는 레코드라 하더라도 연속된 자동 증가값을 보장하지는 않는다. INSERT ... SELECT와 같은 대량 INSERT 문장이 실행되는 중에도 다른 커넥션에서 INSERT를 수행할 수 있으므로 동시 처리 성능이 높아진다. 자동 증가 기능은 유니크한 값이 생성된다는 것만 보장한다. 따라서 소스 서버와 레플리카 서버의 자동 증가 값이 달라질 수 있어 주의해야한다. MySQL 5.7 까지는 기본값이 1이었으나, 8버전 바이너리 로그 포맷이 STATEMENT가 아니라 ROW로 변경되었기 때문에 2로 바뀌었다. ROW 포맷이 아니라 STATEMENT 포맷의 바이너리 로그를 사용한다면 1로 변경해서 사용해야 한다.\n자동 증가 값이 한 번 증가하면 절대 줄어들지 않는 이유는 AUTO_INCREMENT 잠금을 최소화 하기 위해서다. INSERT 쿼리가 실패했더라도 한 번 증가된 AUTO_INCREMENT 값은 다시 줄어들지 않고 그대로 남는다.\n래치(Latch)란?\nDBMS에서 여러 스레드 혹은 프로세스가 동시에 접근할 수 있는 데이터에 대한 접근 제어 기술 중 하나이다.\n래치는 뮤텍스(Mutex)와 유사한 개념이지만, 뮤텍스는 오직 한 스레드 혹은 프로세스만이 해당 자원에 접근할 수 있도록 하는 동기화 기술이고, 래치는 여러 개의 스레드 혹은 프로세스가 읽기만 가능하고 쓰기는 하나의 스레드 혹은 프로세스만 가능하도록 하는 동기화 기술이다.\n래치는 읽기 래치와 쓰기 래치로 구분되며, 읽기 래치는 여러 스레드 혹은 프로세스가 동시에 해당 데이터를 읽을 수 있게 하고, 쓰기 래치는 오직 하나의 스레드 혹은 프로세스만 해당 데이터를 변경할 수 있게 한다.\n래치는 주로 인덱스 구조나 버퍼풀 등에서 사용되며, 동시성을 높이기 위해 적극적으로 활용된다. 그러나 래치를 과도하게 사용하거나 사용 방법이 부적절할 경우 데드락(deadlock) 등의 문제가 발생할 수 있으므로 주의해서 사용해야 한다.\n인덱스와 잠금 InnoDB의 잠금은 레코드를 잠그는 것이 아니라 인덱스를 잠그는 방식으로 처리된다. 즉 변경해야 할 레코드를 찾기 위해 검색한 인덱스의 레코드를 모두 락을 걸어야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 --// KEY ix_firstname (first_name) SELECT COUNT(*) FROM employees WHERE first_name=\u0026#39;Georgi\u0026#39; ; /* +---+ |253| +---+ */ SELECT COUNT(*) FROM employees WHERE first_name=\u0026#39;Georgi\u0026#39; AND last_name=\u0026#39;Klassen\u0026#39; ; /* +---+ | 1| +---+ */ UPDATE employees SET hire_date = NOW() WHERE first_name=\u0026#39;Georgi\u0026#39; AND last_name=\u0026#39;Klassen\u0026#39; ; UPDATE 쿼리가 실행되면 1건의 레코드가 업데이트 되지만, UPDATE 문장의 조건에서 인덱스를 이용할 수 있는 조건은 first_name뿐이며, last_name 컬럼은 인덱스가 없기 때문에 first_name='Georgi'인 레코드 253건의 레코드가 모두 잠긴다.\nUPDATE 문장을 위해 적절히 인덱스가 준비돼 있지 않다면 각 클라이언트 간의 동시성이 상당히 떨어져서 한 세션에서 UPDATE 작업을 하는 중에는 다른 클라이언트는 테이블을 업데이트하지 못하고 기다려야 하는 상황이 발생한다.\n만약 employees 테이블에 인덱스가 하나도 없다면, 테이블을 풀스캔 하면서 UPDATE를 수행하기 때문에 테이블에 있는 모든 레코드를 잠그게 된다.\n레코드 수준의 잠금 확인 및 해제 InnoDB 스토리지 엔진을 사용하는 테이블의 레코드 수준 잠금은 테이블 수준의 잠금보다는 조금 더 복잡하다.\n테이블 잠금에서는 잠금의 대상이 테이블 자체이므로 쉽게 문제의 원인이 발견되고 해결될 수 있으나, 레코드 수준의 잠금은 테이블의 레코드 각각에 잠금이 걸리므로 그 레코드가 자주 사용되지 않는다면 오랜 시간 동안 잠겨진 상태로 남아 있어도 잘 발견되지 않는다.\n예전 버전의 MySQL 서버에서는 레코드 잠금에 대한 메타 정보를 제공하지 않았기 때문에 어려웠지만, 5.1 버전부터는 레코드 잠금과 잠금 대기에 대한 조회가 가능하므로 쿼리 하나만 실행하면 잠금과 잠금 대기 상태를 바로 확인할 수 있다.\nMySQL 5.1 +\ninformation_schema라는 DB에 INNODB_TRX, INNODB_LOCKS, INNODB_LOCK_WAITS 라는 테이블을 통해 확인 MySQL 8.0 +\ninformation_schema의 정보들은 조금씩 제거되고 있다. performance_schema의 data_locks, data_lock_waits로 확인 ","date":"2023-04-22T15:30:10+09:00","image":"https://codemario318.github.io/post/real-mysql/5/2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/5/2/","title":"5.2 잠금"},{"content":"트랜잭션은 작업의 완전성을 보장해 주는 것이다.\n논리적이 작업 셋을 모두 완벽하게 처리하거나, 처리하지 못할 경우에는 원 상태로 복구해서 작업의 일부만 적용되는 현상(Partial update)이 발생하지 않게 만들어주는 기능이다.\n트랜잭션은 꼭 여러 개의 변경 작업을 수행하는 쿼리가 조합됐을 때만 의미있는 개념은 아니다. 트랜잭션은 하나의 논리적인 작업 셋이 100% 적용되거나(COMMIT) 아무것도 적용되지 않아야(ROLLBACK)함을 보장해 주는 것이다.\nMySQL에서의 트랜잭션 MySQL에서 InnoDB와 달리 MyISAM과 MEMORY 스토리지 엔진은 트랜잭션을 제공하지않는다. 처리방식의 차이를 확인하고, 트랜잭션을 사용할 때 주의할 점을 알아야 한다.\n1 2 3 4 5 CREATE TABLE tab_myisam ( fdpk INT NOT NULL, PRIMARY KEY (fdpk) ) ENGINE=MyISAM; INSERT INTO tab_myisam (fdpk) VALUE (3); CREATE TABLE tab_myInnodb ( fdpk INT NOT NULL, PRIMARY KEY (fdpk) ) ENGINE=INNODB; INSERT INTO tab_myInnodb (fdpk) VALUE (3); 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 SET autocommit=ON; INSERT INTO tab_myisam (fdpk) VALUES (1), (2), (3); /* ERROR 1062 (23000): Duplicate entry \u0026#39;3\u0026#39; for key \u0026#39;PRIMARY\u0026#39; +----+ |fdpk| +----+ | 1| +----+ | 2| +----+ | 3| +----+ */ INSERT INTO tab_myinnodb (fdpk) VALUES (1), (2), (3); /* ERROR 1062 (23000): Duplicate entry \u0026#39;3\u0026#39; for key \u0026#39;PRIMARY\u0026#39; +----+ |fdpk| +----+ | 3| +----+ */ 두 테이블의 INSERT 문장이 프라이머리 키 중복 오류로 실패했으나, MyISAM 테이블은 INSERT 문장이 순차적으로 실행되고 즉시 반영된다. 따라서 1과 2를 저장하고 3을 저장할 때 오류가 발생하여 쿼리 실행이 종료되며, 기존에 실행되었던 INSERT로 테이블에 1, 2가 저장되었고 오류로 실패해도 그대로 유지된다. MEMORY 스토리지 엔진도 동일한 방식으로 처리된다.\n반면 InnoDB는 쿼리 중 일부라도 오류가 발생하면 전체를 원 상태로 만든다는 트랜잭션의 원칙대로 INSERT 문장을 실행하기 전 상태로 복구한다.\nMyISAM에서 발생한 이러한 현상을 부분 업데이트(Partial Update)라고 표현하며, 테이블 데이터의 정합성을 맞추기 위해 실패한 쿼리로 인해 남은 레코드를 다시 삭제하는 등 어려운 문제를 만들어 낸다.\n주의사항 트랜잭션 또한 DBMS의 커넥션과 동일하게 트랜잭션의 범위를 최소화 하여, 꼭 필요한 최소의 코드에만 적용하는 것이 좋다.\nAS-IS 처리 시작 데이터베이스 커넥션 생성 트랜잭션 시작 사용자의 로그인 여부 확인 사용자의 글쓰기 내용의 오류 여부 확인 첨부로 업로드된 파일 확인 및 저장 사용자의 입력 내용을 DBMS에 저장 첨부 파일 정보를 DBMS에 저장 저장된 내용 또는 기타 정보를 DBMS에서 조회 게시물 등록에 대한 알림 메일 발송 알림 메일 발송 이력을 DBMS에 저장 트랜잭션 종료(COMMIT) 데이터베이스 커넥션 반납 처리완료 데이터베이스의 커넥션 생성하고 트랜잭션을 시작하는 부분\n실제로 DBMS에 데이터를 저장하는 작업은 5번부터 시작되므로 2,3,4 작업이 아무리 빨리 처리된다고 하더라도 트랜잭션에 포함시킬 필요는 없다. 일반적으로 데이터베이스 커넥션은 개수가 제한적이어서 각 단위 프로그램이 커넥션을 소유하는 시간이 길어질수록 여유 커넥션의 개수는 줄어든다. 이에 따라 커넥션을 기다려야 하는 상황이 발생할 수 있다. 메일 전송이나 FTP 파일 전송 작업 또는 네트워크를 통해 원격 서버와 통신하는 작업\n8번에서 프로그램이 실행되는 동안 통신할 수 없는 상황이 발생한다면 웹 서버뿐 아니라 DBMS 서버까지 위험해지는 상황이 발생하게 될 수 있으므로 트랜잭션 내에서 제거하는 것이 좋다. 작업 단위 분리\n처리 되어야 하는 DBMS 작업을 (5,6), (7), (9) 총 3개로 분리해서 트랜잭션을 만들 수 있고, 7번이 단순 조회라면 트랜잭션을 사용하지 않아도 된다. TO-BE 처리 시작 사용자의 로그인 여부 확인 사용자의 글쓰기 내용의 오류 여부 확인 첨부로 업로드된 파일 확인 및 저장 데이터베이스 커넥션 생성(또는 커넥션 풀에서 가져오기) 트랜잭션 시작 사용자의 입력 내용을 DBMS에 저장 첨부 파일 정보를 DBMS에 저장 트랜잭션 종료(COMMIT) 저장된 내용 또는 기타 정보를 DBMS에서 조회 게시물 등록에 대한 알림 메일 발송 트랜잭션 시작 알림 메일 발송 이력을 DBMS에 저장 트랜잭션 종료(COMMIT) 데이터베이스 커넥션 반납 처리완료 위와 같은 방식으로 개선 될 수 있다.\n","date":"2023-04-22T15:29:10+09:00","image":"https://codemario318.github.io/post/real-mysql/5/1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/5/1/","title":"5.1 트랜잭션"},{"content":"Get the app 1 git clone https://github.com/docker/getting-started.git Build the app’s container image 컨테이너 이미지를 빌드하려면 Dockerfile을 사용해야한다.\nDockerfile? Dockerfile은 파일 확장자가 없는 간단한 텍스트 기반 파일로서 도커 이미지를 만들기 위한 빌드 스크립트로 활용된다. 도커 이미지는 컨테이너를 생성하는 데 사용되며, Dockerfile은 이러한 이미지를 구성하는 데 필요한 모든 정보를 제공한다.\nDockerfile은 도커 이미지를 생성하는 데 필요한 기본 운영 체제 이미지, 필요한 응용 프로그램, 설정 파일 및 다른 종속성 등 모든 구성 요소를 명시하며, 일반적으로 다음과 같은 명령어로 구성된다.\nFROM: 기본이 되는 이미지 RUN: 새로운 레이어에서 실행될 명령어 COPY: 호스트 파일 시스템에서 파일이나 디렉토리를 이미지로 복사 ADD: COPY 명령과 유사하지만, URL에서 파일을 다운로드하거나 tar 파일에서 파일을 추출하는 등의 작업을 수행할 수 있음 WORKDIR: 명령어가 실행될 작업 디렉토리를 설정 ENV: 환경 변수를 설정 EXPOSE: 컨테이너가 사용하는 포트 CMD: 컨테이너가 시작될 때 실행할 명령어를 설정 Dockerfile에 정의된 모든 명령어는 도커 이미지의 각 레이어로 구성된다. 각 레이어는 독립적으로 캐싱되고 이미지를 다시 빌드할 때 이전에 캐싱된 레이어를 재사용하여 빌드 속도를 향상시켜 도커 이미지를 효율적이고 일관적인 방식으로 생성할 수 있다.\nDockerfile 생성 빌드할 어플리케이션이 있는 디렉터리(/getting-started/app)로 이동하여 Dockerfile 생성 후 필요한 내용을 채운다.\n1 2 cd getting-started/app touch Dockerfile 1 2 3 4 5 6 7 8 # syntax=docker/dockerfile:1 FROM node:18-alpine WORKDIR /app COPY . . RUN yarn install --production CMD [\u0026#34;node\u0026#34;, \u0026#34;src/index.js\u0026#34;] EXPOSE 3000 빌드 컨테이너 이미지를 빌드한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 docker build -t getting-started . [+] Building 26.1s (13/13) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 186B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; resolve image config for docker.io/docker/dockerfile:1 2.4s =\u0026gt; docker-image://docker.io/docker/dockerfile:1@sha256:39b85bbfa7536a5feceb7372a0817649ecb2724562a38 3.4s =\u0026gt; =\u0026gt; resolve docker.io/docker/dockerfile:1@sha256:39b85bbfa7536a5feceb7372a0817649ecb2724562a38360f 0.0s =\u0026gt; =\u0026gt; sha256:9d0cd65540a143ce38aa0be7c5e9efeed30d3580d03667f107cd76354f2bee65 10.82MB / 10.82MB 3.2s =\u0026gt; =\u0026gt; sha256:39b85bbfa7536a5feceb7372a0817649ecb2724562a38360f4d6a7782a409b14 8.40kB / 8.40kB 0.0s =\u0026gt; =\u0026gt; sha256:7f44e51970d0422c2cbff3b20b6b5ef861f6244c396a06e1a96f7aa4fa83a4e6 482B / 482B 0.0s =\u0026gt; =\u0026gt; sha256:a28edb2041b8f23c38382d8be273f0239f51ff1f510f98bccc8d0e7f42249e97 2.90kB / 2.90kB 0.0s =\u0026gt; =\u0026gt; extracting sha256:9d0cd65540a143ce38aa0be7c5e9efeed30d3580d03667f107cd76354f2bee65 0.2s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; [internal] load metadata for docker.io/library/node:18-alpine 2.3s =\u0026gt; [internal] load build context 0.1s =\u0026gt; =\u0026gt; transferring context: 4.59MB 0.1s =\u0026gt; [1/4] FROM docker.io/library/node:18-alpine@sha256:ca5d399560a9d239cbfa28eec00417f1505e5e108f3ec6 8.3s =\u0026gt; =\u0026gt; resolve docker.io/library/node:18-alpine@sha256:ca5d399560a9d239cbfa28eec00417f1505e5e108f3ec6 0.0s =\u0026gt; =\u0026gt; sha256:fefc7d195eee885e1f309ca2b5eff078b537b766f6bd949f8eb69fe895088821 2.41MB / 2.41MB 1.8s =\u0026gt; =\u0026gt; sha256:ca5d399560a9d239cbfa28eec00417f1505e5e108f3ec6938d230767eaa81f61 1.43kB / 1.43kB 0.0s =\u0026gt; =\u0026gt; sha256:46f34dda633f5708462f6d6ee7ef829535bd0ee04b82cbe28dcccb5df74b3eb1 1.16kB / 1.16kB 0.0s =\u0026gt; =\u0026gt; sha256:a966e12937d2cdbf1cf501f972673875426566c53820a3fe54c2a15b0ad93639 6.50kB / 6.50kB 0.0s =\u0026gt; =\u0026gt; sha256:c41833b44d910632b415cd89a9cdaa4d62c9725dc56c99a7ddadafd6719960f9 3.26MB / 3.26MB 0.6s =\u0026gt; =\u0026gt; sha256:762c2470eea4dfd0e37925b903f27172a7b89fd8b11bb8cf61554941c3293636 47.28MB / 47.28MB 6.7s =\u0026gt; =\u0026gt; extracting sha256:c41833b44d910632b415cd89a9cdaa4d62c9725dc56c99a7ddadafd6719960f9 0.1s =\u0026gt; =\u0026gt; sha256:06fc22ed341f1d0c400e6972828f8731f3544007cda80ea1d333fe15acf0a28b 448B / 448B 1.2s =\u0026gt; =\u0026gt; extracting sha256:762c2470eea4dfd0e37925b903f27172a7b89fd8b11bb8cf61554941c3293636 1.4s =\u0026gt; =\u0026gt; extracting sha256:fefc7d195eee885e1f309ca2b5eff078b537b766f6bd949f8eb69fe895088821 0.1s =\u0026gt; =\u0026gt; extracting sha256:06fc22ed341f1d0c400e6972828f8731f3544007cda80ea1d333fe15acf0a28b 0.0s =\u0026gt; [2/4] WORKDIR /app 0.1s =\u0026gt; [3/4] COPY . . 0.0s =\u0026gt; [4/4] RUN yarn install --production 8.6s =\u0026gt; exporting to image 0.7s =\u0026gt; =\u0026gt; exporting layers 0.7s =\u0026gt; =\u0026gt; writing image sha256:8b52580feb5e556abf813797e2003cbb30d53dfeee0f865fde0f16de6354c6a1 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/getting-started docker build 명령은 Dockerfile을 사용하여 새 컨테이너 이미지를 빌드한다.\n명령 실행 후 많은 레이어들을 다운로드 하게 되었는데, 이는 도커 파일 작성시 기본이 되는 이미지를 FROM으로 명시했기 때문이다. 하지만 해당 이미지가 존재하지 않았기 때문에 다운로드 하게 된다.\n도커가 이미지를 다운로드한 후에는 Dockerfile에서 지시한대로 애플리케이션을 복사하고 yarn을 사용하여 애플리케이션의 종속성을 설치한다. CMD로 이미지에서 컨테이너를 시작할 때 실행할 기본 명령을 지정할 수 있다.\n명령에 사용된 -t 옵션을 명시하면 이미지에 태그를 설정할 수 있으며, 입력된 getting-started로 이름을 지정하여 컨테이너를 실행할 때 해당 이미지를 참조할 수 있게 된다.\n명령의 마지막에 있는 .는 도커가 현재 디렉토리에서 Dockerfile을 찾아야 한다는 것을 알린다.\nStart an app container 이미지가 있다면 docker run명령을 통해 컨테이너에서 애플리케이션을 실행할 수 있다.\n1 2 3 docker run -dp 3000:3000 getting-started 2b5285b3ebfa65b51a44116c620e56b96b787874c71b25c8643b6a9ee137cb49 -d: 새 컨테이너를 백그라운드 모드로 실행한다. -p 호스트의 포트와 컨테이너의 포트간 매핑을 생성한다. 포트 매핑이 없다면 애플리케이션에 접근할 수 없다. 명령이 정상적으로 수행되었다면 http://localhost:3000 링크를 통해 애플리케이션에 접근할 수 있다.\n컨테이너를 간단히 확인해 보면, getting-started 이미지를 사용하고 있으며 포트 3000으로 실행 중인 컨테이너가 적어도 하나 있어야 하고, 컨테이너를 확인하려면 CLI 또는 Docker Desktop의 그래픽 인터페이스를 사용할 수 있다.\n1 2 3 4 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2b5285b3ebfa getting-started \u0026#34;docker-entrypoint.s…\u0026#34; 3 minutes ago Up 3 minutes 0.0.0.0:3000-\u0026gt;3000/tcp youthful_yonath ","date":"2023-04-21T16:08:25+09:00","image":"https://codemario318.github.io/post/docker/1/docker_cover_hue12353db563619e41ee3a11307d3cf25_62602_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/docker/1/","title":"1. Containerize an application"},{"content":"노드 쿠버네티스는 컨테이너를 파드내에 배치하고 노드에서 실행함으로 워크로드를 구동한다. 노드는 클러스터에 따라 가상 또는 물리적 머신일 수 있다. 각 노드는 컨트롤 플레인에 의해 관리되며 파드를 실행하는데 필요한 서비스를 포함한다.\n일반적으로 클러스터에는 여러개의 노드가 있으며, 학습 또는 리소스가 제한되는 환경에서는 하나만 있을 수도 있다.\n노드의 컴포넌트에는 kubelet, 컨테이너 런타임 그리고 kube-proxy가 포함된다.\n관리 API 서버에 노드를 추가하는 두가지 주요 방법이 있다.\n노드의 kubelet으로 컨트롤 플레인에 자체 등록 사용자(또는 다른 사용자)가 노드 오브젝트를 수동으로 추가 노드 오브젝트 또는 노드의 kubelt으로 자체 등록한 후 컨트롤 플레인은 새 노드 오브젝트가 유효한지 확인한다.\n1 2 3 4 5 6 7 8 9 10 { \u0026#34;kind\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;10.240.79.157\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my-first-k8s-node\u0026#34; } } } 쿠버네티스는 내부적으로 노드 오브젝트를 생성(표시)한다. 쿠버네티스는 kubelet이 노드의 metadata,name필드와 일치하는 API 서버에 등록이 되어있는지 확인한다. 노드가 정상이면(필요한 모든 서비스가 실행중인 경우) 파드를 실행할 수 있게 된다. 그렇지 않으면, 해당 노드는 정상이 될때까지 모든 클러스터 활동에 대해 무시된다.\n💡 쿠버네티스는 유효하지 않은 노드 오브젝트를 유지하고, 노드가 정상적인지 확인한다. 상태 확인을 중지하려면 사용자 또는 컨트롤러에서 노드 오브젝트를 명시적으로 삭제해야 한다.\n노드 오브젝트의 이름은 유효한 DNS 서브 도메인 이름이어야 한다.\n노드 이름 고유성 이름은 노드를 식별한다. 두 노드는 동시에 같은 이름을 가질 수 없다. 쿠버네티스는 또한 같은 이름의 리소스가 동일한 객체라고 가정한다. 노드의 경우, 동일한 이름을 사용하는 인스턴스가 동일한 상태와 노드 레이블과 같은 동일한 속성을 갖는다고 암시적으로 가정한다. 인스턴스가 이름을 변경하지 않고 수정된 경우 이로 인해 불일치가 발생할 수 있다. 노드를 대폭 교체하거나 업데이트 해야 하는 경우, 기존 노드 오브젝트를 먼저 API 서버에서 제거하고 업데이트 후 다시 추가해야 한다.\n노드 상태 kubectl을 사용해서 노드 상태와 기타 세부 정보를 볼수 있다.\n1 kubectl describe node \u0026lt;insert-node-name-here\u0026gt; 주소 주소 필드는 클라우드 제공 사업자 또는 베어메탈 구성에 따라 다양하게 사용된다.\nHostName: 노드의 커널에 의해 알려진 호스트명이다. -hostname-override 파라미터를 통해 치환될 수 있다. ExternalIP: 일반적으로 노드의 IP 주소는 외부로 라우트 가능 (클러스터 외부에서 이용 가능) 하다 . InternalIP: 일반적으로 노드의 IP 주소는 클러스터 내에서만 라우트 가능하다. 컨디션 conditions필드는 모든 Running 노드의 상태를 기술한다.\n노드 컨디션 설명 Ready 노드가 상태 양호하며 파드를 수용할 준비가 되어 있는 경우 True, 노드의 상태가 불량하여 파드를 수용하지 못할 경우 False, 그리고 노드 컨트롤러가 마지막 node-monitor-grace-period (기본값 40 기간 동안 노드로부터 응답을 받지 못한 경우) Unknown DiskPressure 디스크 사이즈 상에 압박이 있는 경우, 즉 디스크 용량이 넉넉치 않은 경우 True, 반대의 경우 False MemoryPressure 노드 메모리 상에 압박이 있는 경우, 즉 노드 메모리가 넉넉치 않은 경우 True, 반대의 경우 False PIDPressure 프로세스 상에 압박이 있는 경우, 즉 노드 상에 많은 프로세스들이 존재하는 경우 True, 반대의 경우 False NetworkUnavailable 노드에 대해 네트워크가 올바르게 구성되지 않은 경우 True, 반대의 경우 False 용량과 할당가능 노드 상에 사용 가능한 리소스를 나타낸다.\nCPU 메모리 스케줄 되어질 수 있는 최대 파드 수 등 용량 블록 필드는 노드에 있는 리소스의 총량을 나타낸다. 할당가능 블록은 일반 파드에서 사용할 수 있는 노드의 리소스 양을 나타낸다.\n정보 노드에 대한 일반적은 정보가 기술된다. kubelet이 노드에서 수집하여 쿠버네티스 API 로 전송한다.\n커널 버전 쿠버네티스 버전 컨테이너 런타임 상세 정보 등 하트비트 쿠버네티스 노드가 보내는 하트비느느 ㄴ클러스터가 개별 노드가 가용한지를 판단할 수 있도록 도움을 주고, 장애가 발견된 경우 조치를 할 수 있게 한다.\n노드의 .status 에 대한 업데이트 kube=-node-lease 네임 스페이스 내의 Lease오브젝트 노드의 .status에 비하면, 리스는 경량의 리소스이다. 큰 규모의 클러스터에서는 리스를 하트비트에 사용하여 업데이트로 인한 성능 영향을 줄일 수 있다.\nkubelet은 노드의 .status 생성과 업데이트 및 관련된 리스의 업데이트를 담당한다.\nkubelet은 상태가 변경되거나 설정된 인터벌보다 오래 업데이트가 없는 경우 노드의 .status를 업데이트한다. 노드의 .status 업데이트에 대한 기본 인터벌은 접근이 불가능한 노드에 대한 타임아웃인 40초 보다 훨씬 긴 5분이다. kubelet은 리스 오브젝트를 (기본 업데이트 인터벌인) 매 10초마다 생성하고 업데이트한다. 리스 업데이트는 노드의 .status 업데이트와는 독립적이다. 만약 리스 업데이트가 실패하면, kubelet은 200밀리초에서 시작하고 7초의 상한을 갖는 지수적 백오프를 사용해서 재시도한다. 노드 컨트롤러 노드 컨트롤러는 노드의 다양한 측면을 관리하는 쿠버네티스 컨트롤 플레인 컴포넌트이다.\n노드 컨트롤러는 노드가 생성되어 유지되는 동안 다양한 역할을 한다.\n등록 시점에 (CIDR 할당이 사용토록 설정된 경우) 노드에 CIDR 블럭을 할당하는 것이다.\n노드 컨트롤러의 내부 노드 리스트를 클라우드 제공사업자의 사용 가능한 머신 리스트 정보를 근거로 최신상태로 유지하는 것이다.\n클라우드 환경에서 동작 중일 경우, 노드상태가 불량할 때마다, 노드 컨트롤러는 해당 노드용 VM이 여전히 사용 가능한지에 대해 클라우드 제공사업자에게 묻는다. 사용 가능하지 않을 경우, 노드 컨트롤러는 노드 리스트로부터 그 노드를 삭제한다. 노드의 동작 상태를 모니터링하는 것이다. 노드가 접근 불가능(unreachable) 상태가 되는 경우, 노드의 .status 필드의 Ready 컨디션을 업데이트한다. 이 경우에는 노드 컨트롤러가 Ready 컨디션을 Unknown으로 설정한다. 노드가 계속 접근 불가능 상태로 남아있는 경우, 해당 노드의 모든 파드에 대해서 API를 이용한 축출을 트리거한다. 기본적으로, 노드 컨트롤러는 노드를 Unknown으로 마킹한 뒤 5분을 기다렸다가 최초의 축출 요청을 시작한다. 기본적으로, 노드 컨트롤러는 5 초마다 각 노드의 상태를 체크한다. 체크 주기는 kube-controller-manager 구성 요소의 --node-monitor-period 플래그를 이용하여 설정할 수 있다.\n축출 빈도 제한 대부분의 경우, 노드 컨트롤러는 초당 --node-eviction-rate(기본값 0.1)로 축출 속도를 제한한다. 이 말은 10초당 1개의 노드를 초과하여 파드 축출을 하지 않는다는 의미가 된다.\n노드 축출 행위는 주어진 가용성 영역 내 하나의 노드가 상태가 불량할 경우 변화한다. 노드 컨트롤러는 영역 내 동시에 상태가 불량한 노드의 퍼센티지가 얼마나 되는지 체크한다(Ready 컨디션은 Unknown 또는 False 값을 가진다).\n상태가 불량한 노드의 비율이 최소 -unhealthy-zone-threshold (기본값 0.55)가 되면 축출 속도가 감소한다. 클러스터가 작으면 (즉 -large-cluster-size-threshold 노드 이하면 - 기본값 50) 축출이 중지된다. 이외의 경우, 축출 속도는 초당 -secondary-node-eviction-rate(기본값 0.01)로 감소된다. 이 정책들이 가용성 영역 단위로 실행되어지는 이유는 나머지가 연결되어 있는 동안 하나의 가용성 영역이 컨트롤 플레인으로부터 분할되어 질 수도 있기 때문이다. 만약 클러스터가 여러 클라우드 제공사업자의 가용성 영역에 걸쳐 있지 않는 이상, 축출 매커니즘은 영역 별 가용성을 고려하지 않는다.\n노드가 가용성 영역들에 걸쳐 퍼져 있는 주된 이유는 하나의 전체 영역이 장애가 발생할 경우 워크로드가 상태 양호한 영역으로 이전될 수 있도록 하기 위해서이다. 그러므로, 하나의 영역 내 모든 노드들이 상태가 불량하면 노드 컨트롤러는 --node-eviction-rate 의 정상 속도로 축출한다.\n모든 영역이 완전히 상태불량(클러스터 내 양호한 노드가 없는 경우)한 경우이다. 이러한 경우, 노드 컨트롤러는 컨트롤 플레인과 노드 간 연결에 문제가 있는 것으로 간주하고 축출을 실행하지 않는다. (중단 이후 일부 노드가 다시 보이는 경우 노드 컨트롤러는 상태가 양호하지 않거나 접근이 불가능한 나머지 노드에서 파드를 축출한다.)\n또한, 노드 컨트롤러는 파드가 테인트를 허용하지 않을 때 NoExecute 테인트 상태의 노드에서 동작하는 파드에 대한 축출 책임을 가지고 있다. 추가로, 노드 컨틀로러는 연결할 수 없거나, 준비되지 않은 노드와 같은 노드 문제에 상응하는 테인트를 추가한다. 이는 스케줄러가 비정상적인 노드에 파드를 배치하지 않게 된다.\n리소스 용량 추적 노드 오브젝트는 노드 리소스 용량에 대한 정보: 예를 들어, 사용 가능한 메모리의 양과 CPU의 수를 추적한다. 노드의 자체 등록은 등록하는 중에 용량을 보고한다. 수동으로 노드를 추가하는 경우 추가할 때 노드의 용량 정보를 설정해야 한다.\n쿠버네티스 스케줄러는 노드 상에 모든 노드에 대해 충분한 리소스가 존재하도록 보장한다. 스케줄러는 노드 상에 컨테이너에 대한 요청의 합이 노드 용량보다 더 크지 않도록 체크한다. 요청의 합은 kubelet에서 관리하는 모든 컨테이너를 포함하지만, 컨테이너 런타임에 의해 직접적으로 시작된 컨 테이너는 제외되고 kubelet의 컨트롤 범위 밖에서 실행되는 모든 프로세스도 제외된다.\n참고: 파드 형태가 아닌 프로세스에 대해 명시적으로 리소스를 확보하려면, 시스템 데몬에 사용할 리소스 예약하기을 본다.\n노드 토폴로지 기능 상태: Kubernetes v1.18 [beta]\nTopologyManager 기능 게이트(feature gate)를 활성화 시켜두면, kubelet이 리소스 할당 결정을 할 때 토폴로지 힌트를 사용할 수 있다. 자세한 내용은 노드의 컨트롤 토폴로지 관리 정책을 본다.\n그레이스풀(Graceful) 노드 셧다운(shutdown) 기능 상태: Kubernetes v1.21 [beta]\nkubelet은 노드 시스템 셧다운을 감지하고 노드에서 실행 중인 파드를 종료하려고 시도한다.\nKubelet은 노드가 종료되는 동안 파드가 일반 파드 종료 프로세스를 따르도록 한다.\n그레이스풀 노드 셧다운 기능은 systemd inhibitor locks를 사용하여 주어진 기간 동안 노드 종료를 지연시키므로 systemd에 의존한다.\n그레이스풀 노드 셧다운은 1.21에서 기본적으로 활성화된 GracefulNodeShutdown 기능 게이트로 제어된다.\n기본적으로, 아래 설명된 두 구성 옵션, shutdownGracePeriod 및 shutdownGracePeriodCriticalPods 는 모두 0으로 설정되어 있으므로, 그레이스풀 노드 셧다운 기능이 활성화되지 않는다. 기능을 활성화하려면, 두 개의 kubelet 구성 설정을 적절하게 구성하고 0이 아닌 값으로 설정해야 한다.\n그레이스풀 셧다운 중에 kubelet은 다음의 두 단계로 파드를 종료한다.\n노드에서 실행 중인 일반 파드를 종료시킨다. 노드에서 실행 중인 중요(critical) 파드를 종료시킨다. 그레이스풀 노드 셧다운 기능은 두 개의 [KubeletConfiguration](https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/) 옵션으로 구성된다.\nshutdownGracePeriod: 노드가 종료를 지연해야 하는 총 기간을 지정한다. 이것은 모든 일반 및 중요 파드의 파드 종료에 필요한 총 유예 기간에 해당한다. shutdownGracePeriodCriticalPods: 노드 종료 중에 중요 파드를 종료하는 데 사용되는 기간을 지정한다. 이 값은 shutdownGracePeriod 보다 작아야 한다. 예를 들어, shutdownGracePeriod=30s, shutdownGracePeriodCriticalPods=10s 인 경우, kubelet은 노드 종료를 30초까지 지연시킨다. 종료하는 동안 처음 20(30-10)초는 일반 파드의 유예 종료에 할당되고, 마지막 10초는 중요 파드의 종료에 할당된다.\n**참고: 그레이스풀 노드 셧다운 과정에서 축출된 파드는 셧다운(shutdown)된 것으로 표시된다. kubectl get pods 명령을 실행하면 축출된 파드의 상태가 Terminated으로 표시된다. 그리고 kubectl describe pod 명령을 실행하면 노드 셧다운으로 인해 파드가 축출되었음을 알 수 있다.\nReason: Terminated Message: Pod was terminated in response to imminent node shutdown.**\n논 그레이스풀 노드 셧다운 기능 상태: Kubernetes v1.24 [alpha]\n전달한 명령이 kubelet에서 사용하는 금지 잠금 메커니즘(inhibitor locks mechanism)을 트리거하지 않거나, 또는 사용자 오류(예: ShutdownGracePeriod 및 ShutdownGracePeriodCriticalPods가 제대로 설정되지 않음)로 인해 kubelet의 노드 셧다운 관리자(Node Shutdown Mananger)가 노드 셧다운 액션을 감지하지 못할 수 있다. 자세한 내용은 위의 그레이스풀 노드 셧다운 섹션을 참조한다.\n노드가 셧다운되었지만 kubelet의 노드 셧다운 관리자가 이를 감지하지 못하면, 스테이트풀셋에 속한 파드는 셧다운된 노드에 \u0026lsquo;종료 중(terminating)\u0026rsquo; 상태로 고착되어 다른 동작 중인 노드로 이전될 수 없다. 이는 셧다운된 노드의 kubelet이 파드를 지울 수 없어서 결국 스테이트풀셋이 동일한 이름으로 새 파드를 만들 수 없기 때문이다. 만약 파드가 사용하던 볼륨이 있다면, 볼륨어태치먼트(VolumeAttachment)도 기존의 셧다운된 노드에서 삭제되지 않아 결국 파드가 사용하던 볼륨이 다른 동작 중인 노드에 연결(attach)될 수 없다. 결과적으로, 스테이트풀셋에서 실행되는 애플리케이션이 제대로 작동하지 않는다. 기존의 셧다운된 노드가 정상으로 돌아오지 못하면, 이러한 파드는 셧다운된 노드에 \u0026lsquo;종료 중(terminating)\u0026rsquo; 상태로 영원히 고착될 것이다.\n위와 같은 상황을 완화하기 위해, 사용자가 node.kubernetes.io/out-of-service 테인트를 NoExecute 또는 NoSchedule 값으로 추가하여 노드를 서비스 불가(out-of-service) 상태로 표시할 수 있다. kube-controller-manager에 NodeOutOfServiceVolumeDetach기능 게이트 가 활성화되어 있고, 노드가 이 테인트에 의해 서비스 불가 상태로 표시되어 있는 경우, 노드에 매치되는 톨러레이션이 없다면 노드 상의 파드는 강제로 삭제될 것이고, 노드 상에서 종료되는 파드에 대한 볼륨 해제(detach) 작업은 즉시 수행될 것이다. 이를 통해 서비스 불가 상태 노드의 파드가 빠르게 다른 노드에서 복구될 수 있다.\n논 그레이스풀 셧다운 과정 동안, 파드는 다음의 두 단계로 종료된다.\n매치되는 out-of-service 톨러레이션이 없는 파드를 강제로 삭제한다. 이러한 파드에 대한 볼륨 해제 작업을 즉시 수행한다. 참고: • node.kubernetes.io/out-of-service 테인트를 추가하기 전에, 노드가 완전한 셧다운 또는 전원 꺼짐 상태에 있는지 (재시작 중인 것은 아닌지) 확인한다. • 사용자가 서비스 불가 상태 테인트를 직접 추가한 것이기 때문에, 파드가 다른 노드로 옮겨졌고 셧다운 상태였던 노드가 복구된 것을 확인했다면 사용자가 서비스 불가 상태 테인트를 수동으로 제거해야 한다.\n파드 우선순위 기반 그레이스풀 노드 셧다운 기능 상태: Kubernetes v1.23 [alpha]\n그레이스풀 노드 셧다운 시 파드 셧다운 순서에 더 많은 유연성을 제공할 수 있도록, 클러스터에 프라이어리티클래스(PriorityClass) 기능이 활성화되어 있으면 그레이스풀 노드 셧다운 과정에서 파드의 프라이어리티클래스가 고려된다. 이 기능으로 그레이스풀 노드 셧다운 시 파드가 종료되는 순서를 클러스터 관리자가 프라이어리티 클래스 기반으로 명시적으로 정할 수 있다.\n위에서 기술된 것처럼, 그레이스풀 노드 셧다운 기능은 파드를 중요하지 않은(non-critical) 파드와 중요한(critical) 파드 2단계(phase)로 구분하여 종료시킨다. 셧다운 시 파드가 종료되는 순서를 명시적으로 더 상세하게 정해야 한다면, 파드 우선순위 기반 그레이스풀 노드 셧다운을 사용할 수 있다.\n그레이스풀 노드 셧다운 과정에서 파드 우선순위가 고려되기 때문에, 그레이스풀 노드 셧다운이 여러 단계로 일어날 수 있으며, 각 단계에서 특정 프라이어리티 클래스의 파드를 종료시킨다. 정확한 단계와 단계별 셧다운 시간은 kubelet에 설정할 수 있다.\n다음과 같이 클러스터에 커스텀 파드 프라이어리티 클래스가 있다고 가정하자.\n파드 프라이어리티 클래스 이름 파드 프라이어리티 클래스 값 custom-class-a 100000 custom-class-b 10000 custom-class-c 1000 regular/unset 0 kubelet 환경 설정 안의 shutdownGracePeriodByPodPriority 설정은 다음과 같을 수 있다.\n파드 프라이어리티 클래스 값 종료 대기 시간 100000 10 seconds 10000 180 seconds 1000 120 seconds 0 60 seconds 이를 나타내는 kubelet 환경 설정 YAML은 다음과 같다.\n1 2 3 4 5 6 7 8 9 **shutdownGracePeriodByPodPriority**: - **priority**: 100000 **shutdownGracePeriodSeconds**: 10 - **priority**: 10000 **shutdownGracePeriodSeconds**: 180 - **priority**: 1000 **shutdownGracePeriodSeconds**: 120 - **priority**: 0 **shutdownGracePeriodSeconds**: 60 위의 표에 의하면 priority 값이 100000 이상인 파드는 종료까지 10초만 주어지며, 10000 이상 ~ 100000 미만이면 180초, 1000 이상 ~ 10000 미만이면 120초가 주어진다. 마지막으로, 다른 모든 파드는 종료까지 60초가 주어질 것이다.\n모든 클래스에 대해 값을 명시할 필요는 없다. 예를 들어, 대신 다음과 같은 구성을 사용할 수도 있다.\n파드 프라이어리티 클래스 값 종료 대기 시간 100000 300 seconds 1000 120 seconds 0 60 seconds 위의 경우, custom-class-b에 속하는 파드와 custom-class-c에 속하는 파드는 동일한 종료 대기 시간을 갖게 될 것이다.\n특정 범위에 해당되는 파드가 없으면, kubelet은 해당 범위에 해당되는 파드를 위해 기다려 주지 않는다. 대신, kubelet은 즉시 다음 프라이어리티 클래스 값 범위로 넘어간다.\n기능이 활성화되어 있지만 환경 설정이 되어 있지 않으면, 순서 지정 동작이 수행되지 않을 것이다.\n이 기능을 사용하려면 GracefulNodeShutdownBasedOnPodPriority 기능 게이트를 활성화해야 하고, kubelet config의 ShutdownGracePeriodByPodPriority를 파드 프라이어리티 클래스 값과 각 값에 대한 종료 대기 시간을 명시하여 지정해야 한다.\n참고: 그레이스풀 노드 셧다운 과정에서 파드 프라이어리티를 고려하는 기능은 쿠버네티스 v1.23에서 알파 기능으로 도입되었다. 쿠버네티스 1.26에서 이 기능은 베타 상태이며 기본적으로 활성화되어 있다.\ngraceful_shutdown_start_time_seconds 및 graceful_shutdown_end_time_seconds 메트릭은 노드 셧다운을 모니터링하기 위해 kubelet 서브시스템에서 방출된다.\n스왑(swap) 메모리 관리 기능 상태: Kubernetes v1.22 [alpha]\n쿠버네티스 1.22 이전에는 노드가 스왑 메모리를 지원하지 않았다. 그리고 kubelet은 노드에서 스왑을 발견하지 못한 경우 시작과 동시에 실패하도록 되어 있었다. 1.22부터는 스왑 메모리 지원을 노드 단위로 활성화할 수 있다.\n노드에서 스왑을 활성화하려면, NodeSwap 기능 게이트가 kubelet에서 활성화되어야 하며, 명령줄 플래그 --fail-swap-on 또는 구성 설정에서 failSwapOn가 false로 지정되어야 한다.\n경고: 메모리 스왑 기능이 활성화되면, 시크릿 오브젝트의 내용과 같은 tmpfs에 기록되었던 쿠버네티스 데이터가 디스크에 스왑될 수 있다.\n사용자는 또한 선택적으로 memorySwap.swapBehavior를 구성할 수 있으며, 이를 통해 노드가 스왑 메모리를 사용하는 방식을 명시한다.\n1 2 memorySwap: swapBehavior: LimitedSwap` swapBehavior에 가용한 구성 옵션은 다음과 같다.\nLimitedSwap: 쿠버네티스 워크로드는 스왑을 사용할 수 있는 만큼으로 제한된다. 쿠버네티스에 의해 관리되지 않는 노드의 워크로드는 여전히 스왑될 수 있다. UnlimitedSwap: 쿠버네티스 워크로드는 요청한 만큼 스왑 메모리를 사용할 수 있으며, 시스템의 최대치까지 사용 가능하다. 만약 memorySwap 구성이 명시되지 않았고 기능 게이트가 활성화되어 있다면, kubelet은 LimitedSwap 설정과 같은 행동을 기본적으로 적용한다.\nLimitedSwap 설정에 대한 행동은 노드가 (\u0026ldquo;cgroups\u0026quot;으로 알려진) 제어 그룹이 v1 또는 v2 중에서 무엇으로 동작하는가에 따라서 결정된다.\ncgroupsv1: 쿠버네티스 워크로드는 메모리와 스왑의 조합을 사용할 수 있다. 파드의 메모리 제한이 설정되어 있다면 가용 상한이 된다. cgroupsv2: 쿠버네티스 워크로드는 스왑 메모리를 사용할 수 없다. 테스트를 지원하고 피드벡을 제공하기 위한 정보는 KEP-2400 및 디자인 제안에서 찾을 수 있다.\n","date":"2023-04-21T14:00:47+09:00","image":"https://codemario318.github.io/post/study/kubernetes/5/kubernetes_cover_hu15504362c54454204bbf321d3bfe3873_12586_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/study/kubernetes/5/","title":"쿠버네티스: 5. 클러스터 아키텍처"},{"content":"관리기법 ⛔ 쿠버네티스 오브젝트는 하나의 기법만 사용하여 관리해야 한다. 동일한 오브젝트에 대해 여러 기법을 혼용하면 오동작이 발생할 수 있다.\n관리기법 대상 권장 환경 지원하는 작업자 수 학습 난이도 명령형 커맨드 활성 오브젝트 개발 환경 1+ 낮음 명령형 오브젝트 구성 개별 파일 프로덕션 환경 1 보통 선언형 오브젝트 구성 파일이 있는 디렉터리 프로덕션 환경 1+ 높음 명령형 커맨드 사용자가 클러스터 내 활성 오브젝트를 대상으로 직접 동작시킨다. 실행할 작업을 인수 또는 플래그로 kubectl 커맨드에 지정한다.\n1 kubectl create deployment nginx --image nginx 트레이드 오프 장점\n커맨드는 하나의 동작을 나타내는 단어로 표현됨 클러스터를 수정하기 위해 단 하나의 단계만을 필요로 한다. 단점\n커맨드는 변경 검토 프로세스와 통합되지 않는다. 변경에 관한 감사 추적(audit trail)을 제공하지 않는다. 활성 동작 중인 경우를 제외하고는 레코드의 소스를 제공하지 않는다. 새로운 오브젝트 생성을 위한 템플릿을 제공하지 않는다. 명령형 오브젝트 구성 kubectl 커맨드로 작업, 선택적 플래그, 파일 이름을 지정하여 실행한다.\n","date":"2023-04-21T13:57:47+09:00","image":"https://codemario318.github.io/post/study/kubernetes/4/kubernetes_cover_hu15504362c54454204bbf321d3bfe3873_12586_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/study/kubernetes/4/","title":"쿠버네티스: 4. 쿠버네티스 오브젝트 관리"},{"content":" 쿠버네티스 오브젝트는 하나의 의도를 담은 레코드이다.\n쿠버네티스 오브젝트는 쿠버네티스 시스템에서 영속성을 가지는 오브젝트이다. 쿠버네티스는 클러스터의 상태를 나타내기 위해 .yaml로 작성된 오브젝트에 상세 내용을 기술한다.\n어떤 컨테이너화된 애플리케이션이 동작 중인지(어느 노드에서 동작중인지) 해당 어플리케이션이 이용할 수 있는 리소스 해당 애플리케이션이 어떻게 재구동 정책, 업그레이드 내고장성과 같은 것에 동작해야 하는지에 대한 정책 오브젝트를 생성하게 되면, 쿠버네티스 시스템은 오브젝트에 담긴 상태를 보장하기 위해 지속적으로 동작하게 된다.\n오브젝트 명세(spec)과 상태(status) 거의 모든 쿠버네티스 오브젝트는 spec 오브젝트와 status 오브젝트로 구성된다.\nspec spec을 가지는 오브젝트는 오브젝트를 생성할 때 리소스에 원하는 특징(의도한 상태)에 대한 설명을 제공하여 설정한다.\nstatus 쿠버네티스 시스템과 컴포넌트에 의해 제공되고 업데이트된 오브젝트의 현재 상태를 설명한다. 컨트롤 플레인은 모든 오브젝트의 실제 상태를 사용자가 의도한 상태와 일치시키기 위해서 끊임없이 능독적으로 관리한다.\nhttps://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md\n쿠버네티스 오브젝트 기술하기 쿠버네티스에서 오브젝트를 생성할 때, 오브젝트에 대한 기본적인 정보와 더불어, 의도한 상태를 기술한 오브젝트 spec을 제시해야 한다.\n대부분의 경우 정보를 .yaml 파일로 kubectl에 제공한다. kubectl은 API 요청이 이루어질 때, JSON 형식으로 정보를 변환시켜준다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 요구되는 필드 생성하고자 하는 쿠버네티스 오브젝트에 대한 .yaml 파일 내, 다음 필드를 윟나 값들을 설정해 줘야한다.\napiVersion: 해당 오브젝트를 생성하기 위해 사용하고 있는 쿠버네티스 API 버전 kind: 오브젝트의 종류 metadata: 오브젝트를 유일하게 구분지어 줄 데이터(이름 문자열, UID, 선택적인 네임스페이스 등) spec에 대한 정확한 포맷은 모든 쿠버네티스 오브젝트마다다르고, 그 오브젝트 특유의 중첩된 필드를 포함한다.\nhttps://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/\n","date":"2023-04-21T13:53:47+09:00","image":"https://codemario318.github.io/post/study/kubernetes/3/kubernetes_cover_hu15504362c54454204bbf321d3bfe3873_12586_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/study/kubernetes/3/","title":"쿠버네티스: 3. 쿠버네티스 오브젝트 이해하기"},{"content":"쿠버네티스 클러스터 컨테이너화 된 애프릴케이션을 실행하는 노드라고 하는 워커 머신의 집합으로, 모든 클러스터는 최소 한 개의 워커 노드를 가진다.\n워커 노드는 애플리케이션의 구성요소인 파드를 호스트하며, 컨트롤 플레인은 워커 노드와 클러스터 내 파드를 관리한다.\n프로덕션 환경에서는 일반적으로 컨트롤 플레인이 여러 컴퓨터에 걸쳐 실행되고, 클러스터는 일반적으로 여러 노드를 실행하므로 내결함성과 고가용성이 제공된다.\n컨트롤 플레인 컴포넌트 컨트롤 플레인 컴포넌트는 클러스터에 관한 전반적인 결정 (스케줄링 등)을 수행하고 클러스터 이벤트를 감지하고 반응한다.\n컨트롤 플레인 컴포넌트는 클러스터 내 어떠한 머신에서도 동작할 수 있으나, 간결성 유지를 위해 구성 스크립트는 보통 동일 머신 상에 모든 컨트롤 플레인 컴포넌트를 구동시키고, 사용자 컨테이너는 해당 머신 상에 동작시키지 않는다.\nkube-apiserver API 서버는 쿠버네티스 API를 노출하는 쿠버네티스 컨트롤 플레인 컴포넌트이다. API 서버는 쿠버네티스 컨트롤 플레인의 프론트 엔드이다.\n쿠버네티스 API서버는 정말 그냥 API서버라구욧\nkube-apiserver는 마스터 노드의 중심에서 모든 클라이언트, 컴포넌트로 부터 오는 요청들을 받는 REST API 서버이다.\nkubectl와 Kubernetes SDK를 이용할 수 있지만, 직접 REST API 호출로 쿠버네티스와 통신할 수 있다.\n쿠버네티스 API를 사용하여 애플리케이션을 작성하는 경우 클라이언트 라이브러리 중 하나를 사용하는 것이 좋다. https://kubernetes.io/ko/docs/reference/using-api/client-libraries/\netcd 모든 클러스터 데이터를 담는 쿠버네티스 뒷단의 저장소로 사용되는 일관성 및 고가용성 키-값 저장소.\nKubernetes 운영을 위한 etcd 기본 동작 원리의 이해\nkubernetes는 기반 스토리지(backing storage)로 etcd를 사용하고 있고, 모든 데이터가 etcd에 보관된다(클러스터에 어떤 노드가 몇 개나 있고 어떤 파드가 어떤 노드에서 동작하고 있는지등 ). 동작중인 클러스터의 etcd 데이터베이스가 유실된다면 컨테이너 뿐만 아니라 클러스터가 사용하는 모든 리소스는 미아가 된다.\n따라서 쿠버네티스 클러스터에서 etcd를 뒷단 저정소로 사용한다면, 이 데이터를 백업하는 계획을 필수적으로 갖추어야 한다.\nkube-scheduler 노드가 배정되지 않은 새로 생성된 파드를 감지하고, 실행할 노드를 선택하는 컨트롤 플레인 컴포넌트.\n아래 항목들에 대해 고려하여 스케줄링이 이루어진다.\n리소스에 대한 개별 및 총체적 요구 사항 하드웨어/소프트웨어/정책적 제약 어피니티(affinity) 및 안티-어피니티(anti-affinity) 명세 데이터 지역성 워크로드간 간섭 데드라인 kube-controller-manager 컨트롤러 프로세스를 실행하는 컨트롤 플레인 컴포넌트.\n각 컨트롤러는 논리적으로 분리된 프로세스이지만, 복잡성을 낮추기 위해 모두 단일 바이너리로 컴파일되고 단일 프로세스 내에서 실행된다.\n노드 컨트롤러: 노드가 다운되었을 때 통지와 대응에 관한 책임을 가진다. 잡 컨트롤러: 일회성 작업을 나타내는 잡 오브젝트를 감시한 다음, 해당 작업을 완료할 때까지 동작하는 파드를 생성한다. 엔드포인트 컨트롤러: 엔드포인트 오브젝트를 채운다.(서비스와 파드를 연결시킨다.) 서비스 어카운트 \u0026amp; 토큰 컨트롤러: 새로운 네임스페이스에 대한 기본 계정과 API 접근 토큰을 생성한다. cloud-controller-manager 클라우드 별 컨트롤 로직을 포함하는 쿠버네티스 컨트롤 플레인 컴포넌트이다.\n클라우드 컨트롤러 매니저를 통해 클러스터를 클라우드 공급자의 API에 연결하고, 해당 클라우드 플랫폼과 상호 작용하는 컴포넌트와 클러스터와만 상호 작용하는 컴포넌트를 구분할 수 있게 해준다.\n클라우드 제공자 전용 컨트롤러만 실행한다. 자신의 사내 또는 PC 내부의 학습 환경에서 쿠버네티스를 실행 중인 경우 클러스터에는 클라우드 컨트롤러 매니저가 없다.\nkube-controller-manager와 마찬가지로 cloud-controller-manager는 논리적으로 독립적인 여러 컨트롤 루프를 단일 프로세스로 실행하는 단일 바이너리로 결합한다. 수평으로 확장(두 개 이상의 복제 실행)해서 성능을 향상시키거나 장애를 견딜 수 있다.\n노드 컨트롤러: 노드가 응답을 멈춘 후 클라우드 상에서 삭제되었는지 판별하기 위해 클라우드 제공 사업자에게 확인하는 것 라우트 컨트롤러: 기본 클라우드 인프라에 경로를 구성하는 것 서비스 컨트롤러: 클라우드 제공 사업자 로드밸런서를 생성, 업데이트 그리고 삭제하는 것 노드 컴포넌트 노드 컴포넌트는 동작 중인 파드를 유지시키고 쿠버네티스 런타임 환경을 제공하며, 모든 노드 상에서 동작한다.\nkubelet 클러스터의 각 노드에서 실행되는 에이전트. kubelet은 파드에서 컨테이너가 확실하게 동작하도록 관리한다.\n다양한 메커니즘을 통해 제공된 파드 스펙(PodSpec)의 집합을 받아 컨테이너가 해당 파드 스펙에 따라 건강하게 동작하는 것을 확실히 한다.\nkubelet은 쿠버네티스를 통해 생성되지 않는 컨테이너는 관리하지 않는다.\nkube-proxy 클러스터의 각 노드에서 실행되는 네트워크 프록시로, 크버네티스의 서비스 개념 구현부이다.\n노드의 네트워크 규칙을 관리한다. 네트워크 규칙이 내부 네트워크 세션이나 클러스터 바깥에서 파드로 네트워크 통신을 하도록 해준다.\n운영체제에 가용한 패킷 필터링 계층이 있는 경우, 이를 사용한다. 그렇지 않으면, kube-proxy는 트래픽 자체를 포워드한다.\n컨테이너 런타임 컨테이너 런타임은 컨테이너 실행을 담당하는 소프트웨어이다.\n쿠버네티스는 containerd, CRI-O와 같은 컨테이너 런타임 및 모든 Kubernetes CRI(컨테이너 런타임 인터페이스) 구현체를 지원한다.\nCRI란?\n클러스터 컴포넌트를 다시 컴파일하지 않아도 Kubelet이 다양한 컨테이너 런타임을 사용할 수 있도록 하는 플러그인 인터페이스이다.\n","date":"2023-04-21T13:42:47+09:00","image":"https://codemario318.github.io/post/study/kubernetes/2/kubernetes_cover_hu15504362c54454204bbf321d3bfe3873_12586_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/study/kubernetes/2/","title":"쿠버네티스: 2. 쿠버네티스 컴포넌트"},{"content":"쿠버네티스란? 쿠버네티스는 컨테이너화된 워크로드와 서비스를 관리하기 위한 이식성이 있고, 확장 가능한 오픈소스 플랫폼이다.\n컨테이너화된 워크로드와 서비스를 관리하기 위한 이식성이 있다. 확장가능한 오픈소스 플랫폼이다. 선언적 구성과 자동화를 모두 용이하게 해준다. 크고 빠르게 성장하는 생태계를 가지고 있다. 쿠버네티스 서비서, 기술 지원 및 도구는 어디서나 쉽게 이용할 수 있다. 컨테이너 장점\n기민한 애플리케이션 생성과 배포\nVM 이미지를 사용하는 것에 비해 컨테이너 이미지 생성이 보다 쉽고 효율적임. 지속적인 개발, 통합 및 배포\n안정적이고 주기적으로 컨테이너 이미지를 빌드해서 배포할 수 있고 (이미지의 불변성 덕에) 빠르고 효율적으로 롤백할 수 있다. 개발과 운영의 관심사 분리\n배포 시점이 아닌 빌드/릴리스 시점에 애플리케이션 컨테이너 이미지를 만들기 때문에, 애플리케이션이 인프라스트럭처에서 분리된다. 가시성(observability)\nOS 수준의 정보와 메트릭에 머무르지 않고, 애플리케이션의 헬스와 그 밖의 시그널을 볼 수 있다. 개발, 테스팅 및 운영 환경에 걸친 일관성\n랩탑에서도 클라우드에서와 동일하게 구동된다. 클라우드 및 OS 배포판 간 이식성\nUbuntu, RHEL, CoreOS, 온-프레미스, 주요 퍼블릭 클라우드와 어디에서든 구동된다. 애플리케이션 중심 관리\n가상 하드웨어 상에서 OS를 실행하는 수준에서 논리적인 리소스를 사용하는 OS 상에서 애플리케이션을 실행하는 수준으로 추상화 수준이 높아진다. 느슨하게 커플되고, 분산되고, 유연하며, 자유로운 마이크로서비스 애플리케이션은 단일 목적의 머신에서 모놀리식 스택으로 구동되지 않고 보다 작고 독립적인 단위로 쪼개져서 동적으로 배포되고 관리될 수 있다. 리소스 격리\n애플리케이션 성능을 예측할 수 있다. 리소스 사용량\n고효율 고집적. 쿠버네티스가 왜 필요하고 무엇을 할 수 있나 기존 \u0026ldquo;전통적인 배포\u0026rdquo;, \u0026ldquo;가상화된 배포\u0026quot;를 거치며 \u0026ldquo;컨테이너를 통한 배포\u0026rdquo; 까지 발전해왔다.\n컨테이너를 통한 개발 환경은 애플리케이션을 포장하고 실행하는 좋은 방법이지만, 프로덕션 환경에서는 애플리케이션을 실행하는 컨테이너를 관리하고 가동 중지 시간이 없는지 확인해야 하는 등 여러 작업이 필요하게 된다.\n쿠버네티스는 분산 시스템을 탄력적으로 실행하기 위한 프레임 워크를 제공한다. 애플리케이션의 확장과 장애 조치를 처리하고, 배포 패턴 등을 제공한다. 예를 들어, 쿠버네티스는 시스템의 카나리아 배포를 쉽게 관리 할 수 있다.\n서비스 디스커버리와 로드 밸런싱 쿠버네티스는 DNS 이름을 사용하거나 자체 IP 주소를 사용하여 컨테이너를 노출할 수 있다. 컨테이너에 대한 트래픽이 많으면, 쿠버네티스는 네트워크 트래픽을 로드밸런싱하고 배포하여 배포가 안정적으로 이루어질 수 있다.\n스토리지 오케스트레이션 쿠버네티스를 사용하면 로컬 저장소, 공용 클라우드 공급자 등과 같이 원하는 저장소 시스템을 자동으로 탑재 할 수 있다.\n자동화된 롤아웃과 롤백 쿠버네티스를 사용하여 배포된 컨테이너의 원하는 상태를 서술할 수 있으며 현재 상태를 원하는 상태로 설정한 속도에 따라 변경할 수 있다. 예를 들어 쿠버네티스를 자동화해서 배포용 새 컨테이너를 만들고, 기존 컨테이너를 제거하고, 모든 리소스를 새 컨테이너에 적용할 수 있다.\n자동화된 빈 패킹(bin packing) 컨테이너화된 작업을 실행하는데 사용할 수 있는 쿠버네티스 클러스터 노드를 제공한다. 각 컨테이너가 필요로 하는 CPU와 메모리(RAM)를 쿠버네티스에게 지시한다. 쿠버네티스는 컨테이너를 노드에 맞추어서 리소스를 가장 잘 사용할 수 있도록 해준다.\n자동화된 복구(self-healing) 쿠버네티스는 실패한 컨테이너를 다시 시작하고, 컨테이너를 교체하며, \u0026lsquo;사용자 정의 상태 검사\u0026rsquo;에 응답하지 않는 컨테이너를 죽이고, 서비스 준비가 끝날 때까지 그러한 과정을 클라이언트에 보여주지 않는다.\n시크릿과 구성 관리 쿠버네티스를 사용하면 암호, OAuth 토큰 및 SSH 키와 같은 중요한 정보를 저장하고 관리 할 수 있다. 컨테이너 이미지를 재구성하지 않고 스택 구성에 시크릿을 노출하지 않고도 시크릿 및 애플리케이션 구성을 배포 및 업데이트 할 수 있다.\n","date":"2023-04-21T13:19:47+09:00","image":"https://codemario318.github.io/post/study/kubernetes/1/kubernetes_cover_hu15504362c54454204bbf321d3bfe3873_12586_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/study/kubernetes/1/","title":"쿠버네티스: 1. 쿠버네티스란 무엇인가?"},{"content":"JUnit 프레임워크 ComparisonCompactor.java 모듈 개선하기 생성자 두 번째 인수와 세 번째 인수를 비교한다. comapct 함수에 문자열을 넣으면 결과 메시지 앞에 문자열이 추가된다. 비교 후 다른 문자열을 “[]”를 써서 강조한다. 첫 번째 인수는 다른 문자열을 출력할 때 “[]” 앞 뒤로 출력할 문자열 개수이다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 package junit.framework; public class ComparisonCompactor { private static final String ELLIPSIS = \u0026#34;...\u0026#34;; private static final String DELTA_END = \u0026#34;]\u0026#34;; private static final String DELTA_START = \u0026#34;[\u0026#34;; private int fContextLength; private String fExpected; private String fActual; private int fPrefix; private int fSuffix; public ComparisonCompactor(int contextLength, String expected, String actual) { fContextLength = contextLength; fExpected = expected; fActual = actual; } public String compact(String message) { if (fExpected == null || fActual == null || areStringsEqual()) return Assert.format(message, fExpected, fActual); findCommonPrefix(); findCommonSuffix(); String expected = compactString(fExpected); String actual = compactString(fActual); return Assert.format(message, expected, actual); } private String compactString(String source) { String result = DELTA_START + source.substring(fPrefix, source.length() - fSuffix + 1) + DELTA_END; if (fPrefix \u0026gt; 0) result = computeCommonPrefix() + result; if (fSuffix \u0026gt; 0) result = result + computeCommonSuffix(); return result; } private void findCommonPrefix() { fPrefix = 0; int end = Math.min(fExpected.length(), fActual.length()); for (; fPrefix \u0026lt; end; fPrefix++) { if (fExpected.charAt(fPrefix) != fActual.charAt(fPrefix)) break; } } private void findCommonSuffix() { int expectedSuffix = fExpected.length() - 1; int actualSuffix = fActual.length() - 1; for (; actualSuffix \u0026gt;= fPrefix \u0026amp;\u0026amp; expectedSuffix \u0026gt;= fPrefix; actualSuffix--, expectedSuffix--) { if (fExpected.charAt(expectedSuffix) != fActual.charAt(actualSuffix)) break; } fSuffix = fExpected.length() - expectedSuffix; } private String computeCommonPrefix() { return (fPrefix \u0026gt; fContextLength ? ELLIPSIS : \u0026#34;\u0026#34;) + fExpected.substring(Math.max(0, fPrefix - fContextLength), fPrefix); } private String computeCommonSuffix() { int end = Math.min(fExpected.length() - fSuffix + 1 + fContextLength, fExpected.length()); return fExpected.substring(fExpected.length() - fSuffix + 1, end) + (fExpected.length() - fSuffix + 1 \u0026lt; fExpected.length() - fContextLength ? ELLIPSIS : \u0026#34;\u0026#34;); } private boolean areStringsEqual() { return fExpected.equals(fActual); } } 멤버 변수 앞에 붙인 접두어 오늘날 사용하는 개발 환경에서는 변수 이름에 범위를 명시할 필요가 없다.\n접두어 f는 중복되는 정보다.\n1 2 3 4 5 private int contextLength; private String expected; private String actual; private int prefix; private int suffix; 캡슐화되지 않은 조건문 1 2 3 4 5 6 7 8 9 10 11 public String compact(String message) { if (expected == null || actual == null || areStringsEqual()) { return Assert.format(message, expected, actual); } findCommonPrefix(); findCommonSuffix(); String expected = compactString(this.expected); String actual = compactString(this.actual); return Assert.format(message, expected, actual); } 의도를 명확히 표현하려면 조건문을 캡슐화 하는것이 좋다. 조건문을 메서드로 뽑아내 적절한 이름을 붙인다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public String compact(String message) { if (**shouldNotCompact**()) { return Assert.format(message, expected, actual); } findCommonPrefix(); findCommonSuffix(); String expected = compactString(this.expected); String actual = compactString(this.actual); return Assert.format(message, expected, actual); } private boolean **shouldNotCompact**() { return expected == null || actual == null || areStringsEqual(); } 중복되는 변수 이름 compact 함수에서 사용하는 this.expected와 this.acutal같은 경우 기존 함수에 expected, actual 이라는 지역 변수가 있었는데, 클래스 변수 fExpected, fAcutal에서 f를 제거하여 같은 이름을 사용하게 되었다.\n함수에서 멤버 변수와 이름이 같은 변수를 사용하는 이유를 파악하고 더 서술적(구체적)인 이름으로 바꾼다.\n1 2 String compactExpected = compactString(expected); String compactActual = compactString(actual); 조건문의 부정문 사용 부정문은 긍정문보다 이해하기 약간 더 어렵다. 첫 문장 if를 긍정으로 만들어 조건문을 반전한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public String compact(String message) { if (**canBeCompacted**()) { return Assert.format(message, expected, actual); } findCommonPrefix(); findCommonSuffix(); String expected = compactString(this.expected); String actual = compactString(this.actual); return Assert.format(message, expected, actual); } private boolean **canBeCompacted**() { return expected **!=** null || actual **!=** null || **!**areStringsEqual(); } 동작, 의미를 잘 표현할 수 있는 이름을 사용 compact 함수는 문자열을 앞축하라는 의미가 강하지만 실제로 canBeCompacted가 false면 압축하지 않는다. compact라는 이름을 붙이면 오류 점검이라는 부가 단계(기능)이 숨겨진다. 함수는 단순히 압축된 문자열이 아니라 형식이 갖춰진 문자열을 반환한다. 실제로는 formatCompatedComparison이라는 이름이 적합하다.\n새 이름에 인수를 고려하면 가독성이 훨신 좋아진다.\n1 2 3 public String formatCompactedComparison(String message) { ... } 함수가 한 가지 책임만 가지게 if 문 안에서는 예상 문자열과 실제 문자열을 진짜로 압축한다. 이 부분을 빼내 compactExpectedAndActual이라는 메서드로 만든다.\n형식을 맞추는 작업은 formatCompactedComparison에게 전적으로 맡긴다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ... private String compactExpected; private String compactActual; ... public String formatCompactedComparison(String message) { if (canBeCompacted()) { **compactExpectedAndActual**(); return Assert.format(message, compactExpected, compactActual); } else { return Assert.format(message, expected, actual); } } private **compactExpectedAndActual**() { findCommonPrefix(); findCommonSuffix(); compactExpected = compactString(expected); compactActual = compactString(actual); } compactExpected, compactActual를 멤버 변수로 승격했다.\n사용 방식 일관성 새 함수compactExpectedAndActual에서 마지막 두 줄은 변수를 반환하지만 첫째 줄과 둘째줄은 반환값이 없다. 멤버변수 함수 사용방식이 일관적이지 못하다.\nfindCommonPrefix, findCommonSuffix는 함수 내부에서 멤버변수 prefix, suffix에 값을 할당하지만, compactString은 반환값을 만들게 되고, 멤버 변수에 반환값을 할당한다. findCommonPrefix, findCommonSuffix 를 변경하여 접두어 값과 접미어 값의 위치를 반환하게 바꾼다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 private void compactExpectedAndActual() { prefixIndex = findCommonPrefix(); suffixIndex = findCommonSuffix(); compactExpected = compactString(expected); compactActual = compactString(actual); } private int findCommonPrefix() { int prefixIndex = 0; int end = Math.min(expected.length(), actual.length()); for (; prefixIndex \u0026lt; end; prefixIndex++) { if (expected.charAt(prefixIndex) != actual.charAt(prefixIndex)) break; } return prefixIndex; } private int findCommonSuffix() { int expectedSuffix = expected.length() - 1; int actualSuffix = actual.length() - 1; for (; actualSuffix \u0026gt;= prefixIndex \u0026amp;\u0026amp; expectedSuffix \u0026gt;= prefixIndex; actualSuffix--, expectedSuffix--) { if (expected.charAt(expectedSuffix) != actual.charAt(actualSuffix)) break; } return expected.length() - expectedSuffix; } 숨겨진 시간적인 결합을 노출 findCommonSuffix 를 주의 깊게 살펴보면 숨겨진 시간적인 결함이 존재한다. findCommonSuffix는 findCommonPrefix가 prefixIndex를 계산한다는 사실에 의존하게 된다.\n만약 findCommonPrefix와 findCommonSuffix를 잘못된 순서로 호출하면 문제를 찾기 어렵다.\n따라서 시간 결함을 외부에 노출하고자 findCommonSuffix를 고쳐 prefixIndex를 인수로 넘겼다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 private void compactExpectedAndActual() { prefixIndex = findCommonPrefix(); suffixIndex = findCommonSuffix(prefixIndex); compactExpected = compactString(expected); compactActual = compactString(actual); } private int findCommonSuffix(int prefixIndex) { int expectedSuffix = expected.length() - 1; int actualSuffix = actual.length() - 1; for (; actualSuffix \u0026gt;= prefixIndex \u0026amp;\u0026amp; expectedSuffix \u0026gt;= prefixIndex; actualSuffix--, expectedSuffix--) { if (expected.charAt(expectedSuffix) != actual.charAt(actualSuffix)) break; } return expected.length() - expectedSuffix; } prefixIndex를 인수로 전달하는 방식은 함수 호출 순서를 확실히 명시할 수 있지만, prefixIndex가 필요한 이유를 명확히 설명하지 못한다.\n필요한 이유가 분명히 드러나지 않으므로 다른 개발자가 원래대로 되돌려놓을지도 모른다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 private void compactExpectedAndActual() { **findCommonPrefixAndSuffix**(); compactExpected = compactString(expected); compactActual = compactString(actual); } private void **findCommonPrefixAndSuffix**() { **findCommonPrefix**(); int expectedSuffix = expected.length() - 1; int actualSuffix = actual.length() - 1; for (; actualSuffix \u0026gt;= prefixIndex \u0026amp;\u0026amp; expectedSuffix \u0026gt;= prefixIndex; actualSuffix--, expectedSuffix--) { if (expected.charAt(expectedSuffix) != actual.charAt(actualSuffix)) break; } suffixIndex = expected.length() - expectedSuffix; } private **void** findCommonPrefix() { prefixIndex = 0; int end = Math.min(expected.length(), actual.length()); for (; prefixIndex \u0026lt; end; prefixIndex++) if (expected.charAt(prefixIndex) != actual.charAt(prefixIndex)) break; } findCommonPrefix와 findCommonSuffix를 되돌리고, findCommonSuffix라는 이름을 findCommonPrefixAndSuffix로 바꾼 후 findCommonPrefixAndSuffix에서 가장 먼저 findComonPrefix를 호출한다.\n두 함수를 호출하는 순서가 앞서 고친 코드보다 훨씬 더 분명해진다.\n그리고 PrefixAndSuffix함수를 정리한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 private void findCommonPrefixAndSuffix() { findCommonPrefix(); int suffixLength = 1; for (; !suffixOverlapsPrefix(suffixLength); suffixLength++) { if (charFromEnd(expected, suffixLength) != charFromEnd(actual, suffixLength)) break; } suffixIndex = suffixLength; } private char charFromEnd(String s, int i) { return s.charAt(s.length() - i); } private boolean suffixOverlapsPrefix(int suffixLength) { return actual.length() - suffixLength \u0026lt; prefixLength || expected.length() - suffixLength \u0026lt; prefixLength; } 적절한 이름으로 바꾸기 코드를 정리하니 suffixIndex가 index가 아닌 실제로는 접미어 길이(length)를 의미한다. 이름이 적절치 않다. prefixIndex도 마찬가지이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 public class ComparisonCompactor { ... private int **suffixLength**; ... private void findCommonPrefixAndSuffix() { findCommonPrefix(); **suffixLength** = 0; for (; !suffixOverlapsPrefix(suffixLength); suffixLength++) { if (charFromEnd(expected, suffixLength) != charFromEnd(actual, suffixLength)) break; } } private char charFromEnd(String s, int i) { return s.charAt(s.length() - i - 1); } private boolean suffixOverlapsPrefix(int suffixLength) { return actual.length() - suffixLength \u0026lt;= prefixLength || expected.length() - suffixLength \u0026lt;= prefixLength; } ... private String compactString(String source) { String result = DELTA_START + source.substring(prefixLength, source.length() - **suffixLength**) + DELTA_END; if (prefixLength \u0026gt; 0) result = computeCommonPrefix() + result; if (**suffixLength** \u0026gt; 0) result = result + computeCommonSuffix(); return result; } ... private String computeCommonSuffix() { int end = Math.min(expected.length() - **suffixLength** + contextLength, expected.length()); return expected.substring(expected.length() - **suffixLength**, end) + (expected.length() - **suffixLength** \u0026lt; expected.length() - contextLength ? ELLIPSIS : \u0026#34;\u0026#34;); } } computeCommonSuffix에서 +1을 없애고 charFromEnd에 -1을 추가하고 suffixOverlapsPrefix에 ≤ 를 사용했다. 그 후 suffixLength로 바꿔 가독성을 크게 높힐 수 있었다.\n불필요한 코드 제거 +1를 제거하던 중 compactString에서 다음 행을 발견했다.\n1 if (suffixlength \u0026gt; 0) suffixLength가 1씩 감소했으므로 당연히 연산자를 ≥로 고쳐야 하지만 지금 상황에서는 ≥는 나올 수 없다.\n코드를 좀 더 분석해보면 if 문은 길이가 0인 접미어를 걸러내어 suffixIndex가 언제나 1 이상이므로 if 문 자체가 필요 없다.\ncompactString에 있는 if문이 모두 필요 없어 보여 제거후 테스트를 하니 모두 통과했다. 불필요한 if문을 제거하여 더 깔끔하게 만든다.\n1 2 3 4 5 6 7 private String compactString(String source) { return computeCommonPrefix() + DELTA_START + source.substring(prefixLength, source.length() - suffixLength) + DELTA_END + computeCommonSuffix(); } 최종 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 package junit.framework; public class ComparisonCompactor { private static final String ELLIPSIS = \u0026#34;...\u0026#34;; private static final String DELTA_END = \u0026#34;]\u0026#34;; private static final String DELTA_START = \u0026#34;[\u0026#34;; private int contextLength; private String expected; private String actual; private int prefixLength; private int suffixLength; public ComparisonCompactor( int contextLength, String expected, String actual) { this.contextLength = contextLength; this.expected = expected; this.actual = actual; } public String formatCompactedComparison(String message) { String compactExpected = expected; String compactActual = actual; if (shouldBeCompacted()) { findCommonPrefixAndSuffix(); compactExpected = compact(expected); compactActual = compact(actual); } return Assert.format(message, compactExpected, compactActual); } private boolean shouldBeCompacted() { return !shouldNotBeCompacted(); } private boolean shouldNotBeCompacted() { return expected == null || actual == null || expected.equals(actual); } private void findCommonPrefixAndSuffix() { findCommonPrefix(); suffixLength = 0; for (; !suffixOverlapsPrefix(); suffixLength++) { if (charFromEnd(expected, suffixLength) != charFromEnd(actual, suffixLength)) break; } } private char charFromEnd(String s, int i) { return s.charAt(s.length() - i - 1); } private boolean suffixOverlapsPrefix() { return actual.length() - suffixLength \u0026lt;= prefixLength || expected.length() - suffixLength \u0026lt;= prefixLength; } private void findCommonPrefix() { prefixLength = 0; int end = Math.min(expected.length(), actual.length()); for (; prefixLength \u0026lt; end; prefixLength++) if (expected.charAt(prefixLength) != actual.charAt(prefixLength)) break; } private String compact(String s) { return new StringBuilder() .append(startingEllipsis()) .append(startingContext()) .append(DELTA_START) .append(delta(s)) .append(DELTA_END) .append(endingContext()) .append(endingEllipsis()) .toString(); } private String startingEllipsis() { return prefixLength \u0026gt; contextLength ? ELLIPSIS : \u0026#34;\u0026#34;; } private String startingContext() { int contextStart = Math.max(0, prefixLength - contextLength); int contextEnd = prefixLength; return expected.substring(contextStart, contextEnd); } private String delta(String s) { int deltaStart = prefixLength; int deltaEnd = s.length() - suffixLength; return s.substring(deltaStart, deltaEnd); } private String endingContext() { int contextStart = expected.length() - suffixLength; int contextEnd = Math.min(contextStart + contextLength, expected.length()); return expected.substring(contextStart, contextEnd); } private String endingEllipsis() { return (suffixLength \u0026gt; contextLength ? ELLIPSIS : \u0026#34;\u0026#34;); } } 코드를 살펴보면 초반에 내렸던 결정 일부를 번복했다.\n처음 추출했던 메서드 몇 개를 formatCompactedComparison에다 도로 집어넣었다. shouldNotBeCompacted 조건을 되돌렸다. 코드를 리펙터링 하다보면 원래 했던 변경을 되돌리는 경우가 흔하다.\n리펙터링은 코드가 어느 수준에 이를때까지 수많은 시행착오를 반복하는 작업이다.\n","date":"2023-04-21T11:11:13+09:00","image":"https://codemario318.github.io/post/clean-code/15/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean-code/15/","title":"클린코드: 15. JUnit 들여다보기"},{"content":" Heruistics(발견법): 경험, 학습 등으로 습득한 직감을 이용해 현실적으로 만족할 만한 수준의 해답을 찾는 것\n리펙토링(마틴 파울러)에서 언급된 내용과 저자의 경험을 합쳐 깨끗하지 못한 코드를 발견하는 방법에 대하여 설명했다.\n아래에 해당된다면 깨끗한 코드인지 고민해서 확인해볼 필요가 있다.\n주석 C1: 부적절한 정보 주석은 코드와 설계에 기술적인 설명을 부연하는 수단이다.\n다른 시스템에 저장할 정보는 주석으로 적절하지 못하다.\n소스 코드 관리 시스템 버그 추적 시스템 이슈 추적 시스템 기타 기록 관리 프로그램 등 예를 들어, 변경 이력은 과도한 정보로 코드를 번잡하게 만든다.\nC2: 쓸모없는 주석 쓸모 없는 주석은 일단 들어가고 나면 업데이트를 주저하기 때문에 변해가는 코드에서 쉽게 멀어지고 코드를 그릇된 방향으로 이끌 여지가 있다.\n오래된 주석, 엉뚱한 주석, 잘못된 주석은 더 이상 쓸모가 없다.\n주석은 빨리 낡는다. 따라서 쓸모 없어질 주석은 달지 않는 편이 좋다. 또한 쓸모 없어진 주석은 재빨리 삭제하는 편이 가장 좋다.\nC3: 중복된 주석 주석은 코드만으로 다하지 못하는 설명을 부연하는 역할을 해야한다.\n서술적인 이름 등 깨끗한 코드로 설명할 수 있는 내용으로 주석을 만들어 중복되는 설명을 만드는 주석은 사용하지 않는다.\nC4: 성의없는 주석 꼭 필요한 주석은 잘 작성해야할 의무가 있다.\n단어를 신중하게 선택한다. 문법과 구두점을 올바로 사용한다. 주절대지 않는다. 당연한 소리를 반복하지 않는다. 간결하고 명료하게 작성한다. C5: 주석 처리된 코드 주석 처리된 코드는 만들어지면 코드의 중요도나 의미와 관계없이, (얼마나 오래된 코드인지, 중요한 코드인지) 누군가가 사용할수도 있을 것이라는 생각 때문에삭제하기 꺼려진다.\n결국 이렇게 남겨진 코드는 읽는 사람을 혼동하게 하는 등 코드를 오염시킨다.\n주석으로 처리된 코드를 발견하면 즉각 지워라! 누군가 필요로 한다면 버전관리 시스템으로 확인할 수 있다.\n주석으로 처리된 코드는 대부분 당장 필요한 경우가 없어서 이전에 필요한 사람도 오랜 기간 후에 소수의 사람일 가능성이 높다. 따라서 주석으로 남겨 코드를 오염시켜 사람들을 혼동시키는 것 보다. 버전관리 시스템을 통해 필요한 사람들이 찾게 남기는 것이 더 올바른 선택이다.\n환경 E1: 여러 단계로 빌드해야 한다. 빌드는 간단히 한 단계로 끝나야 한다. 기타 시스템에 필요한 파일을 찾느라 여기저기 뒤적을 필요가 없어야 한다. 한 명령으로 전체를 체크아웃해서 한 명령으로 빌드할 수 있어야 한다.\nE2: 여러 단계로 테스트해야 한다. 테스트 자체가 귀찮은 행위이다. 따라서 간단하고 빠르고 결과가 명료해야 자주 자주 활용될 수 있고, 이를 통해 코드의 품질, 테스트를 유지할 수 있다.\n모든 단위테스트는 한 명령으로 돌려야 한다. IDE에서 버튼 하나로 모든 테스트를 돌린다면 가장 이상적이다. 아무리 열악한 환경이라도 셸에서 명령 하나로 가능해야 한다.\n함수 F1: 너무 많은 인수 함수에서 인수 개수는 적을수록 고민할 사항들이 적어진다. 따라서 가능하다면 인수가 없는게 좋고 적에 만드는 것이 좋다.\n인수가 4개 이상일 경우 인수가 정말로 필요한지 고민해봐야 한다.\nF2: 출력 인수 출력 인수: 함수 입력으로 사용된 인수가 출력이 되는 경우\n일반적으로 독자는 출력이 아닌 입력으로 생각한다. 함수에서 어떠한 상태를 변경해야 한다면, 변경 대상을 인수로 넣어 사용하는 것이 아니라, 객체의 상태를 표현하게 만들고 함수 내부에서 this 같은 키워드를 통해 객체의 상태를 변경해야 한다.\nF3: 플래그 인수 Boolean 인수는 함수가 여러 기능(참일때 이거 아닐때 이거)을 수행한다는 명백한 증거다. 플래그 인수는 혼란을 초래하므로 피해야한다.\nF4: 죽은 함수 아무도 호출하지 않는 함수는 삭제한다.\n일반 G1: 한 소스 파일에 여러 언어를 사용한다. 오늘날 프로그래밍 환경은 한 소스 파일 내에서 다양한 언어를 지원한다.\n이상적으로 소스 파일 하나에 언어 하나만 사용하는 방식이 가장 좋으나, 현실적으로 여러 언어가 불가피하다.\n하지만 소스 파일에서 언어 수와 범위를 최대한 줄이도록 애써야 한다.\nG2: 당연한 동작을 구현하지 않는다. 최소 놀람 원칙(The Principle of Least Surprise): \u0026ldquo;필요한 기능에 크나큰 깜짝 놀래킬만한 요소가 있다면 해당 기능을 다시 설계할 필요가 있을 수 있다”\n함수나 클래스는 다른 프로그래머가 당연하게 여길만한 동작과 기능을 제공해야 한다.\nG3: 경계를 올바르게 처리하지 않는다. 코드를 올바르게 처리하는것은 당연하지만 복잡하다는 것을 관과하고 직관에 의존하여 만든다. 이러한 과정에서 모든 경계와 구석진 곳에서 증명하지 않는다.\n부지런함을 대신할 지름길은 없다. 모든 경계조건, 모든 구석진 곳, 모든 예외는 우아하고 직관적인 알고리즘을 좌초시킬 암초다.\n스스로의 직관에 의존하지 않고, 모든 경계조건을 찾아내 테스트 하는 테스트 케이스를 만들어야 한다.\nG4: 안전 절차 무시. 실패하는 테스트 케이스를 제껴두고 나중으로 미루는 태도는 신용카드가 공짜 돈이라는 생각만큼 위험하다.\nG5: 중복 DRY(Don’t Reapeat Yourself): 익스트림 프로그래밍의 핵심 규칙중 하나로 선언한 후 “한 번, 단 한번만(Once, and only once)” 론 제프리스는 이 규칙을 “모든 테스트를 통과한다” 규칙 다음으로 중요하게 꼽았다.\n코드에서 중복을 발견할 때마다 추상화할 기회로 간주하라. 중복된 코드르 하위 루틴이나 다른 클래스로 분리하는 등 추상화로 중복을 정리하면 설계 언어의 어휘가 늘어난다.\nG6: 추상화 수준이 올바르지 못하다. 기초 클래스는 구현 정보를 몰라야 한다. (구현에 관계없이 의도한 결과만 받아오고, 이름을 통해 결과물을 잘 표현해야 한다.)\n추상화는 저차원 상세 개념에서 고차원 일반 개념을 분리한다.\n추상화로 개념을 분리할 때는 모든 저차원 개념을 파생 클래스에 넣고, 모든 고차원 개념은 기초 클래스에 넣는다.\n세부 구현과 관련한 상수, 변수, 유틸리티 함수는 기초 클래스에 넣으면 안된다. 1 2 3 4 5 6 7 public interface Stack { Object pop() throw EmptyException; void push(Object o) throws FullException; double percentFull(); class EmptyEception extends Exception {} class FullException extends Exception {} } percentFulll 함수는 추상화 수준이 올바르지 못함 꽉 찬 정도를 의미하는 결과물을 반환하는데, 경우에 따라서 (ex. 크기가 무한대) 꽉 찬 정도를 알아낼 수 없다. 그러므로 BoundedStack 같은 파생 인터페이스에 넣어야 마땅하다. stack의 크기가 논리적으로 무한한 경우 0을 반환하면 된다고 하지만 물리적으로 무한할 수 없다. 1 stack.percentFull() \u0026lt; 50.0; 위 코드의 경우 스택 크기를 확인할때 OutOfMemoryException 예외가 발생할 여지가 있다. 또한 0을 반환하면 거짓된 정보를 반환하게 된다. 일반적으로 stack을 구현할 때 크기를 저장할 변수를 만듦. push 할때 크기값을 증가시키므로 한계를 넘어서는 갯수를 저장할 때 예외가 발생시킬수 있음 percentFull을 구현한다고 하더라도, 무한한 경우를 검사해서 예외를 발생시킬 수 있음 G7: 기초 클래스가 파생 클래스에 의존한다. 개념을 기초 클래스와 파생 클래스로 나누는 가장 흔한 이유는 고차원 기초 클래스 개념을 저차원 파생 클래스 개념으로부터 분리해 독립성을 보장하기 위해서다.\n따라서 기초 클래스는 파생 클래스를 사용한다면 뭔가 문제가 있다는 말이다.\n일반적으로는 기초 클래스와 파생 클래스를 다른 파일로 배포하는 편이 좋다.\n독립적인 개별 컴포넌트 단위로 시스템을 배치할 수 있다. 컴포넌트를 변경한다면 해당 컴포넌트만 다시 배치하면 된다. 즉, 변경이 시스템에 미치는 영향이 아주 작아지므로 현장에서 시스템을 유지보수하기가 한결 수월하게 된다.\nG8: 과도한 정보 클래스나 모듈 인터페이스에 노출할 함수를 제한하여 필요한 기능만 활용하도록 한다.\n자료와 함수를 최대한 숨기고 필요한 기능만 깐깐하게 공개하여 결합도를 높힐 여지를 만들지 마라\n잘 정의된 모듈은 인터페이스가 아주 작다. 작은 인터페이스로도 많은 동작을 구현할 수 있다.\n반면 부식하게 정의된 모듈은 인터페이스가 구질구질하다. 그래서 간단한 동작 하나에도 온갖 인터페이스가 필요하다.\n잘 정의된 인터페이스는 많은 함수를 제공하지 않는다. 함수를 적게 제공하기 때문에 (결합도를 높힐 여지가 적으므로) 결합도가 자연스럽게 낮아진다.\n클래스가 제공하는 메서드 수는 작을수록 좋다. 함수가 아는 변수 수도 작을수록 좋다. 클래스에 들어있는 인스턴스 변수 수도 작을수록 좋다. G9: 죽은 코드 죽은 코드란 실행되지 않는 코드를 가르킨다. 죽은 코드를 발견하면 장례식을 치뤄줘라!\n불가능한 조건을 확인하는 if 문 thorw 문이 없는 try 문에서 catch 블록 아무도 호출하지 않는 유틸리티 함수 switch-case 문에서 불가능한 case 조건 등 죽은 코드는 설계가 변해도 제대로 수정되지 않기 때문에 새로운 규칙이나 표기법을 따르지 않아 일관성을 해치거나 레거시로 남게되고, 다른 코드들도 같이 오염시킬 여지를 준다.\nG10: 수직 분리 변수와 함수는 사용되는 위치에 가깝게 정의한다.\n지역 변수는 처음으로 사용하기 직전에 선언하며 수직으로 가까운 곳에 위치해야 한다. 비공개 함수는 처음으로 호출함 직후에 정의한다. G11: 일관성 부족 간단한 일관성만으로도 코드를 읽고 수정하기 쉬워진다.\n어떤 개념을 특정 방식으로 구현했다면 유사한 개념도 같은 방식으로 구현한다.\n최소 놀람 원칙에도 부합한다. 표기법은 신중하게 선택하고, 선택된 표기법을 준수해야 한다.\nG12: 잡동사니 비어 있는 기본 생성자 아무도 사용하지 않는 변수 아무도 호출하지 않는 함수 정보를 제공하지 못하는 주석 코드만 복잡하게 만들 뿐이므로 제거한다.\nG13: 인위적인 결합 함수, 상수, 변수를 선언할 때는 시간을 들여 올바른 위치를 고민한다.\n서로 무관한 개념을 인위적으로 결합하지 않는다.\nEnum 일반적인 enum은 특정 클래스에 속할 이유가 없다. enum이 클래스에 속하면 사용하는 코드가 특정 클래스를 알아야 한다. 범용 static 변수 일반적으로 인위적인 결합은 직접적인 상호작용이 없는 두 모듈 사이에서 일어난다. 뚜렷한 목적이 없이 변수, 상수, 함수를 당장 편한 위치에(잘못된) 넣어버려 발생한다.\nG14: 기능 욕심 기능 욕심은 한 클래스의 내부를 다른 클래스에 노출하게 되므로, 별다른 문제가 없다면, 제거하는 것이 좋다.\n클래스 메서드는 자기 클래스의 변수와 함수에 관심을 가져야지 다른 클래스의 변수와 함수에 관심을 가져서는 안된다.\n메서드가 다른 객체의 참조자와 변경자를 사용해 그 객체 내용을 조작한다면 메서드가 그 객체 클래스의 범위를 욕심내는 탓이다.\n1 2 3 4 5 6 7 8 9 10 11 public class HourlyPayCalculator { public Money calculateWeeklyPay(HourlyEmployee e) { int tenthRate = e.getTenthRate().getPennies(); int tenthsWorked = e.getTenthsWorked(); int straightTime = Math.min(400, tenthsWorked); int overTime = Math.max(0, tenthsWorked - straightTime); int straightPay = straightTime * tenthRate; int overtimePay = (int)Math.round(overTime * tenthRate * 1.5); return new Money(straightPay + overtimePay); } } calculateWeeklyPay 메서드가 HourlyEmployee 객체에서 많은 정보를 가져와 처리한다. calculateWeeklyPay 메서드는 HourlyEmployee 클래스의 범위를 욕심낸다고 볼 수 있다. 자신이 HourlyEmployee 클래스에 속하는 것 처럼 구현되어 있다.\nHourlyEmployee 클래스에서 적어도 tenthRate, straightTime, overTime을 반환하는 것이 적합해보임\nG15: 선택자 인수 선택자 인수는 목적을 기억하기 어려울 뿐 아니라 각 선택자 인수가 여러 함수를 하나로 조합한다. 선택자 인수는 큰 함수를 작은 함수 여럿으로 쪼개지 않으려는 게으름의 소산이다.\nflag 변수들을 의미한다.\n부울 인수, enum, int 등 함수 동작을 제어하려는(분기하려는) 인수는 하나같이 바람직 하지 않다. 일반적으로 인수를 넘겨 동작을 선택하는 대신 새로운 함수를 만드는 편이 좋다.\n💡 유사한 개념을 통합해 코드를 줄이기 위해 사용되는 경우를 종종 봐왔는데, 이런 경우 유사한 개념을 분리하고 각각 별도 구현하여 분리한 기능을 호출하여 사용되는 것이 올바른 선택인 것 같다.\nG16: 모호한 의도 코드를 짤 때는 의도를 최대한 분명히 밝혀야 한다.\n행을 바꾸지 않고 표현한 수식 헝가리식 표기법(변수 및 함수 인자 이름 앞에 데이터 타입을 명시하는 코딩 규칙) 매직 넘버 등 독자에게 의도를 분명히 표현하도록 시간을 투자해야한다.\nG17: 잘못 지운(전가한) 책임 (MisPlaced responsibility) 최소 놀람 원칙을 적용하여 기능을 적절한 위치에 배치해야 한다.\n독자에게 직관적인 위치가 아니라 개발자에게 편한 함수에 배치하는 것을 비꼬고있다.\n다른 위치에 배치될 경우 함수 이름을 잘 지어 확실하게 표현해야한다.\nG18: 부적절한 static 함수 반드시 static 함수로 정의해야겠다면 오버라이딩할 가능성이 없는지 꼼꼼히 살핀다.\nstatic 함수는 오버라이딩 할 가능성이 없어야 한다.\n1 HourlyPayCalculator.calculatePay(employee, overtimeRate); 위 메서드는 특정 객체와 관련이 없으면서 모든 정보를 인수에서 가져오기 때문에 static 함수로 정의해도 괜찮아 보이지만, 수당을 계산하는 기능이 여러개로 분리될 가능성이 있어(오버라이딩 될 수 있어) 적합하지 않다. 따라서 Employee 클래스에 속하는 인스턴스 함수여야 한다.\n일반적으로 static 함수보다 인스턴스 함수가 좋다. 조금이라도 의심스럽다면 인스턴스 함수로 정의한다.\nG19: 서술적 변수 계산을 몇 단계로 나누고 중간값에 좋은 변수 이름만 붙여도 읽기 쉬운 코드로 바뀐다.\n프로그램 가독성을 높이는 가장 효과적인 방법 중 하나가 계산을 여러 단계로 나누고 중간 값으로 서술적인 변수 이름을 사용하는 방법이다.\n[Book] 켄트벡의 구현 패턴 - gpeegpee/learn-java Wiki\n1 2 3 4 5 6 7 Matcher match = headerPattern.matcher(line); if (match.find()) { String key = match.group(1); String value = match.group(2); headers.put(key.toLowerCase(), value); } 서술적인 변수 이름을 사용하여 match를 이용하여 찾은 첫번째 그룹이 키(key)이고, 두번째로 일치하는 그룹이 값(value)이라는 사실이 명확히 들어난다.\n서술적인 변수 이름은 많이 써도 괜찮다.\nG20: 이름과 기능이 일치하는 함수 1 Date newDate = date.add(5); 위 함수는 5일을 더하는 함수인가? 5주 혹은 5시간? date 인스턴스를 변경하는 함수인가? 아니면 예전 date 인스턴스는 두고 새로운 Date를 반환하는 함수인지 알 수 없다.\ndate 인스턴스에 5일을 더해 date 인스턴스를 변경하는 함수라면 addDaysTo 혹은 increaseByDays date 인스턴스를 변경하지 않으면서 5일 뒤인 새 날짜를 반환 daysLater, daysSince 이름만으로 분명하지 않기에 구현을 살피거나 문서를 뒤적여야 한다면 더 좋은 이름으로 바꾸거나 아니면 더 좋은 이름을 붙이기 쉽도록 기능을 정리해야 한다.\nG21: 알고리즘을 이해하라 코드가 돌아간다는 사실을 아는 것과 돌아가기 위한 알고리즘이 올바르다는 사실을 아는 것은 다르다.\n대다수 괴상한 코드는 사람들이 알고리즘을 충분히 이해하지 않은 채 코드를 구현한 탓이다.\n구현이 끝났다고 선언하기 전에 함수가 돌아가는 방식을 확실히 이해하는지 확인하라. 테스트 케이스를 모두 통과한다는 사실만으로 부족하다.\n알고리즘이 올바르다는 사실을 확인하고 이해하려면 기능이 뻔히 보일 정도로 함수를 깔끔하고 명확하게 재구성하는 방법이 최고다.\nG22: 논리적 의존성은 물리적으로 드러내라 한 모듈이 다른 모듈에 의존한다면 의존하는 모든 정보를 명시적으로 요청하여 물리적인 의존성이 드러낸다.\n의존성의 드러나있지 않다면 코드를 한눈에 파악할 수 없어 잘못 구현하거나, 아니더라도 코드를 분석을 해야 하기때문에 유지보수에 어려움이 생긴다.\nG23: if-else 혹은 switch/case 문보다 다형성을 사용하라 대다수 개발자가 switch문을 사용하는 이유는 그 상황에서 가장 올바른 선택이기보다는 당장 손쉬운 선택이기 때문이다. 유형보다 함수가 더 쉽게 변하는 경우는 긱히 드물다. 저자는 ‘switch 문 하나’ 규칙을 따른다. 선택 유형 하나에는 switch문을 한번만 사용한다. 같은 선택을 수행하는 다른 코드에서는 다형성 객체를 생성해 switch 문을 대신한다.\nG24: 표준 표기법을 따르라 팀은 업계 표준에 기반한 구현 표준을 따라야 한다.\n구현 표준\n표준을 설명하는 문서는 코드 자체로 충분해야 하며 업계 표준을 따라르기 때문에 별도 문서를 만들 필요는 없어야 한다.\n인스턴스 변수 이름을 선언하는 위치 클래스, 메서드, 변수 이름을 정하는 방법 괄호 넣는 위치 G25: 매직 숫자는 명명된 상수로 교체하라 일반적으로 코드에서 숫자를 사용하지 않는것이 좋다. 사용되는 숫자는 명명된 상수 뒤로 숨겨라\n매직 숫자라는 용어는 단순히 숫자만 의미하지 않는다. 의미가 분명하지 않은 토큰을 모두 가르킨다. G26: 정확하라 코드에서 뭔가를 결정할 때는 정확히 결정한다(들어맞게). 결정을 내리는 이유와 예외를 처리할 방법을 분명히 알아야 한다.\n호출하는 함수가 null을 반환할 가능성이 있다면 null을 항상 점검한다. 조회 결과가 하나뿐이라 짐작한다면 하나인지 확실히 확인한다.\n갱신 가능성이 희박하다고 잠금과 트랜잭션 관리를 건너뛰는 행동은 아무리 잘 봐줘도 게으름이다.\nList로 선언할 변수를 ArrayList로 선언하는 행동은 지나친 제약이다.\n코드에서 모호성과 부정확은 의견차나 게으름의 결과다.\nG27: 관례보다 구조를 사용하라 설계 결정을 강제할 때는 규칙보다 관례를 사용한다. 더 나아가 구조 자체로 강제하면 더 좋다.\nenum을 활용한 switch-case 보다 추상 메서드가 있는 기초 클래스가 더 좋다. switch-case문을 매번 똑같이 구현하게 강제하기는 어렵지만, 파생 클래스는 추상 메서드를 모두 구현하지 않으면 안되기 때문이다.\nG28: 조건을 캡슐화하라 부울 논리는 이해하기 어렵다. 조건의 의도를 분명히 밝히는 함수로 표현하라.\nG29: 부정 조건을 피하라 부정 조건은 긍정 조건보다 이해하기 어렵다. 가능하면 긍정 조건으로 표현한다.\nG30: 함수는 한가지만 해야한다 함수를 짜다보면 함수 안에 여러 단락을 이어, 일련의 작업을 수행하고픈 유혹에 빠진다. 이런 함수는 한가지만 수행하는 함수가 아닐 가능성이 높다. 한 가지만 수행하는 좀 더 작은 함수 여럿으로 나눠야 한다.\nG31: 숨겨진 시간적인 결합 때로는 시간적인 결합이 필요하다. 하지만 시간적인 결합을 숨겨서는 안된다. 함수를 만들때 함수 인수를 적절히 배치하여 함수가 호출되는 순서를 명백히 드러낸다.\n함수 인수를 통해 각 함수가 내놓는 결과를 사용하게 구현하면 일종의 연결 소자 역할을 하게 되어 시간적인 결합을 노출하기 쉽다.\nG32: 일관성을 유지하라 코드 구조를 잡을 때는 이유를 고민하라. 그리고 그 이유를 코드 구조로 명백히 표현하라.\n구조에 일관성이 없어 보인다면 남들이 맘대로 바꿔도 괜찮다고 생각한다. 반면 시스템 전반에 걸쳐 구조가 일관성이 있다면 남들도 일관성을 따르고 보존한다.\nG33: 경계 조건을 캡슐화하라 경계 조건은 빼먹거나 놓치기 쉽다. 따라서 경계 조건을 코드 여기저기세서 처리하지 않고 한 곳에서 별도로 처리한다.\nG34: 함수는 추상화 수준을 한 단계만 내려가야 한다 함수 내 모든 문장은 추상화 수준이 동일해야 한다. 그리고 추상화 수준은 함수 이름이 의미하는 작업보다 한 단계만 낮아야 한다.\n개념은 아주 간단하지만 인간은 추강화 수준을 뒤섞는 능력이 너무나도 뛰어나 따르기 어렵다. 1 2 3 4 5 6 7 8 9 10 11 public String render() throws Exception { StringBuffer html = new StringBuffer(\u0026#34;\u0026lt;hr\u0026#34;); if (size \u0026gt; 0) { html.append(\u0026#34; size=\\\u0026#34;\u0026#34;).append(size + 1).append(\u0026#34;\\\u0026#34;\u0026#34;); } html.append(\u0026#34;\u0026gt;\u0026#34;); return html.toString(); } 위 함수는 추상화 수준이 두개 이상 섞여있다.\n수평선에 크기가 있다는 개념 hr 태그 자체의 문법 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public String render() throws Exception { HtmlTag hr = new HtmlTag(\u0026#34;hr\u0026#34;); if (extraDashes \u0026gt; 0 ) { hr.addAttribute(\u0026#34;size\u0026#34;, hrSize(extraDashes)); } return hr.html(); } private String hrSize(int height) { int hrSize = height + 1; return String.format(\u0026#34;d\u0026#34;, hrSize); } G35: 설정 정보는 최상위 단계에 둬라 추상화 최상위 단계에 뒤야 할 기본값 상수나 설정 관련 상수를 저차운 함수에 숨겨서는 안된다. 대신 고차원 함수에서 저차원 함수를 호출할 때 인수로 넘긴다.\nG36: 추이적 탐색을 피하라 일반적으로 한 모듈은 주변 모듈을 모를수록 좋다. 좀 더 구체적으로 A가 B를 사용하고, B가 C를 사용한다 하더라도 A가 C를 알아야 할 필요는 없다는 뜻이다.\nex. a.getB().getC().doSometing(); 이를 “디미터의 법칙”, “부끄럼 타는 코드 작성”이라 부른다.\n요지는 자신이 직접 사용하는 모듈만 알아야 한다는 뜻이다. 내가 아는 모듈이 연이어 자신이 아는 모듈을 따라가며 시스템 전체를 휘저을 필요가 없다.\nJava J1: 긴 import 목록을 피하고 와일드카드를 사용하라. J2: 상수는 상속하지 않는다. 상수를 상속하여 사용하는 것은 언어의 범위 규칙을 속이는 행위이다. 대신 static import를 사용하라\nJ3: 상수 VS enum enum을 사용하면 public static final int라는 옛날 기교를 더 이상 사용할 필요가 없다. int는 코드에서 의미를 잃어버리기도 한다.\n반면 enum은 이름이 부여된 열거체이므로 의미를 잃어버리지 않는다.\n메서드와 필드도 사용할 수 있다. int보다 훨씬 더 유연하고 서술적이다.\n이름 N1: 서술적인 이름을 사용하라 소프트웨어의 가독성은 90% 이름이 결정한다. 그러므로 시간을 들여 현명한 이름을 선택하고 유효한 상태로 유지한다.\n서술적인 이름을 신중히 고른다. 소프트웨어가 진화하면 의미도 변하므로 선택한 이름이 적합한지 자주 되돌아본다. N2: 적절한 추상화 수준에서 이름을 선택하라 구현을 드러내는 이름은 피하는 것이 좋다. 작업 대상 클래스나 함수가 위치하는 추상화 수준을 반영하는 이름을 선택하라.\nN3: 가능하다면 표준 명명법을 사용하라 기존 명명법을 사용하는 이름은 이해하기 더 쉽다.\n디자인 패턴을 활용하면 클래스 이름에 패턴 이름을 사용한다. 자바에서 객체를 문자열로 변환하는 함수는 toString이라는 이름을 쓴다. 등 이름에 기존 관례가 있다면 관례를 따르는 것이 좋다.\n프로젝트에 유효한 의미가 담긴 이름을 많이 사용할수록 독자가 코드를 이해하기 쉬워진다.\nN4: 명확한 이름 함수나 변수의 목적을 명확히 밝히는 이름을 선택한다. 광범위하거나 모호한 이름을 사용하지 않는다.\nN5: 긴 범위는 긴 이름을 사용하라 이름 길이는 범위 길이에 비례해야 한다. 범위가 작으면 아주 짧은 이름을 사용해도 괜찮다. 하지만 범위가 길어지면 긴 이름을 사용한다.\nN6: 인코딩을 피하라 이름에 유형 정보나 범위 정보를 넣어서는 안 된다.\nN7: 이름으로 부수 효과를 설명하라 함수, 변수, 클래스가 하는 일을 모두 기술하는 이름을 사용한다. 이름에 부수 효과를 숨기지 않는다.\n테스트 T1: 불충분한 테스트 테스트 케이스는 잠재적으로 깨질 만한 부분을 보두 테스트해야 한다. 테스트 케이스가 확인하지 않는 조건이나 검증하지 않는 계산이 있다면 그 테스트는 불완전하다.\nT2: 커버리지 도구를 사용하라 커버리지 도구는 테스트가 빠뜨리는 공백을 알려준다. 도구를 사용하며 ㄴ테스트가 불충분한 모듈, 클래스, 함수를 찾기가 쉬워진다.\n상위 15 개 코드 커버리지 도구 (Java, JavaScript, C ++, C #, PHP 용) - 다른\nT3: 사소한 테스트를 건너뛰지 마라 사소한 테스트는 짜기 쉽다. 사소한 테스트가 제공하는 문서적 가치는 구현에 드는 비용을 넘어선다.\nT4: 무시한 테스트는 모호함을 뜻한다 때로는 요구사항이 불분명하기에 프로그램이 돌아가는 방식을 확신하기 어렵다. 불분명한 요구사항은 테스트 케이스를 주석으로 처리하거나 테스트 케이스에 @Ignore를 붙여 표현한다.\n선택 기준은 모호함이 존재하는 테스트 케이스가 컴파일이 가능한지 불가능한지에 달려있다.\nT5: 경계 조건을 테스트하라 경계 조건은 각별히 신경 써서 테스트한다. 알고리즘의 중앙 조건은 올바로 짜놓고 경계 조건에서 실수하는 경우가 흔하기 때문\nT6: 버그 주변은 철저히 테스트하라 버그는 서로 모이는 경향이 있다. 한 함수에서 버그를 발견했다면 그 함수를 철저히 테스트하는 편이 좋다.\nT7: 실패 패턴을 살펴라 때로는 테스트 케이스가 실패하는 패턴으로 문제를 진단할 수 있다.\n합리적인 순서로 정렬된 꼼꼼한 테스트 케이스는 실패 패턴을 드러낸다.\nT8: 테스트 커버리지 패턴을 살펴라 통과하는 테스트가 실행하거나 실행하지 않는 코드를 살펴보면 실패하는 테스트 케이스의 실패 원인이 드러난다.\nT9: 테스트는 빨라야한다 느린 테스트 케이스는 실행하지 않게 된다. 일정이 촉박하면 느린 테스트 케이스를 제일 먼저 건너 뛴다.\n결론 휴리스틱 목록을 익힌다고 소프트웨어 장인이 되지는 못한다. 전문가 정신과 장인 정신은 가치에서 나온다. 그 가치에 기반한 규율과 절제가 필요하다.\n","date":"2023-04-21T11:11:13+09:00","image":"https://codemario318.github.io/post/clean-code/17/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean-code/17/","title":"클린코드: 17. 냄새와 휴리스틱"},{"content":" 프로그래밍은 과학보다 공예에 가깝다. 깨끗한 코드를 짜려면 먼저 지저분한 코드를 짠 뒤에 정리해야 한다.\nArgs 구현 명령행 인수 구문을 분석하기 위한 클래스인 Args 클래스 개선을 예시로 확인해본다.\nMarshalling(마샬링): 객체의 메모리에서 표현방식을 저장 또는 전송에 적합한 다른 데이터 형식으로 변환하는 과정이다.\nArgs: 1차 초안 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 import java.text.ParseException; import java.util.*; public class Args { private String schema; private String[] args; private boolean valid = true; private Set\u0026lt;Character\u0026gt; unexpectedArguments = new TreeSet\u0026lt;Character\u0026gt;(); private Map\u0026lt;Character, Boolean\u0026gt; booleanArgs = new HashMap\u0026lt;Character, Boolean\u0026gt;(); private Map\u0026lt;Character, String\u0026gt; stringArgs = new HashMap\u0026lt;Character, String\u0026gt;(); private Map\u0026lt;Character, Integer\u0026gt; intArgs = new HashMap\u0026lt;Character, Integer\u0026gt;(); private Set\u0026lt;Character\u0026gt; argsFound = new HashSet\u0026lt;Character\u0026gt;(); private int currentArgument; private char errorArgumentId = \u0026#39;\\0\u0026#39;; private String errorParameter = \u0026#34;TILT\u0026#34;; private ErrorCode errorCode = ErrorCode.OK; private enum ErrorCode { OK, MISSING_STRING, MISSING_INTEGER, INVALID_INTEGER, UNEXPECTED_ARGUMENT } public Args(String schema, String[] args) throws ParseException { this.schema = schema; this.args = args; valid = parse(); } private boolean parse() throws ParseException { if (schema.length() == 0 \u0026amp;\u0026amp; args.length == 0) return true; parseSchema(); try { parseArguments(); } catch (ArgsException e) { } return valid; } private boolean parseSchema() throws ParseException { for (String element : schema.split(\u0026#34;,\u0026#34;)) { if (element.length() \u0026gt; 0) { String trimmedElement = element.trim(); parseSchemaElement(trimmedElement); } } return true; } private void parseSchemaElement(String element) throws ParseException { char elementId = element.charAt(0); String elementTail = element.substring(1); validateSchemaElementId(elementId); if (isBooleanSchemaElement(elementTail)) parseBooleanSchemaElement(elementId); else if (isStringSchemaElement(elementTail)) parseStringSchemaElement(elementId); else if (isIntegerSchemaElement(elementTail)) { parseIntegerSchemaElement(elementId); } else { throw new ParseException( String.format(\u0026#34;Argument: %c has invalid format: %s.\u0026#34;, elementId, elementTail), 0); } } private void validateSchemaElementId(char elementId) throws ParseException { if (!Character.isLetter(elementId)) { throw new ParseException( \u0026#34;Bad character:\u0026#34; + elementId + \u0026#34;in Args format: \u0026#34; + schema, 0); } } private void parseBooleanSchemaElement(char elementId) { booleanArgs.put(elementId, false); } private void parseIntegerSchemaElement(char elementId) { intArgs.put(elementId, 0); } private void parseStringSchemaElement(char elementId) { stringArgs.put(elementId, \u0026#34;\u0026#34;); } private boolean isStringSchemaElement(String elementTail) { return elementTail.equals(\u0026#34;*\u0026#34;); } private boolean isBooleanSchemaElement(String elementTail) { return elementTail.length() == 0; } private boolean isIntegerSchemaElement(String elementTail) { return elementTail.equals(\u0026#34;#\u0026#34;); } private boolean parseArguments() throws ArgsException { for (currentArgument = 0; currentArgument \u0026lt; args.length; currentArgument++) { String arg = args[currentArgument]; parseArgument(arg); } return true; } private void parseArgument(String arg) throws ArgsException { if (arg.startsWith(\u0026#34;-\u0026#34;)) parseElements(arg); } private void parseElements(String arg) throws ArgsException { for (int i = 1; i \u0026lt; arg.length(); i++) parseElement(arg.charAt(i)); } private void parseElement(char argChar) throws ArgsException { if (setArgument(argChar)) argsFound.add(argChar); else { unexpectedArguments.add(argChar); errorCode = ErrorCode.UNEXPECTED_ARGUMENT; valid = false; } } private boolean setArgument(char argChar) throws ArgsException { if (isBooleanArg(argChar)) setBooleanArg(argChar, true); else if (isStringArg(argChar)) setStringArg(argChar); else if (isIntArg(argChar)) setIntArg(argChar); else return false; return true; } private boolean isIntArg(char argChar) { return intArgs.containsKey(argChar); } private void setIntArg(char argChar) throws ArgsException { currentArgument++; String parameter = null; try { parameter = args[currentArgument]; intArgs.put(argChar, new Integer(parameter)); } catch (ArrayIndexOutOfBoundsException e) { valid = false; errorArgumentId = argChar; errorCode = ErrorCode.MISSING_INTEGER; throw new ArgsException(); } catch (NumberFormatException e) { valid = false; errorArgumentId = argChar; errorParameter = parameter; errorCode = ErrorCode.INVALID_INTEGER; throw new ArgsException(); } } private void setStringArg(char argChar) throws ArgsException { currentArgument++; try { stringArgs.put(argChar, args[currentArgument]); } catch (ArrayIndexOutOfBoundsException e) { valid = false; errorArgumentId = argChar; errorCode = ErrorCode.MISSING_STRING; throw new ArgsException(); } } private boolean isStringArg(char argChar) { return stringArgs.containsKey(argChar); } private void setBooleanArg(char argChar, boolean value) { booleanArgs.put(argChar, value); } private boolean isBooleanArg(char argChar) { return booleanArgs.containsKey(argChar); } public int cardinality() { return argsFound.size(); } public String usage() { if (schema.length() \u0026gt; 0) return \u0026#34;-[\u0026#34; + schema + \u0026#34;]\u0026#34;; else return \u0026#34;\u0026#34;; } public String errorMessage() throws Exception { switch (errorCode) { case OK: throw new Exception(\u0026#34;TILT: Should not get here.\u0026#34;); case UNEXPECTED_ARGUMENT: return unexpectedArgumentMessage(); case MISSING_STRING: return String.format(\u0026#34;Could not find string parameter for -%c.\u0026#34;, errorArgumentId); case INVALID_INTEGER: return String.format(\u0026#34;Argument -%c expects an integer but was \u0026#39;%s\u0026#39;.\u0026#34;, errorArgumentId, errorParameter); case MISSING_INTEGER: return String.format(\u0026#34;Could not find integer parameter for -%c.\u0026#34;, errorArgumentId); } return \u0026#34;\u0026#34;; } private String unexpectedArgumentMessage() { StringBuffer message = new StringBuffer(\u0026#34;Argument(s) -\u0026#34;); for (char c : unexpectedArguments) { message.append(c); } message.append(\u0026#34; unexpected.\u0026#34;); return message.toString(); } private boolean falseIfNull(Boolean b) { return b != null \u0026amp;\u0026amp; b; } private int zeroIfNull(Integer i) { return i == null ? 0 : i; } private String blankIfNull(String s) { return s == null ? \u0026#34;\u0026#34; : s; } public String getString(char arg) { return blankIfNull(stringArgs.get(arg)); } public int getInt(char arg) { return zeroIfNull(intArgs.get(arg)); } public boolean getBoolean(char arg) { return falseIfNull(booleanArgs.get(arg)); } public boolean has(char arg) { return argsFound.contains(arg); } public boolean isValid() { return valid; } private class ArgsException extends Exception { } } 문제점 인스턴스 변수가 너무 많다. “TILT” 와 같은 알수 없는 문자열이 있다. HashSets, TreeSets, try-catch-catch 블록 등 지저분해 보인다. Boolean만 지원하는 Args.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 package com.objectmentor.utilities.getopts; import java.util.*; public class Args { private String schema; private String[] args; private boolean valid; private Set\u0026lt;Character\u0026gt; unexpectedArguments = new TreeSet\u0026lt;Character\u0026gt;(); private Map\u0026lt;Character, Boolean\u0026gt; booleanArgs = new HashMap\u0026lt;Character, Boolean\u0026gt;(); private int numberOfArguments = 0; public Args(String schema, String[] args) { this.schema = schema; this.args = args; valid = parse(); } public boolean isValid() { return valid; } private boolean parse() { if (schema.length() == 0 \u0026amp;\u0026amp; args.length == 0) return true; parseSchema(); parseArguments(); return unexpectedArguments.size() == 0; } private boolean parseSchema() { for (String element : schema.split(\u0026#34;,\u0026#34;)) { parseSchemaElement(element); } return true; } private void parseSchemaElement(String element) { if (element.length() == 1) { parseBooleanSchemaElement(element); } } private void parseBooleanSchemaElement(String element) { char c = element.charAt(0); if (Character.isLetter(c)) { booleanArgs.put(c, false); } } private boolean parseArguments() { for (String arg : args) parseArgument(arg); return true; } private void parseArgument(String arg) { if (arg.startsWith(\u0026#34;-\u0026#34;)) parseElements(arg); } private void parseElements(String arg) { for (int i = 1; i \u0026lt; arg.length(); i++) parseElement(arg.charAt(i)); } private void parseElement(char argChar) { if (isBoolean(argChar)) { numberOfArguments++; setBooleanArg(argChar, true); } else unexpectedArguments.add(argChar); } private void setBooleanArg(char argChar, boolean value) { booleanArgs.put(argChar, value); } private boolean isBoolean(char argChar) { return booleanArgs.containsKey(argChar); } public int cardinality() { return numberOfArguments; } public String usage() { if (schema.length() \u0026gt; 0) return \u0026#34;-[\u0026#34; + schema + \u0026#34;]\u0026#34;; else return \u0026#34;\u0026#34;; } public String errorMessage() { if (unexpectedArguments.size() \u0026gt; 0) { return unexpectedArgumentMessage(); } else return \u0026#34;\u0026#34;; } private String unexpectedArgumentMessage() { StringBuffer message = new StringBuffer(\u0026#34;Argument(s) -\u0026#34;); for (char c : unexpectedArguments) { message.append(c); } message.append(\u0026#34; unexpected.\u0026#34;); return message.toString(); } public boolean getBoolean(char arg) { return booleanArgs.get(arg); } } 간결하고 단순하며 이해하기도 쉬웠다.\n하지만 코드를 잘 살펴보면 나중에 엉망으로 변해갈 씨앗이 보인다.\nBoolean과 String을 지원하는 Args.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 package com.objectmentor.utilities.getopts; import java.text.ParseException; import java.util.*; public class Args { private String schema; private String[] args; private boolean valid = true; private Set\u0026lt;Character\u0026gt; unexpectedArguments = new TreeSet\u0026lt;Character\u0026gt;(); private Map\u0026lt;Character, Boolean\u0026gt; booleanArgs = new HashMap\u0026lt;Character, Boolean\u0026gt;(); private Map\u0026lt;Character, String\u0026gt; stringArgs = new HashMap\u0026lt;Character, String\u0026gt;(); private Set\u0026lt;Character\u0026gt; argsFound = new HashSet\u0026lt;Character\u0026gt;(); private int currentArgument; private char errorArgument = \u0026#39;\\0\u0026#39;; enum ErrorCode { OK, MISSING_STRING } private ErrorCode errorCode = ErrorCode.OK; public Args(String schema, String[] args) throws ParseException { this.schema = schema; this.args = args; valid = parse(); } private boolean parse() throws ParseException { if (schema.length() == 0 \u0026amp;\u0026amp; args.length == 0) return true; parseSchema(); parseArguments(); return valid; } private boolean parseSchema() throws ParseException { for (String element : schema.split(\u0026#34;,\u0026#34;)) { if (element.length() \u0026gt; 0) { String trimmedElement = element.trim(); parseSchemaElement(trimmedElement); } } return true; } private void parseSchemaElement(String element) throws ParseException { char elementId = element.charAt(0); String elementTail = element.substring(1); validateSchemaElementId(elementId); if (isBooleanSchemaElement(elementTail)) parseBooleanSchemaElement(elementId); else if (isStringSchemaElement(elementTail)) parseStringSchemaElement(elementId); } private void validateSchemaElementId(char elementId) throws ParseException { if (!Character.isLetter(elementId)) { throw new ParseException( \u0026#34;Bad character:\u0026#34; + elementId + \u0026#34;in Args format: \u0026#34; + schema, 0); } } private void parseStringSchemaElement(char elementId) { stringArgs.put(elementId, \u0026#34;\u0026#34;); } private boolean isStringSchemaElement(String elementTail) { return elementTail.equals(\u0026#34;*\u0026#34;); } private boolean isBooleanSchemaElement(String elementTail) { return elementTail.length() == 0; } private void parseBooleanSchemaElement(char elementId) { booleanArgs.put(elementId, false); } // // here! private boolean parseArguments() { for (currentArgument = 0; currentArgument \u0026lt; args.length; currentArgument++) { String arg = args[currentArgument]; parseArgument(arg); } return true; } private void parseArgument(String arg) { if (arg.startsWith(\u0026#34;-\u0026#34;)) parseElements(arg); } private void parseElements(String arg) { for (int i = 1; i \u0026lt; arg.length(); i++) parseElement(arg.charAt(i)); } private void parseElement(char argChar) { if (setArgument(argChar)) argsFound.add(argChar); else { unexpectedArguments.add(argChar); valid = false; } } private boolean setArgument(char argChar) { boolean set = true; if (isBoolean(argChar)) setBooleanArg(argChar, true); else if (isString(argChar)) setStringArg(argChar, \u0026#34;\u0026#34;); else set = false; return set; } private void setStringArg(char argChar, String s) { currentArgument++; try { stringArgs.put(argChar, args[currentArgument]); } catch (ArrayIndexOutOfBoundsException e) { valid = false; errorArgument = argChar; errorCode = ErrorCode.MISSING_STRING; } } private boolean isString(char argChar) { return stringArgs.containsKey(argChar); } // private void setBooleanArg(char argChar, boolean value) { booleanArgs.put(argChar, value); } private boolean isBoolean(char argChar) { return booleanArgs.containsKey(argChar); } public int cardinality() { return argsFound.size(); } public String usage() { if (schema.length() \u0026gt; 0) return \u0026#34;-[\u0026#34; + schema + \u0026#34;]\u0026#34;; else return \u0026#34;\u0026#34;; } public String errorMessage() throws Exception { if (unexpectedArguments.size() \u0026gt; 0) { return unexpectedArgumentMessage(); } else switch (errorCode) { case MISSING_STRING: return String.format(\u0026#34;Could not find string parameter for -%c.\u0026#34;, errorArgument); case OK: throw new Exception(\u0026#34;TILT: Should not get here.\u0026#34;); } return \u0026#34;\u0026#34;; } private String unexpectedArgumentMessage() { StringBuffer message = new StringBuffer(\u0026#34;Argument(s) -\u0026#34;); for (char c : unexpectedArguments) { message.append(c); } message.append(\u0026#34; unexpected.\u0026#34;); return message.toString(); } public boolean getBoolean(char arg) { return falseIfNull(booleanArgs.get(arg)); } private boolean falseIfNull(Boolean b) { return b == null ? false : b; } // here! public String getString(char arg) { return blankIfNull(stringArgs.get(arg)); } private String blankIfNull(String s) { return s == null ? \u0026#34;\u0026#34; : s; } public boolean has(char arg) { return argsFound.contains(arg); } public boolean isValid() { return valid; } } String유형 관련 코드 추가 후 코드가 어지러워졌다. 이후 Integer 인수 유형을 추가하니 코드가 완전히 엉망이 되었다.\n그래서 멈췄다. 인수 유형을 더 추가해야 했는데, 코드가 훨씬 더 나빠질 것이라는 확신으로 리펙터링을 시작했다.\n새 인수 유형을 추가하려면 주요 지점 세곳에다 코드를 추가해야 한다는 사실을 알았다. (parse, get, set)\n인수 유형에 해당하는 HashMap을 선택하기 위해 스키마 요소의 구문 분석 명령행 인수에서 인수 유형을 분석해 진짜 유형으로 변환. getXXX메서드를 구현해 호출자에게 진짜 유형을 반환 💡 인수 유형은 다양하지만 모두가 유사한 메서드를 제공하므로 클래스 하나가 적합하다 판단했다. 그래서 ArgumentMarshaler라는 개념이 탄생했다.\n점진적으로 개선하다. 프로그램을 망치는 가장 좋은 방법 중 하나는 개선이라는 이름 아래 구조를 크게 뒤집는 행위다. 개선 전과 똑같은 결과를 만들게 하는 것이 어렵기 때문이다.\n그래서 테스트 주도 개발이라는 기법을 사용하여, 변경 후에도 시스템이 변경 전과 똑같이 돌아가는 것을 확인한다.\nArgumentMarshaler 각 인수 유형을 처리하는 코드를 Args내부에 선언된 ArgumentMarshaler 클래스에 옮긴 후, 이후 ArgumentMarshaler 파생 클래스를 만들어 분리했다.\n프로그램 구조를 조금씩 변경하는 동안에도 시스템의 정상 동작을 유지하기 쉬워지기 때문 아래 코드를 Args.java끝에 추가했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 private class ArgumentMarshaler { private boolean booleanValue = false; public void setBoolean(boolean value) { booleanValue = value; } public boolean getBoolean() { return booleanValue; } } private class BooleanArgumentMarshaler extends ArgumentMarshaler { } private class StringArgumentMarshaler extends ArgumentMarshaler { } private class IntegerArgumentMarshaler extends ArgumentMarshaler { } 처음 Boolean 인수를 저장하는 HashMap에서 Boolean 인수 유형을 ArgumentMarshaler유형으로 바꿨다. 1 private Map\u0026lt;Character, **ArgumentMarshaler**\u0026gt; booleanArgs = new HashMap\u0026lt;Character, **ArgumentMarshaler**\u0026gt;(); 그 후 깨진 부분에 변경을 반영했다. 1 2 3 4 5 6 7 8 9 10 11 private void parseBooleanSchemaElement(char elementId) { booleanArgs.put(elementId, **new BooleanArgumentMarshaler()**); } ... private void setBooleanArg(char argChar, boolean value) { booleanArgs.**get**(argChar).**setBoolean**(value); } ... public boolean getBoolean(char arg) { return falseIfNull(booleanArgs.get(arg).**getBoolean**()); } 새 인수 유형을 추가하려면 세 곳 (parse, get, set)을 변경해야 했는데, 정확히 해당 영역에서 변경이 이루어졌다.\nbooleanArgs가 ArgumentMarshaler 를 값으로 사용하게 변경되었으므로 위 getBoolean에서 사용하는 기존 falseIfNull이 NullPointerException을 막을 수 없었다.\n1 2 3 4 5 6 7 8 9 10 11 12 ... private Map\u0026lt;Character, Boolean\u0026gt; booleanArgs = new HashMap\u0026lt;Character, Boolean\u0026gt;(); ... private class ArgumentMarshaler { ... public boolean getBoolean(char arg) { Args.ArgumentMarshaler am = booleanArgs.get(arg); return am != null \u0026amp;\u0026amp; am.getBoolean(); } } ArgumentMarshaler.getBoolean 에서 null을 검사하게 구현하고 기존 falseIfNull을 제거했다.\nString 인수 추가 Boolean 인수를 추가하는 과정과 유사하게 String 인수를 추가했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 private Map\u0026lt;Character, **ArgumentMarshaler**\u0026gt; stringArgs = new HashMap\u0026lt;Character, **ArgumentMarshaler**\u0026gt;(); private void parseStringSchemaElement(char elementId) { stringArgs.put(elementId, **new StringArgumentMarshaler()**); } // ... private void setStringArg(char argChar) throws ArgsException { currentArgument++; try { stringArgs.**get**(argChar).**setString**(args[currentArgument]); } catch (ArrayIndexOutOfBoundsException e) { valid = false; errorArgumentId = argChar; errorCode = ErrorCode.MISSING_STRING; throw new ArgsException(); } } // ... public String getString(char arg) { **Args.ArgumentMarshaler am** = stringArgs.get(arg); return am == null ? \u0026#34;\u0026#34; : am.getString(); } // ... private class ArgumentMarshaler { private String stringValue; public void setString(String s) { stringValue = s; } public String getString() { return stringValue == null ? \u0026#34;\u0026#34; : stringValue; } } 테스트 케이스가 하나라도 실패하면 다음 변경으로 넘어가기 전에 오류를 수정했다.\n마찬가지로 int 인수 기능도 추가했다.\n추상클래스를 이용한 분리 Boolean, String, Integer 인수 처리에 대한 모든 기능을 Args의 ArgumentMarshaler로 옮겼으므로 미리 선언해둔 파생 클래스로 분산한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 private **abstract** class ArgumentMarshaler { **protected** boolean booleanValue = false; private String stringValue; private int integerValue; public void setBoolean(boolean value) { booleanValue = value; } public boolean getBoolean() { return booleanValue; } public void setString(String s) { stringValue = s; } public String getString() { return stringValue == null ? \u0026#34;\u0026#34; : stringValue; } public void setInteger(int i) { integerValue = i; } public int getInteger() { return integerValue; } **public abstract void set(String s);** } 점진적으로 한단계씩 개선하기 위해 ArgumentMarshaler를 추상 클래스로 바꾸고 하나씩 옮긴다.\n1 2 3 4 5 private class BooleanArgumentMarshaler extends ArgumentMarshaler { public void set(String s) { booleanValue = true; } } ArgumentMarshaler를 상속받은 유형 별 클래스에 set메서드를 구현하고, 기존 ArgumentMarshaler.setXXX을 set으로 대체한다.\n→ BooleanArgumentMarshaler.set 에서 s를 사용하지 않음에도 선언한 이유는 ArgumentMarshaler 를 상속받는 다른 클래스들이 String 인자를 받아 사용하기 때문\nget 1 2 3 4 5 private abstract class ArgumentMarshaler { protected boolean booleanValue = false; ... **public abstract void get();** } get을 추상 메서드로 만든 후 로직을 추가한다.\n1 2 3 4 5 6 7 8 9 10 private class BooleanArgumentMarshaler extends ArgumentMarshaler { **private boolean booleanValue = false;** public void set(String s) { booleanValue = true; } **public Object get() { return booleanValue; }** } 그 후 ArgumentMarshaler 에 protected로 선언되어있던 변수를 xxxArgumentMarshaler의 private변수로 선언한다.\nArgumentMarshaler 검증 1 2 3 4 5 private abstract class ArgumentMarshaler { ... **public abstract Object get(); public abstract void set(String s) throws ArgsException;** } set을 통해 ArgumentMarshaler를 상속받은 클래스의 private변수인 xxxValue에 값을 할당하게 되는데 자료형이 다를경우 오류가 발생한다.\nArgs 통합 xxxArgs를 통합하기 위한 첫걸음으로 marshalers 를 추가했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class Args { // ... private Map\u0026lt;Character, ArgumentMarshaler\u0026gt; booleanArgs = new HashMap\u0026lt;Character, ArgumentMarshaler\u0026gt;(); private Map\u0026lt;Character, ArgumentMarshaler\u0026gt; stringArgs = new HashMap\u0026lt;Character, ArgumentMarshaler\u0026gt;(); private Map\u0026lt;Character, ArgumentMarshaler\u0026gt; intArgs = new HashMap\u0026lt;Character, ArgumentMarshaler\u0026gt;(); **private Map\u0026lt;Character, ArgumentMarshaler\u0026gt; marshalers = new HashMap\u0026lt;Character, ArgumentMarshaler\u0026gt;();** // ... private void parseBooleanSchemaElement(char elementId) { ArgumentMarshaler m = new BooleanArgumentMarshaler(); booleanArgs.put(elementId, m); **marshalers.put(elementId, m);** } private void parseIntegerSchemaElement(char elementId) { ArgumentMarshaler m = new IntegerArgumentMarshaler(); intArgs.put(elementId, m); **marshalers.put(elementId, m);** } private void parseStringSchemaElement(char elementId) { ArgumentMarshaler m = new StringArgumentMarshaler(); stringArgs.put(elementId, m); **marshalers.put(elementId, m);** } } 그 후 parseXXX 메서드에 marshalers.put을 수행하여 테스트를 통과하지 않는지 살폈다. 테스트는 실패없이 돌아갔다.\n1 2 3 private boolean isBooleanArg(char argChar) { return booleanArgs.containsKey(argChar); } 그 후 Args.isBooleanArgs를 아래로 수정했다.\n1 2 3 4 private boolean isBooleanArg(char argChar) { ArgumentMarshaler m = marshalers.get(argChar); return m instanceof BooleanArgumentMarshaler; } 수정 후 테스트를 모두 통과하여 다른 항목들에 대해서도 동일한 수정을 했다. 변경후에도 테스트를 통과하여 marshalers.get을 호출하는 코드를 통합했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 private boolean setArgument(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); if (isBooleanArg(m)) setBooleanArg(argChar); else if (isStringArg(m)) setStringArg(argChar); else if (isIntArg(m)) setIntArg(argChar); else return false; return true; } private boolean isIntArg(ArgumentMarshaler m) { return m instanceof IntegerArgumentMarshaler; } private boolean isStringArg(ArgumentMarshaler m) { return m instanceof StringArgumentMarshaler; } private boolean isBooleanArg(ArgumentMarshaler m) { return m instanceof BooleanArgumentMarshaler; } 다음으로 set함수에서 기존 HashMap을 marshalers HashMap으로 교체한다.\n1 2 3 4 5 6 private void setBooleanArg(ArgumentMarshaler m) { try { m.set(\u0026#34;true\u0026#34;); // was: booleanArgs.get(argChar).set(\u0026#34;true\u0026#34;); } catch (ArgsException e) { } } 각 setXXX의 형식을 통합하여 예외처리를 동일하게 적용할 수 있었다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 private boolean setArgument(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); try { if (m instanceof BooleanArgumentMarshaler) setBooleanArg(m); else if (m instanceof StringArgumentMarshaler) setStringArg(m); else if (m instanceof IntegerArgumentMarshaler) setIntArg(m); else return false; } catch (ArgsException e) { valid = false; errorArgumentId = argChar; throw e; } return true; } private void setIntArg(ArgumentMarshaler m) throws ArgsException { currentArgument++; String parameter = null; try { parameter = args[currentArgument]; m.set(parameter); } catch (ArrayIndexOutOfBoundsException e) { errorCode = ErrorCode.MISSING_INTEGER; throw new ArgsException(); } catch (ArgsException e) { errorParameter = parameter; errorCode = ErrorCode.INVALID_INTEGER; throw e; } } private void setStringArg(ArgumentMarshaler m) throws ArgsException { currentArgument++; try { m.set(args[currentArgument]); } catch (ArrayIndexOutOfBoundsException e) { errorCode = ErrorCode.MISSING_STRING; throw new ArgsException(); } } HashMap 통합 ArgumentMarshaler를 이용하여 만든 유형별 HashMap을 하나의 HashMap으로 통합한다.\n1 2 3 4 5 6 7 8 9 10 public boolean getBoolean(char arg) { Args.ArgumentMarshaler am = marshalers.get(arg); boolean b = false; try { b = am != null \u0026amp;\u0026amp; (Boolean) am.get(); } catch (ClassCastException e) { b = false; } return b; } 이에따라 다른 인수유형도 아래와 같이 변경 가능했고,\n1 2 3 4 5 private void parseBooleanSchemaElement(char elementId) { ArgumentMarshaler m = new BooleanArgumentMarshaler(); ~~booleanArgs.put(elementId, m);~~ marshalers.put(elementId, m); } 결과적으로 하나의 Hashmap으로 통합 가능했다.\nsetArgument 유형을 일일이 확인하는 코드를 없애고 ArgumentMarshaler.set만으로 만든다.\nsetXXXArg를 각각 ArgumentMarshaler 로 내려야 하는데, setIntArgs는 args와 currentArgument라는 인스턴스 변수 두개가 쓰인다.\n해당 메서드를 내리려면 args와 currentArgument를 인수로 넘겨야 하므로(인수가 많아지므로) 코드가 지저분해진다.\n따라서 args를 list로 변환 후 Iterator를 set 함수로 전달한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 public class Args { private String schema; private String[] args; private boolean valid = true; private Set\u0026lt;Character\u0026gt; unexpectedArguments = new TreeSet\u0026lt;Character\u0026gt;(); private Map\u0026lt;Character, ArgumentMarshaler\u0026gt; marshalers = new HashMap\u0026lt;Character, ArgumentMarshaler\u0026gt;(); private Set\u0026lt;Character\u0026gt; argsFound = new HashSet\u0026lt;Character\u0026gt;(); **private Iterator\u0026lt;String\u0026gt; currentArgument;** private char errorArgumentId = \u0026#39;\\0\u0026#39;; private String errorParameter = \u0026#34;TILT\u0026#34;; private ErrorCode errorCode = ErrorCode.OK; **private List\u0026lt;String\u0026gt; argsList;** private enum ErrorCode { OK, MISSING_STRING, MISSING_INTEGER, INVALID_INTEGER, UNEXPECTED_ARGUMENT } public Args(String schema, String[] args) throws ParseException { this.schema = schema; **argsList = Arrays.asList(args);** valid = parse(); } private boolean parse() throws ParseException { if (schema.length() == 0 \u0026amp;\u0026amp; **argsList.size()** == 0) return true; parseSchema(); try { parseArguments(); } catch (ArgsException e) { } return valid; } // --- private boolean parseArguments() throws ArgsException { for (currentArgument = **argsList.iterator()**; currentArgument.**hasNext()**;) { String arg = currentArgument.**next()**; parseArgument(arg); } return true; } // --- private void setIntArg(ArgumentMarshaler m) throws ArgsException { String parameter = null; try { parameter = currentArgument.**next()**; m.set(parameter); } catch (**NoSuchElementException** e) { errorCode = ErrorCode.MISSING_INTEGER; throw new ArgsException(); } catch (ArgsException e) { errorParameter = parameter; errorCode = ErrorCode.INVALID_INTEGER; throw e; } } private void setStringArg(ArgumentMarshaler m) throws ArgsException { try { m.set(currentArgument.**next()**); } catch (**NoSuchElementException** e) { errorCode = ErrorCode.MISSING_STRING; throw new ArgsException(); } } } 따라서 args를 list로 변환 후 Iterator를 set 함수로 전달하는 변경 후에 테스트를 모두 통과했다. set 함수를 적절한 파생 클래스로 내려도 괜찮아져 처음으로 setArgument를 아래로 변경했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 private boolean setArgument(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); if (m == null) return false; try { if (m instanceof BooleanArgumentMarshaler) setBooleanArg(m); else if (m instanceof StringArgumentMarshaler) setStringArg(m); else if (m instanceof IntegerArgumentMarshaler) setIntArg(m); ~~else return false;~~ } catch (ArgsException e) { valid = false; errorArgumentId = argChar; throw e; } return true; } 연쇄적인 if-else 구문을 완전히 제거하기 위해 제거하고 오류 코드를 따로 꺼냈다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 private boolean setArgument(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); if (m == null) return false; try { if (m instanceof BooleanArgumentMarshaler) setBooleanArg(m, currentArgument); else if (m instanceof StringArgumentMarshaler) setStringArg(m); else if (m instanceof IntegerArgumentMarshaler) setIntArg(m); } catch (ArgsException e) { valid = false; errorArgumentId = argChar; throw e; } return true; } // --- private void setBooleanArg(ArgumentMarshaler m, Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { ~~try {~~ m.set(\u0026#34;true\u0026#34;); } ~~catch (ArgsException e) { }~~ } 단계적으로 조금씩 변경하며 매번 테스트를 돌려야 하므로 코드를 옮기고 제거하는 일이 많아진다.\n위와 마찬가지로 iterator를 인자로 받는 이유는 추상 메서드로 호출하기 위해서다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 private abstract class ArgumentMarshaler { public abstract void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException; public abstract void set(String s) throws ArgsException; public abstract Object get(); } private class BooleanArgumentMarshaler extends ArgumentMarshaler { private boolean booleanValue = false; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { booleanValue = true; } public void set(String s) { booleanValue = true; } public Object get() { return booleanValue; } } private class StringArgumentMarshaler extends ArgumentMarshaler { private String stringValue = \u0026#34;\u0026#34;; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { } public void set(String s) { stringValue = s; } public Object get() { return stringValue; } } private class IntegerArgumentMarshaler extends ArgumentMarshaler { private int intValue = 0; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { } public void set(String s) throws ArgsException { try { intValue = Integer.parseInt(s); } catch (NumberFormatException e) { throw new ArgsException(); } } public Object get() { return intValue; } } 새로운 추상 메서드를 추가하고 각각 변경한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 private boolean setArgument(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); if (m == null) return false; try { if (m instanceof BooleanArgumentMarshaler) m.set(currentArgument); else if (m instanceof StringArgumentMarshaler) m.set(currentArgument); else if (m instanceof IntegerArgumentMarshaler) m.set(currentArgument); } catch (ArgsException e) { valid = false; errorArgumentId = argChar; throw e; } return true; } private class StringArgumentMarshaler extends ArgumentMarshaler { private String stringValue = \u0026#34;\u0026#34;; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { try { stringValue = currentArgument.next(); } catch (NoSuchElementException e) { errorCode = ErrorCode.MISSING_STRING; throw new ArgsException(); } } public void set(String s) { } public Object get() { return stringValue; } } private class IntegerArgumentMarshaler extends ArgumentMarshaler { private int intValue = 0; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { String parameter = null; try { parameter = currentArgument.next(); set(parameter); } catch (NoSuchElementException e) { errorCode = ErrorCode.MISSING_INTEGER; throw new ArgsException(); } catch (ArgsException e) { errorParameter = parameter; errorCode = ErrorCode.INVALID_INTEGER; throw e; } } public void set(String s) throws ArgsException { try { intValue = Integer.parseInt(s); } catch (NumberFormatException e) { throw new ArgsException(); } } public Object get() { return intValue; } } setArgument가 테스트에 통과하면 m.set이 정상 동작을 의미하므로 통합할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 private boolean setArgument(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); if (m == null) return false; try { m.set(currentArgument); return true; } catch (ArgsException e) { valid = false; errorArgumentId = argChar; throw e; } } 마지막으로 ArgumentMarshaler를 인터페이스로 변경하면 완료된다.\n1 2 3 4 private interface ArgumentMarshaler { void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException; Object get(); } 결론 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 package com.objectmentor.utilities.args; import static com.objectmentor.utilities.args.ArgsException.ErrorCode.*; import java.util.*; public class Args { private Map\u0026lt;Character, ArgumentMarshaler\u0026gt; marshalers; private Set\u0026lt;Character\u0026gt; argsFound; private ListIterator\u0026lt;String\u0026gt; currentArgument; public Args(String schema, String[] args) throws ArgsException { marshalers = new HashMap\u0026lt;Character, ArgumentMarshaler\u0026gt;(); argsFound = new HashSet\u0026lt;Character\u0026gt;(); parseSchema(schema); parseArgumentStrings(Arrays.asList(args)); } private void parseSchema(String schema) throws ArgsException { for (String element : schema.split(\u0026#34;,\u0026#34;)) if (element.length() \u0026gt; 0) parseSchemaElement(element.trim()); } private void parseSchemaElement(String element) throws ArgsException { char elementId = element.charAt(0); String elementTail = element.substring(1); validateSchemaElementId(elementId); if (elementTail.length() == 0) marshalers.put(elementId, new BooleanArgumentMarshaler()); else if (elementTail.equals(\u0026#34;*\u0026#34;)) marshalers.put(elementId, new StringArgumentMarshaler()); else if (elementTail.equals(\u0026#34;#\u0026#34;)) marshalers.put(elementId, new IntegerArgumentMarshaler()); else if (elementTail.equals(\u0026#34;##\u0026#34;)) marshalers.put(elementId, new DoubleArgumentMarshaler()); else if (elementTail.equals(\u0026#34;[*]\u0026#34;)) marshalers.put(elementId, new StringArrayArgumentMarshaler()); else throw new ArgsException(INVALID_ARGUMENT_FORMAT, elementId, elementTail); } private void validateSchemaElementId(char elementId) throws ArgsException { if (!Character.isLetter(elementId)) throw new ArgsException(INVALID_ARGUMENT_NAME, elementId, null); } private void parseArgumentStrings(List\u0026lt;String\u0026gt; argsList) throws ArgsException { for (currentArgument = argsList.listIterator(); currentArgument.hasNext();) { String argString = currentArgument.next(); if (argString.startsWith(\u0026#34;-\u0026#34;)) { parseArgumentCharacters(argString.substring(1)); } else { currentArgument.previous(); break; } } } private void parseArgumentCharacters(String argChars) throws ArgsException { for (int i = 0; i \u0026lt; argChars.length(); i++) parseArgumentCharacter(argChars.charAt(i)); } private void parseArgumentCharacter(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); if (m == null) { throw new ArgsException(UNEXPECTED_ARGUMENT, argChar, null); } else { argsFound.add(argChar); try { m.set(currentArgument); } catch (ArgsException e) { e.setErrorArgumentId(argChar); throw e; } } } public boolean has(char arg) { return argsFound.contains(arg); } public int nextArgument() { return currentArgument.nextIndex(); } public boolean getBoolean(char arg) { return BooleanArgumentMarshaler.getValue(marshalers.get(arg)); } public String getString(char arg) { return StringArgumentMarshaler.getValue(marshalers.get(arg)); } public int getInt(char arg) { return IntegerArgumentMarshaler.getValue(marshalers.get(arg)); } public double getDouble(char arg) { return DoubleArgumentMarshaler.getValue(marshalers.get(arg)); } public String[] getStringArray(char arg) { return StringArrayArgumentMarshaler.getValue(marshalers.get(arg)); } } 1 2 3 public interface ArgumentMarshaler { void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class BooleanArgumentMarshaler implements ArgumentMarshaler { private boolean booleanValue = false; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { booleanValue = true; } public static boolean getValue(ArgumentMarshaler am) { if (am != null \u0026amp;\u0026amp; am instanceof BooleanArgumentMarshaler) return ((BooleanArgumentMarshaler) am).booleanValue; else return false; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import static com.objectmentor.utilities.args.ArgsException.ErrorCode.*; public class StringArgumentMarshaler implements ArgumentMarshaler { private String stringValue = \u0026#34;\u0026#34;; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { try { stringValue = currentArgument.next(); } catch (NoSuchElementException e) { throw new ArgsException(MISSING_STRING); } } public static String getValue(ArgumentMarshaler am) { if (am != null \u0026amp;\u0026amp; am instanceof StringArgumentMarshaler) return ((StringArgumentMarshaler) am).stringValue; else return \u0026#34;\u0026#34;; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import static com.objectmentor.utilities.args.ArgsException.ErrorCode.*; public class IntegerArgumentMarshaler implements ArgumentMarshaler { private int intValue = 0; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { String parameter = null; try { parameter = currentArgument.next(); intValue = Integer.parseInt(parameter); } catch (NoSuchElementException e) { throw new ArgsException(MISSING_INTEGER); } catch (NumberFormatException e) { throw new ArgsException(INVALID_INTEGER, parameter); } } public static int getValue(ArgumentMarshaler am) { if (am != null \u0026amp;\u0026amp; am instanceof IntegerArgumentMarshaler) return ((IntegerArgumentMarshaler) am).intValue; else return 0; } } 리펙토링을 안전하게 수행하기 위해 조금씩 변경하는 과정을 거쳤다. 결과적으로 절차적인 코드가 객체지향적으로 리펙토링 되었다.\n규모가 있는 코드를 한번에 변경하는 것은 매우 어렵다. 따라서 테스트를 기반으로 조금씩 변경하는 과정을 거쳤다. 테스트는 매우 중요하다!\n","date":"2023-04-21T11:00:13+09:00","image":"https://codemario318.github.io/post/clean-code/14/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean-code/14/","title":"클린코드: 14. 점진적인 개선"},{"content":" 창발성: 떠오름 현상, 創發性, emergent property, emergence\n하위 계층에서 없는 특성이나 행동이 상위 계층에서 자발적으로 돌연히 출현하는 현상, 불시에 솟아나는 특성(wikipedia) 남이 모르거나 하지 아니한 것을 처음으로 또는 새롭게 밝혀내거나 이루어 내는 성질(naver dict) 창발적 설계로 깔끔한 코드를 구현하자 간단한 기본 규칙들(하위 계층)을 지키게 되면 자연스럽게 좋은 코드(상위 계층)가 만들어진다(발생한다).\n단순한 설계 규칙 모든 테스트를 실행한다. 중복을 없앤다. 프로그래머의 의도를 표현한다. 크래스와 메서드 수를 최소로 줄인다. 위 4가지 규칙을 지키면 자연스럽게 깨끗한 코드가 만들어 진다.\n코드 구조와 설계를 파악하기 쉬워짐 이에 따라 SRP, DIP 같은 원칙을 적용하기 쉬워짐 모든 테스트를 실행하라 설계는 의도한 대로 돌아가는 시스템을 내놓아야 한다. 완벽한 설계를 따라 시스템을 만들었다고 하더라도 검증이 불가능하다면 완벽한 설계라고 확언할 수 없다.\n이러한 의도한 대로 돌아가는 시스템을 검증하는 방법은 테스트이며, 의도한 대로 돌아가는 시스템은 모든 테스트 케이스를 통과하는 시스템이다.\n따라서 테스트가 불가능한 시스템은 검증이 불가능한 시스템이고, 검증이 불가능하다면 어디서 문제가 발생할 지 예측할 수 없다.\n테스트가 가능한 시스템을 만들려고 애쓰면 설계 품질이 더불어 높아진다.\n크기가 작고 목적 하나만 수행하는 클래스가 나온다. SRP를 준수하는 클래스는 테스트가 훨씬 더 쉽다.\n테스트 케이스가 많을수록 개발자는 테스트가 용이하게 코드를 작성하게 된다. 따라서 철저한 테스트가 가능한 시스템을 만들면 더 나은 설계가 얻어진다\n결합도가 높으면 테스트 케이스를 작성하기 어렵다. 테스트 케이스를 많이 작성할수록 개발자는 DIP와 같은 원칙을 적용하고, 의존성 주입, 인터페이스, 추상화 등과 같은 도구를 사용해 결합도를 낮춘다.\n“테스트 케이스를 만들고 계속 돌려라\u0026ldquo;라는 간단하고 단순한 규칙을 따르면 시스템은 낮은 결합도와 높은 응집력이라는, 객체 지향 방법론이 지향하는 목표를 저절로 달성한다.\n2~4. 리펙터링 테스트 케이스를 모두 작업했다면 코드와 클래스를 정리해도 괜찮다. 코드를 점직적으로 리팩터링 해나간다.\n테스트 케이스로 인해 코드를 정리하면서 시스템이 깨질까 걱정할 필요가 없다.\n리팩터링 단계에서는 소프트웨어 설계 품질을 높이는 기법이라면 무엇이든 적용해도 괜찮다.\n응집도를 높이기 결합도를 낮추기 관심사를 분리하기 시스템 관심사를 모듈로 나누기 함수와 클래스 크기를 줄이기 더 나은 이름을 선택하기 다양한 기법을 동원한다.\n이 단계는 단순한 설계 규칙 중 나머지 3개를 적용해 중복을 제거하고, 프로그래머의 의도를 표현하고, 클래스와 메서드 수를 최소로 줄이는 단계이다.\n중복을 없애라 중복은 추가 작업, 추가 위험, 불필요한 복잡도를 뜻한다. 따라서 우수한 설계에서 중복은 커다란 적이다.\n똑같은 코드 비슷한 코드 구현 중복 비슷한 코드는 더 비슷하게 고쳐주면 리팩터링이 쉬워진다. 구현 중복도 중복의 한 형태이다.\nSet 클래스 예시: 구현 중복 1 2 int size() {} boolean isEmpty() {} Set 클래스에서 위와 같은 메서드가 있다고 가정할 때, 각 메서드를 따로 구현하는 방법도 있다. isEmpty는 부울 값을 반환하고, size는 개수를 반환한다.\n1 2 3 boolean isEmpty() { return 0 == size(); } 여기서 isEmpty를 별도로 구현하지 않고, size를 이용하면 코드를 중복해서 구현할 필요가 없어진다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public void scaleToOneDimension(float desiredDimension, float imageDimension) { if (Math.abs(desiredDimension - imageDimension) \u0026lt; errorThreshold) { return; } float scalingFactor = desiredDimension / imageDimension; scalingFactor = (float)(Math.floor(scalingFactor * 100) * 0.01f); RenderedOp newImage = ImageUtilities.getScaledImage(image, scalingFactor, scalingFactor); image.dispose(); System.gc(); image = newImage; } public synchronized void rotate(int degrees) { RenderedOp newImage = ImageUtilities.getRotatedImage(image, degrees); image.dispose(); System.gc(); image = newImage; } scaleToOneDimension 메서드와 rotate 메서드를 살펴보면 일부 코드가 동일하다. 다음과 같이 코드를 정리해 중복을 제거한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public void scaleToOneDimension(float desiredDimension, float imageDimension) { if (Math.abs(desiredDimension - imageDimension) \u0026lt; errorThreshold) { return; } float scalingFactor = desiredDimension / imageDimension; scalingFactor = (float)(Math.floor(scalingFactor * 100) * 0.01f); replaceImage(ImageUtilities.getScaledImage(image, scalingFactor, scalingFactor)); } public synchronized void rotate(int degrees) { replaceImage(ImageUtilities.getrotatedImage(image, degrees)); } public void replaceImage(RenderedOp newImage) { image.dispose(); System.gc(); image = newImage; } 코드 3줄을 공통 replaceImage 메서드로 뽑아서 중복을 제거했다. 새 메서드로 뽑아내니 기존 클래스가 SRP를 위반하므로, replaceImage를 적합한 다른 클래스로 옮기면 된다.\n이러한 공통 코드 분리를 통해 메서드의 가시성이 높아졌다. 따라서 다른 팀원이 새 메서드를 좀 더 추상화해 다른 맥락에서 재사용할 기회를 포착할지도 모른다.\n이러한 소규모 재사용은 시스템 복잡도를 극적으로 줄여준다. 소규모 재사용을 제대로 익혀야 대규모 재사용이 가능하다.\nTEMPLATE METHOD 패턴 [Design Pattern] 템플릿 메서드 패턴이란 - Heee\u0026rsquo;s Development Blog\n고차원 중복을 제거할 목적으로 자주 사용하는 기법으로 어떤 작업을 처리하는 일부분을 서브 클래스로 캡슐화하여 전체 일을 수행하는 구조는 바꾸지 않으면서 특정 단계에서 수행하는 내역을 바꾸는 패턴\n전체적으로 동일하면서 부분적으로는 다른 구문으로 구성된 메서드의 코드 중복을 최소화 할때 유용함 동일한 기능을 상위 클래스에서 정의하면서 확장/변화가 필요한 부분만 서브 클래스에서 구현할 수 있도록 한다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public class VacationPolicy { public void accrueUSDivisionVacation() { // 지금까지 근무한 시간을 바탕으로 휴가 일수를 계산하는 코드 // ... // 휴가 일수가 미국 최소 법정 일수를 만족하는지 확인하는 코드 // ... // 휴가 일수를 급여 대장에 적용하는 코드 // ... } public void accfrueEUDivisionVacation() { // 지금까지 근무한 시간을 바탕으로 휴가 일수를 계산하는 코드 // ... // 휴가 일수가 유럽연합 최소 법정 일수를 만족하는지 확인하는 코드 // ... // 휴가 일수를 급여 대장에 적용하는 코드 // ... } } 위 코드에서 최소 법정 일수를 계산하는 코드만 제외하면 두 메서드는 거의 동일하다. 최소 법정 일수를 계산하는 알고리즘은 직원 유형에 따라 조금 변한다.\n이러한 구조에서 Template Method 패턴을 적용해 중복을 제거한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 abstract public class Vacation Policy { public void accrueVacation() { calculateBaseVacationHours(); alterForLegalMinimums(); applyToPayroll(); } private void calculateBaseVacationHours() { // 지금까지 근무한 시간을 바탕으로 휴가 일수를 계산하는 코드 // ... }; abstract protected void alterForLegalMinimums(); private void applyToPayroll() { // 휴가 일수를 급여 대장에 적용하는 코드 // ... }; } public class USVacationPolicy extends VacationPolicy { @override protected void alterForLegalMinimums() { // 미국 최소 법정 일수 사용; } } public class EUVacationPolicy extends VacationPolicy { @override protected void alterForLegalMinimums() { // 유럽연합 최소 법정 일수 사용; } } 하위 클래스는 중복되지 않는 정보만 제공해 accrueVacation 알고리즘에서 빠진 구멍(alterForLegalMinimums)을 메운다.\n표현하라 코드르 짜는동안 문제에 푹 빠져 코드를 구석구석 이해하게 되어, 자신만 이해하는 코드를 짜기가 쉽다. 이런 코드는 이후 유지보수할 사람이 코드를 짜는 사람만큼이나 문제를 깊이 이해할 가능성은 희박하다.\n소프트웨어 프로젝트 비용 중 대다수는 장기적인 유지보수에 들어가게 되는데, 코드를 변경하면서 버그 여지를 주지 않으려면 유지보수 개발자가 시스템을 제대로 이해해야 한다.\n하지만 시스템이 점차 복잡해지면서 유지보수 개발자가 시스템을 이해하느라 보내는 시간은 점점 늘어나고 동시에 코드를 오해할 가능성도 점점 커진다.\n따라서 이해하기 쉬운 코드를 짜야하고, 코드에서 개발자의 의도를 분명히 표현하고 명백하게 짤수록 다른 사람이 코드르 이해하기 쉬워진다.\n표현력을 높이는 방법 좋은 이름을 선택한다. 이름과 기능이 완전히 딴판인 클래스나 함수로 혼란을 주면 안된다. 함수와 클래스 크기를 가능한 줄인다. 작은 클래스와 작은 함수는 이름 짓기도 쉽고, 구현하기도 쉽고, 이해하기도 쉽다. 표준 명칭을 사용한다. 디자인 패턴은 의사소통과 표현력 강화가 주요 목적이다. 클래스가 COMMAND나 VISITOR와 같은 표준 패턴을 사용한다면 클래스 이름에 패턴 이름을 넣어준다. 단위 테스트 케이스를 꼼꼼히 작성한다. 테스트 케이스는 소위 ‘예제로 보여주는 문서’다. 잘 만든 테스트 케이스를 읽어보면 클래스 기능이 한눈에 들어온다. 표현력을 높이는 방법은 노력이다. 함수와 클래스에 조금 더 시간을 투자하자. 더 나은 이름을 선택하고, 큰 함수를 작은 함수 여럿으로 나누고, 자신의 작품에 조금만 더 주의를 기울이자!\n나중에 코드를 읽을 사람은 바로 자신일 가능성이 높다.\n클래스와 메소드 수를 최소로 줄여라 중복을 제거하고, 의도를 표현하고, SRP를 준수한다는 기본적인 개념도 극단으로 치달으면 득보다 실이 많아진다.\n클래스와 메서드 크기를 줄이자고 조그만 클래스와 메서드를 수없이 만드는 사례도 없지 않다. 그래서 이 규칙은 함수와 클래스 수를 가능한 줄이라고 제안한다.\n때로는 무의미하고 독단적인 전책 탓에 클래스 수와 메서드 수가 늘어나기도 한다.\n클래스마다 무조건 인터페이스를 생성하라고 요구하는 구현 표준 자료 클래스와 동작 클래스는 무조건 분리해야 한다. 가능한 독단적인 견해는 멀리하고 실용적인 방식을 택한다.\n목표는 함수와 클래스 크기를 작게 유지하면서 시스템 크기도 작게 유지하는 데 있다.\n이 규칙은 간단한 설계 규칙 네 개중 가장 우선순위가 낮다. 즉, 틀래스와 함수 수를 줄이는 작업도 중요하지만, 테스트 케이스를 만들고 중복을 제거하고 의도를 표현하는 작업이 더 중요하다는 뜻이다.\n결론 경험을 대신할 단순한 개발 기법은 없다. 하지만 소개하는 기법은 저자들이 수십 년 동안 쌓은 경험의 정수다.\n💡 단순한 설계 규칙을 따른다면 우수한 기법과 원칙을 단번에 활용할 수 있다.\n","date":"2023-04-21T10:52:13+09:00","image":"https://codemario318.github.io/post/clean-code/12/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean-code/12/","title":"클린코드: 12. 창발성"},{"content":"코드의 표현력과 그 코드로 이루어진 함수에 아무리 신경 쓸지라도 좀 더 차원 높은 단계까지 신경쓰지 않으면 깨끗한 코드를 얻기는 어렵다.\n클래스 체계 표준 자바 관례에 의하면 아래 순서로 선언하게 된다.\npublic static 상수 private static 변수 비공개 인스턴스 변수 공개 변수가 필요한 경우는 거의 없다 공개 함수 비공개 함수 자신을 호출하는 공개 함수 직후에 넣는다. 즉 추상화 단계가 순차적으로 내려간다. 그래서 프로그램은 신문 기사처럼 읽힌다.\n캡슐화 변수와 유틸리티 함수는 가능한 공개하지 않는 편이 낫지만 반드시 숨겨야한다는 법칙도 없다. 때로는 변수나 유틸리티 함수를 protected로 선언해 테스트 코드에 접근을 허용하기도 한다.\n같은 패키지 안에서 테스트 코드가 함수를 호출하거나 변수를 사용해야 한다면 그 함수나 변수를 protected로 선언하거나 패키지 전체로 공개한다.\n하지만 그 전에 비공개 상태를 유지할 방법을 강구해야 한다.\n💡 캡슐화를 풀어주는 결정은 언제나 최후의 수단이다.\n클래스는 작아야 한다. 클래스를 만들 때 첫 번째 규칙은 크기다. 클래스는 작아야 한다. 클래스를 설계할 때도, 함수와 마찬가지로, 작게가 기본 규칙이다.\n얼마나 작아야 할까? 함수는 물리적인 행 수로 크기를 측정했지만 클래스는 맡은 책임 수로 측정한다.\n1 2 3 4 5 6 7 public class SuperDashboard extends JFrame implements MetaDataUser { public Component getLastFocusedCompnent(); public void setLastFocused(Component lastFocused); public int getMajorVersionNumber(); public int getMinorVersionNumber(); public int getBuildNumber(); } 메서드 개수를 5개 정도로 줄인 클래스이다. 하지만 메서드 수가 적음에도 불구하고 책임이 너무 많다.\n클래스 이름 클래스 이름은 해당 클래스 책임을 기술해야 한다. 실제로 클래스 작명은 크기를 줄이는 첫 번째 관문이다.\n간결한 이름이 떠오르지 않는다면 대부분 클래스 크기가 너무 커서(책임이 많아서) 그렇다. 예를 들어 클래스 이름에 Processor, Manager, Super 등과 같이 모호한 단어가 있다면 클래스가 여러 책임을 떠안고있다는 뜻이다.\n클래스 설명 클래스 설명은 만일(if), 그리고(and), (하)며(or), 하지만(but)을 사용하지 않고서 25단어 내외로 가능해야 한다.\n위 SuperDashboard 의 설명을 만들면\n“마지막으로 포커스를 얻었던 컴포넌트에 접근하는 방법을 제공하며, 버전과 빌드 번호를 추적하는 메커니즘을 제공한다.”\n인데 “~하며”가 포함되므로 책임이 많다고 생각할 수 있다.\n단일 책임 원칙 - SRP: Single Responsibility Principle 단일 책임 원칙은 클래스나 모듈을 변경할 이유가 단 하나뿐이어야 한다는 원칙이다.\nSRP는 ‘책임’이라는 개념을 정의하며 적절한 클래스 크기를 제시한다. 클래스는 책임, 즉 변경할 이유가 하나여야 한다는 의미다.\n겉보기에 작아보이는 위 SuperDashboard는 변경할 이유가 2가지다.\n소프트웨어 버전 정보를 추적한다. 버전 정보는 소프트웨어를 출시할 때마다 달라진다. 자바 스윙 컴포넌트를 관리한다. SuperDashboard는 최상위 GUI 윈도의 스윙 표현인 JFrame에서 파생한 클래스다. 즉, 스윙 코드를 변경할 때마다 버전 번호가 달라진다. 책임(변경할 이유)을 파악하려 애쓰다 보면 코드를 추상화 하기도 쉬워진다.\n1 2 3 4 5 public class Version { public int getMajorVersionNumber(); public int getMinorVersionNumber(); public int getBuildNumber(); } 버전 정보를 다루는 메서드 세 개를 따로 빼내 version이라는 독자적인 클래스를 만든다.\nSRP는 객체 지향 설계에서 더욱 중요한 개념이다. 또한 이해하고 지키기 수월한 개념이기도 하다. 하지만 이상하게도 SRP는 클래스 설계자가 가장 무시하는 규칙 중 하나다.\n소프트웨어를 돌아가게 만드는 활동과 소프트웨어를 깨끗하게 만드는 활동은 완전히 별개다.\n개발자는 자잘한 책임 클래스가 많아지면 큰 그림을 이해하기 어려워진다고 우려한다. 큰 그림을 이해하려면 이 클래스 저 클래스를 수없이 넘나들어야 한다고 걱정한다.\n하지만 작은 클래스가 많은 시스템이든 큰 클래스가 몇 개뿐인 시스템이든 익힐 내용은 그 양이 비슷하다.\n클래스를 작게 유지하는 행위는 코드를 체계적으로 정리하는 과정으로, 단일 책임을 가진 클래스를 만들고 이름을 잘 붙인다면 자연스럽게 코드가 정리된다.\n책임이 많은 클래스는 큰 공구함에 여러 공구를 마구 때려넣는 행동과 비슷하다. 응집도 클래스는 인스턴스 변수 수가 적어야 한다. 각 클래스 메서드는 클래스 인스턴스 변수를 하나 이상 사용해야 한다.\n일반적으로 메서드가 변수를 더 많이 사용할수록 메서드와 클래스는 응집도가 더 높다. 모든 인스턴스 변수를 메서드마다 사용하는 클래스는 응집도가 가장 높다.\n일반적으로 이처럼 응집도가 가장 높은 클래스는 가능하지도 바람직하지도 않다. 그렇지만 대부분 응집도가 높은 클래스를 선호한다. 응집도가 높다는 말은 클래스에 속한 메서드와 변수가 서로 의존하며 논리적인 단위로 묶인다는 의미다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class Stack { private int topOfStack = 0; List\u0026lt;Integer\u0026gt; elements = new LinkedList\u0026lt;Integer\u0026gt;(); public int size() { return topOfStack; } public void push(int element) { topOfStack++; elements.add(element); } public int pop() throws PoppedWhenEmpty { if (topOfStack == 0) { throw new PoppedWhenEmpty(); } int element = elements.get(--topOfStack); elements.remove(topOfStack); return element; } } 위 클래스는 응집도가 아주 높다. size()를 제외한 다른 두 메서드는 두 변수를 모두 사용한다.\n함수를 작게, 매개변수 목록을 짧게라는 전략을 따르다 보면 때때로 몇몇 메서드만이 사용하는 인스턴스 변수가 아주 많아진다.\n응집도가 높아지도록 변수와 메서드를 적절히 분리해 새로운 클래스 두세 개로 쪼개준다.\n응집도를 유지하면 작은 클래스 여럿이 나온다. 큰 함수를 작은 함수 여럿으로 나누기만 해도 클래스 수가 많아진다.\n예를 들어 변수가 아주 많은 큰 함수 하나가 있다. 큰 함수 일부를 작은 함수 하나로 빼내고 싶은데, 빼내려는 코드가 큰 함수에 정의된 변수 넷을 사용한다. 그렇다면 변수 네 개를 새 함수에 인수로 넘겨야 옳은가?\n전혀 아니다. 만약 네 변수를 클래스 인스턴스 변수로 승격한다면 새 함수는 인수가 필요없다. 그만큼 함수를 쪼개기 쉬워진다.\n이렇게 하면 클래스가 응집력을 잃는다. 몇몇 함수만 사용하는 인스턴스 변수가 점점 더 늘어나기 때문이다.\n몇몇 함수가 몇몇 변수만 사용한다면 독자적인 클래스로 분리하면 된다. 응집력을 잃는다면 쪼개라\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 package literatePrimes; public class PrintPrimes { public static void main(String[] args) { final int M = 1000; final int RR = 50; final int CC = 4; final int WW = 10; final int ORDMAX = 30; int P[] = new int[M+1]; int PAGENUMBER; int PAGEOFFSET; int ROWOFFSET; int C; int J; int K; boolean JPRIME; int ORD; int SQUARE; int N; int MULT[] = new int[ORDMAX+1]; J = 1; K = 1; P[1] = 2; ORD = 2; SQUARE = 9; while (K \u0026lt; M) { do { J = J+2; if (J == SQUARE) { ORD = ORD + 1; SQUARE = P[ORD] * P[ORD]; MULT[ORD-1] = J; } N = 2; JPRIME = true; while (N \u0026lt; ORD \u0026amp;\u0026amp; JPRIME) { while (MULT[N] \u0026lt; J) { MULT[N] = MULT[N] + P[N] + P[N]; } if (MULT[N] == J) { JPRIME = false; } N = N + 1; } } while (!JPRIME); K = K + 1; P[K] = J; } { PAGENUMBER = 1; PAGEOFFSET = 1; while (PAGEOFFSET \u0026lt;= M) { System.out.println(\u0026#34;The First \u0026#34; + M + \u0026#34; Prime Numbers --- Page \u0026#34; + PAGENUMBER); System.out.println(\u0026#34;\u0026#34;); for (ROWOFFSET = PAGEOFFSET; ROWOFFSET \u0026lt; PAGEOFFSET + RR; ROWOFFSET++) { for (C = 0; C \u0026lt; CC; C++) { if (ROWOFFSET + C * RR \u0026lt;= M) { System.out.println(\u0026#34;%10d\u0026#34;, P[ROWOFFSET + C * RR]); } } System.out.println(\u0026#34;\u0026#34;); } System.out.println(\u0026#34;\\f\u0026#34;); PAGENUMBER = PAGENUMBER + 1; PAGEOFFSET = PAGEOFFSET + RR * CC; } } } } 함수가 하나뿐인 위 프로그램은 엉망이다.\n들여쓰기 심함 이해하기 힘든 변수 이름 구조가 빽빽함 → 여러함수로 나눠야한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package literatePrimes; public class PrimePrinter { public static void main(String[] args) { final int NUMBER_OF_PRIMES = 1000; int[] primes = PrimeGenerator.generate(NUMBER_OF_PRIMES); final int ROWS_PER_PAGE = 50; final int COLUMNS_PER_PAGE = 4; RowColumnPagePrinter tablePrinter = new RowColumnPagePrinter(ROWS_PER_PAGE, COLUMNS_PER_PAGE, \u0026#34;The First \u0026#34; + NUMBER_OF_PRIMES + \u0026#34; Prime Numbers\u0026#34;); tablePrinter.print(primes); } } ... 가장 먼저 눈에 띄는 변화가 프로그램이 길어졌다. 길이가 늘어난 이유는 여러가지다.\n리팩터링한 프로그램은 좀 더 길고 서술적인 변수 이름을 사용한다. 리팩터링한 프로그램은 코드에 주석을 추가하는 수단으로 함수 선언과 클래스 선언을 활용한다. 가독성을 높이고자 공백을 추가하고 형식을 맞추었다. PrimePrinter 클래스는 main 함수 하나만 포함하며 실행 환경을 책임진다. 호출 방식이 달라지면 클래스도 바뀐다. 예를들어 SOAP 서비스로 바꾸려면 PrimePrinter 클래스를 고친다.\nPrimeGenerator 클래스는 소수 목록을 생성하는 방법을 안다. 코드를 살펴보면 알겠지만, 객체로 인스턴스화하는 클래스가 아니다. 단순히 변수를 선언하고 감추려고 사용하는 유용한 공간일 뿐이다.\n소수를 계산하는 알고리즘이 바뀐다면 PrimeGenerator 클래스를 고친다. 변경하기 쉬운 클래스 대다수 시스템은 지속적인 변경이 가해진다. 그리고 뭔가 변경할 때마다 시스템이 의도대로 동작하지 않을 위험이 따른다. 깨끗한 시스템은 클래스를 체계적으로 정리해 변경에 수반하는 위험을 낮춘다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class Sql { public Sql(String table, Column[] columns); public String create(); public String insert(Object[] fields); public String selectAll(); public String findByKey(String keyColumn, String keyValue); public String select(Column column, String pattern); public String select(Criteria criteria); public String preparedInsert(); private String columnList(Column[] columns); private String valuesList(Object[] fields, final Column[] columns); private String selectWithCriteria(String criteria); private String placeholderList(Column[] columns); } update를 아직 지원하지 않는 코드다. 지원할 시점이 오면 클래스에 손대어 코쳐야 한다. 문제는 코드에 손대면 위험이 생긴다는 사실이다. 어떤 변경이든 클래스에 손대면 다른 코드를 망가뜨릴 잠정적인 위험이 존재한다.\n새로운 SQL문을 지원하려면 반드시 Sql 클래스에 손대야 한다. 또한 기존 SQL문 하나를 수정할 때도 반드시 Sql 클래스에 손대야 한다. 이렇듯 변경할 이유가 두 가지이므로 Sql 클래스는 SRP를 위반한다.\n단순히 구조적인 관점에서도 Sql은 SRP를 위반한다. selectWithCriteria라는 비공개 메서드가 있는데, 이 메서드는 select문을 처리할 때만 사용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 abstract public class Sql { public Sql(String table, Column[] columns) abstract public String generate(); } public class CreateSql extends Sql { public CreateSql(String table, Column[] columns) @Override public String generate(); } public class SelectSql extends Sql { public SelectSql(String table, Column[] columns) @Override public String generate(); } public class InsertSql extends Sql { public InsertSql(String table, Column[] columns) @Override public String generate(); } public class SelectWithCriteraSql extends Sql { public SelectWithCriteraSql(String table, Column[] columns) @Override public String generate(); } public class SelectWithMatchSql extends Sql { public SelectWithMatchSql(String table, Column[] columns, Criteria criteria) @Override public String generate(); } public class FindByKeySql extends Sql { public FindByKeySql(String table, Column[] columns, Column column, String pattern) @Override public String generate(); } public class CreateSql extends Sql { public CreateSql(String table, Column[] columns, String keyColumn, String keyValue) @Override public String generate(); } public class PreparedInsertSql extends Sql { public PreparedInsertSql(String table, Column[] columns) @Override public String generate(); private String placeholderList(Column[] columns); } public class Where extends Sql { public Where(String criteria) @Override public String generate(); } public class ColumnList extends Sql { public ColumnList(Column[] columns) @Override public String generate(); } 공개 인터페이스를 각각 Sql클래스에서 파생하는 클래스로 만들었다. valueList와 같은 비공개 메서드는 해당하는 파생 클래스로 옮겼다. 모든 파생 클래스가 공통으로 사용하는 비공개 메서드는 Where와 ColumnList라는 두 유틸리티 클래스에 넣었다.\n클래스를 서로 분리하여 각 클래스는 극도로 단순하며 이해하기 쉽다. 함수 하나를 수정했다고 다른 함수가 망가질 위험도 사실상 없다. 테스트 관점에서 모든 논리를 구석구석 증명하기도 쉬워졌다. 새로운 sql을 넣을때도 기존 클래스를 변경할 필요가 전혀 없다.\n새 기능을 수정하거나 기존 기능을 변경할 때 건드릴 코드가 최소인 시스템 구조가 바람직하다. 이상적인 시스템이라면 새 기능을 추가할 때 시스템을 확장할 뿐 기존 코드를 변경하지 않는다. 변경으로부터 격리 요구사항은 변하기 마련이다. 따라서 코드도 변하기 마련이다.\n상세한 구현에 의존하는 코드는 테스트가 어렵다. 예를들어 Portfolio 클래스를 만든다고 가정하자.\nPortfolio 클래스는 외부 TokyoStockExchange API를 이용해 포트폴리오 값을 계삲나다. 따라서 테스트 코드는 시세 변화에 영향을 받게 된다.\n5분마다 값이 달라지는 API로 테스트 코드를 짜기는 어렵다.\n1 2 3 public interface StorckExchange { Money CurrentPrice(String symbol); } Portfolio에서 TokyoStockExchange API를 직접 호출하는 대신 StockExchange라는 인터페이스를 생성한 후 메서드 하나를 선언했다.\n다음으로 StockExchange 인터페이스를 구현하는 TokyoStockExchange 클래스를 구현한다.\n1 2 3 4 5 6 7 public Portfolio { private StockExchange exchange; public Portfolio(StockExchange exchange) { this.exchange = exchange; } ... } Portfolio를 수정해 StockExchange 참조자를 인수로 받는다.\n이를 통해 TokyoStockExchange 클래스를 흉내내는 테스트용 클래스를 만들 수 있게 되었다.\n테스트용 클래스는 StockExchange 인터페이스를 구현하며 고정된 주가를 반환한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public class PortfolioTest { private FixedStockExchangeStub exchange; private Portfolio portfolio; @Before protected void setUp() throws Exception { exchange = new FixedStockExchangeStub(); exchange.fix(\u0026#34;MSFT\u0026#34;, 100); protfolio = new Portfolio(exchange); } @Test public void GivenFiveMSFTTotalShouldBe500() throws Exception { portfolio.add(5, \u0026#34;MSFT\u0026#34;); Assert.assertEquals(500, portfolio.value()); } } 위와 같은 테스트가 가능할 정도로 시스템의 결함도를 낮추면 유연성과 재사용성도 더욱 높아진다.\n결합도가 낮다는 소리는 각 시스템 요소가 다른 요소로부터 그리고 변경으로부터 잘 격리되어 있다는 의미다. 시스템 요소가 서로 잘 결리되어 있으면 각 요소를 이해하기도 더 쉬워진다. 이렇게 결합도를 최소로 줄이면 자연스럽게 또 다른 클래스 설계 원칙인 DIP(Dependency Inversion Principle)를 따르는 클래스가 나온다.\n본질적으로 DIP는 클래스가 상세한 구현이 아니라 추상화에 의존해야 한다는 원칙이다.\n결론 공개가 필요할땐 최대한 캡슐화를 유지하는 방식을 강구해야 한다. 클래스도 작게 유지해야 한다. 클래스의 크기는 책임 수로 측정한다. 책임이 많은 클래스는 이름과 설명에 모호한 표현이 섞이게 된다. 응집도를 유지하면 자연스럽게 클래스가 작아진다. 변경은 여러 문제들을 수반한다. 따라서 기존 코드 변경이 없는것이 좋다. 그대로 변경은 항상 필요할 수 있으므로 변경하기 쉽게 만들어야 한다. 클래스를 깨끗하게 정리하면 변경하기 쉽다. 클래스를 작게 만들면 자연스럽게 정리된다. 결합도를 최대한 낮추면 변경으로 부터 격리할 수 있다. ","date":"2023-04-20T16:30:13+09:00","image":"https://codemario318.github.io/post/clean-code/10/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean-code/10/","title":"클린코드: 10. 클래스"},{"content":"애자일과 TDD 덕택에 단위 테스트를 자동화하는 개발자들이 이미 많아졌으며 점점 더 늘어나는 추세다.\n테스트를 급하게 서두르는 와중에 많은 개발자들이 제대로 된 테스트 케이스를 작성해야한다는 중요한 사실을 놓친다.\nTDD 법칙 세 가지 실패하는 단위 테스트를 작성할 때까지 실제 코드를 작성하지 않는다. 컴파일은 실패하지 않으면서 실행이 실패하는 정도로만 단위 테스트를 작성한다. 현재 실패하는 테스트를 통과할 정도로만 실제 코드를 작성한다. 위 세 가지 규칙을 따르면 개발과 테스트가 대략 30초 주기로 묶인다. 테스트 코드와 실제 코드가 함께 나올뿐더러 테스트 코드가 실제 코드보다 불과 몇 초 전에 나온다.\n이렇게 일하면 실제 코드를 사실상 전부 테스트하는 테스트 케이스가 나오지만, 실제 코드와 맞먹을 정도로 방대한 테스트 코드는 심각한 관리 문제를 유발하기도 한다.\n깨끗한 테스트 코드 유지하기 테스트 코드는 실제 코드 못지 않게 중요하다. 실제 코드 못지 않게 깨끗하게 짜야한다.\n지저분한 테스트 코드는 테스트를 하지 않는 것 보다 더 안좋은 결과를 만들 수 있다.\n단위 테스트는 코드 변경에 대한 안전함을 보장하여 유연성, 유지보수성, 재사용성을 지키는 버팀목이 된다.\n실제 코드가 진화하면 테스트 코드도 변해야 하는데, 테스트 코드가 지저분할수록 실제 코드를 변경하기 어려워진다. 결과적으로 테스트 코드가 복잡할수록 실제 코드를 짜는 시간보다 테스트 케이스를 추가하는 시간이 더 걸리게 된다.\n실제 코드를 변경해 기존 테스트 케이스가 실패하기 시작하면, 지저분한 코드로 인해, 실패하는 테스트 케이스를 점점 더 통과시키키 어려워진다. 그래서 테스트 코드는 계속해서 늘어나는 부담이 되고 악순환이 발생할 수 있다.\n테스트 슈트가 없으면 개발자는 자신이 수정한 코드가 제대로 도는지 확인할 방법이 없다. 시스템을 수정했을때 다른 문제가 발생하지 않는다는 것을 확인할 방법이 없다. 결함율이 높아지기 시작한다. 의도하지 않은 결함 수가 많아지면 개발자는 변경을 주저한다. 변경하면 득보다 해가 크다 생각해 더 이상 코드를 정리하지 않는다. 테스트는 유연성, 유지보수성, 재사용성을 제공한다. 테스트 코드를 깨끗하게 유지하지 않으면 결국은 잃어버린다. 그리고 테스트 케이스가 없으면 실제 코드를 유연하게 만드는 버팀목도 사라진다.\n테스트 케이스가 없다면 모든 변경이 잠정적인 버그다. 아무리 아키텍처가 유연하더라도, 설계를 아무리 잘 했어도 버그가 없다는 것을 증명할 수 없기 때문이다.\n깨끗한 테스트 코드 가독성, 가독성, 가독성!\n가독성은 실제 코드보다 테스트 코드에 더더욱 중요하다. 테스트 코드의 가독성을 높히려면 여느 코드와 마찬가지로 명료성, 단순성, 풍부한 표현력이 필요하다.\n테스트 코드는 최소 표현으로 많은 것을 나타내야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public void testGetPageHierarchyAsXml() throws Exception { makePages(\u0026#34;pageOne\u0026#34;, \u0026#34;PageOne.ChildOne\u0026#34;, \u0026#34;PageTwo\u0026#34;); submintRequest(\u0026#34;root\u0026#34;, \u0026#34;type:pages\u0026#34;); assertResponseIsXML(); assertResponseContains( \u0026#34;\u0026lt;name\u0026gt;PageOne\u0026lt;/name\u0026gt;\u0026#34;, \u0026#34;\u0026lt;name\u0026gt;PageTwo\u0026lt;/name\u0026gt;\u0026#34;, \u0026#34;\u0026lt;name\u0026gt;ChildOne\u0026lt;/name\u0026gt;\u0026#34; ); assertResponseDoesNotContain(\u0026#34;SymPage\u0026#34;); } public void testGetDataAsXml() throws Exception { makePageWithContent(\u0026#34;TestPageOne\u0026#34;, \u0026#34;test page\u0026#34;); submintRequest(\u0026#34;TestPageOne\u0026#34;, \u0026#34;type:data\u0026#34;); assertResponseIsXML(); assertResponseContains(\u0026#34;test page\u0026#34;, \u0026#34;\u0026lt;Test\u0026#34;); } BUILD-OPERATE-CHECK 패턴 Usual production patterns applied to Integration tests\n테스트를 세 부분으로 나눈다.\nBUILD: 테스트 자료를 만든다. OPERATE: 테스트 자료를 조작한다. CHECK: 조작한 결과가 올바른지 확인한다. BUILD-OPERATE-CHECK 패턴을 사용하면 잡다하고 세세한 코드를 거의 다 없앨 수 있다.\n도메인에 특화된 테스트 언어 도메인 특화 언어(DSL)에 관한 설명 | JetBrains MPS\n흔히 쓰는 시스템 조작 API를 사용하는 대신 API 위에다 함수와 유틸리티를 구현한 후 그함수와 유틸리티를 사용하므로 테스트 코드를 짜기도 읽기도 쉬어진다.\n이렇게 구현한 함수와 유틸리티는 테스트 코드에서 사용하는 특수 API가 된다. 즉 테스트를 구현하는 당사자와 나중에 테스트를 읽어볼 독자를 도와주는 테스트 언어이다.\n이중 표준 테스트 API 코드에 적용하는 표준은 실제 코드에 적용하는 표준과 확실히 다르다.\n단순하고, 간결하고, 표현력이 풍부해야 하지만, 실제 코드만큼 효율적일 필요는 없다.\n또한 실제 환경과 테스트 환경은 요구사항이 판이하게 다르다.\n1 2 3 4 5 6 7 8 9 10 @Test public void turnOnLoTempAlarmAtThreashold() throws Exception { hw.setTemp(WAY_TOO_COLD); controller.tic(); assertTrue(hw.heaterState()); assertTrue(hw.blowerState()); assertFalse(hw.coolerState()); assertFalse(hw.hiTempAlarm()); assertTrue(hw.loTempAlarm()); } 1 2 3 4 5 @Test public void turnOnLoTempAlarmAtThreashold() throws Exception { wayTooCold(); assertEquals(\u0026#34;HBchL\u0026#34;, hw.getState()); } 실제 시스템이 돌아가는 환경은 가혹할 지 모르나, 대부분 테스트 환경은 자원이 제한적일 가능성이 낮다.\n실제 환경에서는 절대로 안 되지만 테스트 환경에서는 전현 문제없는 방식이 있다. 대개 메모리나 CPU 효율과 관련있는 경우인데 코드의 깨끗함과는 철저히 무관하다.\n테스트 당 assert 하나 assert문이 단 하나인 함수는 결론이 하나라서 코드를 이해하기 쉽고 빠르다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 public void testGetPageHierarchyAsXml() throws Exception { givenPages(\u0026#34;PageOne\u0026#34;, \u0026#34;pageOne.ChildOne\u0026#34;, \u0026#34;PageTwo\u0026#34;); whenRequestIsIssued(\u0026#34;root\u0026#34;, \u0026#34;type:pages\u0026#34;); thenResponseShouldBeXML(); } public void testGetPageHiereachyHasRightTags() throws Exception { givenPages(\u0026#34;PageOne\u0026#34;, \u0026#34;pageOne.ChildOne\u0026#34;, \u0026#34;PageTwo\u0026#34;); whenRequestIsIssued(\u0026#34;root\u0026#34;, \u0026#34;type:pages\u0026#34;); thenResponseShouldContain( \u0026#34;\u0026lt;name\u0026gt;PageOne\u0026lt;/name\u0026gt;\u0026#34;, \u0026#34;\u0026lt;name\u0026gt;PageTwo\u0026lt;/name\u0026gt;\u0026#34;, \u0026#34;\u0026lt;name\u0026gt;ChildOne\u0026lt;/name\u0026gt;\u0026#34; ); } 테스트를 분리하면 위 함수 1~2처럼 중복되는 코드가 많아지는데, TEMPLATE METHOD 패턴을 사용하면 중복을 제거할 수 있다.\ngiven/when 부분을 부모 클래스에 두고 소두 부분을 자식 클래스에 두면 된다. 아니면 완전히 독자적인 테스트 클래스를 만들어 @Before 함수에 given/when 부분을 넣고 @Test 함수에 then부분을 넣어도 된다.\n하지만 두 경우 모두 과도한 느낌이 있는데 이것저것 감안해 보면 결국 assert 문을 여렷 사용하는 편이 좋을수도 있다.\n💡 단일 assert를 지원하는 해당 분야 테스트 언어를 만드는 것이 이점이 많으나, 과하다면 꼭 지킬 필요는 없다. 하지만 assert 문 개수를 최대한 줄이려고 노력해야 한다.\n테스트당 개념 하나 테스트 함수마다 한 개념만 테스트하라. 이것저것 잡다한 개념을 연속으로 테스트하는 긴 함수는 피한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 /** * addMonths() 메서드를 테스트하는 장황한 코드 */ public void testAddMonths() { SerialDate d1 = SerialDate.createInstance(31, 5, 2004); SerialDate d2 = SerialDate.addMonths(1, d1); assertEquals(30, d2.getDayOfMonth()); assertEquals(6, d2.getMonth()); assertEquals(2004, d2.getYYYY()); SerialDate d3 = SerialDate.addMonths(2, d1); assertEquals(31, d3.getDayOfMonth()); assertEquals(7, d3.getMonth()); assertEquals(2004, d3.getYYYY()); SerialDate d4 = SerialDate.addMonths(1, SerialDate.addMonths(1, d1)); assertEquals(30, d4.getDayOfMonth()); assertEquals(7, d4.getMonth()); assertEquals(2004, d4.getYYYY()); } 위 테스트 코드는 독자적인 개념 세 개를 테스트하므로 독자적인 테스트 세개로 쪼개는 것이 좋다. 새 개념을 한 함수로 몰아넣으면 독자가 각 절이 존재하는 이유와 테스트하는 개념을 모두 이해해야 하기 때문이다.\n31일로 끝나는 달의 마지막 날짜가 주어지는 경우 30일로 끝나는 한 달을 더하면 날짜는 30일이 되어야지 31일이 되어서는 안된다. 두 달을 더하면 그리고 두 번째 달이 31일로 끝나면 날짜는 31일이 되어야 한다. 30일로 끝나는 달의 마지막 날짜가 주어지는 경우 31일로 끝나는 한 달을 더하면 날짜는 30일이 되어야지 31일이 되면 안된다. 위 테스트코드는 assert문이 여렷이라는 사실이 문제가 아니라, 한 테스트 함수에서 여러 개념을 테스한다는 사실이 문제다.\n“개념 당 assert 문 수를 최소로 줄여라”와 “테스트 함수 하나는 개념 하나만 테스트하라”\nF.I.R.S.T 깨끗한 테스트는 다음 다섯가지 규칙을 따른다.\nF(Fast) 빠르게\n테스트는 빨라야 한다. 테스트가 느리면 자주 돌릴 엄두를 못 낸다, 자주 돌리지 않으면 초반에 문제를 찾아내기 힘들다. 이에 따라 코드를 마음껏 정리하지 못해지고, 결국 코드 품질이 망가지기 시작한다.\nI(Independent) 독립적으로\n각 테스트는 서로 의존하면 안된다.\n한 테스트가 다음 테스트가 실행될 환경을 준비해서는 안된다. 각 테스트는 독립적으로 그리고 어떤 순서로 실행해도 괜찮아야 한다.\n테스트가 서로에게 의존하면 하나가 실패할 때 나머지도 잇달아 실패하므로 원인을 진단하기 어려워지며 후반 테스트가 찾아내야 할 결함을 숨긴다.\nR(Repeatable) 반복가능하게\n테스트는 어떤 환경에서도 반복 가능해야 한다.\n테스트가 돌아가지 않는 환경이 하나라도 있다면 테스트가 실패한 이유를 둘러댈 변명이 생긴다. 또한 환경이 지원되지 않기에 테스트를 수행하지 못하는 상황에 직면한다.\nS(Self-Validating) 자가검증하는\n테스트는 부울 값으로 결과를 내야 한다.\n성공 아니면 실패다. 통과 여부를 알려고 로그 파일을 읽게 만들어서는 안 된다. 통과 여부르 보려고 텍스트 파일 두개를 수작업으로 비교하게 만들어서도 안 된다.\nT(Timely) 적시에\n테스트는 적시에 작성해야 한다.\n단위 테스트는 테스트하려는 실제 코드를 구현하기 직전에 구현한다.\n실제 코드를 구현한 다음에 테스트 코드를 만들면 실제 코드가 테스트에 적합하지 않은 구조로 만들어 질 수 있다.\n결론 테스트 코드는 실제 코드만큼이나 프로젝트 건강에 중요하다. 어쩌면 실제 코드보다 더 중요할지도 모른다.\n테스트 코드가 방치되어 망가지면 실제 코드도 망가진다.\n내 결론 테스트 코드는 변경에 대한 안전장치, 버팀목이며 이를 통해 코드에 유연성, 유지보수성, 재사용성을 보존하고 강화한다. 테스트 코드를 깨끗히 유지하지 않으면, 테스트 코드 작성, 유지가 어려워져 방치하게 된다. 결국 코드 변경에 대한 두려움을 만들게 되고 이러한 두려움이 실제 코드의 품질을 점점 떨어뜨린다. 테스트 코드는 가독성이 가장 중요하다. 테스트 API를 구현해 도메인 특화 언어를 만들자. 도메인에 최적화된 테스트 코드를 만들자. assert문은 최소한으로 유지하자 여러개의 assert문이 있다는 뜻은 코드 자체 가독성을 떨어뜨린다. assert 문이 여러개면 여러 개념을 테스트 하고 있을 지도 모른다. 테스트 코드 하나는 하나의 개념만 테스트 해야한다. 여러 개념을 테스트하면 테스트 코드가 어떤 동작을 테스트하는지 알기 위해 고민해야한다. (가독성이 떨어진다) 테스트 코드를 먼저 만드는 이유는 테스트에 적합한 실제 코드를 만드는데 도움이 되기 때문이다. ","date":"2023-04-20T16:22:13+09:00","image":"https://codemario318.github.io/post/clean-code/9/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean-code/9/","title":"클린코드: 9. 단위 테스트"},{"content":" 뭔가 잘못되면 바로잡을 책임은 바로 우리 프로그래머에게 있다.\n깨끗한 코드와 오류처리는 연관이 있다. 상당수 코드들은 전적으로 오류 처리 코드에 좌우된다. 여기저기 흩어진 오류 처리 코드 때문에 실제 코드가 하는 일을 파악하기가 거의 불가능하다는 의미다.\n💡 오류 처리는 중요하지만 이로 인해 프로그램 논리를 이해하기 어려워진다면 깨끗한 코드라 부르기 어렵다.\n오류 코드보다 예외를 사용하라 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public class DeviceController { ... piblic void sendShutDown() { DeviceHandle handle = getHandle(DEV1); if (handle != DeviceHandle.INVALID) { retrieveDeviceRecord(handle); if (record.getStatus() != DEVICE_SUSPENDED) { pauseDevice(handle); clearDeviceWorkQueue(handle); closeDevice(handle); } else { logger.log(\u0026#34;Device suspended. Unable to shut down\u0026#34;); } } else { logger.log(\u0026#34;Invalid handle for: \u0026#34; + DEV1.toString()); } } ... } 위와 같은 방법을 사용하면 호출자 코드가 복잡해진다. 함수를 호출한 즉시 오류를 확인해야 하기 때문이다.\n오류가 발생하면 예외를 던지면, 실제 구현 로직이 오류 처리 코드와 섞이지 않게 되어 호출자가 깔끔해진다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class DeviceController { ... public void sendShutDown() { try { tryToShutDown(); } catch (DeviceShutDownError e) { logger.log(e); } } private void tryToshutDown() throw DeviceShutDownError { DeviceHandle hadle = getHandle(DEV1); DeviceRecord record = retrieveDeviceRecord(handle); pauseDevice(handle); clearDeviceWorkQueue(handle); closeDevice(handle); } private DeviceHandle getHandle(DeviceId id) { ... throw new DeviceShutDownError(\u0026#34;Invalid handle for: \u0026#34; + id.toString()); } ... } 뒤섞여있던 디바이스 종료 로직과 오류 처리 로직이 분리되었다. 이를 통해 각 개념을 독립적으로 살펴보고 이해할 수 있다.\nTry-Catch-Finally 문부터 작성하라 예외처리는 로직 내부에 범위를 정의한다.\ntry-catch-finally 문에서 try 블록에 들어가는 코드를 실행하면 어느 시점에서든 실행이 중단된 후 catch 블록으로 넘어갈 수 있다.\ntry 블록에서 무슨 일이 생겨도 catch 블록은 프로그램 상태를 일관성 있게 유지해야 한다.\n예외가 발생할 코드를 짤 때는 try-catch-finally 문을 만들어놓고 시작하면 try 블록에서 무슨 일이 생기든지 호출자가 기대하는 상태를 정의하기 쉬워진다.\n1 2 3 4 5 // 파일이 없으면 예외를 던지는지 알아보는 단위 테스트 @Test(expcted = StroageException.class) public void retrieveSectionShouldThrowOnInvalidFileName() { sectionStore.retrieveSection(\u0026#34;invalid - file\u0026#34;); } 1 2 3 public List\u0026lt;RecordedGrip\u0026gt; retrieveSection(String sectionName) { return new ArrayList\u0026lt;RecordedGrip\u0026gt;(); } 코드가 예외를 던지지 않으므로 단위 테스트는 실패한다. 아래 코드는 예외를 던진다.\n1 2 3 4 5 6 7 8 public List\u0026lt;RecordedGrip\u0026gt; retrieveSevtion(String sectionName) { try { FileInputStream stream = new FileInputStream(sectionName); } catch (Exception e) { throw new StorageException(\u0026#34;retrieval error\u0026#34;, e); } return new ArrayList\u0026lt;RecordedGrip\u0026gt;(); } 코드가 예외를 던지므로 이제는 테스트가 성공한다. 이 시점에서 리팩터링이 가능하다.\ncatch 블록에서 예외 유형을 좁혀 실제로 FileInputStream 생성자가 던지는 FileNotFoundException을 잡아낸다.\n1 2 3 4 5 6 7 8 9 public List\u0026lt;RecordedGrip\u0026gt; retrieveSevtion(String sectionName) { try { FileInputStream stream = new FileInputStream(sectionName); stream.close(); } catch (FileNotFoundException e) { throw new StorageException(\u0026#34;retrieval error\u0026#34;, e); } return new ArrayList\u0026lt;RecordedGrip\u0026gt;(); } try-catch 구조로 범위를 정의했으므로 TDD를 이용해 필요한 나머지 논리를 추가한다. 나머지 논리는 FileInputStream을 생성하는 코드와 close 호출문 사이에 넣으며 오류나 예외가 전혀 발생하지 않는다고 가정한다.\n미확인(unchecked) 예외를 사용하라 여러 해 동안 자바 프로그래머들은 확인된(checked) 예외의 장단점을 놓고 논쟁을 벌여왔다. 안정적인 소프트웨어를 제작하는 요소로 확인된 예외가 반드시 필요하지 않다는 사실이 분명해졌다.\nJava의 체크 된 예외와 체크되지 않은 예외의 차이점\n확인된 예외: 컴파일 과정에서 발견되는 예외\nRuntimeExeption 클래스를 제외한 Exeption 클래스의 모든 하위 클래스 Error클래스와 그 하위 클래스 확인되지 않은 예외: 프로그램 실행 중 발생하는 예외, 컴파일러가 확인할 수 없는 예외\nRuntimeExeption 클래스와 해당 하위 클래스 확인된 예외는 OCP(Open Closed Principle) 를 위반한다.\n메서드에서 확인된 예외를 던졌는데 catch 블록이 세 단계 위에 있다면 그사이 메서드 모두가 선언부에 해당 예외를 정의해야 한다.\n하위 단계에서 코드를 변경하면 상위 단계 메서드 선언부를 전부 고쳐야 한다. 최하위 함수를 변경하여 새로운 오류를 던진다고 가정하고 확인된 오류를 던진다면 함수는 선언부에 throws 절을 추가해야 한다.\n변경한 함수를 호출하는 함수 모두가 catch 블록에서 새로운 예외를 처리한다. 선언부에 throw 절을 추가한다. 결과적으로 최하위 단계에서 최상위 단계까지 연쇄적인 수정이 일어난다. 다르게 해석하면, throws 경로에 위치하는 모든 함수가 최하위 함수에서 던지는 예외를 알아야 하므로 캡슐화가 깨진다.\n💡 확인된 예외를 이용하여 구현하게 되면 안전한 로직을 만들 수 있지만, 일반적인 애플리케이션은 확인된 예외를 통한 이익보다, 의존성이 더 중요하다.\n예외에 의미를 제공하라 예외를 던질 때 전후 상황을 충분히 덧붙히면, 오류가 발생한 원인과 위치를 찾기 쉬워진다.\n실패한 코드의 의도를 파악하려면 호출 수택 만으로 부족한 경우가 많아. 오류 메시지에 정보(실패한 연산 이름과 실패 유형)를 담아 예외와 함께 던진다.\n애플리케이션에서 로깅 기능을 사용한다면 catch 블록에서 오류를 기록하도록 충분한 정보를 넘겨주면 좋다.\n호출자를 고려해 예외 클래스를 정의하라 오류를 분류하는 방법은 수없이 많다.\n오류가 발생한 위치 오류가 발생한 컴포넌트 오류 유형 디바이스 실패 네트워크 실패 프로그래밍 오류 어플리케이션 오류를 정의할 때 프로그래머에게 가장 중요한 관심사는 오류를 잡아내는 방법이 되어야 한다.\n나쁜예 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ACMEport port = new ACMEport(12); try { port.open(); } catch (DeviceResponseException e) { reportPortError(e); logger.log(\u0026#34;Device response exception\u0026#34;, e); } catch (ATM1212UnlokedException e) { reportPortError(e); logger.log(\u0026#34;Unlock exception\u0026#34;. e); } catch (GMXRrror e) { reportPortError(e); logger.log(\u0026#34;Device response exeption\u0026#34;); } finally { ... } 위 코드는 중복이 심하여, 대다수 상황에서 오류를 처리하는 방식은 비교적 일정하다.\n오류를 기록한다. 프로그램을 계속 수행해도 좋은지 확인한다. 위 경우는 예외에 대응하는 방식이 예외 유형과 무관하게 거의 동일하다. 그래서 코드를 간결하게 고치기 쉽다.\n1 2 3 4 5 6 7 8 9 10 LocalPort port = new LocalPort(12); try { port.open(); } catch (portDeviceFailure e) { reportError(e); logger.log(e.getMessage(), e); } finally { ... } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class LocalPort { private ACMEPort innerPort; public LocalPort(int portNumber) { innerPort = new ACMEPort(portNumber); } public void open() { try { innerPort.open(); } catch (DeviceResponseException e) { throw new PortDeviceFailure(e); } catch (ATM1212UnlockedException e) { throw new PortDeviceFailure(e); } catch (GMXError e) { throw new PortDeviceFailure(e); } } ... } 여기서 LocalPort클래스는 단순히 ACMEport클래스가 던지는 예외를 잡아 변환하는 wrapper 클래스 일뿐이다.\n하지만 이러한 wrapper 클래스는 매우 유용하다. 실제로 외부 API를 사용할 때는 감싸기 기법이 최선이다.\n외부 API를 감싸면 외부 라이브러리와 프로그램 사이에서 의존성이 크게 줄어든다. 나중에 다른 라이브러리로 갈아타도 비용이 적다. 감싸기 클래스에서 외부 API를 호출하는 대신 테스트 코드를 넣어주는 방법으로 프로그램을 테스트하기도 쉬워진다. 특정 업체가 API를 설계한 방식에 발목 잡히지 않는다. 프로글매이 사용하기 편한 API를 정의하면 그만이다. 예외 클래스가 하나만 있어도 충분한 코드가 많다.\n예외 클래스에 포함된 정보로 오류를 구분해도 괜찮은 경우. 한 예외는 잡아내고 다른 예외는 무시해도 괜찮은 경우라면 여러 예외 클래스를 사용하는 것이 좋다.\n정상 흐름을 정의하라 정상 흐름을 정확히 정의하지 않으면 딴길로 샌다.\n1 2 3 4 5 6 try { MealExpenses expenses = expenseReportDAO.getMeals(employee.getID()); m_total += expenses.getTotal(); } catch(MealExpensesNotFound e) { m_total += getMealPerDiem(); } 위에서 식비를 비용으로 청구했다면 직원이 청구한 식비를 총계에 더한다. 식비를 비용으로 청구하지 않았다면 일일 기본 식비를 총계에 더한다. 그런데 예외가 논리를 따라가기 어렵게 만든다.\n1 2 MealExpenses expenses = expenseReportDAO.getMeals(employee.getID()); m_total += expenses.getTotal(); ExpensesReportDAO를 고쳐 청구한 식비가 없다면, 일일 기본 식비를 반환하는 MealExpense객체를 반환한다.\n1 2 3 4 5 public class PerDiemMealExpenses implements MealExpenses { public in getTotal() { // 기본값으로 일일 기본 식비를 반환 } } 이러한 형태를 **특수 사례 패턴(Special Case Pattern)**이라 부른다. 클래스를 만들거나 객체를 조작해 특수 사례를 처리하는 방식이다.\n클라이언트 코드가 예외적인 상황을 처리할 필요가 없어진다.\nnull을 반환하지 마라 null 반환은 흔히 사용되어 오류를 유발하는 경우가 많다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 public void registerItem(Item item) { if (item != null) { ItemRegistry registry = peristentStore.getItemRegistry(); if (registry != null { Item existing = registry.getItem(item.getID()); if (existing.getBillingPeriod().hasRetailOwner()) { existing.register(item); } } } } null을 반환하기 때문에 정상적인 동작 처리를 위해 null 인지 아닌지 확인하는 로직이 필요하여 일거리를 늘린다.\n이러한 코드는 호출자가 null 처리를 잊을 경우 문제가 발생하게는 구조로, 문제를 호출자에게 떠넘기기 때문에 좋은 구조라고 볼 수 없다.\n이러한 경우 특수 사례가 좋은 해결책이 될 수 있다.\n1 2 3 4 5 6 7 List\u0026lt;Empoloyee\u0026gt; employees = getEmpoloyees(); if (employees != null { for(Employee e : employees) { totalPay += e.getPay(); } } 위 getEmployees는 null도 반호나한다. 하지만 이런 경우 null반환하는 것이 아니라 빈 리스트를 반환한다면 코드가 훨씬 깔끔해진다.\n1 2 3 4 5 List\u0026lt;Employee\u0026gt; employees = getEmployees(); for ( Employees e L employees) { totalPay += e.getPay(); } 자바에는 Collections.emptyList()가 있어 미리 정의된 읽기 전용 리스트를 반환한다.\n1 2 3 4 5 public List\u0026lt;Employee\u0026gt; getEmployees() { if ( ... 직원이 없다면 ..) { return Collections.emptyList(); } } 위와 같이 코드를 변경하면 코드도 깔끔해지고 NullPointerException이 발생할 가능성도 줄어든다.\nnull을 전달하지 마라 메서드에서 null을 반환하는 방식도 나쁘지만 메서드로 null을 전달하는 방식은 더 나쁘다.\n1 2 3 4 5 6 public class MetricsCalculator { public double xProjection(Point p1, Point p2) { return (p2.x - p1.x) * 1.5; } ... } 누군가 인수로 null을 전달하면 NullPointerException이 발생한다.\n1 2 3 4 5 6 7 8 9 public class MetricsCalculator { public double xProjection(Point p1, Point p2) { if (p1 == null || p2 === null) { throw InvalidArgumentException( \u0026#34;Invalid argument for MetricsCalculator.xProjection\u0026#34;); } return (p2.x - p1.x) * 1.5; } } 위 코드로 입력이 null일 경우를 예방 했지만, InvalidArgumentException 잡아내는 처리기를 추가해야 한다.\n1 2 3 4 5 6 7 public class MetricsCalculator { public double xProjection(Point p1, Point p2) { assert p1 != null : \u0026#34;p1 should not be null\u0026#34;; assert p2 != null : \u0026#34;pw should not be null\u0026#34;; return (p2.x - p1.x) * 1.5; } } 위 코드는 assert문을 사용하여 처리했다.\nJava의 assert 키워드\n문서화가 잘 되어 코드 읽기는 편하지만 문제를 해결하지는 못한다. 누군가 null을 전달하면 여전히 실행 오류가 발생한다.\n💡 대다수 프로그래밍 언어는 호출자가 실수로 넘기는 null을 적절히 처리하는 방법이 없기 때문에, 애초에 null을 넘기지 못하도록 금지하는 정책이 합리적일 수 있다.\n결론 깨끗한 코드는 읽기도 좋아야 하지만 안정성도 높아야 한다.\n이 둘은 상충하는 목표가 아님 오류 처리를 프로그램 논리와 분리해 별도의 사안으로 고려해야함 더욱 튼튼하고 깨끗한 코드를 작성할 수 있게됨 독립적인 추론이 가능해지며 코드 유지보수성도 크게 높아짐 프로그램 논리와 분리해 별도로 고민하라. 오류 처리를 만들기 전에 흐름을 정확히 파악하면 안 쓸수도 있다. 정상 흐름을 정의하라. 특수 상황 패턴등을 이용 null은 사용자의 실수를 유발하므로 되도록 자제해야 한다. ","date":"2023-04-20T16:12:13+09:00","image":"https://codemario318.github.io/post/clean-code/7/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean-code/7/","title":"클린코드: 7. 오류 처리"},{"content":"자료 추상화 사용자가 구현을 모른 채 자료의 핵심을 조작할 수 있어야 진정한 의미의 클래스다.\n1 2 3 4 public class Point { public double x; public double y; } 1 2 3 4 5 6 7 8 public interface Point { double getX(); double getY(); void setCartesian(double x, double y); double getR(); double getTheta(); void setPolar(double r, double theta); } 두 클래스 모두 2차원 점을 표현하지만 6-1은구현을 외부로 노출하고, 6-2는 구현을 완전히 숨긴다.\n6-1은 변수를 private 으로 선언하더라도 각 값마다 getter, setter를 제공한다면 외부로 노출하는 것이다.\n변수 사이에 함수라는 계층을 넣는다고 구현이 저절로 감춰지지는 않는다. 구현을 감추려면 어느정도의 추상화가 필요하다.\n예시 1\n1 2 3 4 public interface Vehicle { double getFuelTankCapacityInGallons(); double getGallonsOfGasoline(); } 예시 2\n1 2 3 public interface Vehicle { double getPercentFuelRemaining(); } 예시 1은 연료탱크의 용량과 휘발유의 양을 받아올 수 있고, 예시 2는 백분율로 얼마나 더 남았는가를 받아올 수 있다.\n둘 중 예시 2가 더 추상적이지만, 백분율의 기준값을 알 수 없으므로 정확한 정보를 표현하지 못한다.\n인터페이스나 조회/설정 함수만으로는 추상화가 이뤄지지 않는다. 개발자는 객체가 포함하는 자료를 표현 할 가장 좋은 방법을 심각하게 고민해야 한다.\n자료/객체 비대칭 객체: 추상화 두로 자료를 숨긴 채 자료를 다루는 함수만 공개한다. 자료구조: 자료를 그대로 공개하며 별다른 함수는 제공하지 않는다. 절차적인 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 public class Square { public Point topLeft; public double side; } public class Rectangle { public Point topLeft; public double height; public double width; } public class Circle { public Point center; public double radius; } public class Geometry { public final double PI = 3.141592653589793; public double area(Object shape) throws NoSuchShapeExetion { if (shape instanceof Square) { Square s = (Square)shape; return s.side * s.side; } else if (shape instanceof Rectangle) { Rectangle r = (Rectangle)shape; return r.height * r.width; } else if (shape instanceof Circle) { Circle c = (Circle)shape; return PI * c.radious * c.radious; } throw new NoSuchShapeExcption(); } } 위 예시에서 만약 Geometry 클래스에 새로운 기능들을 추가해도 도형 클래스들은 영향을 받지 않는다. 위 예시에서 새 도형을 추가하고 싶다면 Geometry의 속한 기능들에 새로운 도형을 위한 코드를 추가해야 한다. 객체 지향적인 도형 클래스 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public class Square implements Shape { private Point topLeft; private double side; public double area() { return s.side * s.side; } } public class Rectangle implements Shape { public Point topLeft; public double height; public double width; public double area() { return height * width; } } public class Circle implements Shape { public Point center; public double radius; public double area() { return PI * radius * radius; } } 위 예시에서 만약 Shape에 새로운 기능들을 추가하면 상속받은 모든 클래스에 변경이 필요하다. 위 예시에서 Shape를 상속받은 다른 도형을 추가하고 싶다면 하나 새로 만들면 된다. 목록 절차적인 구현과 객체 지향적인 구현은 근본적으로 반대이며 상호 보완적인 특성을 가진다. 그래서 객체와 자료 구조는 근본적으로 양분된다.\n💡 객체 지향 코드에서 어려운 변경은 절차적인 코드에서 쉬우며, 절차적인 코드에서 어려운 변경은 객체지향 코드에서 쉬운 경우가 많다. 따라서, 때로는 단순한 자료 구조와 절차적인 코드가 가장 적합한 상황도 있다.\n디미터 법칙 디미터의 법칙은 잘 알려진 휴리스틱으로, 모듈은 자신이 조작하는 객체의 속사정을 몰라야 한다는 법칙이다.\n디미터 법칙(Law of Demeter)\n이러한 이유로 Don’t Talk to Strangers(낮선 이에게 말하지 마라) 또는 Principle of least Knowledge(최소 지식 원칙)으로도 알려져 있다.\n객체는 자료를 숨기고 함수를 공개한다.\n즉, 객체는 조회 함수로 내부 구조를 공개하면 안된다는 의미다. 조금 더 정확하게 표현하자면, 디미터 법칙은 “클래스 C의 메서드 f는 다음과 같은 객체의 메서드만 호출해야 한다”고 주장한다.\n클래스 C f가 생성한 객체 f 인수로 넘어온 객체 C 인스턴스 변수에 저장된 객체 위 객체에서 허용된 메서드가 반환하는 객체의 메서드는 호출하면 안된다.\n1 final String ouputDir = ctxt.getOptions().getScratchDir().getAbsolutePath(); getOptions() 함수가 반환하는 객체의 getScratchDir() 함수를 호출한 후 getScratchDir() 함수가 반환하는 객체의 getAbsolutePath() 함수를 호출하기 때문이다 디미터의 법칙을 위반했다. 클래스 C의 메서드가 지정된 메서드만을 호출하지 않고 있다. 기차 충돌 메서드를 연속적으로 호출하여 코드가 여러 객차가 한 줄로 이어진 기차처럼 보이기 때문에 기차 충돌이라 부른다.\n1 2 3 Options opts = ctxt.getOptions(); File scratchDir = opts.getScratchDir(); final String outputDir = scratchDir.getAbsolutePath(); 위와 같이 기차 충돌을 제거했을 경우 연속적으로 호출되었을 때는 반환되는 객체를 정확히 파악할 수 없었는데 나눠놓으니 더 쉽게 파악할 수 있다.\n하지만 위 구조도 아래와 같은 전제를 이미 알고 있어야 하므로 디미터 법칙을 만족한다고 볼 수 없다.\nctxt 객체가 Options를 포함한다. Options가 ScratchDir을 포함한다. ScratchDir이 AbsolutePath를 포함한다. 위 코드를 사용하는 함수는 많은 객체에 대한 정보를 알고 있기 때문에 최소 지식 원칙을 만족한다고 볼 수 없다.\n디미터 법칙을 위반하는지는 호출된 ctxt, Options, ScratchDir이 객체인지 자료 구조 인지에 달렸다.\n객체라면 내부 구조를 숨겨야 하므로 디미터 법칙을 위반한다. 자료 구조라면 내부 구조를 노출하기 때문에 디미터 법칙 대상이 아니다. 위 예제는 조회 함수를 사용하는 바람에 혼란을 일으키게 되는데, 코드를 다음과 같이 구현했다면 디미터 법칙을 거론할 필요가 없어진다.\n1 final String ouputDir = ctxt.options.scratchDir.absolutePath; 자료구조를 이용하게 되어 다른 클래스의 내부 구조에 대해 알 수 없게 되므로 디미터의 법칙 대상이 아니게 된다.\n자료 구조는 무조건 함수 없이 공개 변수만 포함하고 객체는 비공개 변수와 공개 함수를 포함한다면, 문제는 훨씬 간단해진다.\n잡종 구조 하지만 단순한 자료 구조에도 조회 함수와 설정 함수를 정의하라 요구하는 프레임워크와 표준(ex. bean)이 존재하고 있다. 이런 혼란 때문에 절반은 객체, 절반은 자료 구조인 잡종 구조가 나온다.\n잡종 구조는 중요한 기능을 수행하는 함수도 있고, 공개 변수나 공개 조회/설정 함수도 있다.\n공개/조회 함수는 비공개 변수를 그대로 노출하게 되며, 이 때문에 다른 함수가 절차적인 프로그래밍의 자료 구조 접근 방식처럼 비공개 변수를 사용하게 유혹한다.\n위처럼 만들어진 잡종 구조는 새로운 함수는 물론이고 새로운 자료 구조도 추가하기 어렵다. 그러므로 이러한 구조는 되도록 피하는 편이 좋다.\n구조체 감추기 ctxt, options, scratchDir이 진짜 객체라면 앞에서 본 예제처럼 기차 충돌을 만들면 안된다.\n1 ctxt.getAbsolutePathOfScratchDirectoryOption(); ctxt 객체가 여러 메서드를 조작해야 하므로 공개해야 하는 메서드가 너무 많아진다. 1 ctxt.getScratchDiretoryOption().getAQbsolutePath(); 객체가 아니라 자료 구조를 반환한다고 가정한다. ctxt가 객체라면 뭔가를 하라고 말해야지 속으로 드러내라고 말하면 안된다.\n고칠때는 무슨 목적으로 만들어진 기능인지 살피면 더 좋은 코드로 바꿀 수 있다. 위 기능에서 절대 경로를 얻으려는 이유는 임시 파일을 생성하기 위해서였다.\n1 2 3 String outFile = outputDir + \u0026#34;/\u0026#34; + className.replace(\u0026#39;.\u0026#39;, \u0026#39;/\u0026#39;) + \u0026#34;.class\u0026#34;; FileOutputStream fout = new FileOutputStream(outFile); BufferedOutputStream bos = new BufferedOutputStream(fout); 이러한 경우 ctxt객체에 임시 파일을 생성하는 기능을 만드는 것이 더 좋아보인다.\n1 BufferedOutputStream bos = ctxt.createScratchFileStream(classFileName); 위 코드로 변경하여 ctxt는 내부 구조를 드러내지 않으며, 모듈에서 해당 함수는 자신이 몰라야 하는 여러 객체를 탐색할 필요가 없어졌다.\n❓ 메서드 체인으로 만들었던 outputDir 은 결국 className을 함께 이용하여 파일 경로를 만들어 주게 되므로 별도로 분리하는 것이 아니라 묶어서 감추는 것이 더 좋다는 의미인가?\n자료 전달 객체 DTO 자료 구조체의 전형적인 형태는 공개 변수만 있고 함수가 없는 클래스다. 이런 자료 구조체를 때로는 자료 전달 객체(Data Transfer Object, DTO)라 한다.\nDTO는 데이터베이스와 통신하거나 소켓에서 받은 메시지의 구문을 분석할 때 유용하다.\n흔히 DTO는 데이터 베이스에 저장된 가공되지 않은 정보를 애플리케이션 코드에서 사용할 객체로 변환하는 단계에서 가장 처음으로 사용하는 구조체다.\nDTO의 좀 더 일반적인 형태는 ‘빈(bean)’구조다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 public class Address { private String street; private String streetExtra; private String city; private String state; private String zip; public Address(String street, String streetExtra, String city, String state, String zip) { this.street = street; this.streetExtra = streetExtra; this.city = city; this.state = state; this.zip = zip; } public String getStreet() { return street; } public String getStreetExtra() { return streetExtra; } public String getCity() { return city; } public String getState() { return state; } public String getZip() { return zip; } } 빈은 비공개 변수를 조회/설정 함수로 조작한다. 일종의 사이비 캡슐화로, 별 다른 이익을 제공하지는 않는다.\n활성 레코드 활성 레코드는 데이터베이스 테이블이나 다른 소스에서 자료를 직접 변환한 결과로 DTO의 특수한 형태다. 공개 변수가 있거나 비공개 변수에 조회/설정 함수가 있는 자료 구조지만, 대개 save나 find와 같은 탐색 함수도 제공한다.\n활성 레코드를 사용할 때 비즈니스 로직을 추가해 이런 자료 구조를 객체로 취급하는 개발자가 많은데, 이러한 방식은 결국 잡종 구조를 만들게 된다.\n활성 레코드는 자료 구조로 취급하여, 비즈니스 규칙을 담으면서 내부 자료를 숨기는 객체를 따로 생성해야 한다.\n결론 객체는 동작을 공개하고 자료를 숨긴다. 그래서 기존 동작을 변경하지 않으면서 새 객체 타입을 추가는 쉽다. 하지만 기존 객체에 새 동작을 추가하는 것은 어렵다. 자료구조는 별다른 동작 없이 자료를 노출한다. 그래서 기존 자료 구조에 새 동작을 추가는 쉽다. 기존 함수에 새 자료 구조를 추가하기는 어렵다. 구현할 때, 새로운 자료 타입을 추가하는 유연성이 필요하면 객체가 더 적합하다. 다른 경우로 새로운 동작을 추가하는 유연성이 필요하면 자료 구조와 절차적인 코드가 더 적합하다.\n💡 우수한 개발자는 편견 없이 이 사실을 이해해 직면한 문제에 최적인 해결책을 선택해야 한다.\n읽고 싶은 책 엘레강트 오브젝트 객체지향을 조금 다른 관점에서 접근하여 깨끗한 코드를 만드는 방법을 추천하는 책 ","date":"2023-04-20T15:53:13+09:00","image":"https://codemario318.github.io/post/clean-code/6/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean-code/6/","title":"클린코드: 6. 객체와 자료구조"},{"content":"어떤 프로그램이든 가장 기본적인 단위는 함수다\n어떻게 해야 읽기 쉽고 이해하기 쉬운 함수를 만들 수 있을까 의도를 분명히 표현하는 함수를 어떻게 구현할수 있을까 함수에 어떤 속성을 부여해야 처음 읽는 사람이 프로그램 내부를 직관적으로 파악할 수 있을까 작게 만들어라 함수를 만드는 첫번째 규칙은 ‘작게!’, 두번째 규칙은 ‘더 작게!’ 다.\n이 규칙은 증명하긴 어렵지만 작가의 경험으로 작은 함수가 좋다고 확신한다.\n함수가 작을수록 한가지 일만 처리하게 만들기 용이하고 명백해진다.\n블록과 들여쓰기 조건문을 통해 처리될 블록은 한줄로 표현해야한다. 즉 코드를 함수로 만들어야 한다.\n블록에 들어가게 되는 함수 이름을 적절하게 사용한다면 코드를 이해하기 쉬워진다.\n이 말은 중첩 구조가 생길만큼 함수가 커져서는 안 된다는 뜻으로 함수의 들여쓰기 깊이는 2단을 넘지 않게 만드는 것이 좋다.\n한가지만 해라 함수는 한 가지 일을 해야야한다. 그 한가지를 잘 해야한다. 그 한 가지만을 해야한다.\n추상화 수준이 하나인 단위로 함수를 만들면 한가지 일을 하는 함수를 만들 수 있다.\n함수를 만드는 이유는 큰 개념(기능)을 다음 추상화 수준에서 여러 단계로 나눠 수행하기 위해서다. 의미를 유지하면서 더 쪼갤 수 없는 수준까지 줄여야한다.\n단순히 다른 표현이 아니라 의미 있는 이름으로 다른 함수를 추출할 수 있다면 그 함수는 여러 작업을 하는 셈이다.\n함수 내 섹션 섹션이 여러개 만들어진다면 함수가 여러개의 일을 한다는 뜻이다. 한가지 일 만 하는 함수는 자연스럽게 섹션으로 나누기 어렵다.\n함수 내 추상화 수준은 하나로 함수가 확실히 한 가지 작업만 하려면 함수 내 모든 문장의 추상화 수준이 동일해야 한다.\n한 함수 내에 추상화 수준을 섞으면 특정 표현이 근본 개념인지 세부사항인지 구분하기 어려워, 코드를 읽는 사람이 헷갈린다.\n또한 근본 개념과 세부사항을 뒤섞기 시작하면, 깨진 유리창처럼 사람들이 함수에 세부사항을 점점 더 추가한다.\n위에서 아래로 코드 읽기: 내려가기 규칙 코드는 위에서 아래로 이야기처럼 읽혀야 좋다. 한 함수 다음에는 추상화 수준이 한 단계 낮은 함수가 온다.\n즉 위에서 아래로 읽히려면 함수 추상화 수준이 한번에 한 단계씩 낮아진다.\n💡 내려간다는건 단순히 위에서 아래로 읽힌다는 의미보다, 깊이가 깊어질수록 조금 더 낮은 수준으로 표현되야 한다는 뜻같다.\n추상화 수준이 하나인 함수를 구현하는것은 어렵다. 핵심은 짧으면서도 한가지 일만 수행하는 함수이다. 한 단계씩 깊어지는 코드를 구현하면 추상화 수준을 일관되게 유지하기 쉬워진다.\nSwitch 문 Switch문은 본질적으로 switch 문은 N 가지를 처리하기 때문에 작게, 한 가지 작업만 수행하게 만들기 어렵다.\n완전히 사용하지 않을 방법은 없기 때문에 다형성을 사용하여 저차원 클래스에 숨기고 반복하지 않게 만들어야 한다.\n","date":"2023-04-20T15:48:13+09:00","image":"https://codemario318.github.io/post/clean-code/3/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean-code/3/","title":"클린코드: 3. 함수 잘 만드는 법"},{"content":" 코드는 요구사항을 표현하는 도구이다. 고도로 추상화된 언어나 특정 응용 분야 언어로 기술하는 명세 역시 코드이다. 제대로 명시한 요구사항은 코드만큼 정형적이며 테스트 케이스로 사용해도 좋다! 나쁜 코드 잘나가던 회사를 망하게한 원인은 나쁜코드였다. 그들은 출시에 바빠 코드를 마구 짰다. 기능을 추가할수록 코드는 엉망이 되어갔고, 결국은 감당이 불가능한 수준에 이르렀다.\n언급 되었던 회사는 20년 후 망했다. 장기적으로 살피지 않아도, 빠르게 진행되던 프로젝트가 1~2년 만에 정체되는 경우가 꽤 많이 일어난다.\n이렇게 만들어진 나쁜 코드들은 생산성 악순환을 만든다.\n나쁜 (더러운) 코드를 지속적으로 개선하지 않는다면 장기적으로 부정적인 결과를 가져오게 된다.\n나중은 절대 오지 않는다 - Later == Never ( later equals never!) 나중은 절대 오지 않는다. 따라서 시간을 들여 깨끗한 코드를 만드는 노력이 비용을 절감하는 방법일 뿐만 아니라 전문가로서 꼭 필요하다.\n좋은 코드가 나쁜 코드가 되는 이유 요구사항 변경 짧은 일정 등등 외적인 요인들 💡 모두 변명임. 잘못은 사실 개발자 자신에게 있다.\n기획, 사업 등 에게 적극적으로 정보를 제공하여 사전에 방지해야 한다.\n커뮤니케이션을 통해 이러한 요인들을 예방하는 것도 개발자의 역량 나쁜 코드의 위험을 이해하지 못하는 관리자 말을 그대로 따르는 행동은 전문가답지 못하다.\n원초적 난제 개발자는 근본적인 가치에서 난제에 봉착한다.\n기한을 맞추려면 나쁜 코드를 양산할 수 밖에 없다고 느끼지만, 오히려 엉망진창인 상태로 인해 속도가 늦어지고 기한을 놓치게 된다.\n💡 기한을 맞추는 유일한 방법은, 즉 빨리 가는 유일한 방법은, 언제나 코드를 최대한 깨끗하게 유지하는 습관이다.\n깨끗한 코드는 어떻게 작성할까? 코드감각 깨끗한 코드를 작성하려면 청결이라는 힘겹게 습득한 감각을 활용해 자잘한 기법들을 적용하는 절제와 규율이 필요하다.\n깨끗한 코드란? 비야네 스트롭스트룹 - Bjarne Stroustrup (C++ 창시자) 간단한 논리\n낮은 의존성\n성능을 최적화\n깨끗한 코드는 한 가지 일을 제대로 한다.\n나쁜 코드는 나쁜 코드를 유혹한다.\n나쁜 코드를 고치면서 오히려 더 나쁜 코드를 만든다. 깨진 창문 깨진 유리창 이론 - 위키백과, 우리 모두의 백과사전\n그래디 부치 - Grady Booch : UML 창시자 깨끗한 코드는 단순하고 직접적이다. 잘 쓴 문장처럼 읽힌다. 결코 설계자의 의도를 숨기지 않는다. 명쾌한 추상화와 단순한 제어문으로 가득하다. 가독성을 강조하고 있다.\n","date":"2023-04-20T15:42:13+09:00","image":"https://codemario318.github.io/post/clean-code/1/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean-code/1/","title":"클린코드: 1. 깨끗한 코드"},{"content":"Go 언어에서 채널은 고루틴을 연결해주는 통로(파이프)다. 기본적으로 채널은 양방향이고 고루틴이 아래 이미지와 같이 동일한 채널을 통해 데이터를 보내거나 받을 수 있다.\nGo 채널은 그 채널을 통하여 데이터를 주고 받는 통로라 볼 수 있다. 채널은 make() 함수를 통해 미리 생성되어야 하며, 채널 연산자 \u0026lt;- 을 통해 데이터를 보내고 받는다.\n채널은 흔이 고루틴들 사이에 데이터를 주고 받을때 사용되는데, 상대편이 준비될 때까지 채널에서 대기함으로써 별도로 lock을 걸지 않고 데이터를 동기화하는데 사용된다.\n아래 예제는 정수형 채널을 생성하고, 한 고루틴에서 그 채널에 123이란 정수 데이터를 보낸 후, 이를 다시 메인 루틴에서 채널로부터 123 데이터를 받는 코드이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main func main() { // 정수형 채널을 생성한다 ch := make(chan int) go func() { ch \u0026lt;- 123 //채널에 123을 보낸다 }() var i int i = \u0026lt;- ch // 채널로부터 123을 받는다 println(i) } 채널을 생성할 때는 make()함수에 어떤 타입 데이터를 채널에서 주고받을 지 미리 지정해 주어야 한다. 채널로 데이터를 보낼 때는 채널명 \u0026lt;- 데이터 와 같이 사용하고, 채널로부터 데이터를 받을 경우에는 \u0026lt;- 채널명 와 같이 사용한다.\n메인 루틴은 마지막에서 채널로부터 데이터를 받고 있는데, 상대편 고루틴에서 데이터를 전송할 때까지는 계속 대기하게 된다. 따라서, 이 예제에서는 time.Sleep()이나 fmt.Scanf() 같이 고루틴이 끝날 때까지 기다리는 코드를 적지 않는다.\nGo 채널의 수신자와 송신자가 서로를 기다리는 속성을 이용하여 Go루틴이 끝날 때까지 기다리는 기능을 구현할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main import \u0026#34;fmt\u0026#34; func main() { done := make(chan bool) go func() { for i := 0; i \u0026lt; 10; i++ { fmt.Println(i) } done \u0026lt;- true }() // 위의 Go루틴이 끝날 때까지 대기 \u0026lt;-done } 익명함수를 사용한 고루틴에서 어떤 작업이 실행되고 있을 때, 메인 루틴은 ←done에서 계속 수신하며 대기하고 있게 된다.\n익명함수 고루틴에서 작업이 끝난 후, done채널에 true를 보내면, 수신자 메인루틴은 이를 받고 프로그램을 끝내게 된다.\nGo 채널 버퍼링 Go 채널은 Unbufferd Channel과 Buffered Channel 2가지 형태가 있다.\nUnbufferd Channel 위 예제에서 Go 채널은 Unbuffered Channel로서 이 채널에서는 하나의 수신자가 데이터를 받을 때까지 송신자가 데이터를 보내는 채널에 묶여있게 된다. (결과를 반환할 때까지 기다린다. blocking?)\nBufferd Channel Bufferd Channel을 사용하면 수신자가 받을 준비가 되어 있지 않아도 지정된 버퍼만큼 데이터를 보내고 계속 다른 일을 수행할 수 있다.(unblocking?) 버퍼 크기까지 입력 작업이 블락되지 않는다.\n버퍼 채널은 make(chan type, N) 함수를 통해 생성되는데, 두번째 파라미터 N에 사용할 버퍼 갯수를 넣는다.\n예를들어 make(chan int, 10)은 10개의 정수형을 갖는 버퍼 채널을 만든다.\n1 2 3 4 5 6 7 8 9 package main import \u0026#34;fmt\u0026#34; func main() { c := make(chan int) c \u0026lt;- 1 //수신루틴이 없으므로 데드락 fmt.Println(\u0026lt;-c) //코멘트해도 데드락 (별도의 Go루틴없기 때문) } 버퍼 채널을 이용하지 않는 경우, 위 코드는 데드락 에러를 발생시킨다. 메인루틴에서 채널에 1을 보내면서 상대편 수신자를 기다리고 있는데, 이 채널을 받는 수신자 고루틴이 없기 때문이다.\n1 2 3 4 5 6 7 8 9 10 11 12 package main import \u0026#34;fmt\u0026#34; func main() { ch := make(chan int, 1) //수신자가 없더라도 보낼 수 있다. ch \u0026lt;- 101 fmt.Println(\u0026lt;-ch) } 위처럼 버퍼채널을 사용하면, 수신자가 당장 없더라도 최대버퍼 수까지 데이터를 보낼 수 있으므로 에러가 발생하지 않는다\n버퍼에 적재된 데이터를 언젠가 가져갈 것이라 판단하고 최대 버퍼수를 넘지 않을때까지 보내도 데드락 에러를 발생시키지 않는다.\nUnbufferd Channel 예제코드 오류 분석 1 2 3 4 5 6 7 8 package main import \u0026#34;fmt\u0026#34; func main() { c := make(chan int) c \u0026lt;- 1 } unbuffered channel에서 발생하는 일반적인 데드락의 의미인 2개 이상의 작업 서로의 작업이 완료되기를 대기하는 교착 상태와는 약간 다르다고 볼 수도 있다.\n왜냐하면 sender와 receiver중 누군가가 먼저 작업을 끝내야지 그 다음으로 누군가가 작업을 수행할 수 있는 것이 아니라 서로 동시에 협력해야만 unbuffered channel에 대한 대기를 끝낼 수 있는데 이 경우는 동시에 협력해줄 그 누군가(receiver)가 없어 무한정 기다리게되는 상황이다.\n1 2 3 4 5 6 7 8 9 package main import \u0026#34;fmt\u0026#34; func main() { c := make(chan int) c \u0026lt;- 1 \u0026lt;- c } 그렇다고해서 위와 같이 자기 혼자 send와 receive를 하려 해도 Unbuffered channel은 sender와 receiver가 모두 ready여야 작업을 진행할 수 있기 때문에 불가능하다.\n1 2 3 4 5 6 7 8 9 10 11 package main import \u0026#34;fmt\u0026#34; func main() { c := make(chan int) go func() { \u0026lt;- c }() c \u0026lt;- 1 } 1 2 3 4 5 6 7 8 9 10 11 package main import \u0026#34;fmt\u0026#34; func main() { c := make(chan int) go func() { c \u0026lt;- 1 }() \u0026lt;- c } 따라서 다른 goroutine에서 A에 대한 receiver or sender 역할을 해주면 된다.\n채널 파라미터 채널을 함수의 파라미터로 전달할 때, 일반적으로 송수신을 모두 하는 채널을 전달하지만, 특별히 해당 채널로 송신만 할 것인지 혹은 수신만 할 것 인지를 지정할 수 있다.\n송신 파라미터: chan\u0026lt;- 수신 파라미터: \u0026lt;-chan 파라미터에 다른 송수신을 바꿔 넣게 되면 에러가 발생한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import \u0026#34;fmt\u0026#34; func main() { ch := make(chan string, 1) sendChan(ch) receiveChan(ch) } func sendChan(ch chan\u0026lt;- string) { ch \u0026lt;- \u0026#34;Data\u0026#34; // x := \u0026lt;-ch // 에러발생 } func receiveChan(ch \u0026lt;-chan string) { data := \u0026lt;-ch fmt.Println(data) } 채널 닫기 채널을 열어 데이터를 보낸 후, close()함수를 사용하여 채널을 닫을 수 있다. (송신자만 가능) 채널을 닫게되면, 해당 채널로는 더이상 송신할 수 없지만, 계속 수신은 가능하다.\n채널 수신에 사용되는 \u0026lt;- ch 은 채널 메시지, 정상 수신 여부 2개의 반환값을 갖는다. 채널이 닫혔을 경우 두번째 리턴값은 false이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main func main() { ch := make(chan int, 2) // 채널에 송신 ch \u0026lt;- 1 ch \u0026lt;- 2 // 채널을 닫는다 close(ch) // 채널 수신 println(\u0026lt;-ch) println(\u0026lt;-ch) if _, success := \u0026lt;-ch; !success { println(\u0026#34;더이상 데이타 없음.\u0026#34;) } } 채널 range 문 채널에서 송신자가 송신을 한 후, 채널을 닫을 수 있다. 수신자는 임의의 갯수의 데이터를 채널이 닫힐 때까지 수신할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package main func main() { ch := make(chan int, 2) // 채널에 송신 ch \u0026lt;- 1 ch \u0026lt;- 2 // 채널을 닫는다 close(ch) // 방법1 // 채널이 닫힌 것을 감지할 때까지 계속 수신(무한 for 루프) for { if i, success := \u0026lt;-ch; success { println(i) } else { break } } // 방법2 // 위 표현과 동일한 채널 range 문 (방법 1의 간결한 표현 세부 동작은 같음) for i := range ch { println(i) } } 채널 range문은 range 키워드 다음에 명시한 채널로부터 데이터를 계속 수신하다가 채널이 닫힌 것을 감지하면 for 루프를 종료한다.\n채널 select 문 select문은 복수 채널들을 기다리면서 준비된 채널을 실행하는 기능을 제공한다.\nselect문은 여러개의 case문에서 각각 다른 채널을 기다리다가 준비가 된 채널 case를 실행한다. select문은 case 채널들이 준비되지 않으면 계속 대기하게 되고, 가장 먼저 도착한 채널의 case를 실행한다. 만약 복수 채널에 신호가 오면, Go 런타임이 랜덤하게 그중 한 개를 선택하게 된다. select문에 default문이 있으면, case문 채널이 준비되지 않더라도 계속 대기하지 않고 바로 default문을 실행한다.\n","date":"2023-04-20T13:38:40+09:00","image":"https://codemario318.github.io/post/golang/13/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/golang/13/","title":"Golang: 13. Go 채널"},{"content":"Go루틴(goroutine)은 Go 런타임이 관리하는 Lightweight 논리적 (혹은 가상) 쓰레드이다. Go에서 go 키워드를 사용하여 함수를 호출하면, 런타임시 새로운 goroutine을 실행한다.\n고루틴은 비동기적으로 함수루틴을 실행하므로, 여러 코드를 동시에 실행하는데 사용된다.\n고루틴은 OS 쓰레드보다 훨씬 가볍게 비동기 Concurrent(동시성) 처리를 구현하기 위하여 만든것으로, 기본적으로 Go 런타임이 자체 관리한다.\nGo 런타임 상에서 관리되는 작업단위인 여러 고루틴들을 하나의 OS 쓰레드 1개로도 실행되곤 한다. 고루틴들은 OS 쓰레드와 1대1로 대응되지 않고, Multiplexing(다중화)으로 훨씬 적은 OS 쓰레드를 사용한다. 메모리 측면에서도 OS쓰레드가 1MB 스택을 갖는 반면, 고루틴은 이보다 훨씬 작은 몇 KB 스택을 갖는다(필요시 동적으로 증가함). Go 런타임은 Go루틴을 관리하면서 Go 채널을 통해 Go 루틴간의 통신을 쉽게 할 수 있도록 했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func say(s string) { for i := 0; i \u0026lt; 10; i++ { fmt.Println(s, \u0026#34;***\u0026#34;, i) } } func main() { // 함수를 동기적으로 실행 say(\u0026#34;Sync\u0026#34;) // 함수를 비동기적으로 실행 go say(\u0026#34;Async1\u0026#34;) go say(\u0026#34;Async2\u0026#34;) go say(\u0026#34;Async3\u0026#34;) // 3초 대기 time.Sleep(time.Second * 3) } main 함수를 보면, 먼저 say() 함수를 동기적으로 호출하고, 다음으로 동일한 say() 함수를 비동기적으로 3번 호출하고 있다. 첫번째 동기적 호출은 say() 함수가 완전히 끝났을 때 다음 문장으로 이동하고, 다음 3개의 go say() 비동기 호출은 별도 고루틴들에서 동작하면서, 메인루틴은 계속 다음 문장을 실행한다. 고루틴들은 비동기이므로 처리 순서가 일정하지 않으므로 프로그램 실행시 마다 다른 출력 결과를 나타낼 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 Sync *** 0 Sync *** 1 Sync *** 2 Sync *** 3 Sync *** 4 Sync *** 5 Sync *** 6 Sync *** 7 Sync *** 8 Sync *** 9 Async3 *** 0 Async3 *** 1 Async3 *** 2 Async3 *** 3 Async3 *** 4 Async3 *** 5 Async3 *** 6 Async3 *** 7 Async3 *** 8 Async3 *** 9 Async2 *** 0 Async2 *** 1 Async2 *** 2 Async2 *** 3 Async2 *** 4 Async1 *** 0 Async1 *** 1 Async1 *** 2 Async1 *** 3 Async1 *** 4 Async1 *** 5 Async1 *** 6 Async1 *** 7 Async1 *** 8 Async1 *** 9 Async2 *** 5 Async2 *** 6 Async2 *** 7 Async2 *** 8 Async2 *** 9 익명함수 Go루틴 고루틴은 익명함수에 대해 사용할 수도 있다. 즉, go 키워드 뒤에 익명함수를 바로 정의하는 것으로, 익명함수를 비동기로 실행하게 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func main() { // WaitGroup 생성. 2개의 Go루틴을 기다림. var wait sync.WaitGroup wait.Add(2) // 익명함수를 사용한 goroutine go func() { defer wait.Done() //끝나면 .Done() 호출 fmt.Println(\u0026#34;Hello\u0026#34;) }() // 익명함수에 파라미터 전달 go func(msg string) { defer wait.Done() //끝나면 .Done() 호출 fmt.Println(msg) }(\u0026#34;Hi\u0026#34;) wait.Wait() //Go루틴 모두 끝날 때까지 대기 } 첫번째 익명함수는 간단히 Hello를 출력하는데, 이를 고루틴으로 실행하면 비동기적으로 그 익명함수를 실행하게 된다. 두번째 익명함수는 파라미터를 전달하는 예제로 익명함수에 파라미터가 있는 경우, go 익명함수 바로 뒤에 파라미터를 함께 전달하게 된다.\n여기서 sync.WaitGroup을 사용하고 있는데, 이는 기본적으로 여러 고루틴들이 끝날 때까지 기다리는 역할을 한다. WaitGroup을 사용하기 위해서는 먼저 Add() 메소드에 몇 개의 Go루틴을 기다릴 것인지 지정하고, 각 고루틴에서 Done() 메서드를 호출한다.(여기서는 defer 사용)\n그리고 메인루틴에서는 Wait() 메서드를 호출하여, Go루틴들이 모두 끝나기를 기다린다.\n다중 CPU 처리 go는 디폴트로 CPU 1개를 사용한다. 여러 개 고루틴을 만들더라도, CPU 1개에서 작업을 시분할하여 처리한다. 만약 머신이 CPU 여러개를 가진 경우, Go 프로그램을 다중 CPU에서 병렬처리(Perallel)하게 할 수 있는데, 병렬처리를 위해서는 아래와 같이 runtime.GOMAXPROCS(CPU수) 함수를 호출하여야 한다.\nCPU 수는 Logical CPU 수를 가리킨다. 1 2 3 4 5 6 7 8 9 10 11 12 package main import ( \u0026#34;runtime\u0026#34; ) func main() { // 4개의 CPU 사용 runtime.GOMAXPROCS(4) //... } 동시성과 병렬성은 비슷하게 들리지만 전혀 다른 개념이다.\n","date":"2023-04-20T13:28:40+09:00","image":"https://codemario318.github.io/post/golang/12/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/golang/12/","title":"Golang: 12. Go 루틴"},{"content":"지연실행 defer Go 언어의 defer 키워드는 특정 문장 혹은 함수를 나중에 (defer를 호출하는 함수의 결과를 반환하기 직전에) 실행된다.\n일반적으로 defer는 C#, Java 같은 언어에서 finally 블럭처럼 마지막에 Clena-up 작업을 위해 사용된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import \u0026#34;os\u0026#34; func main() { f, err := os.Open(\u0026#34;1.txt\u0026#34;) if err != nil { panic(err) } // main 마지막에 파일 close 실행 defer f.Close() // 파일 읽기 bytes := make([]byte, 1024) f.Read(bytes) println(len(bytes)) } 파일을 Open 한 후 바로 파일을 Close하는 작업을 defer로 쓰고 있다. 이는 차후 문장에서 어떤 에러가 발생하더라도 항상 파일을 Close할 수 있도록 한다. panic 함수 Go 내장함수인 panic()함수는 현재 함수를 즉시 멈추고 현재 함수에 defer 함수들을 모두 실행한 후 즉시 리턴한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package main import \u0026#34;os\u0026#34; func main() { // 잘못된 파일명을 넣음 openFile(\u0026#34;Invalid.txt\u0026#34;) // openFile() 안에서 panic이 실행되면 // 아래 println 문장은 실행 안됨 println(\u0026#34;Done\u0026#34;) } func openFile(fn string) { f, err := os.Open(fn) if err != nil { panic(err) } defer f.Close() } 이러한 panic 모드 실행 방식은 다시 상위함수에도 똑같이 적용되고, 계속 콜스택을 타고 올라가며 적용된다. 마지막에는 프로그램이 에러를 내고 종료하게 된다. recover 함수 Go 내장합수인 recover() 함수는 panic 함수에 의한 패닉상태를 다시 정상상태로 되돌리는 함수이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) func main() { // 잘못된 파일명을 넣음 openFile(\u0026#34;Invalid.txt\u0026#34;) // recover에 의해 // 이 문장 실행됨 println(\u0026#34;Done\u0026#34;) } func openFile(fn string) { // defer 함수. panic 호출시 실행됨 defer func() { if r := recover(); r != nil { fmt.Println(\u0026#34;OPEN ERROR\u0026#34;, r) } }() f, err := os.Open(fn) if err != nil { panic(err) } defer f.Close() } recover 함수를 사용하면 panic 상태를 제거하고 openFile() 다음 문장인 println()을 호출하게 된다. ","date":"2023-04-20T13:26:40+09:00","image":"https://codemario318.github.io/post/golang/11/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/golang/11/","title":"Golang: 11. defer와 panic"},{"content":"Go는 내장 타입으로 error 라는 인터페이스 타입을 갖는다. Go 에러는 이 error 인터페이스를 통해서 주고 받게 되는데, 메서드 하나를 갖는다.\n1 2 3 type error interface { Error() string } 개발자는 error 인터페이스를 구현하는 커스텀 에러 타입을 만들 수 있다.\nGo 에러처리 Go 함수가 결과와 에러를 함께 반환한다면, 이 에러가 nil 인지 체크하여 에러를 확인할 수 있다.\n예를들어 os.Open() 함수는 func Open(name string) (file *File, err error) 과 같은 함수 원형을 갖는데, File 포인터와 error 인터페이스를 함께 반환한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import ( \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; ) func main() { f, err := os.Open(\u0026#34;C:\\\\temp\\\\1.txt\u0026#34;) if err != nil { log.Fatal(err.Error()) } println(f.Name()) } 그래서 반환된 두번째 error을 체크하여 nil 이면 에러가 없는 것이고, 아니라면 err.Error()로 부터 해당 에러를 알 수 있다.\nlog.Fatal()은 메시지를 출력하고 os.Exit(1)을 호출하여 프로그램을 종료한다. error Type을 이용한 에러처리 또 다른 에러처리로 error의 Type을 체크하여 에러 타입별로 별도의 에러 처리를 하는 방식이 있다.\n1 2 3 4 5 6 7 8 9 _, err := otherFunc() switch err.(type) { default: // no error println(\u0026#34;ok\u0026#34;) case MyError: log.Print(\u0026#34;Log my error\u0026#34;) case error: log.Fatal(err.Error()) } 예제에서 otherFunc()를 호출한 후 반환된 error의 타입을 통해 여러 유형의 에러를 처리할 수 있다.\n모든 에러는 error 인터페이스를 구현하므로 마지막 case문은 모든 에러에 적용된다. ","date":"2023-04-20T13:25:40+09:00","image":"https://codemario318.github.io/post/golang/10/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/golang/10/","title":"Golang: 10. 에러"},{"content":"구조체가 필드들의 집합체라면, interface는 메서드들의 집합체이다.\n인터페이스는 타입(type)이 구현해야 하는 메서드 원형(prototype)들을 정의한다. 어떠한 사용자 정의 타입이 인터페이스를 구현하려면 선언한 인터페이스가 갖는 모든 메서드들을 구현하면 된다.\n1 2 3 4 type Shape interface { area() float64 perimeter() float64 } 인터페이스는 구조체와 마찬가지로 type문을 사용하여 정의한다.\n인터페이스 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //Rect 정의 type Rect struct { width, height float64 } //Circle 정의 type Circle struct { radius float64 } //Rect 타입에 대한 Shape 인터페이스 구현 func (r Rect) area() float64 { return r.width * r.height } func (r Rect) perimeter() float64 { return 2 * (r.width + r.height) } //Circle 타입에 대한 Shape 인터페이스 구현 func (c Circle) area() float64 { return math.Pi * c.radius * c.radius } func (c Circle) perimeter() float64 { return 2 * math.Pi * c.radius } Shape 인터페이스를 구현하기 위해서는 area(), perimeter() 2개 메서드만 구현하면 된다.\n인터페이스 사용 인터페이스를 사용하는 일반적인 예로 함수가 파라미터로 인터페이스를 받을 수 있다.\n함수 파라미터가 인터페이스인 경우, 어떤 타입이든 해당 인터페이스를 구현하기만 하면 모두 입력 파라미터로 사용될 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func main() { r := Rect{10., 20.} c := Circle{10} showArea(r, c) } func showArea(shapes ...Shape) { for _, s := range shapes { a := s.area() //인터페이스 메서드 호출 p := s.perimeter() println(a, p) } } Rect 구조체와 Circle 구조체는 Shape 인터페이스에서 선언한 area, perimeter 메서드를 구현하고 있기 때문에 Shape 인터페이스를 상속했다고 볼 수 있다. 따라서 showArea에서 Shape 인터페이스 파라미터 입력으로 사용 가능하다.\n인터페이스 타입 Go 프로그래밍을 하다보면 흔히 빈 인터페이스(empty interface)를 자주 접할 수 있는데, 이는 인터페이스 타입(interface type)으로 불린다.\n여러 표준패키지들의 함수 Prototype을 살펴보면, 빈 인터페이스가 자주 등장한다.\n빈 인터페이스는 interface{} 로 표현한다. 1 2 3 func Marshal(v interface{}) ([]byte, error); func Println(a ...interface{}) (n int, err error); 빈 인터페이스는 메서드를 전혀 갖지 않는 인터페이스이다.\nGo의 모든 Type은 적어도 0개의 메서드를 구현하므로, Go에서 모든 Type을 의미한다.\n즉, 빈 인터페이스는 어떠한 타입도 담을 수 있는 컨테이너로 볼 수 있고, 여러 다른 언어에서 흔히 일컫는 Dynamin Type으로 사용할 수 있다. (C#, java ⇒ object, C,C++ ⇒ void*)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import \u0026#34;fmt\u0026#34; func main() { var x interface{} x = 1 x = \u0026#34;Tom\u0026#34; printIt(x) } func printIt(v interface{}) { fmt.Println(v) //Tom } Type Assertion 인터페이스 타입의 x와 타입 T에 대하여 x.(T)로 표현했을 때, 이는 x가 nil이 아니며, x는 T 타입에 속한다는 것을 확인한다. 이러한 표현방식을 Type Assertion 이라고 부른다.\n만약 x가 nil 이거나 x의 타입이 T가 아니라면, 런타임 에러가 발생하고, x가 T 타입인경우는 T 타입 x를 반환한다.\n","date":"2023-04-20T13:08:40+09:00","image":"https://codemario318.github.io/post/golang/9/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/golang/9/","title":"Golang: 9. 인터페이스"},{"content":"Go 언어는 OOP를 구조체와 메서드를 이용하는 방식으로 지원한다.\n다른 언어들이 클래스가 내부에 데이터와 메서드를 함께 갖는 것과 달리 Go 언어에서는 구조체가 데이터만을 가지고, 메서드는 별도로 분리하여 정의한다.\nGo 메서드는 특별한 형태의 함수이다. 메서드는 함수 정의에서 func 키워드와 함수명 사이에 “그 함수가 어떤 구조체를 위한 메서드인지” 표시한다.\nreceiver로 부르며 메서드가 속한 구조체 타입과 변수명을 지정하는데, 구조체 변수명은 함수 내에서 입력 파라미터처럼 사용된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main //Rect - struct 정의 type Rect struct { width, height int } //Rect의 area() 메소드 func (r Rect) area() int { return r.width * r.height } func main() { rect := Rect{10, 20} area := rect.area() //메서드 호출 println(area) } area는 Rect의 메소드로 선언 되어 메서드를 입력처럼 사용할 수 있다.\n(value VS 포인터) receiver Value receiver value receiver는 구조체의 데이터를 복사하여 전달한다. value receiver는 메서드 내의 필드값 변경되더라도 실제 데이터는 변경되지 않는다.\n1 2 3 4 5 6 7 8 9 10 func (r Rect) area2() int { r.width++ return r.width * r.height } func main() { rect := Rect{10, 20} area := rect.area2() //메서드 호출 println(rect.width, area) // 10 220 출력 } 포인터 리시버 포인터 리시버는 구조체의 포인터를 전달한다. 메서드 내의 필드값 변경이 그대로 호출자에서 반영된다.\n","date":"2023-04-20T12:34:40+09:00","image":"https://codemario318.github.io/post/golang/8/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/golang/8/","title":"Golang: 8. 메서드"},{"content":"Go 언어는 객체지향 프로그래밍을 고유의 방식으로 지원한다.\n클래스, 객체, 상속 개념이 없다. 전통적인 OOP의 클래스는 Go 언어에서 Custom Type을 정의하는 struct(구조체)로 표현한다.\nStruct Go에서 struct는 Custom Data Type을 표현하는데 사용된다(C 처럼)\n필드들의 집합체이며 필드들의 컨테이너이다. struct는 필드 데이터만을 가지며(자료 구조 역할), 메서드는 표현하지 않는다. 메서드는 별도로 분리하여 정의한다.\nStruct 선언 구조체를 정의하기 위해서 Custom Type을 정의하는데 사용하는 type 문을 사용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import \u0026#34;fmt\u0026#34; // struct 정의 type person struct { name string age int } func main() { // person 객체 생성 p := person{} // 필드값 설정 p.name = \u0026#34;Lee\u0026#34; p.age = 10 fmt.Println(p) } person 구조체를 패키지 외부에서 사용할 수 있게 하려면 struct명을 Person으로 변경하면 된다.\nstruct 객체 생성 선언된 struct 타입으로부터 객체를 생성하는 방법은 몇 가지 방법들이 있다.\n빈 객체를 먼저 할당하고, 나중에 그 필드값을 채워넣는 방법\n1 2 3 4 p := person{} p.name = \u0026#34;Lee\u0026#34; p.age = 10 초기값을 함께 할당하는 방법\n1 2 3 var p1 person p1 = person{\u0026#34;Bob\u0026#34;, 20} p2 := person{name: \u0026#34;Sean\u0026#34;, age: 50} 초기화가 생략된 필드들은 Zero value (정수인 경우 0, float인 경우 0.0, string인 경우 \u0026ldquo;\u0026rdquo;, 포인터인 경우 nil 등)를 갖는다.\nnew 내장함수 사용\n1 2 p := new(person) p.name = \u0026#34;Lee\u0026#34; // p가 포인터라도 . 을 사용한다 new()를 사용하면 모든 필드를 Zero value로 초기화하고 person 객체의 포인터를 반환한다.\n객체 포인터인 경우에도 필드 엑세스시 . 을 사용하는데 이 때 포인터는 자동으로 역참조된다. (c에서 → 과 동일한 동작)\nGo에서 struct는 기본적으로 mutable 개체로서 필드값이 변화할 경우 해당 개체 메모리에서 직접 변경된다. 하지만 struct 개체를 다른 함수의 파라미터로 넘기면, Pass by Value에 따라 객체를 복사해서 전달한다. 따라서 struct 개체의 값을 변경하려면 포인터를 전달해야 한다.\n생성자 함수 구조체 필드가 사용 전에 초기화되어야 하는 경우가 있다. 예를 들어 struct 필드가 map 타입인 경우 구조체 할당 후 map을 다시 할당받고 초기화 해야한다.\n이럴 때 map을 사전에 미리 초기화 하면, 외부에서 구조체를 사용할 때 별도로 초기화하는 번거로움을 줄일 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main type dict struct { data map[int]string } //생성자 함수 정의 func newDict() *dict { d := dict{} d.data = map[int]string{} return \u0026amp;d //포인터 전달 } func main() { dic := newDict() // 생성자 호출 dic.data[1] = \u0026#34;A\u0026#34; } 생성자 함수인 newDict()는 dict라는 구조체의 map 필드를 초기화한 후 구조체 포인터를 반호나한다. main 함수에서 구조체 개체를 만들 때 dict를 직접 생성하지 않고 대신 생성자 함수를 통해 이미 초기환 된 data 맵 필드를 사용할 수 있다.\n","date":"2023-04-20T12:30:40+09:00","image":"https://codemario318.github.io/post/golang/7/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/golang/7/","title":"Golang: 7. 구조체"},{"content":"Go는 패키지를 통해 모듈화, 재사용 기능을 제공한다. 패키지를 사용해서 작은 단위의 컴포넌트를 작성하고, 이러한 작은 패키지들을 활용해서 프로그램을 작성할 것을 권장한다.\nGo는 실제 프로그램 개발에 필요한 많음 패키지들을 표준 라이브러리로 제공한다. 이러한 표준 라이브러리 패키지들을 GOROOT/pkg 안에 존재한다. GOROOT 환경변수는 Go 설치 디렉토리를 가르키는데, 보통 Go 설치시 자동으로 추가된다.\nGo에 사용하는 표준 패키지들은 https://golang.org/pkg 에 자세히 설명되어 있다.\nMain 패키지 일반적으로 패키지는 라이브러리로서 사용되지만, main이라고 명명된 패키지는 Go Compiler에 의해 특별하게 인식된다. 패키지명이 main인 경우, 컴파일러는 해당 패키지를 공유 라이브러리가 아닌 실행 프로그램으로 만든다. 그리고 이 main 패키지 안의 main() 함수가 프로그램의 시작점, 즉 Entry Point가 된다. 패키지를 공유 라이브러리로 만들 때에는, main패키지나 main 함수를 사용해서는 안된다.\n패키지 Import 다른 패키지를 프로그램에서 사용하기 위해서는 import 키워드를 사용하여 패키지를 포함시킨다.\n예를 들어 Go 표준 라이브러리인 fmt 패키지를 사용하기 위해 import “fmt”와 같이 해당 패키지를 포함시킬 것을 선언해 준다.\n1 2 3 4 5 6 7 package main import \u0026#34;fmt\u0026#34; func main(){ fmt.Println(\u0026#34;Hello\u0026#34;) } 패키지를 import 할 때, Go 컴파일러는 GOROOT 혹은 GOPATH 환경 변수를 검색하는데, 표준 패키지는 GOROOT/pkg 에서 사용자 패키지나 3rd party 패키지의 경우 GOPATH/pkg에서 패키지를 찾는다.\nGOROOT 환경변수는 Go 설치시 자동으로 시스템에 설정되지만, GOPATH는 사용자가 지어해 주어야 한다. GOPATH 환경변수는 3rd party 패키지를 갖는 라이브러리 디렉토리나 사용자 패키지가 있는 작업 디렉토리를 지정하게 되는데, 복수 개일 경우 세미콜론을 사용하여 연결한다.\n패키지 scope 패키지 내에는 함수, 구조체, 인터페이스, 메서드 등이 존재하는데, 이름의 첫 문자를 대문자로 시작하면 public으로 사용할 수 있다. 패키지 외부에서 이들을 호출하거나 사용할 수 있게 된다.\n반면 , 이름이 소문자로 시작하면 이는 non-public으로 패키지 내부에서만 사용될 수 있다.\n패키지 init 함수와 alias 개발자가 패키지를 작성할 때, 패키지 실행시 처음으로 호출되는 init() 함수를 작성할 수 있다. init 함수는 패키지가 로드되면서 실행되는 함수로 별도의 호출 없이 자동으로 호출된다.\n1 2 3 4 5 6 7 package testlib var pop map[string]string func init() { // 패키지 로드시 map 초기화 pop = make(map[string]string) } 패키지를 import 하면서 그 패키지 안의 init() 만 호출하길 원한다면, import 시 _ 라는 alias를 지정한다.\n1 2 package main import _ \u0026#34;other/xlib\u0026#34; 패키지 이름이 동일하지만, 서로 다른 버전 혹은 서로 다른 위치에서 로딩하고자 할 때는 패키지 alias를 사용하여 구분할 수 있다.\n1 2 3 4 5 6 7 8 9 import ( mongo \u0026#34;other/mongo/db\u0026#34; mysql \u0026#34;other/mysql/db\u0026#34; ) func main() { mondb := mongo.Get() mydb := mysql.Get() //... } 사용자 정의 패키지 생성 사용자 정의 패키지를 만들어 재사용 가능한 컴포넌트를 만들어 사용할 수 있다. 사용자 정의 라이브러리 패키지는 일반적으로 폴더를 하나 만들고 그 폴더 안에 .go 파일들을 만들어 구성한다.\n하나의 서브 폴더안데 있는 .go 파일들은 동일한 패키지명을 가지며, 패키지명은 해당 폴더의 이름과 같게 한다. 해당 폴더에 있는 여러 .go 파일들은 하나의 패키지로 묶인다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package testlib import \u0026#34;fmt\u0026#34; var pop map[string]string func init() { pop = make(map[string]string) pop[\u0026#34;Adele\u0026#34;] = \u0026#34;Hello\u0026#34; pop[\u0026#34;Alicia Keys\u0026#34;] = \u0026#34;Fallin\u0026#39;\u0026#34; pop[\u0026#34;John Legend\u0026#34;] = \u0026#34;All of Me\u0026#34; } // GetMusic : Popular music by singer (외부에서 호출 가능) func GetMusic(singer string) string { return pop[singer] } func getKeys() { // 내부에서만 호출 가능 for _, kv := range pop { fmt.Println(kv) } } 패키지명은 폴더명과 동일하게 정해야 한다. 패키지 폴더 안에 여러 파일들이 있을 경우에도, 동일한게 폴더 이름을 패키지 이름으로 사용한다.\n💡 사이즈가 큰 목잡한 라이브러리 같은 경우, go install 명령을 사용하여 라이브러리를 컴파일 하여 cache 할 수 있는데, 다음 빌드시 빌드 타임을 크게 줄일 수 있다.\n1 2 cd {{package_dir}} go install 패키지를 찾기 위해 GOROOT와 GOPATH를 사용하는데, GOROOT와 GOPATH에 있는 각 루트폴더의 해당 패키지를 찾게 된다.\nGOPATH가 C:\\GoApp;C:\\GoSrc라면 지정된 라이브러리를 찾기 위해 폴더를 순차적으로 검색하게 된다.\n1 2 3 C:\\Go\\src\\testlib (from $GOROOT) C:\\GoApp\\src\\testlib (from $GOPATH) C:\\GoSrc\\src\\testlib ","date":"2023-04-20T12:28:40+09:00","image":"https://codemario318.github.io/post/golang/6/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/golang/6/","title":"Golang: 6. 패키지"},{"content":"배열 Array 배열은 연속적인 메모리 공간에 동일한 타입의 데이터를 순차적으로 저장하는 자료구조이다.\n배열의 선언은 var 변수명 [배열크기] 데이터타입 과 같이 한다. Go에서 배열크기는 Type을 구성하는 한 요소이다. 따라서, [3]int와 [5]int는 서로 다른 타입으로 인식된다.\n1 2 3 4 5 6 7 8 9 package main func main() { var a [3]int //정수형 3개 요소를 갖는 배열 a 선언 a[0] = 1 a[1] = 2 a[2] = 3 println(a[1]) // 2 출력 } 배열 초기화 배열을 정의할 때, 초기값을 설정할 수 있다. 초기값은 [배열크기] 데이터타입 {초기값0, 초기값1 ...} 로 할당한다.\n초기화 과정에서 [\u0026hellip;]를 사용하여 배열 크기를 생략하면 자동으로 초기화 요소 개수만큼 배열 크기가 정해진다.\n1 2 var a1 = [3]int{1, 2, 3} var a3 = [...]int{1, 2, 3} //배열크기 자동으로 다차원 배열 1 2 var multiArray [3][4][5]int // 정의 multiArray[0][1][2] = 10 다차원 배열 초기화 1 2 3 4 5 6 7 func main() { var a = [2][3]int{ {1, 2, 3}, {4, 5, 6}, //끝에 콤마 추가 } println(a[1][2]) } 슬라이스 Slice Go 배열은 고정된 크기 안에 동일한 타입의 데이터를 연속적으로 저장하지만, 배열의 크기를 동적으로 증가시키거나 부분 배열을 추출하는 등의 기능은 없다.\nGo 슬라이스는 내부적으로 배열에 기초하여 만들어졌지만 편리하고 유용한 기능들을 제공한다.\n고정된 크기를 지정하지 않을 수 있음 크기를 동적으로 변경할 수 있음 부분 배열 추출 가능 등 Go 슬라이스 선언은 배열을 선언하듯이 var v []T 처럼 하는데 배열과 달리 크기는 지정하지 않는다.\n1 2 3 4 5 6 7 8 9 package main import \u0026#34;fmt\u0026#34; func main() { var a []int //슬라이스 변수 선언 a = []int{1, 2, 3} //슬라이스에 리터럴값 지정 a[1] = 10 fmt.Println(a) // [1, 10, 3]출력 } make() 슬라이스를 생성하는 다른 방법으로 내장함수 make()를 이용할 수 있다.\nmake함수로 슬라이스를 생성하면 슬라이스 길이(length), 용량(Capacity)을 임의로 지정할 수 있는 장점이 있다.\n1 2 3 4 func main() { s := make([]int, 5, 10) println(len(s), cap(s)) // len 5, cap 10 } → 슬라이스의 길이는 len(), 용량은 cap()을 써서 확인할 수 있다.\n1 2 3 4 5 6 7 8 func main() { var s []int if s == nil { println(\u0026#34;Nil Slice\u0026#34;) } println(len(s), cap(s)) // 모두 0 } make함수로 슬라이스를 생성하면 모든 요소가 Zero value인 슬라이스를 만들게 된다. 또한 슬라이스에 별도의 길이와 용량을 지정하지 않으면, 기본적으로 길이와 용량이 0인 슬라이스를 만드는데 이를 Nill Slice 라고 하며, nil 과 비교하면 참을 반환한다.\n#$# 부분 슬라이스\n슬라이스에서 일부를 발췌하여 부분 슬라이스를 만들 수 있다. 부분 슬라이스는 슬라이스[시작인덱스:마지막인덱스] 형식으로 만든다.\n시작 인덱스는 inclusive이며 마지막 인덱스는 Exclusive이다(파이썬과 동일함)\n1 2 3 4 5 6 7 8 package main import \u0026#34;fmt\u0026#34; func main() { s := []int{0, 1, 2, 3, 4, 5} s = s[2:5] fmt.Println(s) //2,3,4 출력 } 부분 슬라이스에서 인덱스는 생략 가능하다.\n처음 인덱스 생략: 0 자동 대입 마지막 인덱스 생략: 슬라이스 길이 자동대입 따라서 모두 생략하면 전체를 가져온다.\n1 2 3 4 s := []int{0, 1, 2, 3, 4, 5} s = s[2:5] // 2, 3, 4 s = s[1:] // 3, 4 fmt.Println(s) // 3, 4 출력 슬라이스 추가 배열은 고정된 크기로 그 크기 이상의 데이터를 임의로 추가할 수 없지만, 슬라이스는 자유롭게 새로운 요소를 추가할 수 있다.\n슬라이스에 새로운 요소를 추가하려면 내장함수 append()를 사용한다.\n1 2 3 4 5 6 7 8 9 10 func main() { s := []int{0, 1} // 하나 확장 s = append(s, 2) // 0, 1, 2 // 복수 요소들 확장 s = append(s, 3, 4, 5) // 0,1,2,3,4,5 fmt.Println(s) } append 동작 과정 슬라이스 용량이 남아있는 경우 슬라이스의 길이를 변경하여 데이터를 추가 슬라이스 용량을 초과하는 경우 현재 용량의 2배에 해당하는 새로운 Underlying array를 생성하고 기존 배열 값들을 모두 새 배열에 복제한 후 다시 슬라이스를 할당. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main import \u0026#34;fmt\u0026#34; func main() { // len=0, cap=3 인 슬라이스 sliceA := make([]int, 0, 3) // 계속 한 요소씩 추가 for i := 1; i \u0026lt;= 15; i++ { sliceA = append(sliceA, i) // 슬라이스 길이와 용량 확인 fmt.Println(len(sliceA), cap(sliceA)) } fmt.Println(sliceA) // 1 부터 15 까지 숫자 출력 } 병합 한 슬라이스를 다른 슬라이스 뒤에 병합하기 위해서는 append()와 ellipsis(...)를 사용한다.\nellipsis(...) 은 파이썬 asterisk(*)와 같이 동작한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 package main import \u0026#34;fmt\u0026#34; func main() { sliceA := []int{1, 2, 3} sliceB := []int{4, 5, 6} sliceA = append(sliceA, sliceB...) //sliceA = append(sliceA, 4, 5, 6) fmt.Println(sliceA) // [1 2 3 4 5 6] 출력 } 복사 슬라이스는 내장함수 copy()를 사용하여 한 슬라이스를 다른 슬라이스로 복사할 수도 있다.\n1 2 3 4 5 6 7 func main() { source := []int{0, 1, 2} target := make([]int, len(source), cap(source)*2) copy(target, source) fmt.Println(target) // [0 1 2 ] 출력 println(len(target), cap(target)) // 3, 6 출력 } 슬라이스 내부구조 슬라이스는 내부적으로 사용하는 배열의 부분 영역인 세그먼트에 대한 메타 정보를 가지고 있다. 슬라이스는 크게 3개의 필드로 구성되어 있다.\n내부적으로 사용하는 배열에대한 포인터 세그먼트 길이 세그먼트 최대 용량 처음 슬라이스가 생성될 때, 길이와 용량이 지정되었다면, 내부적으로 용량만큼 크기의 배열을 생성하고, 슬라이스 첫번째 필드에 그 배열의 처음 메모리 위치를 지정한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main const limit = 10 func main() { slice := []int{0, 1, 2, 3, 4, 5} subSlice := slice[2:5] subSlice[0] = 20 subSlice[1] = 30 subSlice[2] = 40 for _, v := range slice { println(v) } } 1 2 3 4 5 6 0 1 20 30 40 5 서브 슬라이스를 만들면 슬라이스 생성시와 마찬가지로 지정 인덱스만큼의 길이와 용량을 설정하게 되며, 배열 포인터는 시작 슬라이스 위치로 초기화 된다.\n위의 예시의 서브 슬라이스의 길이는 3 용량은 4로 만들어진다.\n따라서 서브 슬라이스의 값을 변경하면 원본 슬라이스의 값도 변경된다.\nMap 선언 Map은 키에 대응하는 값을 신속히 찾는 해시테이블을 구현한 자료구조이다. Go 언어는 Map 타입을 내장하고 있는데, map[key 타입]값타입 로 선언할 수 있다.\n1 var idMap map[int]string 이때 선언된 idMap은 레퍼런스 타입이므로 nil 값을 갖으며, 이를 Nil Map이라고 부른다. Nil map은 어떤 데이터를 쓸 수 없는데, map을 초기화하기 위해 make()함수를 사용할 수 있다.\n초기화 make() 1 idMap = make(map[int]string) make() 함수의 첫번째 파라미터로 map 키워드와 [키타입]값타입을 지정하는데, 이때 make()함수는 해시테이블 자료구조를 메모리에 생성하고 그 메모리를 가리키는 map value를 리턴한다.\n→ map value는 내부적으로 runtime.hmap 구조체를 가리키는 포인터이다.\n따라서 idMap 변수는 이 해시테이블을 가리키는 map을 가리키게 된다.\n초기화 - 리터럴 map은 make() 함수를 써서 초기화할 수도 있지만, 리터럴을 사용해 초기화할 수도 있다. 리터럴 초기화는 map[key타입]value타입 {key:value} 와 같이 Map 타입 뒤 중괄호 안에 \u0026lsquo;키:값\u0026rsquo;들을 결거하면 된다.\n1 2 3 4 5 ticker := map[string]string{ \u0026#34;GOOG\u0026#34;: \u0026#34;Google Inc\u0026#34;, \u0026#34;MSFT\u0026#34;: \u0026#34;Microsoft\u0026#34;, \u0026#34;FB\u0026#34;: \u0026#34;FaceBook\u0026#34;, } Map 사용 처음 map이 make() 함수에 의해 초기화 되었을 때는, 아무 데이터가 없는 상태이다. 이때 새로운 데이터를 추가하기 위해서는 map변수[키] = 값 처럼 해당 키에 그 값을 할당하면 된다.\n만약 키 값이 이미 존재한다면 추가 대신 값만 갱신한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package main func main() { var m map[int]string m = make(map[int]string) m[901] = \u0026#34;Apple\u0026#34; m[134] = \u0026#34;Grape\u0026#34; m[777] = \u0026#34;Tomato\u0026#34; str := m[134] println(str) noData := m[999] println(noData) println(m[777]) delete(m, 777) println(m[777]) } 만약 map안에 찾는 키가 존재하지 않는다면 reference 타입인 경우 nil, value 타입인 경우 zero를 리턴한다.\nmap에서 특정 키와 그 값을 삭제하기 위해서는 delete() 함수를 이용한다.\nMap 키 체크 map을 사용하는 경우 종종 map안에 특정 키가 존재하는지를 체크할 필요가 있다. 이를 위해 go에선 “map 변수[키]” 읽기를 수행할 때 2개 값을 반환한다.\n키에 상응하는 값 키 존재 여부 (bool) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main func main() { tickers := map[string]string{ \u0026#34;GOOG\u0026#34;: \u0026#34;Google Inc\u0026#34;, \u0026#34;MSFT\u0026#34;: \u0026#34;Microsoft\u0026#34;, \u0026#34;FB\u0026#34;: \u0026#34;FaceBook\u0026#34;, \u0026#34;AMZN\u0026#34;: \u0026#34;Amazon\u0026#34;, } // map 키 체크 val, exists := tickers[\u0026#34;MSFT\u0026#34;] if !exists { println(\u0026#34;No MSFT ticker\u0026#34;) } } for 루프를 사용한 Map 열거 Map이 가진 모든 요소들을 출력하기 위해, for range 루프를 사용할 수 있다. Map 컬렉션에 for range를 사용하면, Map 키와 Map 값 2개 데이터를 반환한다.\n","date":"2023-04-20T12:17:40+09:00","image":"https://codemario318.github.io/post/golang/5/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/golang/5/","title":"Golang: 5. 컬렉션"},{"content":"func 키워드를 사용하여 정의한다.\n함수 파라미터는 0개 이상 사용할 수 있는데, 각 파라미터는 파라미터명 뒤에 int, string 등 파라미터 타입을 정의한다.\n함수의 반환 타입은 파라미터 괄호 뒤에 적는다.\n함수는 패키지 안에 정의되며 호출되는 함수가 호출하는 함수의 반드시 앞에 위치해야 할 필요는 없다.\n1 2 3 4 5 6 7 8 9 package main func main() { msg := \u0026#34;Hello\u0026#34; say(msg) } func say(msg string) { println(msg) } Pass By Reference Go에서 파라미터를 전달하는 방식은 크게 Pass By Value와 Pass By Reference로 나뉜다.\nPass By Value 함수를 사용할 때 변수를 그대로 사용하면 함수 인자로 사용된 변수들의 값이 복사되어 함수에게 전달된다.\n따라서 함수 인자로 받은 값을 함수 내부에서 변경해도 실제 변수값은 영향을 전혀 받지 않는다.\nPass By Reference 변수 앞에 \u0026amp; 를 붙이면 주소를 표시한다. 흔히 포인터라 부르며, 포인터를 사용하면 함수에서 해당 변수를 사용할 때 복사본이 아닌 실제 메모리에 접근하여 변수를 지정하므로 함수 내에서 변경이 인자에 넘겨진 주소를 가진 변수의 실제 값이 변경된다.\n1 2 3 4 5 6 7 8 9 10 11 package main func main() { msg := \u0026#34;Hello\u0026#34; say(\u0026amp;msg) println(msg) //변경된 메시지 출력 } func say(msg *string) { println(*msg) *msg = \u0026#34;Changed\u0026#34; //메시지 변경 } 함수에서 파라미터를 선언할 때 *string 처럼 포인터임을 표시하면 해당 파라미터가 문자열이 아닌 문자열을 저장하고있는 메모리의 주소를 갖는다.\n함수 내에서 포인터 파라미터의 주소에 저장된 데이터를 변경하려면 *변수명 = 값 형태로 변수 이름에 역참조 심볼인 *을 붙여 접근하고 변경할 수 있다.\n가변인자함수 함수에 여러개ㅢ 파라미터를 전달하려면 가변 파라미터를 나타내는 ... 을 타입 앞에 붙여 사용한다.\n가변 파라미터를 갖는 함수를 호출할 때 n개 동일 타입 파라미터를 전달할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 package main func main() { say(\u0026#34;This\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;book\u0026#34;) say(\u0026#34;Hi\u0026#34;) } func say(msg ...string) { for _, s := range msg { println(s) } } 함수 반환값 기본형 함수에서 반환값이 있는 경우 func 문 마지막에 리턴 타입을 정의한다. 그리고 return 키워드를 사용해야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main func main() { total := sum(1, 7, 3, 5, 9) println(total) } func sum(nums ...int) **int** { s := 0 for _, n := range nums { s += n } return **s** } 복수 개 반환값 go 언어에서 함수는 반환값이 여러개일 수 있다.\n여러개 값을 반환하기 위해서는 해당 반환 타입들을 괄호 안에 적어준다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main func main() { count, total := sum(1, 7, 3, 5, 9) println(count, total) } func sum(nums ...int) (int, int) { s := 0 // 합계 count := 0 // 요소 갯수 for _, n := range nums { s += n count++ } return count, s } Named Return Parameter Named Return Parameter라는 기능을 제공하는데, 이는 반환되는 값들을 미리 선언하며, 값들이 여러 개 일때, 코드 가독성을 높힌다.\n1 2 3 4 5 6 7 func sum(nums ...int) (**count** int, **total** int) { for _, n := range nums { total += n } count = len(nums) return } 마지막에 빈 return이 있지만 named return parameter로 정의된 형태로 반환된다. return 은 반환값에 아무런 영향을 주지 않지만 생략하면 에러가 발생한다.\n","date":"2023-04-20T12:10:40+09:00","image":"https://codemario318.github.io/post/golang/4/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/golang/4/","title":"Golang: 4. 함수"},{"content":"for 문 Go 언어에서 반복문은 for 하나뿐이다. 다른 언어들과 비슷하게 for 초기값; 조건식; 증감 {...} 형식을 따른다.\n초기값, 조건식, 증감식은 경우에 따라 생략할 수 있고, 초기값; 조건식; 증감 을 둘러싸는 괄호를 추가하면 에러가 발생한다.\n1 2 3 4 5 6 7 8 9 package main func main() { sum := 0 for i := 1; i \u0026lt;= 100; i++ { sum += i } println(sum) } 조건식만 사용하는 for 루프 초기값과 증감식을 생략하고 조건식만 사용하면 다른 언어의 while 루프와 같게 동작한다.\n1 2 3 4 5 6 7 8 9 10 11 12 package main func main() { n := 1 for n \u0026lt; 100 { n *= 2 //if n \u0026gt; 90 { // break //} } println(n) } 무한루프 초기값, 조건식, 증감을 모두 생략하면 무한루프로 동작한다.\n1 2 3 4 5 6 7 package main func main() { for { println(\u0026#34;Infinite loop\u0026#34;) } } range 문 for range문은 컬렉션으로 부터 한 요소씩 가져와 차례로 for 블럭 문장들을 실행한다. 다른 언어의 foreach와 비슷하다.\nfor range 문은 for 인덱스, 요소값 := range 컬렉션 형태로 for 루프를 구성하는데, range 키워드 다음에 명시한 컬렉션으로부터 하나씩 요소를 반환하여 그 요소의 인덱스와 값을 for 키워드 다음 2개 변수에 각각 할당한다.\n1 2 3 4 5 names := []string{\u0026#34;홍길동\u0026#34;, \u0026#34;이순신\u0026#34;, \u0026#34;강감찬\u0026#34;} for index, name := range names { println(index, name) } break, countinue, goto break: for 루프 내에서 즉시 빠져나옴 continue: 루프 중간에서 나머지 문장들을 실행하지 않고 다음 루프를 시작함 goto: 임의의 문장으로 이동 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package main func main() { var a = 1 for a \u0026lt; 15 { if a == 5 { a += a continue // for루프 시작으로 } a++ if a \u0026gt; 10 { break //루프 빠져나옴 } } if a == 11 { goto END //goto 사용예 } println(a) END: println(\u0026#34;End\u0026#34;) } break 레이블 break문은 보통 단독으로 사용되지만, 경우에 따라 레이블을 붙여 지정된 레이블로 이동할 수도 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main func main() { i := 0 L1: for { if i == 0 { break L1 } } println(\u0026#34;OK\u0026#34;) } break의 레이블은 현재 보통 현재 for 루프 바로 위에 적게 되는데, 현재 루프를 빠져나와 지정된 레이블로 이동하고, 동작한 break문이 속한 for 루프 전체의 다음 문장을 실행한다.\n","date":"2023-04-20T12:05:40+09:00","image":"https://codemario318.github.io/post/golang/3/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/golang/3/","title":"Golang: 3. 반복문"},{"content":"if 문 if 문은 해당 조건이 맞으면 {} 블럭 안의 내용을 실행한다.\n조건식을 ()로 둘러 싸지 않아도 된다. 조건 블럭 시작 { 를 if문과 같은 라인에 두어야 한다. 1 2 3 4 5 6 7 if k == 1 { println(\u0026#34;One\u0026#34;) } else if k == 2 { //같은 라인 println(\u0026#34;Two\u0026#34;) } else { //같은 라인 println(\u0026#34;Other\u0026#34;) } Optional Statement if문에서 조건식을 사용하기 이전에 간단한 문장(Optional Statement)을 함께 실행할 수 있다.\n1 2 3 4 5 6 if val := i * 2; val \u0026lt; max { println(val) } // 아래 처럼 사용하면 Scope 벗어나 에러 val++ val := i * 2 처럼 조건식 이전에 연산을 실행할 수 있는데, 정의된 변수 val의 범위는 if-else 블럭이다.\nif문 외에도 switch, for 에서도 사용할 수 있다.\nswitch 문 다른 언어들과 비슷하게 하나의 변수를 지정하고 case문에 해당 변수가 가질 수 있는 값들을 지정하여 case 블럭을 실행한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package main func main() { var name string var category = 1 switch category { case 1: name = \u0026#34;Paper Book\u0026#34; case 2: name = \u0026#34;eBook\u0026#34; case 3, 4: name = \u0026#34;Blog\u0026#34; default: name = \u0026#34;Other\u0026#34; } println(name) // Expression을 사용한 경우 switch x := category \u0026lt;\u0026lt; 2; x - 1 { //... } } 다른 언어와 차이점 C++, C#, Java 등과 조금 다르게 동작한다.\nswitch 뒤에 expression이 없을 수 있음 다른 언어는 switch 키워드 뒤에 변수나 조건식을 두지만, Go는 쓰지 않아도 된다. 이런 경우 true로 간주하고 첫번째 case문으로 이동하여 검사한다.\ncase문에 조건식 쓸 수 있음 다른 언어는 일반적으로 값을 갖지만, Go는 조건식을 쓸 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func grade(score int) { switch { case score \u0026gt;= 90: println(\u0026#34;A\u0026#34;) case score \u0026gt;= 80: println(\u0026#34;B\u0026#34;) case score \u0026gt;= 70: println(\u0026#34;C\u0026#34;) case score \u0026gt;= 60: println(\u0026#34;D\u0026#34;) default: println(\u0026#34;No Hope\u0026#34;) } } No default fall through case문에 기본적으로 break를 적용한다. 다음 case로 넘어가려면 fallthrough 키워드를 쓴다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func check(val int) { switch val { case 1: fmt.Println(\u0026#34;1 이하\u0026#34;) fallthrough case 2: fmt.Println(\u0026#34;2 이하\u0026#34;) fallthrough case 3: fmt.Println(\u0026#34;3 이하\u0026#34;) fallthrough default: fmt.Println(\u0026#34;default 도달\u0026#34;) } } Type switch switch 뒤 변수의 타입으로 분기할 수 있다.\n1 2 3 4 5 6 7 8 9 10 switch v.(type) { case int: println(\u0026#34;int\u0026#34;) case bool: println(\u0026#34;bool\u0026#34;) case string: println(\u0026#34;string\u0026#34;) default: println(\u0026#34;unknown\u0026#34;) } ","date":"2023-04-20T12:00:40+09:00","image":"https://codemario318.github.io/post/golang/2/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/golang/2/","title":"Golang: 2. 조건문"},{"content":"변수 기본 선언 변수는 Go 키워드 var을 사용하여 선언한다. var 키워드 뒤에 변수명을 적고 변수 타입을 적는다.\n변수를 선언하면서 초기값을 지정하지 않으면, Go는 Zero Value를 기본적으로 할당한다.\n1 var a int 초기값 지정 변수 선언문에서 변수 초기값을 할당할 수도 있다. float32 타입 변수 f에 11.0이라는 초기값을 할당하기 위해 아래같이 쓸 수 있다.\n1 var f float32 = 11. 할당 선언된 변수는 이후 해당 타입 값을 할당할 수 있다.\n1 2 a = 10 f = 12.0 →선언된 변수가 Go 프로그램 내에서 사용되지 않는다면, 에러를 발생시킨다.\nShort Assignment Statement 함수 내부라면 Short Assignment Statement를 사용할 수 있다.\n1 2 i := 1 s := \u0026#34;Hello\u0026#34; 함수 밖에서 선언시에는 var를 사용해야 한다.\n여러 개 변수 동일한 타입 변수가 여러개 있으면, 변수들을 나열하고 마지막에 타입을 한번만 지정할 수 있다.\n1 2 3 4 var i, j, k int // 선언과 동시에 초기화 가능 var i, j, k int = 1, 2, 3 타입 추론 Go 에서는 할당되는 값을 보고 그 타입을 추론하는 기능이 자주 사용된다.\n1 2 var i = 1 var s = \u0026#34;Hello\u0026#34; i는 정수형으로 1이 할당되고, s는 문자열로 Hello가 할당된다.\n상수 상수는 Go 키워드 const를 사용하여 선언한다. const 키워드 뒤에 상수명을 적고, 그 뒤에 상수 타입, 그리고 상수 값을 할당한다.\n1 2 const i int = 1 const s string = \u0026#34;Hello\u0026#34; 타입 추론 변수와 마찬가지로 타입을 생략하고 초기화 하면 Go에서 자동으로 타입을 추론한다.\n1 2 const i = 1 const s = \u0026#34;Hello\u0026#34; 여러개 상수를 묶어서 지정할 수 있다.\n1 2 3 4 5 const ( Visa = \u0026#34;Visa\u0026#34; Master = \u0026#34;Master\u0026#34; Amex = \u0026#34;American Express\u0026#34; ) iota iota 키워드를 사용하면 상수값을 0부터 순차적으로 부여할 수 있다.\n","date":"2023-04-20T11:56:40+09:00","image":"https://codemario318.github.io/post/golang/1/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/golang/1/","title":"Golang: 1. 변수와 상수"},{"content":"Vue? 개발자에게 더 쉽고, 가볍고, 누구나 빨리 배울 수 있는 접근성이 뛰어난 프레임워크를 목표로 개발됨\n기존 HTML 마크업 기반의 템플릿을 그대로 활용 CSS를 작성하는 스타일도 기존 문법을 그대로 따름 라우팅, 상태 관리, 빌드 도구 등 공식적으로 지원하는 라이브러리와 패키지를 통해 배포하여 복잡한 로직의 프론트엔드 개발을 비교적 단순하고 쉽게 만듦 React, Anguler에 비해서 처음 접하는 사용자들이 진입하기에 부담스럽지 않음\nVue3 개선점 가상돔 최적화 AS-IS\n기존 Vue의 렌더링을 위한 가상 DOM 설계는 HTML 기반의 템플릿을 제공하고 이 템플릿 구문을 가상 DOM 트리로 반환한 후 실제로 DOM의 어떤 영역이 업데이트 되어야 하는지 재귀적으로 탐색하는 방식\n이러한 방식은 변경사항 확인을 위해 DOM 트리를 완전 탐색을 하게 되므로, 작은 변경도 트리 전체를 확인하게 되어 비효율적임 TO-BE\n불필요한 탐색을 위한 코드를 제거하여 렌더링 성능을 향상시켜 가상돔 최적화를 진행함\n탬플릿 구문에서 정적 요소와 동적 요소를 구분하여 트리를 탐색할 때 변경이 발생하는 동적 요소만 탐색할 수 있게 변경 렌더링 시 객체가 여러 번 생성되는 것을 방지하기 위해 컴파일러가 미리 템플릿 구문 내에서 정적 요소, 서브 트리, 데이터 객체 등을 탐지해 렌더러 함수 밖으로 호이스팅함 컴파일러가 미리 템플릿 구문 내에서 동적 바인딩 요소에 대해 플래그를 생성함 특정 요소가 동적 클래스 바인딩을 가지고 있고 정적인 값이 지정된 속성을 갖고 있다면 클래스만 처리하면 되므로, 컴파일러가 미리 생성해둔 플래그로 필요한 부분만 처리하여 렌더링 속도 향상 트리쉐이킹 강화 트리쉐이킹이란?\n나무를 흔들어 잎을 떨어트리듯 모듈을 번들링하는 과정에서 사용하지 않는 코드를 제거하여 파일 크기를 줄이는 최적화 방안\nVue3는 템플릿 컴파일러가 실제 사용하는 코드만 임포트 하도록 하였음.\n양방향 데이터 바인딩을 지원하는 v-model 디렉티브와 같은 대부분의 사용자 정의 기능에서 트리쉐이킹이 가능했는데, 이를 강화하여 번들 크기를 절반 이상으로 대폭 줄일 수 있음\nCompoistion API AS-IS\n기존 Vue에서 하나의 컴포넌트에 여러 기능이 포함되면, 기능별로 데이터영역, 메소드 영역, 컴퓨티드 영역, 라이프 사이클 훅, 와치 등 많은 로직이 추가되고, 이러한 기능 별로 분리된 코드들이 각각 기능에 맞는 메서드에 포함되게 되어 필연적으로 섞이게 됨.\n이에 따라 여러 기능이 활용될수록 코드는 커지며 복잡도가 증가함\nTO-BE\n컴포지션 API는 모든 코드를 독립적으로 정의할 수 있다. 각 기능을 함수로 묶어 모듈화 하기 때문에 특정 기능의 유지 보수를 위해 해당 기능을 수행하는 함수만 확인하면 된다.\n코드 재사용 AS-IS\n기존 Vue 에서도 mixin, slot 등으로 컴포넌트 코드를 재사용 할 수 있었으나, 믹스인은 한계가 존재했음\n프로젝트가 커져 믹스인을 다중으로 상속하게 되면 이름 충돌로 인해 컨벤션 룰이 필요했음 매개변수를 믹스인을 통해 전달할 수 없어 유연성이 떨어짐 TO-BE\n컴포지션 API를 사용하면 인스턴스의 특정 기능 단위로 모듈화된 로직을 여러 컴포넌트에서 재사용 할 수 있다.\nmixin?\nVue 컴포넌트에 재사용 가능한 기능을 배포하는 유연한 방법. mixin 객체는 모든 구성요서 옵션을 포함할 수 있으며, 컴포넌트에 mixin을 사용하면 해당 mixin의 모든 옵션이 컴포넌트의 고유 옵션에 “혼합”됨\n그 외 주요 변화 텔레포트 리엑트에서 기본으로 제공하는 포털과 유사한 기능. vue가 기존에 Portal-Vue 플러그인을 통해 제공하고 있었던 기능.\n모달이나 알림 등과 같은 요소를 렌더링하려는 위치가 템플릿 구문이 속하는 컴포넌트와 다른 컴포넌트에 존재할 때, 다른 태그 위치로 모달의 위치를 조정하는 것 처럼 보이게 만드는 것을 CSS를 통해 해결하기 번거롭기 때문에, 보통 모달이 포함된 컴포넌트를 하나 더 만들어 컴포넌트의 구조를 변경하는 방식으로 구현되었음.\nvue3는 텔레포트를 사용하여 모달 컴포넌트를 분리하지 않고도 내부의 HTML을 특정 태그로 옮겨 렌더링 할 수 있게 되었음\n서스펜스 서스팬스 컴포넌트는 리액트가 지원하던 컴포넌트 종류 중 하나로, 컴포넌트 내에 있는 조건인 Async 구문이 충족되지 않으면 조건이 충족될 때까지 템플릿 내에 Fallback 구문을 렌더링함.\n컴포지션 API를 통해 setup() 함수 내에서 외부 API에 접근해 데이터를 가져오는 비동기 작업을 수행하면 데이터를 모두 가져올 때까지 로딩 표시를 해야 할 수 있다. 이럴 때 서스펜스를 사용해 컴포넌트를 감싸면 대체할 템플릿 구문을 렌더링 할 수 있다.\n데이터를 가져오는 도중 오류가 발생하면 Vue3의 새로운 라이프사이클 훅인 OnErrorCaptured를 제공하여, 에러에 대한 처리 구문을 Fallback 구문 대신 표시할 수 있다.\n리액티비티 API 이전 버전의 Vue는 인스턴스 내부에 오브젝트를 선언하고 새로운 속성을 추가하는 것을 감지할 수 없었다. 그래서 기존에는 Vue.set 메소드를 사용하여 기존 객체에 반응성을 부여했다.\nVue3는 이러한 데이터 반응성을 해결하기 위해 리액티비티 API를 지원한다. 객체에 반응성을 추가하기 위해서 리액티브 메소드를 사용하면 된다.\n단순 값이라면 ref 메소드를 사용한다. 이외에도 Readonly, ToRef 등 반응성을 지원하는 여러 API 가 추가되었다.\n","date":"2023-04-18T20:05:16+09:00","image":"https://codemario318.github.io/post/vue3/vue3_cover_huce45f5603be21a224ec2957025110a35_3700_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/vue3/","title":"Vue3 살펴보기"},{"content":" 화면이 수정될 때, 렌더링 과정을 최적화하는 방법\n재배치(Reflow)와 다시 그리기(Repainting) 처음 화면이 그려진 후 사용자의 인터렉션에 의해 요소가 변경되는 일이 발생하였을때, Render 트리가 변경되면서 발생한다.\n이 과정이 빈번하게 발생할 수록 성능 및 사용자 경험이 저하되기 때문에 이를 최소화하는 것이 좋다.\n다시 그리기(Repainting) 재배치가 발생하거나 요소의 색상등이 변경된 경우, 다시 화면에 표현하는 동작.\n화면의 구조가 변경되었을 때\nReflow 과정을 거쳐 화면 구조를 다시 계산한 후 Repaint 과정을 통해 화면을 다시 그린다.\n화면의 구조가 변경되었을 때에는 Reflow와 Repaint 모두 발생합니다. 화면의 구조가 변경되지 않는 화면 변화의 경우\nRepaint만 발생화면 구조(Layout)이 변경되었을 때, 뷰포트 내에서 렌더 트리의 노드의 정확한 위치와 크기를 계산하는 과정을 다시 수행해야 한다.\nopacity, background-color, visibility, outline 등의 스타일 변경 시에는 Repaint만 동작한다. Repaint는 변경된 화면을 실제 화면에 반영하는 과정으로 최적화할 수 있는 방법은 화면 변화를 최소화할 수 있는 방법 뿐이다.\n재배치(Reflow) 화면 구조(Layout)이 변경되었을 때, 뷰포트 내에서 렌더 트리의 노드의 정확한 위치와 크기를 계산하는 과정\nReflow가 발생하는 경우 DOM 노드의 추가, 제거 DOM 노드의 위치 변경 DOM 노드의 크기 변경(margin, padding, border, width, height 등..) CSS3 애니메이션과 트랜지션 폰트 변경, 텍스트 내용 변경 이미지 크기 변경 offset, scrollTop, scrollLeft과 같은 계산된 스타일 정보 요청 페이지 초기 렌더링 윈도우 리사이징 위의 내용에서 빠졌더라도 화면의 구조가 변경되었다면 Reflow가 발생한다.\nReflow 최적화 재배치 작업은 다시 그리기 작업을 동반하기 때문에 경우에 따라 Render 트리 전체를 재구성할 수도 있으므로 다시 그리기만 발생할 떄에 비해서 비용이 훨씬 비싸다.\n재배치, 다시 그리기 작업을 최소화 하는 과정을 통해 최적화를 한다.\nReflow는 렌더 트리의 변화를 최소화하는 등.. DOM의 depth를 최소화\nDOM의 깊이와 크기를 작게 구성하여 재배치를 더 빠르게 처리하게 만든다. 스타일 변경을 한번에 처리한다.\n1 2 3 4 5 .newstyles { width: 100px; height: 200px; margin: 10px; } 1 2 3 4 5 // 비효율적인 코드 예시 var myelement = document.getElementById(\u0026#39;myelement\u0026#39;); myelement.width = \u0026#39;100px\u0026#39;; myelement.height = \u0026#39;200px\u0026#39;; myelement.style.margin = \u0026#39;10px\u0026#39;; 1 2 3 // 개선된 예시 var myelement = document.getElementById(\u0026#39;myelement\u0026#39;); myelement.classList.add(\u0026#39;newstyles\u0026#39;); 미리 만들어놓은 스타일을 한번에 적용하여 재배치를 최소화 할 수 있다.\n주변에 영향을 주는 요소를 제한한다.\n인터렉션에 의해 크기나 위치가 변경되는 요소는 병경될 때 주변 요소들이 최대한 영향받지 않도록 정의한다.\n스타일을 변경할 경우 가장 하위 노드의 클래스를 변경 애니메이션이 있는 노드는 position을 fixed 또는 absolute 로 지정한다. 개발자 도구를 이용하여 분석\n개발자 도구를 이용하여 재배치와 다시 그리기가 얼마나 발생하는지 확인하고 해당 요소를 최적화 시도한다.\n라이브러리를 사용한다.\nReact, Vue는 트리 형태의 Object를 통해 Virtual DOM을 구성하고 요소가 변경될 때 업데이트한다. 그 후 최종 상태의 Virtual DOM을 실제 DOM에 반영하여 재배치와 다시 그리기를 최소화 시켜 렌더링 최적화를 구현한다.\n스타일을 변경할 경우 가장 하위 노드의 클래스를 변경\nDOM 노드의 크기 또는 위치가 변경되면 하위 노드와 상위 노드까지 영향을 미칠 수 있다. 따라서 가장 하위 노드의 스타일을 변경할 경우, 전체 노드가 아니니 일부 노드로 영향을 최소화 할 수 있다.\n하지만 실무에서는 보통 변경해야 할 노드들이 정해져 있기 때문에 적용 범위가 크지 않을 수 있다.\n애니메이션이 있는 노드는 position을 fixed 또는 absolute로 지정한다. 애니메이션 효과는 많은 Reflow 비용이 발생하게 됨.\nposition 속성을 fixed 또는 absolute 로 지정하면, 해당 노드를 전체 노드에서 분리시켜 일부만 Reflow가 발생하도록 제한시킬 수 있다.\n애니메이션 효과를 줘야 하는 노드에 position 속성이 적용되지 않았다면 애니메이션 시작 시 position 속성 값을 fixed 또는 absolute로 변경하였다가 애니메이션 종료 후 다시 원복 시켜 렌더링을 최적화할 수 있다.\n\u0026lt;table\u0026gt; 레이아웃을 피한다. \u0026lt;table\u0026gt; 은 점진적으로 렌더링 되지 않고, 모두 로드되고 테이블 너비가 계산된 후 화면에 그려진다. 테이블 안의 콘텐츠의 값에 따라 테이블 너비가 계산된다.\n콘텐츠의 값에 따라 테이블 너비가 계산되기 때문에, 테이블 콘텐츠의 작은 변경만 있어도 테이블 너비가 다시 계산되고 테이블의 모든 노드들이 Reflow가 발생한다.\n부득이하게 \u0026lt;table\u0026gt;을 사용할 때는 table-layout:fixed 값을 지정하는 것이 좋다.\ntable-layout:fixed는 테이블의 콘텐츠의 길이에 따라 테이블의 너비가 계산되는 것이 아니기 때문에, table-layout의 기본 값인 auto에 비해 성능이 더 좋다. \u0026lt;table\u0026gt;을 레이아웃 용도가 아닌 데이터 표시 용도로 사용할 때도 table-layout:fixed를 지정하는 것이 성능 면에서 더 좋습니다. IE의 CSS 표현식을 사용하지 않는다. CSS 표현식은 비용이 매우 높기 때문에 사용을 피해야 함\n1 2 3 .expression { width: expression(document.documentElement.clientWidth \u0026gt; 0 ? \u0026#39;1000px\u0026#39; : \u0026#39;auto\u0026#39;); } Reflow가 발생할 때마다 자바스크립트 표현식이 다시 계산되기 때문에 CSS 표현식은 비용이 비싸다.\n애니메이션이 동작한다면, 애니메이션에 의한 Reflow가 발생할 때마다 자바스크립트 표현식이 계산됨. CSS 하위 선택자를 최소화한다. 1 2 3 4 5 6 7 8 /* 잘못된 예 */ .reflow_box .reflow_list li .btn{ display:block; } /* 올바른 예 */ .reflow_list .btn { display:block; } CSS 하위 선택자를 최소화하는 것이 렌더링 성능에 더 좋다.\n렌더 트리는 DOM과 CSSOM이 합쳐져서 만들어 지는데, DOM은 HTML이 파싱 되어 만들어진 트리이고, CSSOM은 CSS가 파싱 되어 만들어진 트리이다.\n두 트리를 결합하여 렌더 트리를 만드는데, CSS 하위 선택자가 많아지만 CSSOM 트리의 깊이가 깊어지게 되고 결국 렌더 트리를 만드는 시간이 더 오래 걸릴 수 있다.\n숨겨진 노드의 스타일을 변경한다. display:none으로 숨겨진 노드를 변경할 때는 Reflow가 발생하지 않기 때문에 숨겨진 노드를 표시하기 전에 노드의 콘텐츠를 먼저 변경한 후 화면에 나타내면 Reflow를 줄일 수 있다.\n클래스를 사용하여 한 번에 스타일을 변경한다. 스타일을 변경할 때, 스타일을 각각 변경할 경우 추가 Reflow가 발생할 수 있기 때문에 한번에 스타일을 변경하는 것이 좋다.\n요약 Repaint(Redraw)는 화면에 변화가 있을 때 화면을 그리는 과정 Reflow(Layout)는 뷰포트 내에서 렌더 트리의 노드의 정확한 위치와 크기를 계산하는 과정 Repaint가 발생하는 경우는 화면이 변경되는 모든 경우 Reflow가 발생하는 경우는 화면의 구조가 바뀌었을 경우 Reflow를 최적화하는 방법 스타일을 변경할 경우 가장 하위 노드의 클래스를 변경한다. 인라인 스타일을 사용하지 않는다. 애니메이션이 있는 노드는 position을 fixed 또는 absolute로 지정한다. 퀄리티, 퍼포먼스의 타협점을 찾는다. \u0026lt;table\u0026gt; 레이아웃을 피한다. IE의 CSS 표현식을 사용하지 않는다. CSS 하위 선택자를 최소화한다. 숨겨진 노드의 스타일을 변경한다. 클래스를 혹은 cssText 사용하여 한 번에 스타일을 변경한다. DOM 사용을 최소화한다. 캐시를 활용한다. 라이브러리를 사용한다. ","date":"2023-04-18T19:48:26+09:00","image":"https://codemario318.github.io/post/rendering_optimize/browser_cover_huc408e1e4bc0026ab219b7f7573db946e_26010_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/rendering_optimize/","title":"랜더링 업데이트 최적화"},{"content":" CRP 최적화란 HTML, CSS 및 JS 간 종속성을 이해하고 최적화 하는것이다.\nCritical Rendering Path란? 브라우저가 페이지의 초기 출력을 위해 실행해야 하는 순서\nDOM 트리 구축 CSSOM 트리 구축 JS 실행 렌더트리 구축 레이아웃 생성 페인팅 CSS CSS는 렌더링 차단 리소스이므로 최초 렌더링에 걸리는 시간을 최적화하려면 클라이언트에 최대한 빠르게 다운로드되어야 한다.\n렌더 트리를 만들 때 사용되는 HTML, CSS 모두 렌더링 차단 리소스 CSS가 없는 페이지는 상대적으로 사용성이 떨어지기 때문에 브라우저는 DOM과 CSSOM을 모두 사용할 수 있게 될 때까지 렌더링을 차단한다.\nCSS를 간단하게 유지하고 가능한 빨리 제공하고 최대한 빨리 렌더링의 차단을 해제해야 한다.\n미디어 쿼리, 미디어 유형 미디어 쿼리를 사용하면 특정한 사용 사례와 동적인 조건에 맞게 렌더링이 차단되므로 효율을 높힐 수 있다.\n미디어 유형과 미디어 쿼리를 통해 일부 CSS 리소스를 렌더링 비차단 리소스로 표시할 수 있음 브라우저는 차단 동작이든 비차단 동작이든 관계없이 모든 CSS 리소스를 다운로드함 미디어 쿼리는 하나의 미디어 유형과 특정 미디어 기능의 조건을 확인하는 0개 이상의 식으로 구성된다.\n1 2 3 4 5 6 \u0026lt;!-- 1 --\u0026gt; \u0026lt;link href=\u0026#34;style.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;!-- 2 --\u0026gt; \u0026lt;link href=\u0026#34;print.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; media=\u0026#34;print\u0026#34;\u0026gt; \u0026lt;!-- 3 --\u0026gt; \u0026lt;link href=\u0026#34;other.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; media=\u0026#34;(min-width: 40em)\u0026#34;\u0026gt; 조건이 없는 경우\n미디어 유형이나 미디어 쿼리를 제공하지 않아서 모든 경우에 적용됨 즉 항상 렌더링을 차단함 미디어 유형을 적용\n콘텐츠가 인쇄될 때만 적용 처음에 로드될 때 페이지 렌더링을 차단할 필요가 없음 미디어 쿼리 적용\n조건이 일치하면 스타일시트가 다운로드되고 처리될 때까지 브라우저가 렌더링을 차단. JS 자바스크립트는 파서 차단 리소스(parser blocking resource)이며, JS를 사용하면 콘텐츠, 스타일, 사용자와의 상호작용등 거의 모든것을 수정할 수 있다.\nJS실행은 DOM 생성을 차단하고 페이지 렌더링을 지연하게 된다.\n자바스크립트를 비동기로 설정하고, CRP에서 불필요한 JS를 제거해야 한다. JS와 HTML의 종속성 HTML 파서는 script 태그를 만나면 DOM 생성 프로세스를 중지하고 자바스크립트 엔진에 권한을 넘긴다. 자바스크립트 엔진의 실행이 완료된 후 브라우저가 중지했던 시점부터 DOM 생성을 다시 시작하게 된다.\nscript 태그의 뒷부분에서 정의된 어떠한 태그들도 아직 생성되지 않았기 때문에 노드를 찾을 수 없다. 또한, 인라인 스크립트를 실행하면 DOM 생성이 차단되고, 이로 인해 초기 렌더링도 지연된다.\n이러한 이유로 인하여 자바스크립트는 화면에 그려지는 태그들이 모두 파싱 된 후인, \u0026lt;body\u0026gt; 태그를 닫기 직전에 \u0026lt;script\u0026gt; 태그를 선언하는 것이 좋다.\nJS와 CSS의 종속성 CSS를 파싱 하는 동안 자바스크립트에서 스타일 정보를 요청하는 경우, CSS가 파싱이 끝나지 않은 상태라면 자바스크립트 오류가 발생할 수 있다. CSS 파싱으로 생성되는 CSSOM과 JavaScript에서 스타일 수정 시 발생하는 CSSOM 수정 사이에 경쟁 조건(race condition)이 발생할 수 있다.\n브라우저는 이 문제를 해결하기 위해 CSSOM을 생성하는 작업이 완료할 때까지 자바스크립트 실행 및 DOM 생성을 지연시킨다. DOM, CSSOM, 자바스크립트 실행 간에 종속성 때문에 브라우저가 화면에 페이지를 처리하고 렌더링 할 때 상당한 지연이 발생할 수 있습니다.\n비동기 JS HTML을 파싱 하면서 script 태그를 만나면 DOM 생성을 중지시키고 자바스크립트 엔진에게 제어 권한을 넘겨 자바스크립트를 실행한 후, DOM 생성을 진행한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;!-- index.html --\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width,initial-scale=1\u0026#34;\u0026gt; \u0026lt;link href=\u0026#34;style.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Critical Path: Script\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Hello \u0026lt;span\u0026gt;web performance\u0026lt;/span\u0026gt; students!\u0026lt;/p\u0026gt; \u0026lt;script src=\u0026#34;app.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 1 2 3 4 5 6 7 8 9 // app.js var span = document.getElementsByTagName(\u0026#39;span\u0026#39;)[0]; span.textContent = \u0026#39;interactive\u0026#39;; // change DOM text content span.style.display = \u0026#39;inline\u0026#39;; // change CSSOM property // create a new element, style it, and append it to the DOM var loadTime = document.createElement(\u0026#39;div\u0026#39;); loadTime.textContent = \u0026#39;You loaded this page on: \u0026#39; + new Date(); loadTime.style.color = \u0026#39;blue\u0026#39;; document.body.appendChild(loadTime); 위에서 살펴본 인라인 스크립트뿐만 아니라 위의 코드와 같이 script 태그를 통해 포함된 자바스크립트 역시 파싱을 중지시킨다.\nscript 태그를 사용하여 자바스크립트를 실행할 경우, 서버에서 자바스크립트를 가져올 때까지 기다려야하며 이로 인해 수십~수천 밀리초의 지연이 추가로 발생할 수 있다.\n기본적으로 자바스크립트가 실행될 때, 스크립트가 페이지에서 무엇을 수행할지 모르기 때문에 브라우저는 최악의 대비하여 파서를 차단한다.\n브라우저에 자바스크립트를 바로 실행할 필요가 없음을 알려준다면, 브라우저는 계속해서 DOM을 생성할 수 있고 DOM 생성이 끝난 후에 자바스크립트를 실행할 수 있게 된다.\n이때 사용할 수 있는 것이 비동기 자바스크립트이다.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width,initial-scale=1\u0026#34;\u0026gt; \u0026lt;link href=\u0026#34;style.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Critical Path: Script Async\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Hello \u0026lt;span\u0026gt;web performance\u0026lt;/span\u0026gt; students!\u0026lt;/p\u0026gt; \u0026lt;script src=\u0026#34;app.js\u0026#34; async\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 위의 코드와 같이 단순히 script 태그에 async 속성을 추가해 주면 됩니다. async 속성을 script 태그에 추가하여 자바스크립트가 사용 가능해질 때까지 브라우저에게 DOM 생성을 중지하지 않아도 괜찮다는 것을 알릴 수 있다.\n리소스 우선순위 지정 브라우저는 가장 중요한 리소스(스크립트나 이미지보다 CSS 우선)를 우선 로드하기 위해 가장 중요하다 생각되는 리소스를 추측하여 먼저 로드한다. 하지만 브라우저에게 리소스의 우선순위를 전달하여 중요한 리소스를 먼저 처리하게 할 수 있다.\npreload 속성 현재 페이지에서 빠르게 가져와야 하는 리소스에 사용되는 속성이다.\n\u0026lt;link rel=\u0026quot;preload\u0026quot; as=\u0026quot;...\u0026quot;\u0026gt;는 브라우저에게 현재 리소스가 필요하며, 가능한 한 빨리 가져오기를 시도해야 한다고 알리는 역할을 한다.\n1 2 \u0026lt;link rel=\u0026#34;preload\u0026#34; as=\u0026#34;script\u0026#34; href=\u0026#34;super-important.js\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;preload\u0026#34; as=\u0026#34;style\u0026#34; href=\u0026#34;critical.css\u0026#34;\u0026gt; as 속성을 사용하여 리소스의 유형을 알려줘야 한다. 브라우저는 올바른 유형이 설정되어 있지 않으면 미리 가져온 리소스를 사용하지 않는다. preload는 브라우저가 반드시 리소스를 가져오게 만들며, 리소스를 두 번 가져오게 하거나, 필요하지 않은 것을 가져오지 않도록 주의해야한다.\npreload를 이용하여 리소스를 가져왔지만 현재 페이지에서 즉시 사용되지 않는 리소스는 위의 그림과 같이 워닝 로그가 노출된다.\nprefetch 속성 미래에 필요할 수 있는 리소스를 가져와야 할 때 사용되는 속성이다. \u0026lt;link rel=\u0026quot;prefetch\u0026quot;\u0026gt;는 현재 페이지 로딩이 마치고 다운로드할 여유가 생겼을 때 가장 낮은 우선순위로 리소스를 가져온다.\nprefetch는 사용자가 다음에 할 행동을 미리 준비하는 역할을 한다. 예를 들어, 현재 페이지가 1페이지 라면,\n1 \u0026lt;link rel=\u0026#34;prefetch\u0026#34; href=\u0026#34;page-2.html\u0026#34;\u0026gt; 위의 코드와 같이 사용하여 2페이지를 먼저 가져와 준비하게 된다.\npage-2.html의 HTML만 가져오고 page-2.html에서 사용되는 리소스는 가져오지 않는다.\n요약 CSS 최적화 방법\n미디어 유형, 미디어 쿼리를 사용 JavaScript 최적화 방법\nbody 태그 닫기 직전 \u0026lt;script\u0026gt; 태그를 선언 \u0026lt;script ... async\u0026gt;와 같이 async 속성을 사용 리소스 우선순위 지정\n현재 페이지에서 빠르게 가져와야 하는 리소스에 \u0026lt;link rel=\u0026quot;preload\u0026quot; as=\u0026quot;...\u0026quot;\u0026gt;와 같이 preload 속성을 사용 미래에 사용할 수 있는 리소스는 \u0026lt;link rel=\u0026quot;prefetch\u0026quot;\u0026gt;와 같이 prefetch 속성을 사용 ","date":"2023-04-18T19:27:35+09:00","image":"https://codemario318.github.io/post/crp_optimize/browser_cover_huc408e1e4bc0026ab219b7f7573db946e_26010_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/crp_optimize/","title":"Critical Rendering Path 최적화"},{"content":"브라우저 기본 구조 사용자 인테페이스\n요청한 페이지를 보여주는 부분의 제외한 나머지 부분\n브라우저 엔진\n사용자 인터페이스와 렌더링 엔진 사이의 동작을 제어\n랜더링 엔진\n요청한 콘텐츠 표시\nHTML을 요청하면 HTML과 CSS를 파싱하여 화면에 표시함 통신\nHTTP 요청과 같은 네트워크 호출에 사용됨.\n플랫폼 독립적인 인터페이스이고 각 플랫폼 하부에서 실행됨. UI 백앤드\n콤보 박스와 창 같은 기본적인 장치를 그림\n플랫폼에서 명시하지 않은 일반적인 인터페이스로서, OS 사용자 인터페이스 체계를 사용 자바스크립트 해석기\n자바스크립트 코드를 해석하고 실행\n자료 저장소\n자료를 저장하는 계층으로 쿠키와 같은 모든 종류의 자원을 하드디스크에 저장.\nHTML5에는 브라우저가 지원하는 \u0026lsquo;웹 데이터 베이스\u0026rsquo; 가 정의되어 있다. 크롬은 대부분의 브라우저와 달리 각 탭마다 별도의 렌더링 엔진 인스턴스를 유지하여, 각 탭이 독립된 프로세스로 처리된다.\n렌더링 렌더링 엔진 요청받은 내용을 브라우저 화면에 표시함. HTML 및 XML 문서와 이미지를 표시할 수 있다.\n렌더링 엔진의 역할은 요청받은 내용을 브라우저 화면에 나타내는 일이다. HTML, CSS JS 등의 파일을 브라우저가 화면에 표시할 수 있도록 변환하여 픽셀 단위로 나타낸다.\n렌더링 엔진 동작과정 렌더링 엔진은 요청한 문서의 내용을 얻는 것에서 시작하며, 보통 8KB 단위로 전송된다.\n렌더링 엔진은 HTML 문서를 파싱하고 콘텐츠 트리 내부에서 태그를 DOM 노드로 변환한다. 그 다음 외부 CSS 파일과 함께 포함된 스타일 요소도 파싱한다. 스타일 정보와 HTML 표시 규칙은 렌더 트리 라고 부르는 또 다른 트리를 생성한다.\n파싱 문서를 통해 파싱트리를 만드는 과정\n어휘분석: Tokenizer\n문서를 읽어 정해놓은 규칙을 통해 토큰을 추출하는 도구, 과정을 의미한다.\nToken: 의미적으로 더이상 나눌 수 없는 기본적인 언어 요소를 표현하는 데이터 단위\n구문분석: Lexer\n토큰에 약속된 의미를 부여하는 도구, 과정\n어휘분석과 구문분석의 결과물을 이용하여 파싱트리를 만드는 과정이다. 컴파일 파싱을 통해 만들어진 결과물을 기계 코드로 변환하는 과정이다\nDOM - 문서 객체 모델: Document Object Model HTML, XML 문서의 프로그래밍 인터페이스이다. HTML 문서의 객체 표현\n문서의 구조화된 표현을 제공하며 프로그래밍 언어가 DOM 구조에 접근할 수 있는 방법을 제공하여 그들이 문서 구조, 스타일, 내용 등을 변경할 수 있게 돕는다.\nnodes와 property와 method 를 갖고 있는 objects로 문서를 표현한다. 이들은 웹 페이지를 스크립트 또는 프로그래밍 언어들에서 사용될 수 있게 연결시켜주는 역할을 담당한다.\n동일한 문서를 표현, 저장, 조작하는 방법을 제공하는 웹 페이지의 객체 지향 표현\n따라서, DOM은 마크업과 1:1 관계를 맺는다.\n1 2 3 4 5 6 \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Hello World\u0026lt;/p\u0026gt; \u0026lt;div\u0026gt;\u0026lt;img src=\u0026#34;example.png\u0026#34; /\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; CSSOM(CSS Object Model) 위의 그림과 같이 DOM을 생성하는 과정 그대로 CSSOM을 생성한다.\n브라우저는 DOM을 생성하는 동안 외부 CSS를 참조하는 \u0026lt;link\u0026gt; 태그를 만나게 되면 브라우저에 리소스를 요청함. CSS의 원시 바이트가 문자열로 변환 된 후 차례로 토큰과 노드로 변환되고 마지막으로 CSSOM이라는 트리 구조를 만든다. CSSOM은 하향식으로 규칙을 적용하기 때문에 트리 구조를 가진다. 최종 스타일을 계산할 때 브라우저는 해당 노드에 적용 가능한 가장 일반적인 규칙으로 시작해 더 구체적인 규칙을 적용하는 방식으로 처리된다.\nDOM 트리를 바탕으로 CSSOM 트리를 만들기 때문에 해당 하는 스타일룰이 적용이 된 트리 형태로 구성되는 것 같다.\n어태치먼트: 랜더링 트리 생성 과정 DOM 트리와 CSSOM 트리를 결합하여, 표시해야 할 순서로 내용을 그려냏 수 있도록 하기 위해 렌더트리를 형성한다. 이 과정을 웹킷에서는 어테치먼트라고 한다.\n렌더트리는 화면에 표시되는 각 노드의 위치를 계산하는 레이아웃에 사용되고 픽셀을 하면에 그리는 페인트 과정에도 사용됨\n랜더 트리 생성 과정 DOM 트리 구축을 위한 HTML 파싱 HTML 파싱 → DOM 트리 구축 CSSOM 트리 구축을 위한 CSS 파싱 CSS → CSSOM 트리 생성 DOM 트리와 CSSOM 트리를 활용하여 랜더 트리 구축 DOM Tree + CSSOM Tree = Rendering Tree 랜더링 트리 생성 과정 2 DOM 트리의 루트에서 시작하여 표시되는 노드 각각을 탐색함 스크립트 태그, 메타 태그 등 랜더링된 출력에 반영되지 않는 트리들이 생략됨 CSS를 속성을 통해 숨겨지는 노드들이 생략됨 ex) display: none 표시된 각 노드에 대해 적절하게 일치하는 CSSOM 규칙을 찾아 적용 표시된 노드를 콘텐츠 및 계산된 스타일과 함께 내보냄 visibility: hidden은 비어 있는 상자로 렌더링되지만 display: none은 랜더링에서 제외됨. 따라서 후자는 스크린 리더기에서 읽을 수 없음, 전자는 일부 스크린 리더기에서 인식하지 않기 때문에 접근성을 위한 IR 처리시 주의해야 한다.\n최종 출력은 화면에 표시되는 모든 노드의 콘텐츠 및 스타일 정보를 포함하는 렌더링 트리\n레이아웃 렌더 트리가 생성되고, 기기의 뷰포트 내에서 렌더 트리의 노드가 정확한 위치와 크기를 계산하는 과정.\n모든 상대적인 측정값은 화면에서 절대적인 픽셀로 변환됨. 즉 CSS에 상대적인 값인 %로 할당된 값들은 절대적인 값인 PX 단위로 변환.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width,initial-scale=1\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Critial Path: Hello world!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div style=\u0026#34;width: 50%\u0026#34;\u0026gt; \u0026lt;div style=\u0026#34;width: 50%\u0026#34;\u0026gt;Hello world!\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 뷰포트 내에서 각 요소의 정확한 위치와 크기를 정확하게 캡처하는 상자 모델이 출력된다. 모든 상대적인 측정값은 화면에서 절대적인 픽셀로 변환된다.\n페인팅 렌더링 트리의 각 노드를 화면의 실제 픽셀로 변환하는 마지막 단계로 이러한 정보를 전달할 수 있습니다.\n\u0026lsquo;Layout\u0026rsquo; 이벤트는 타임라인에서 렌더링 트리 생성, 위치 및 크기 계산을 캡처. 레이아웃이 완료될 때 브라우저가 Paint Setup 및 Paint 이벤트를 발생. 이러한 작업은 렌더링 트리를 화면의 픽셀로 변환. 렌더링 트리 생성, 레이아웃 및 페인트 작업을 수행하는 데 필요한 시간은 문서의 크기, 적용된 스타일 및 실행 중인 기기에 따라 달라진다.\n문서가 클수록 브라우저가 수행해야 하는 작업도 더 많아지며, 스타일이 복잡할수록 페인팅에 걸리는 시간도 늘어나게 된다.\nex) 단색은 페인트하는 데 시간과 작업이 적게 필요한 반면, 그림자 효과는 계산하고 렌더링하는 데 시간과 작업이 더 필요하다. 요약 HTML 마크업을 처리하고 DOM 트리를 빌드 (DOM 파싱) CSS 마크업을 처리하고 CSSOM 트리를 빌드 (CSS 파싱) DOM 및 CSSOM을 결합하여 렌더 트리를 형성 (Attachment) 렌더 트리에서 레이아웃을 실행하여 각 노드의 기하학적 형태를 계산 (Layout) 개별 노드를 화면에 페인트(Painting) 참고 자료\nhttps://developers.google.com/web/fundamentals/performance/critical-rendering-path?hl=ko https://janghanboram.github.io/2018/06/06/browser-rendering/ https://d2.naver.com/helloworld/59361 http://taligarsiel.com/Projects/howbrowserswork1.htm#Render_tree_construction https://grosskurth.ca/papers/browser-refarch.pdf https://yilpe93.github.io/2018/06/18/etc/web-browser/ https://sangbui.com/sb-files/BrowserArchitecture_ClientSide.pdf https://medium.com/@monica1109/how-does-web-browsers-work-c95ad628a509 https://blog.lgcns.com/1911 https://cisctbd.github.io/Report.pdf https://blog.asamaru.net/2017/05/04/understanding-the-critical-rendering-path/ ","date":"2023-04-18T18:48:11+09:00","image":"https://codemario318.github.io/post/browser/browser_cover_huc408e1e4bc0026ab219b7f7573db946e_26010_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/browser/","title":"브라우저"},{"content":"Message Queue란? MQ란 메시지 기반의 미들웨어로 메시지를 이용하여 여러 어플리케이션, 시스템, 서비스들을 연결해주는 솔루션이다. MOM(Message Oriented Middleware)를 구현한 솔루션으로 비동기 메시지를 사용하는 서비스들 사이에서 데이터를 교환해주는 역할을 한다.\nProducer(sender)가 메시지를 큐에 전송하면 Consumer(receiver)가 처리하는 방식으로, Producer와 Consumer에 message 프로세스가 추가되는 것이 특징이다.\nMQ를 사용하여 비동기로 요청을 처리하고 Queue 에 저장하여 Consumer에게 병목을 줄여줄 수 있으나 제품마다 구현이 다르고 장단이 있다.\n대표 솔루션 IBM MQ 가장 많이 사용되는 상용 MQ 제품으로 표준 JMS 메시지 ㅇ기반으로 MQTT 프로토콜을 지원한다.\nApache ActiveMQ 자바 기반의 JMS Queue를 지원하는 오픈소스로 MQTT, AMQP, OpenWire, STOMP 프로토콜을 지원한다. 다양한 언어를 지원하며 크러스터링이 가능하다. 단 모니터링 도구는 없다. REST API를 통해 웹기반 메시징 API를 지원하며 Ajax를 통해 순수한 DHTML을 사용한 웹스트리밍을 지원 Rabbit MQ 고성능을 목표로 AMQP 프로토콜을 사용하여 개발된 MQ 로 Erlang OTP 기반으로 개발되었다. 실시간 모니터링이 용이하고 다양한 언어 및 OS 지원, RabbitMQ 서버간 클러스터링이 가능하다. Kafka Linkedin에서 구직정보들을 처리할 수 있는 플랫폼으로 개발이 시작되었다. 실시간 로그 처리에 특화되어 설계된 시스템으로 개발되어 타 MQ 대비 TPS가 매우 우수하나 특화된 솔루션이기 때문에 타 MQ솔루션에서 제공하는 다양한 기능들은 제공되지 않는다. AMQP, JMS 이 아닌 단순 메시지 헤더를 이용한 TCP 통신이다. MQ는 broker가 pruducer에게 메세지를 받아서 consumer 에게 push해주는 방식인데 반해, kafka는 consumer가 Broker로 부터 직접 메시지를 가지고 가는 pull 방식으로 동작한다. Consumer는 자신의 처리능력 만큼의 메시지만 broker로부터 가져오기 때문에 최적의 성능을 낼 수 있다. 많은 데이터 전송과 최대 처리량을 유지하기에 대량 데이터 스트리밍에 적합하다. 상태 변경이 시간순으로 기록되어야 하는 응용 프로그램인 이벤트 소싱(Event Sourcing) 저장소로 적합하다. ","date":"2023-04-18T17:19:16+09:00","image":"https://codemario318.github.io/post/message_queue/mq_cover_hu0d801aafbbe1fc1c7c4e885cbcd16691_10519_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/message_queue/","title":"Message Queue"},{"content":"What is Type Annotation in TypeScript 타입스크립트는 타입 어노테이션을 사용하여 변수, 함수, 객체 등과 같은 식별자의 유형을 명시적으로 지정한다.\n: type 구문을 식별자 뒤에 붙이는 타입 어노테이션으로 사용한다.\ntype은 모든 유효한 유형 가능 식별자에 타입 어노테이션을 추가하면 해당 타입으로만 사용할 수 있다. 식별자가 다른 유형으로 사용되면 타입스클입트 컴파일러에서 오류를 발생시킨다.\nType annotations in variables and constants 1 2 3 let variableName: type; let variableName: type = value; const constantName: type = value; 위 코드에서 타입 어노테이션은 변수, 상수 이름 뒤에 :을 붙이고 그 뒤에 타입이 온다.\n지정한 타입 외 다른 타입 값을 할당하면 컴파일 에러를 발생시킨다.\nType annotation examples 배열\n배열 타입 어노테이션은 : type[] 로 표시한다.\n1 2 let arrayName: type[]; let names: string[] = [\u0026#39;John\u0026#39;, \u0026#39;Jane\u0026#39;, \u0026#39;Peter\u0026#39;, \u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;]; 객체\n1 2 3 4 5 6 7 8 9 let person: { name: string; age: number }; person = { name: \u0026#39;John\u0026#39;, age: 25 }; 함수 인자와 반환형\n1 2 3 4 5 let greeting : (name: string) =\u0026gt; string; greeting = function (name: string) { return `Hi ${name}`; }; 위와 같이 타입 어노테이션을 사용할 경우 string을 인자로 받고, string을 반환하는 모든 함수를 greeting에 할당할 수 있다.\nBase Type number\n타입스크립트에서 모든 숫자는 부동 소수점값과 큰 정수값에 포함된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 let price: number; let price = 9.95; let counter: number = 0; let x: number = 100, y: number = 200; let bin = 0b100; let anotherBin: number = 0B010; let octal: number = 0o10; let hexadecimal: number = 0XA; let big: bigint = 9007199254740991n; string\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 let firstName: string = \u0026#39;John\u0026#39;; let title: string = \u0026#34;Web Developer\u0026#34;; let description = `This TypeScript string can span multiple lines `; let firstName: string = `John`; let title: string = `Web Developer`; let profile: string = `I\u0026#39;m ${firstName}. I\u0026#39;m a ${title}`; console.log(profile); /** result I\u0026#39;m John. I\u0026#39;m a Web Developer. **/ boolean\n1 2 3 4 5 let pending: boolean; pending = true; // after a while // .. pending = false; Object Type 타입스크립트 object 타입은 원시 타입이 아닌 모든 값을 표현할 수 있다.\n타입스크립트 원시 타입 number bigint string boolean null undfined symbol object로 선언된 변수는 원시 타입을 제외한 모든 자료형을 할당할 수 있다.\n1 2 3 4 5 6 7 8 9 10 let employee: object; employee = { firstName: \u0026#39;John\u0026#39;, lastName: \u0026#39;Doe\u0026#39;, age: 25, jobTitle: \u0026#39;Web Developer\u0026#39; }; console.log(employee); 1 2 3 4 5 6 { firstName: \u0026#39;John\u0026#39;, lastName: \u0026#39;Doe\u0026#39;, age: 25, jobTitle: \u0026#39;Web Developer\u0026#39; } 만약 object로 선언된 변수를 다른 원시 타입으로 다시 할당하려고 하면 에러가 발생한다.\n1 2 employee = \u0026#34;Jane\u0026#34;; // error TS2322: Type \u0026#39;\u0026#34;Jane\u0026#34;\u0026#39; is not assignable to type \u0026#39;object\u0026#39;. object 타입은 선언될 때 지정한 프로퍼티 목록을 고정적으로 가지게 되어, 선언되지 않은 프로퍼티를 호출하게 되면 에러를 발생시킨다.\n1 2 console.log(employee.hireDate); // error TS2339: Property \u0026#39;hireDate\u0026#39; does not exist on type \u0026#39;object\u0026#39;. 💡 자바스크립트는 보유하지 않은 프로퍼티를 호출하면 undefined를 반환한다.\nobject 타입 내부에 지정되는 프로퍼티들의 타입을 지정할 수 도 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 let employee: { firstName: string; lastName: string; age: number; jobTitle: string; }; employee = { firstName: \u0026#39;John\u0026#39;, lastName: \u0026#39;Doe\u0026#39;, age: 25, jobTitle: \u0026#39;Web Developer\u0026#39; }; 1 2 3 4 5 6 7 8 9 10 11 let employee: { firstName: string; lastName: string; age: number; jobTitle: string; } = { firstName: \u0026#39;John\u0026#39;, lastName: \u0026#39;Doe\u0026#39;, age: 25, jobTitle: \u0026#39;Web Developer\u0026#39; }; object VS Object 타입스크립트는 첫 번째 대문자만 다른 Object 타입도 있다.\nobject 타입은 원시 타입이 아닌 모든 값들을 표현하고, Object 타입은 동작하는 모든 객체들을 의미한다.\n💡 자바스크립트에 포함된 모든 생성자들은 Object를 extend한다. 따라서 Object에는 모든 객체가 할당될 수 있다. 즉 동작하는 모든 객체를 의미한다.\nThe empty type 타입스크립트에서 비어있는 타입을 표현하는 다른 방법으로 {} 가 있는데, object 타입과 유사하게 동작한다.\n1 2 3 4 let vacant: {}; vacant.firstName = \u0026#39;John\u0026#39;; //error TS2339: Property \u0026#39;firstName\u0026#39; does not exist on type \u0026#39;{}\u0026#39;. 1 2 3 4 let vacant: {} = {}; console.log(vacant.toString()); // [object Object] 1 2 3 4 5 6 7 8 9 let vacant: {} = { firstName: \u0026#39;John\u0026#39; }; console.log(vacant); // { firstName: \u0026#39;John\u0026#39; } console.log(vacant.firstName); // error TS2339: Property \u0026#39;firstName\u0026#39; does not exist on type \u0026#39;{}\u0026#39;. Array 타입스크립트의 Array는 순서가 있는 데이터 리스트를 의미한다.\n1 let 이름: 타입[]; 1 2 3 4 5 6 let skills: string[]; skills[0] = \u0026#34;Problem Solving\u0026#34;; skills[1] = \u0026#34;Programming\u0026#34;; skills.push(\u0026#39;Software Design\u0026#39;); 1 let skills = [\u0026#39;Problem Sovling\u0026#39;,\u0026#39;Software Design\u0026#39;,\u0026#39;Programming\u0026#39;]; 1 2 let skills: string[]; skills = [\u0026#39;Problem Sovling\u0026#39;,\u0026#39;Software Design\u0026#39;,\u0026#39;Programming\u0026#39;]; 위처럼 선언된 skills에 다른 타입을 넣을 경우 에러가 발생한다\n1 2 skills.push(100); // Argument of type \u0026#39;number\u0026#39; is not assignable to parameter of type \u0026#39;string\u0026#39;. 배열 타입이 가진 속성과 메소드 1 2 let series = [1, 2, 3]; console.log(series.length); // 3 유용한 메서드로 forEach(), map(), reduce(), filter() 등이 있다.\n1 2 3 4 5 let series = [1, 2, 3]; let doubleIt = series.map(e =\u0026gt; e* 2); console.log(doubleIt); [ 2, 4, 6 ] 여러 타입 섞어 저장하기 1 let scores = [\u0026#39;Programming\u0026#39;, 5, \u0026#39;Software Design\u0026#39;, 4]; 1 2 let scores : (string | number)[]; scores = [\u0026#39;Programming\u0026#39;, 5, \u0026#39;Software Design\u0026#39;, 4]; Tuple Tuple은 array에 추가 제약사항이 붙은 형태이다.\n내부 요소 갯수 고정 내부 요소의 타입을 선언할 때 같을 필요는 없음 1 2 let skill: [string, number]; skill = [\u0026#39;Programming\u0026#39;, 5]; 1 2 3 let skill: [string, number]; skill = [5, \u0026#39;Programming\u0026#39;]; // error TS2322: Type \u0026#39;string\u0026#39; is not assignable to type \u0026#39;number\u0026#39;. 1 let color: [number, number, number] = [255, 0, 0]; Optional Tuple Elements 타입스크립트 3.0부터 선택적으로 사용할 수 있는 요소 선언이 추가되었다. 선택적으로 사용할 요소 앞에 ?를 앞에 붙인다.\n1 2 3 let bgColor, headerColor: [number, number, number, number?]; bgColor = [0, 255, 255, 0.5]; headerColor = [0, 255, 255]; Enum Enum은 상수 값들을 그룹으로 묶어 이름 붙인 타입이다.\n1 enum name {constant1, constant2, ...}; constant1, constant2, ...는 enum의 멤버가 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 enum Month { Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec }; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 function isItSummer(month: Month) { let isSummer: boolean; switch (month) { case Month.Jun: case Month.Jul: case Month.Aug: isSummer = true; break; default: isSummer = false; break; } return isSummer; } isItSummer의 인자인 month는 Month enum에 해당하는 값을 받을 수 있다.\n1 2 3 console.log(isItSummer(Month.Dec)); //false console.log(isItSummer(11)); //false console.log(isItSummer(MonthSecond.Dec)); //error TS2345: Argument of type \u0026#39;MonthSecond.Dec\u0026#39; is not assignable to parameter of type \u0026#39;Month\u0026#39;. ","date":"2023-04-18T17:09:29+09:00","image":"https://codemario318.github.io/post/typescript/tutorial/2/ts_cover_hua836966f7fcda4b3d2856eb0c525b4e2_8490_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/typescript/tutorial/2/","title":"TypeScript Tutorial - 2. Basic Types"},{"content":"Setup Tools Node.js: TS 컴파일러 실행을 위해 필요함 TypeScript 컴파일러: TS를 JS로 컴파일해준다. ts-node TS 코드를 실행하고 REPL(read-eval-print loop: 스크립트 실행 환경) 사용할 수 있다. TypeScript 컴파일러 설치 npm을 이용해 Typescript를 전역 환경에 설치 1 npm install -g typescript 설치 완료 후 tsc 가 정상적으로 설치되었는지 확인 1 tsc --v ts-node 를 전역 환경에 설치 1 npm install -g ts-node What is TypeScript 타입스크립트는 자바스크립트 상위집합(superset)이다. 타입스크립트가 자바스크립트의 특성을 침범하지 않고 지원한다. 타입스크립트는 자바스크립트를 기반으로 만들어졌다. 타입스크립트 코드를 작성 후, 타입스크립트 컴파일러를 사용하여 일반 자바스크립트 코드로 컴파일 한다.\n평범한 자바스크립트 코드가 만들어지기 때문에 자바스크립트가 실행 가능한 모든 환경에서 배포할 수 있다. 기본적으로 자바스크립트 문법을 사용하고, 타입 지원을 위한 추가 구문이 있다.\n결과적으로 syntax 에러가 없는 자바스크립트 코드는 타입 스크립트 코드 이다.\n모든 자바스크립트 프로그램이 타입스크립트 프로그램이라는 뜻 기존 자바스크립트 코드 베이스를 타입 스크립트로 마이그레이션하는 경우 유용하다.\nWhy TypeScript 타입스크립트는 버그를 피할 수 있게 도와주기 때문에 생산성을 향상시킨다. 타입은 많은 실수를 피하는 데 도움을 준다. 이를 통해 런타임에 버그가 발생하지 않고 컴파일 시간에 버그를 처리할 수 있다.\n1 2 3 4 5 6 function add(x, y) { return x + y; } let result = add(input1.value, input2.value); console.log(result); // result of concatenating strings add 함수는 x+y를 반환한다. 하지만 input이 html \u0026lt;input\u0026gt;이라면 .value는 값이 숫자인 것과 상관없이 문자열로 결과를 반환하는데 자바스크립트는 문자열 + 연산을 concat으로 처리하므로 add 함수는 의도한 동작을 수행하지 않고 이어진 문자열을 반환하게 된다.\n1 2 3 4 5 function add(x: number, y: number) { return x + y; } let result = add(input1.value, input2.value); 타입스크립트는 인자에 들어올 타입을 정할 수 있어서 input.value가 number타입이 아닐 경우 컴파일 에러를 발생시킨다.\n따라서 실수로 다른 값을 넣었을 때 발생하는 오동작을 예방할 수 있다.\n타입스크립트는 미래의 자바스크립트를 현재로 가져온다.\n타입스크립트는 현재 자바스크립트 엔진에 대해 ES Next에서 계획된 기능을 미리 지원한다. 따라서 웹 브라우저나 다른 환경에서 새로운 기능을 완전히 지원하기 전에 새로운 자바스크립트 기능을 사용할 수 있다. 매년 TC39에서 자바스크립트 표준인 ECMAScript에 대한 새로운 기능을 출시하는데 일반적으로 5단계를 거쳐 완전히 적용된다.\nStage 0: Strawperson(?: Strowman, 허수아비?) Stage 1: Proposal(제안) Stage 2: Draft(초안, 신인 선발) Stage 3: Candidate(후보) Stage 4: Finished() 타입스크립트는 일반적으로 Stage 3부터 지원한다.\n\u0026ldquo;Hello, World!\u0026rdquo; in node.js 1 2 let message: string = \u0026#39;Hello, World!\u0026#34;; console.log(message); tsc 을 이용해 .ts 파일을 컴파일하여 js 파일을 만들 수 있다.\n1 tsc app.ts 1 node app.js ts-node 이용 시 컴파일 하지 않아도 실행 가능\n1 ts-node app.ts in Web Browsers 1 2 3 4 5 6 7 8 9 10 11 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;TypeScript: Hello, World!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script src=\u0026#34;app.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 1 2 3 4 5 6 const message: string = \u0026#39;Hello, World!\u0026#39;; const heading = document.createElement(\u0026#39;h1\u0026#39;); heading.textContent = message; document.body.appendChild(heading); 1 tsc app.ts ","date":"2023-04-18T16:43:29+09:00","image":"https://codemario318.github.io/post/typescript/tutorial/1/1/ts_cover_hua836966f7fcda4b3d2856eb0c525b4e2_8490_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/typescript/tutorial/1/1/","title":"TypeScript Tutorial - 1. Gettig Started(1)"},{"content":"타입 스크립트를 사용해야하는 2가지 중요한 이유가 있다.\nJS의 동적 타이핑으로 발생하는 여러 문제들을 예방할 수 있다. 앞으로 반영될 JS 문법을 당장 사용할 수 있다. (ES Next) JS 동적 타이핑으로 발생하는 문제를 예방하는 코딩은 매우 귀찮기때문에 방치되는 경우가 많다.\nJS 최신 문법은 간편한 기능을 제공하는 경우가 많고, 여러 오류들이 수정되어있다.\nJS 동적 타이핑이란 1 2 3 let box; box = \u0026#34;hello\u0026#34;; box = 100; 1 2 3 4 5 6 7 8 let box; console.log(typeof(box)); // undefined box = \u0026#34;Hello\u0026#34;; console.log(typeof(box)); // string box = 100; console.log(typeof(box)); // number 동적 타이핑은 할당 코드 수행 시점에 해석하여 자료형을 결정한다.\n이로 인해 코드가 간결해지고, 로직을 명확하게 보여줄 수 있지만. 할당 될 때 마다 자료형을 결정하는 과정으로 인해 정적 타이핑보다 상대적으로 느리고, 변수를 여러 유형 값을 사용할 여지를 만들어 코드에 혼란을 준다.\n동적 타이핑의 문제 1 2 3 4 5 6 7 function getProduct(id){ return { id: id, name: `Awesome Gadget ${id}`, price: 99.5 } } 1 2 3 4 5 const product = getProduct(1); console.log(`The product ${product.Name} costs $${product.price}`); // result // The product undefined costs $99.5 오타로 인해 undefined가 출력되었다. 1 2 3 const showProduct = (name, price) =\u0026gt; { console.log(`The product ${name} costs ${price}$.`); }; 1 2 3 4 5 const product = getProduct(1); showProduct(product.price, product.name); // result // The product 99.5 costs $Awesome Gadget 1 인자 순서 실수로 인해 다른 의도로 출력되었다. 타입스크립트는 어떤 방식으로 동적 타이핑 문제를 해결했을까? interface 1 2 3 4 5 interface Product{ id: number, name: string, price: number }; interface를 통해 Product객체의 형태를 정의했다.\n1 2 3 4 5 6 7 function getProduct(id) : Product{ return { id: id, name: `Awesome Gadget ${id}`, price: 99.5 } } getProduct함수를 Product타입을 반환하도록 선언할 수 있다.\n1 2 const product = getProduct(1); console.log(`The product ${product.Name} costs $${product.price}`); 함수에 반환 인터페이스를 설정하여 입력이 미리 정의한 타입이 아닐 경우 코드 에디터에서 잘못된 코드임을 표시한다.\n실수를 빠르게 알아차릴 수 있다. 1 2 3 const showProduct = (name: string, price:number) =\u0026gt; { console.log(`The product ${name} costs ${price}$.`); }; 인자로 받을 타입도 지정할 수 있다. 1 2 const product = getProduct(1); showProduct(product.price, product.name); 결론 자바스크립트는 동적 타이핑을 지원하여 유연하지만 그로 인해 많은 문제들이 발생한다. 타입스크립트는 타입을 통해 동적 타이핑에서 발생하는 문제들을 예방한다. ","date":"2023-04-18T16:43:29+09:00","image":"https://codemario318.github.io/post/typescript/tutorial/1/2/ts_cover_hua836966f7fcda4b3d2856eb0c525b4e2_8490_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/typescript/tutorial/1/2/","title":"TypeScript Tutorial - 1.2 Why use TypeScript"},{"content":"\nStop-the-world GC를 실행하기 위해 JVM이 애플리케이션 실행을 멈추는 것이다.\nstop-the-world가 발생하면 GC를 실행하는 쓰레드를 제외한 나머지 쓰레드는 모두 작업을 멈춘다. GC작업을 완료한 이후에야 중단했던 작업을 다시 시작한다. 어떤 GC 알고리즘을 사용하더라도 Stop-the-world는 발생한다. 대개의 경우 GC 튜닝이란 이 Stop-the-world 시간을 줄이는 것이다. GC 필요성 Java의 경우 (대부분의 언어) 프로그램 코드에서 메모리를 명시적으로 지정하여 헤제하지 않는다. 따라서 가비지 컬렉터가 더이상 필요 없는 객체를 찾아 지우는 작업을 수행하여 메모리 공간을 확보한다.\n가끔 명시적으로 해제하려고 해당 객체를 null로 지정하거나 System.gc() 메서드를 호출하는 경우가 있다. null로 지정하는 것은 큰 문제가 안되지만, System.gc() 메서드를 호출하는 것은 시스템의 매우 큰 영향을 끼치므로 System.gc() 메서드는 절대 사용하면 안된다.\nGarbage Collector 가비지 컬렉터는 두 가지 가설 하에 만들어졌다.(가정 또는 전제 조건이라 표현하는 것이 맞다.)\nWeak generational hypothesis 대부분의 객체는 금방 접근 불가능 상태(Unreachable)가 된다. 오래된 객체에서 젊은 객체로의 참조는 아주 적게 존재한다. Young 영역과 Old 영역 이 가설의 장점을 최대한 살리기 위해 HotSpot VM 에서는 크게 2개로 물리적 공간을 나누었다.\nYoung 영역\n새롭게 생성한 객체의 대부분이 여기에 위치한다. 대부분의 객체가 금방 접근 불가능 상태가 되기 때문에 매우 많은 객체가 young 영역에 생성되었다가 사라진다. 이 영역에서 객체가 사라질 때 Minor GC가 발생한다고 말한다.\nOld 영역\n접근 불가능 상태로 되지 않아 Young 영역에서 살아남은 객체가 여기로 복사된다. 대부분 Young 영역보다 크게 할당하며, 크기가 큰 만큼 Young 영역보다 GC는 적게 발생한다. 이 영역에서 객체가 사라질 때 Major GC(Full GC)가 발생한다고 말한다.\nMinor GC 새로 생성된 대부분의 객체는 Eden 영역에 위치한다. Eden 영역에서 GC가 한 번 발생한 후 살아남은 객체는 Survivor 영역 중 하나로 이동된다.\n이 과정을 반복하다가 계속해서 살아남아 있는 객체는 일정시간 참조되고 있다는 뜻이므로 Old 영역으로 이동시킨다.\nMajor GC Old 영역에 있는 모든 객체들을 검사하여 참조되지 않은 객체들을 한꺼번에 삭제한다.\n가비지 컬렉션은 어떤 원리로 소멸시킬 대상을 선정하는가? 알고리즘에 따라 동작 방식이 매우 다양하지만 공통적인 원리가 있다.\n가비지 컬렉터는 힙 내의 객체 중에서 가비지를 찾아내고 찾아낸 가비지를 처리해서 힙의 메모리를 회수한다.\n참조되고 있지 않은 객체를 가비지라고 하며 객체가 가비지인지 아닌지 판단하기 위해서 reachablitiy라는 개념을 사용한다.\n어떤 힙 영역에 할당된 객체가 유효한 참조가 있으면 reachability, 없다면 unreachability로 판단한다. 하나의 객체는 다른 객체를 참조하고, 다른 객체는 또 다른 객체를 참조할 수 있기 때문에 참조 사슬이 형성되는데, 이 참조 사슬 중 최초에 참조한 것을 Root Set이라고 칭한다.\n힙 영역에 있는 객체들은 총 4가지 경우에 대하여 참조를 가지게 된다.\n힘 내의 다른 객체에 의한 참조 Java스택, 즉 Java 메서드 실행 시에 사용하는 지역변수와 파라미터들에 의한 참조 네이티브 스택에 의해 생성된 객체에 대한 참조 메서드 영역의 정적 변수에 의한 참조 2, 3, 4는 Root set이다. 즉 참조 사슬 중 최초에 참조한 것이다.\n인스턴스가 가비지 컬렉션의 대상이 되었다고 해서 바로 소멸이 되는 것은 아니다. 빈번한 가비지 컬렉션의 실행은 시스템에 부담이 될 수 있기에 성능에 영향을 미치지 않도록 가비지 컬렉션 실행 타이밍은 별도의 알고리즘을 기반으로 계산이 되며, 이 계산 결과를 기반으로 가비지 컬렉션이 수행된다.\nSerial GC 적은 메모리와 CPU 코어 개수가 적을 때 적합한 방식으로 위에서 언급한 방식으로 동작\nParallel GC 기본적인 GC 알고리즘은 Serial GC와 동일하지만 Parallel GC는 GC를 처리하는 스레드가 여러 개라서 보다 빠른 GC를 수행할 수 있다.\n메모리가 충분하고 코어의 개수가 많을 때 유리하다.\nParallel Old GC (Parallel Compacting GC) 별도로 살아있는 객체를 식별한다는 부분에서 보다 복잡한 단계로 수행된다.\n","date":"2023-04-18T16:24:22+09:00","image":"https://codemario318.github.io/post/jvm_gc/jvm_cover_hu0ae05cc4d1c0ca93a8c2f5fc57548620_55838_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/jvm_gc/","title":"JVM - Garbage Collection"},{"content":"JVM 이란? JVM이란 Java Virtual Machine, 자바 가상 머신의 약자를 따서 줄여 부르는 용어이다. JVM의 역할은 자바 애플리케이션을 클래스 로더를 통해 읽어 들여 자바 API와 함께 실행하는 것이다.\nJVM은 Java와 OS 사이에서 중개자 역할을 수행하여 JAVA가 OS에 구애받지 않고 재사용을 가능하게 해준다. 메모리 관리, Garbage collction을 수행한다. 스택기반의 가상머신이다. ARM 아키텍쳐 같은 하드웨어는 레지스터 기반으로 동작하는데 비해 JVM은 스택 기반으로 동작한다.\n자바프로그램 실행과정 프로그램이 실행되면 JVM은 OS로 부터 프로그램이 필요로 하는 메모리를 할당받는다. JVM은 이 메모리를 용도에 따라 여러 영역으로 나누어 관리한다. 자바 컴파일러(javac)가 자바 소스코드(.java)를 읽어들여 자바 바이트 코드(.class)로 변환시킨다. Class Loader를 통해 class 파일들을 JVM으로 로딩한다. 로딩된 class 파일들은 Execution engine을 통해 해석된다. 해석된 바이트 코드는 Runtime Data Areas에 배치되어 실질적인 수행이 이루어지게 된다. 이러한 실행 과정속에서 JVM은 필요에 따라 Thread Synchronizeation과 GC같은 관리작업을 수행한다.\nJVM 구성 클래스 로더 Class Loader JVM 내로 클래스 파일을 로드하고, 링크를 통해 배치하는 작업을 수행하는 모듈이다. Runtime시에 동적으로 클래스를 로드한다. jar파일 내 저장된 크래스들을 JVM위에 탑재하고 사용하지 않는 클래스들은 메모리에서 삭제한다. (컴파일러 역할)\n자바는 동적코드, 컴파일 타임이 아니라 런타임에 참조한다. 즉, 클래스를 처음으로 참조할 때, 해당 클래스를 로드하고 링크한다. 그 역할을 클래스 로더가 수행한다.\n실행 엔진 Execution Engine 클래스를 실행시킨다. 클래스 로더가 JVM내의 런타임 데이터 영역에 바이트 코드를 배치하면 실행엔진에 의해 실행된다. 자바 바이트코드는 기계가 바로 수행할 수 있는 언어보다는 비교적 인간이 보기 편한 형태로 기술된 것이다.\n그래서 실행엔진은 이와 같은 바이트코드를 실제로 JVM내부에서 기계가 실행할 수 있는 형태로 변경한다. 이 때 두가지 방식을 사용하게 된다.\n인터프리터 Interpreter\n실행 엔진은 자바 바이트 코드를 명령어 단위로 읽어서 실행한다.\n인터프리터 언어의 단점을 그대로 갖고 있다. 한 줄 씩 수행하기 때문에 느리다. JIT Just-In-Time\n인터프리터 방식의 단점을 보완하기 위해 도입된 JIT 컴파일러이다. 인터프리터 방식으로 실행하다가 적절한 시점에 바이트 코드 전체를 컴파일하여 네이트브 코드로 변경하고, 이후에는 해당 코드를 더이상 인터프리팅 하지 않고 네이티브 코드로 직접 실행한다.\n네이티브 코드는 캐시에 보관하기 때문에 한 번 컴파일 된 코드는 빠르게 수행하게 된다. JIT 컴파일러가 컴파일 하는 과정은 바이트코드를 인터프리팅하는 것보다 훨씬 오래걸리므로 한 번만 실행되는 코드라면 컴파일하지 않고 인터프리팅 하는 것이 유리하다. JIT 컴파일러를 사용하는 JVM들은 내부적으로 해당 메서드가 얼마나 자주 수행되는지 체크하고, 일정 정도를 넘을 때에만 컴파일을 수행한다. Garbage Collector GC를 수행하는 모듈 (쓰레드)를 가진다.\nRuntime Data Area 프로그램을 수행하기 위해 OS에서 할당받은 메모리 공간\nPC Register Thread가 시작될 때 생성되며 생성될 때마다 생성되는 공간으로 스레드마다 하나씩 존재한다.\n쓰레드가 어떤 부분을 어떤 명령으로 실행해야할 지에 대한 기록을 하는 부분으로 현재 수행중인 JVM 명령의 주소를 갖는다.\nJVM 스택 영역 프로그램 실행과정에서 임시로 할당되었다가 메소드를 빠져나가면 바로 소멸되는 특성의 데이터를 저장하기 위한 영역이다.\n각종 형태의 변수나 임시 데이터, 스레드나 메소드의 정보를 저장한다. 메소드 호출 시마다 각각의 스택 프레임(그 메서드 만을 위한 공간)이 생성된다. 메서드 수행이 끝나면 프레임 별로 삭제를 한다. 메소드 안에서 사용되는 값들(local variable)을 저장한다. 호출된 메소드의 매개변수, 지역변수, 리턴 값 및 연산 시 일어나는 값들을 임시로 저장한다. Native method stack 자바 프로그램이 컴파일 되어 생성되는 바이트 코드가 아닌 실제 실행할 수 있는 기계어로 작성된 프로그램을 실행시키는 영역이다.\n자바가 아닌 다른 언어로 작성된 코드를 위한 공간이다. Java Native Interface를 통해 바이트 코드로 전환하여 저장하게 된다. 일반 프로그램처럼 커널이 스택을 잡아 독자적으로 프로그램을 실행시키는 영역이다. 이 부분을 통해 C code를 실행시켜 Kernel에 접근할 수 있다. Method Area (= Class area = Static area) 클래스 정보를 처음 메모리 공간에 올릴 때 초기화되는 대상을 저장하기 위한 메모리 공간.\n올라가게 되는 메소드의 바이트코드는 프로그램의 흐름을 구성하는 바이트 코드이다. 자바 프로그램은 메인 메소드의 호출에서 부터 계속된 메소드의 호출로 흐름을 이어가기 때문이다. 대부분 인스턴스의 생성도 메소드 내에서 명령하고 호출한다. 사실상 컴파일 된 파이트코드의 대부분이 메소드 바이트코드이기 때무넹 거의 모든 바이트코드가 올라간다고 봐도 상관없다. Runtime Constat Pool이라는 별도의 관리 영역도 함께 존재하여, 상수 자료형을 저장하여 참조하고 중복을 막는 역할을 수행한다. 올라가는 정보의 종류 Feild Information 멤버 변수의 이름 데이터 타입 접근 제어자에 대한 정보 Method Information 메소드의 이름, 리턴타입, 매개변수, 접근 제어자에 대한 정보 Type Information Class인지 interface인지의 여부 저장 Type의 속성 전체 이름 Super class의 전체 이름 (interface이거나 object인 경우 제외) Heap 객체를 저장하는 가상 메모리 공간\n생성된 객체와 배열을 저장한다. class area영역에 올라온 클래스들만 객체로 생성할 수 있다. Permanent Generation 생성된 객체들의 정보 주소값이 저장된 공간이다. class loader에 의해 load되는 class, method 등에 대한 meta 정보가 저장되는 영역이고, JVM에 의해 사용된다.\nReflection을 사용하여 동적으로 클래스가 로딩되는 경우에 사용된다. 내부적으로 Reflection 기능을 자주 사용하는 Spring Framework를 이용할 경우 이 영역에 대한 고려가 필요하다.\nNew/Young 영역 Eden 객체들이 최초로 생성되는 공간 Survivor 0 / 1 Eden에서 참조되는 객체들이 저장되는 공간 Old 영역 New area에서 일정 시간 참조되고 있는, 살아남은 객체들이 저장되는 공간\nEden 영역에 객체가 가득차게 되면 첫번째 GC(minor GC)가 발생한다. Eden 영역에 있는 값들을 Survivor 1 영역에 복사하고 이 영역을 제외한 나머지 영역의 객체를 삭제한다.\n인스턴스는 소멸 방법과 소멸 시점이 지역 변수와는 다리기에 힙이라는 별도의 영역에 할당된다. 자바 가상머신은 매우 합리적으로 인스턴스를 소멸시킨다. 더이상 인스턴스의 존재 이유가 없을 때 소멸시킨다.\n","date":"2023-04-18T16:07:19+09:00","image":"https://codemario318.github.io/post/jvm/jvm_cover_hu0ae05cc4d1c0ca93a8c2f5fc57548620_55838_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/jvm/","title":"JVM"},{"content":"제너레이터는 제너레이터 함수가 호출될 때 반환되는 이터러블 객체이다. 제네레이터 함수는 일반적인 함수와 비슷하게 생겼지만 yield 구문을 사용해 데이터를 원하는 시점에 반환하고 처리를 다시 시작할 수 있다. 일반적인 함수는 진입점이 하나라면 제네레이터는 진입점이 여러개라고 생각할 수 있다. 이러한 특성때문에 제네레이터를 사용하면 원하는 시점에 원하는 데이터를 받을 수 있게된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def generator(): yield 1 yield \u0026#39;string\u0026#39; yield True gen = generator() print(gen) # \u0026lt;generator object generator at 0x10a47c678\u0026gt; next(gen) #1 next(gen) # \u0026#39;string\u0026#39; next(gen) # True next(gen) \u0026#39;\u0026#39;\u0026#39; Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; StopIteration \u0026#39;\u0026#39;\u0026#39; 동작 yield 문이 포함된 함수를 실행하면 제너레이터 객체가 반환되는데 이 때는 함수의 내용이 실행되지 않는다. next() 라는 빌트인 메서드를 통해 제네레이터를 실행시킬 수 있으며 next() 메서드 내부적으로 iterator 를 인자로 받아 이터레이터의 __next__() 메서드를 실행시킨다. 처음 __next__() 를 호출하면 함수의 내용을 실행하다 yield 문을 만났을 때 처리를 중단한다. 이 때 모든 local state는 유지되는데 변수의 상태, 명령어 포인터, 내부 스택, 예외 처리 상태를 포함한다. 그 후 제어권을 상위 컨텍스트로 양보(yield)하고 또 __next__() 가 호출되면 제네레이터는 중단된 시점부터 다시 시작한다. yield 문의 값은 어떤 메서드를 통해 제네레이터가 다시 동작했는지에 따라 다른데, __next__() 를 사용하면 None이고 send() 를 사용하면 메서드로 전달 된 값을 갖게되어 외부에서 데이터를 입력받을 수 있게 된다.\n장점 List, Set, Dict 표현식은 iterable 하기에 for 표현식 등에서 유용하게 쓰일 수 있다. 하지만 해당 객체들은 Collection 특성상 가진 데이터를 메모리에 담고 있어야 하기 때문에 큰 값을 다룰 때는 성능상 불리하다. 제너레이터는 yield 를 통해 필요한 값만 받아 쓰기 때문에 모든 값을 메모리에 들고 있을 필요가 없게 된다.\n1 2 3 4 5 6 7 import sys a = [i for i in range(100000)] sys.getsizeof(a) #824464 b = (i for i in range(100000)) sys.getsizeof(b) #88 리스트가 여러번 사용될 수 있는 반면 b 제네레이터는 한번 사용된 후 소진된다. 이는 모든 이터레이터가 마찬가지인데 List, Set 등은 이터러블하지만 이터레이터는 아니기에 소진되지 않는다.\n1 2 len(list(b)) # 100000 len(list(b)) # 0 while True 구분으로 제공받을 데이터가 무한하거나, 모든 값을 한번에 계산하기엔 시간이 많이 소요되어 그때 그때 필요한 만큼만 받아 계산하고 싶을 때 제네레이터를 활용할 수 있다.\n","date":"2023-04-18T14:31:00+09:00","image":"https://codemario318.github.io/post/python_cover/python_cover_hu071c6006b6148c050030e26fb108bd62_83564_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/python_cover/","title":"Python - Generator"},{"content":"\n엘라스틱서치는 검색 엔진인 아파치 루씬 (Apache Lucene)으로 구현한 RESTful API 기반의 검색 엔진이다. 엘라스틱서치 아키텍쳐는 클러스터 기반으로 구성되어 있다.\n클러스터 기본 특징 수평 확장\n클러스터를 사실상 무한으로 확장할 수 있다.\n인덱스 샤딩\n엘라스틱서치는 인덱스를 조각내서 \u0026ldquo;샤드 (shard)\u0026ldquo;라는 조각난 데이터로 만든다. 때문에 나누어진 데이터를 편하게 각 호스트에 분리해서 보관할 수 있다.\n엘라스틱서치 특징 Schemaless와 문서지향 엘라스틱 서치는 JSON 구조를 사용하여 기존 RDBMS와 같은 엄격한 구조를 적용하지 않는다.\n스키마가 다이나믹하게 변할 수 있다\n전통적인 관계형 구조로 구성할 경우 프로젝트 막바지에 모든 스키마르 ㄹ변경하고, 데이터를 다시 구성하는 문제에 봉착할 수 있는데 JSON 구조는 이런 문제들을 사전에 막을 수 있다. 데이터 공간을 절약할 수 있다.\n컬럼을 동적으로 정의하여, 필요한 데이터만 넣게 되어 데이터 공간 및 CPU 사용량, 네트워크 트래픽도 줄일 수 이쓴 효과를 볼 수 있다. 검색능력(Searching) 기본적인 검색 기능 뿐만 아니라 특히 Full-text 검색 능력이라는 강력한 기능을 탑재하고 있다.\n관계형 데이터베이스의 문자열 컬럼에 대해 실행되는 단순한 SQL 질의와는 다르다.\n기본적으로 엘라스틱은 검색을 할 수 있는 Term(의미의 최소한위)로 단어의 형태소 분석을 수행하고, 이 단어들과 연관된 문서를 매핑하여 검색을 시켜주는 개념으로 문서를 통쨰로 like 검색하는 DB와는 구조적으로 다르다.\n분석(Analytics) 엘라스틱 서치를 탑재하여 만든 사이트에 접속하는 사람들의 OS가 무엇인지, 어느나라에서 접속했는지 등을 알고 싶을 때 분석 기능을 사용하면 편리하게 알 수 있다.\n풍부한 API와 REST 지원 기본적으로 20개의 프로그래밍 언어를 지원하며, 기본적으로 REST API를 제공하여 REST API를 사용하는 모든 언어에서 HTTP 형식으로 사용할 수 있다.\n쉬운 작동, 쉬운 확장 Single Node Instance로 작동하며, 수백개의 스케일 아웃을 쉽게 할 수 있다. 대부분의 빅데이터 플랫폼들이 그러하듯 Horizontal Scaling을 사용한다.\nNear real-time(근접 실시간) 검색엔진은 기본적으로 형태소를 분석하고 색인을 해야 하는 시간이 다른 DBMS보다 오래 걸린다. 엘라스틱 역시 데이터를 삽입한 순간 약 몇 초 정도는 이 단계를 거친 후 검색을 할 수 있다.\nLightning-fast (빠른 속도) 엘라스틱 서치의 DNA는 루씬이기 떄문에 단어 입력후 문서를 찾는 속도가 다른 NoSQL들에 비해 매우 빠르다.\nFault-tolerant(내고장성) 노드 실패시 replicate된 다른 노드에서 데이터를 가져오며, 네트워크 실패 시 다른 마스터 복제본으로 선택한다.\n엘라스틱서치 데이터 구조 엘라스틱서치는 위와 같이 문서를 엘라스틱 인덱스로 만든 뒤, 샤드로 분리하여 보관한다.\n샤드는 논리적/물리적으로 분할 된 인덱스인데, 각각의 엘라스틱서치 샤드는 루씬 인덱스이기도 하다.\n루씬은 새로운 문서를 엘라스틱서치 인덱스에 저장할 때 \u0026ldquo;세그먼트\u0026quot;를 생성하는데, 루씬의 인덱스 조각인 이 세그먼트를 조합해 저장한 데이터의 검색을 할 수 있다.\n색인 처리량이 매우 중요할 때는 세그먼트를 더 생성하기도 한다. 루씬은 순차적으로 세그먼트를 검색하므로 세그먼트 수가 많아지면 검색속도도 따라서 느려지게 된다.\n엘라스틱서치 데이터 설명 인덱스(색인) 데이터를 저장 및 색인 하는 곳으로, 관계형 DB의 데이터베이스 개념과 유사하다.\n실제로는 각 샤드를 가리키고 있는 논리적인 네임스페이스 Shard 샤드는 엘라스틱서치에서 사용하는 검색 엔진인 루씬의 인스턴스.\n인덱스를 한 개의 샤드로 구성할 수도 있지만, 인덱스 사이즈가 증가할 경우 여러개의 물리서버에 나누어 보관하기 위해 보통은 여러개의 샤드로 구성함. Segment 각 샤드는 다수의 세그먼트를 가지고 있고, 샤드에서 검색 시, 먼저 각 세그먼트를 검색하여 결과를 조합한 최종 결과를 해당 샤드의 결과로 리턴하게 된다.\nsearchable\n엘라스틱서치에 데이터(문서)를 저장하면, 엘라스틱서치는 이것을 메모리에 모아두고 새로운 세그먼트를 디스크에 기록하여 검색을 리프레시함. 이로 인해 새로운 검색 가능한 세그먼트가 만들어지게 된다.\ncommited\n그러나 세그먼트가 fsync되지 않았으므로 여전히 데이터 손실의 위험이 남아있다. 그래서 엘라스틱서치는 세그먼트를 fsync하는 \u0026ldquo;flush\u0026quot;를 주기적으로 진행하고, 불필요한 트랜젝션 로그를 비운다.\nmerge process\n세그먼트는 불변임, 데이터(document)가 업데이트되면 실제로는 그저 삭제되었다고 마크하고 새로운 데이터(document)를 가리킬 뿐이다. 이러한 오래된 삭제된 데이터를 지우는 것\n엘라스틱서치 클러스터 구조 위 다이어그램은 3개의 엘라스틱서치 인스턴스 환경에서, 4개의 샤드를 2개의 복제본으로 구성했을 때의 구조이다.\n엘라스틱서치는 클러스터 구조로 구성되어 있으며 샤드와 복제본의 수를 설정해두면 스스로 각 노드에 샤드를 분배하여 장애발생 시 데이터 손실을 최소화한다.\n프라이머리 샤드가 손실되었을 경우에는 레플리카를 프라이머리로 승격시켜 데이터 손실을 방지한다.\n","date":"2023-04-18T14:13:19+09:00","image":"https://codemario318.github.io/post/elasticsearch/elasticsearch_cover_hu19466caf2459bbcd9b4d95bc6d53e495_16781_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/elasticsearch/","title":"Elasticsearch"},{"content":" 검색 엔진은 컴퓨터 시스템에 저장된 정보를 찾아주는 것을 도와주도록 설계된 정보 검색 시스템이다. 검색 엔진을 사용하여정보를 찾는데 필요한 시간을 최소화할 수 있다.\n웹 검색 엔진 웹 사이트를 검색하기 위한 프로그램이다.\nFTP 서버나 웹 사이트의 파일 검색 결과를 포함하며, 이미지나 토렌트 파일 또는 특정 부류의 웹 사이트에 특화된 웹 검색 엔진도 있다.\n서버에서는 \u0026lsquo;로봇\u0026rsquo;이라 불리는 특별한 프로그램을 이용하 웹 사이트들을 돌아다니며 웹 사이트들에 대한 정보를 미리 자동적으로 수집한다. 이휴 검색 엔진 사이트에서 특정 검색어를 입력하면 검색 엔진이 수집한 정보 중 그에 맞는 결과를 볼 수 있다.\n로봇이 참조와 어휘를 분헉하는 방식에 따라 검색 품질이 결정됨 원리 검색 엔진은 사용자가 검색 엔진을 사용하기 전에 미리 웹 상에서 정보를 수집하여 색인을 만들어 놓는다. 그리고 나서 사용자가 찾고자 하는 정보의 키워드를 입력하면, 미리 만들어 놓은 색인 중에서 입력된 키워드에 해당하는 정보들을 찾아서 보여준다.\n문서 수집 현재 대부분의 검색 엔진에서는 엡상의 방대한 정보들을 검색하고 색인화 하는 과정을 크롤러라고 부르는 정보 수집 프로그램을 사용하여 수행하고 있다.\n크롤러가 주기적으로 웹에 접속된 사이트들을 방문하여 해당 웹 사이트가 가지고 있는 정보에 대해 색인을 작성한 후 그것을 데이터베이스에 저장하여 검색시 활용하게된다.\n크롤러\n웹상의 문서나 이미지, 영상 등을 주기적으로 검색하고 취합하여, 자동으로 데이터베이스화 시키는 프로그램으로 봇(Bot)이라고도 부른다.\n검색 엔진의 종류 수집한 정보를 색인하는 방법에 따라 구분된다.\n로봇 검색 엔진 크롤라라고 불리는 로봇을 이용하여 웹상의 데이터를 효율적으로 수집하고, 이렇게 수집한 데이터 키워드 색인을 통해 사용자에게 제공하는 검색 엔진\nGoogle, Naver등 현재 사용되는 대부분의 검색 엔진이 이 방식을 채택하고 있다. 디렉토리 검색 엔진 주제 분류에 의한 검색을 제공하는 검색 엔진이며, 데이터의 분류를 사람이 직접 슈행해야 한다.\n현재 주류인 방식은 아니며, 1990년대 Yahoo등에서 사용되었음 메타 검색 엔진 자체적으로 정보를 보유하고 있지 않으면서 사용자가 입력한 키워드를 복수의 다른 검색 엔진으로 전송하여 결과를 얻고, 그 결과들을 종합하여 표시만 해주는 검색 엔진\n여러 검색 엔진의 결과를 동시에 보여주기 때문에 결과를 한눈에 살펴보기에는 편하지만, 메타 검색이라는 과정을 한 번 더 거쳐야 하므로 속도가 느를 수 있다.\n검색 엔진 최적화(Search Engine Optimization, SEO) 검색 결과의 상위에 자신의 웹 페이지가 노출되기 위해 검색 엔진이 자료를 수집하고 결과를 산출하는 방식에 맞춰 웹 페이지의 구성을 조정하는 것을 의미한다.\n각각의 검색 엔진에 맞처 웹 페이지 내의 키워드나 링크 등을 최적화 하는 작업을 SEO라고 한다.\n","date":"2023-04-18T14:04:15+09:00","image":"https://codemario318.github.io/post/search_engine/search_engine_cover_hu920de5c22e59a77d3210239e6515a52e_9451_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/search_engine/","title":"검색 엔진"},{"content":"용어정리 MOM (Message Oriented Middleware, 메시지 지향 미들웨어) 독립된 어플리케이션 간에 데이터를 주고받을 수 있도록 하는 시스템 디자인 함수 호출, 공유메모리 등이 방식이 아닌, 메시지 교환을 이용하는 중간 계층에 대한 인프라 아키텍쳐 개념 분산 컴퓨팅이 가능해지며, 서비스간의 결합성이 낮아짐 지동기로 메시지를 전달하는 것이 특징 Queue, Broadcast, Multicast 등의 방식으로 메시지 전달 Pub/Sub 구조 메시지를 발행하는 Publisher(Producer), 메시지를 소비하는 Subscribe(Consumer)로 구성 Message Broker 메시지처리 또는 메시지 수신자에게 메시지를 전달하는 시스템이며, 일반적으로 MOM 기반으로 구축됨 MQ (Message Queue, 메시지 큐) Message bBroker와 MOM을 구현한 소프트웨어 (RabbitMQ, ActiveMQ, Kafka 등) MOM은 메시지 전송 보장을 해야하므로 AMQP를 구현함 AMQP (Advanced Message Queueing Protocol) 메시지를 안정적으로 주고박기 위한 인터넷 프로토콜 RabbitMQ, Kafka 등은 AMQP를 구현한 MOM 시스템이다.\n메시징 시스템이란? 로그 데이터, 이벤트 메시지 등 API로 호출할 떄 보내는 데이터들을 처리하는 시스템\n예를 들어, 다음과 같이 자동 메일을 발송 시스템이 있다고 가정하면,\n회원가입을 했을 때, 이메일을 발송하는 MemberService 주문완료가 되었을 때, 이메일을 발송하는 OrderService 메일을 실제 발송하는 MailService 이렇게 서비스가 분리되었을 때 프로세스는 다음과 같이 처리될 수 있다.\nMemberService에서 회원가입, OrderService에서 주문완료 이벤트가 발생 Messaging Client로 메일 전송에 필요한 데이터( 받는/보내는 사람 이메일 주소, 메시지 제목/내용 등.. )를 API 호출 Messaging Client에서 MOM을 구현한 소프트웨어(ex. kafka)로 메시지를 생산 MailService에서 메시지가 존재하는지 구독하고 있다가 메시지가 존재하면 메시지를 소비 MailService에서 API 정보들을 통해 User에게 메일 발송 장점 서비스간의 결합성이 낮아지므로 각자의 비즈니스 로직에만 집중할 수 있다. 메시지 처리 방식은 Message Broker에 위임 각 서비스는 Client를 통해 메시지를 보내고 받기만 하면 됨 각 서비스는 비동기 방식으로 메시지를 보내기만 하면, Message Broker에서 순서 보장, 메시지 전송 보장 등을 처리 메시징 시스템이 잠깐 다운되어도 각 서비스에는 직접적인 영향을 미치지 않음 단점 Message Broker 구축, 예를 들면 kafka 클러스터 구축에 필요한 금전, 인적 자원에 대한 비용 비동기의 양면성 - 정말 메시지가 잘 전달되었는가? 함수호출, 공유 메모리 사용 방식보다 메시징 시스템을 사용했을 때 호출 구간이 늘어나므로 네트워크 비용 발생 MOM, 메세지 지향 미들웨어(Message-Oriented-Middleware) 미들웨어: 어플리케이션들을 연결해 이들이 서로 데이터를 교환할 수 있게 해주는 소프트웨어 메시지 지향(=메시징 시스템): 메시지 API를 통해 각 분산되어 있는 어플리케이션간의 다리 역할을 함으로써 데이터를 교환 할 수 있도록 하는 시스템 메시지 지향 미들웨어란? 메시지를 통해 여러 분산되어 있는 시스템 간의 Connector 역할로 결합성을 낮추고, 이들이 서로 실시간 비동기식 데이터를 교환할 수 있도록 하는 소프트웨어\nMessage Queue 기반 패턴 메시지 대기열 패턴은 일종의 지점 간(peer to peer) 메시징 시스템이다. Queue 대기열의 메시지는 Consumer가 소비하면 지워지는 형태\n소비하면 지워지는 형태라는 의미는 Producer 서버가 메시지를 Queue에 보내고 서버가 다운이 되도 Consumer가 소비하지 않았다면 Queue 대기열에 데이터가 존재한다는 걸 의미한다.\n발행(Publish)-구독(Subscribe) 메시지 패턴 메시지 큐와 마찬가지로 메시지를 생산하는 Producer와 메시지를 소비하는 Consumer로 구성되어 있다.\n차이점은 여러 소비자가 하나의 주제에서 각 메시지를 수신할 수 있다는 점. 또한 모든 Consumer가 메시지를 사용하는 경우에만 메시지가 대기열에서 지워진다.\nkafka와 같은 메시징 시스템에는 메시지가 대기열에 있어야 하는 기간을 지정한 보존 정책이 있다. 따라서 메시지는 모든 Consumer가 소비하더라도 지정된 기간 동안 대기열에 사용할 수 있다.\n언제 쓰이는가? 분산 시스템 여러 컴퓨터를 분산시켜 네트워크를 연결하여 데이터들을 나눠서 처리하면 서버의 과부하를 분산할 수 있으며, 성는개선과 장애요소를 최소화하기 위해 분산 시스템을 사용함 과거 분산 시스템의 단점과 웹 API 통신의 한계 과거 분산시스템의 단점 수많은 데이터를 처리하기 위하여 분산 시스템을 운영하였지만, 시스템이 거대해질수록 분산 시스템의 설계도의 복잡성 문제가 발생한다. 하나의 응용프로그램이 변경되면, 다른 응용프로그램에도 영향을 미쳐 분산 시스템 간의 결합도가 강한 단점을 가지고 있었다.\n웹 API 통신의 특성 MSA를 사용한 분산 시스템은 웹 API 서버로 요청 시 응답을 기다려야 한다. 여러 분산되어있는 서비스 간에는 실시간으로 비동기식으로 데이터를 처리해야 하기 떄문에 웹 API로도 비동기식 구현이 가능하지만 순서가 보장되지 않는다는 특성이 있다. 메시지를 보내는 A어플리케이션은 메시지를 보낼 때 B라는 어플리케이션의 목적지(도착점)을 알아야 통신할 수 있다. 두 어플리케이션간 불필요한 결합도가 발생되고, 응답을 취하는 B어플리케이션이 서버 장애시 요청되었던 데이터 때문에 A어플리케이션에게도 장애가 전파될 수 있다. 메시징 지향 미들웨어의 필요성 메시지 API는 비동기 프로토콜을 지원하며, 메시지 순서를 보장합니다. 메시지가 대기열에 전달되면, 응답을 즉시 할 필요가 없다. 메시지 대기열에 전달 된 상황이라면 메시지는 시스템에 보존되어 있어, 다른 어플리케이션간의 의존성이 낮게 된다. Message Broker 송신자와 수신자 사이에서 메시지의 전달을 중재하는 컴퓨터 프로그램 모듈\n메시지 브로커는 정형화된 메시지의 교환을 통해 어플리케이션간의 소통이 이뤚디는 네트워크 엘리먼드이다.\n목적 메시지의 유효성, 정송, 라우팅을 위한 아키텍처 패턴\n어플리케이션 사이의 커뮤니케이션 중재 어플리케이션간의 메시지 전달을 위한 상호 인식(mutal awareness)를 줄여 어플리케이션간의 결합성을 낮춘다(decoupling) 기능 엔드 포인트 분리 NFR(non-functional requirement) 충조 중재함수 (intermediary function)의 간편한 재사용 하나이상의 목적지로의 메시지 라우팅 메시지의 형태 변형 메시지를 수집하여 여러 메시지로 분해하고 대상으로 보내 응답을 하나의 메시지로 재구성하여 사용자에게 반환 메시지 양 증가 또는 저장을 위한 외부 저장소와 상호작용 데이터 검색을 위한 웹 서비스 호출 이벤트 또는 에러의 응담 발행-구독 패턴을 활용한 컨텐츠와 토픽 기반 메시지 라우팅 제공 설계 허브 앤 스포크(hub and spoke)\n중앙 서버가 통합 서비스를 제공하는 메커니즘으로 작동 메시지 버스(message bus)\n메시지 브로커가 버스에서 작동하는 통신 백본 또는 분산 서비스 ","date":"2023-04-17T19:42:33+09:00","image":"https://codemario318.github.io/post/messaging_system/messaging_cover_huc80ec853f6ab161a17ff43aa6052ff01_60754_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/messaging_system/","title":"메시징 시스템이란?"},{"content":"Not Only SQL: SQL만을 사용하지 않는 데이터베이스 관리 시스템을 지칭하는 단어. \u0026lsquo;데이터를 저장하는 데에는 SQL 외에 다른 방법들도 있다.\u0026rsquo;\n정의 NoSQL에 내려진 구체적인 정의는 없으나 공통적인 성향을 가지고 있다.\n대부분 클러스터에서 실행할 목적으로 만들어졌기 때문에 관계형 모델을 사용하지 않는다. 오픈 소스이다. 스키마 없이 동작하며, 구조에 대한 정의를 변경할 필요 없이 데이터베이스 레코드에 자유롭게 필드를 추가할 수 있다. 21세기 초반에 개발 된 SQL을 사용하지 않는 Schema-less 데이터베이스\n클러스터\n저렴한 상용 제품 여러 대를 조합하여 더 빠르고 안정적인 시스템을 목표로 만들어진 방법\n등장배경 여러 대의 컴퓨터에 데이터를 분산 저장하는 것을 목표로 등장했다.\n기존에는 안정적인 데이터 관리가 가장 중요했기 때문에, 트랙잭션을 통한 관리가 가능한 RDBMS가 많이 이용되었지만 웹 2.0 환경과 빅데이터가 등장하면서 RDBMS는 데이터와 트래픽 양이 기하급수적으로 증가함에 따라 한 대에서 실행되도록 설계된 RDBMS를 사용하는 데 필요한 비용 증가 이슈가 생겨났다.\nNoSQL은 데이터의 일관성을 약간 포기한 대신 여러 대의 컴퓨터에 데이터를 분산하여 저장하는 것을 목표로 등장하였고, NoSQL의 등장으로 작고 값싼 장비 여러 대로 대량의 데이터와 컴퓨팅 부하를 처리하는 것이 가능하게 되었다.\n특징 일관성과 확장성 사이의 Trade-off\n일관성이 데이터베이스의 절대적인 요소가 아님을 주장하는 움직임이 생기기 시작했다. 다수가 동시에 읽고 쓰는 상황에서의 성능 향상을 위해서. 분산 환경에서 노드들이 잘 작동하고 있음에도, 시스템의 일부가 고장나면 데이터베이스를 사용할 수 없게 되는 문제를 해결하기 위해서. 분산 저장\n데이터와 트래픽이 증가함에 따라 기존의 장비로는 원할한 데이터의 처리가 어려워졌다. 이를 해결하기 위한 방법으로 장비의 성능을 키우는 수직적 확장과 처리하는 장비 수를 늘리는 수평적 확장이 있다. 수직적 확장은 큰 비용적인 문제가 발생하므로 수평적 확장을 고려했지만, RDBMS가 클러스터 상에서 효율적으로 동작하도록 설계되지 않았다. 샤딩(Sharding)\n샤드키를 기준으로 하나의 테이블을 수평 분할하여 서로 다른 클러스터에 분산 저장하고 질의할 수 있는 기법. RDBMS에서도 사용 가능하지만 어플리케이션 레벨에서 모든 샤딩을 제어해야 한다.(어떤 데이터를 어느 클러스터에서 처리해야 하는지 등) 또한 여러 샤드에 걸치는 쿼리나 참조 정합성, 트랜잭션, 일관성 문제가 발생할 수 있다. 분산 저장을 지원하는 NoSQL 데이터베이스의 경후, 집합-지향(Aggregtae-oriented) 모델을 사용하여 이러한 문제를 해결한다. 연관된 데이터들이 함께 분산되므로, 관계형 모델에서처럼 복잡한 제어가 필요하지 않게 된다. 데이터 일치\nRDBMS에서 관계형 튜플 안의 값은 단순해야 하며 중첩된 레코드나 리스트 등 다른 구조를 포함할 수 없느 반면, 메모리 내 데이터 구조에서는 이런 제약이 없어 훨씬 복잡한 구조를 사용한다.(리스트, 딕셔너리, 중첩된 객체 구조) 그 결과 복잡한 메모리내 데이터 구조를 데이터베이스에 저장하려면 먼저 관계형 표현으로 변환해야 한다. (ORM 프레임워크등을 이용) NoSQL은 메모리 내의 데이터가 어떤 구조이든지 상관하지 않고 하나의 Aggregation으로 취급하여 저장하기 때문에 자유롭다. Impedance mismatch\n관계형 모델과 메모리 내 데이터 구조 간의 불일치\nSchema-less\nNoSQL 데이터베이스의 공통적인 특징은 스키마 없이 동작한다는 점이다. 장점 데이터 구조를 미리 정의할 필요가 없다. 시간이 지나더라도 언제든지 바꿀 수 있기 때문에 비형식적인 데이터를 저장하는 데 용이하다. 단점 단일 값에 대한 데이터 타입에서 불일치가 발생할 수 있다. 데이터베이스가 스키마를 직접 관리하지 않는 것을 의미할 뿐, 데이터 타입에 따른 암묵적인 스키마는 여전히 존재하기 때문 종류 네 가지 모델로 나눌 수 있다.\nkey-value Document Column-family Graph 이 중 그래프 모델을 제외한 나머지 세 모델은 집합-지향(Aggregate-orented)모델이다.\n집합-지향 (Agregate-orented) 모델 집합 지향 데이터베이스는 집합 자료구조로 이루어져 있다.\n집합\n연산의 한 단위로 취급되는 연관된 객체들의 집합.\n관계형 모델처람 하나의 엔티티에 대한 ACID 트랜잭션을 지원하지는 않지만, 하나의 집합에 대한 연산에서는 트랜잭션을 지원한다.\n장점\n집합 지향 데이터베이스는 여러 대의 클러스터로 이루어진 시스템에서 사용하기 적합하다. 수평적 확장이 용이하다. 이는 관계형 데이터베이스와는 달리 연관된 데이터들이 함께 움직이기 떄문이다. 메모리 내의 자료구조와 집합 간 데이터가 잘 일치하므로, 관계형 데이터베이스처럼 객체-관계 매핑 프레임워크가 필요하지 않다. 데이터의 검색도 아주 쉬운편으로, key나 ID를 사용하면 쉽게 집합 레코드를 찾아낼 수 있다. 단점\n집합 지향 데이터베이스는 조인 연산이 불가능 MongoDB나 Cassandra등의 데이터베이스에서는 맵리듀스(MapReduce) 기능을 제공함으로써 조인과 유사한 연산을 가능하도록 설계했지만 사용법이 어렵고 Hadoop의 맵 리듀스에 비하면 속도도 매우 느리다. Key-Value 키 값 저장소는 가장 단순한 형태의 NoSQL\n장점\n수평적 확장이 용이하다. 아주 간단한 API만을 제공하기 떄문에 배우는 것이 어렵지 않다. 간단한 API를 제공하는 만큼 질의의 속도가 굉장히 빠른편 어떠한 형태의 데이터라도 담을 수 있다. 이미지나 비디오도 가능 단점\n값의 내용을 사용한 쿼리가 불가능하다. 키를 사용해 값을 읽어들인 뒤, 어플리케이션 레벨에서 적절히 처리해야 한다. Document 데이터가 키와 문서 형태로 저장되는 키-값 모델의 개선 형태\n키-값 모델과의 차이점\nValue가 계층적인 형태인 도큐먼트로 저장된다. 객체지향의 객체와 유사하며, 하나의 단위로 취급되어 저장된다.\n장점\n하나의 객체를 여러 테이블에 나눠 저장할 필요가 없다. 도큐먼트 내의 item을 이용한 쿼리가 가능하다. 단, Xquery나 다른 도큐먼트 질의 언어가 필요 객체-관계 매핑이 필요하지 않다. 객체를 도큐먼트의 형태로 바로 저장 가능하기 떄문 검색에 최적화 되어있다. 단점\n사용이 번거롭고 쿼리가 SQL과 다르다. 질의의 결과가 JSON이나 XML 형태로 출력되기 때문에 사용방법이 RDBMS와 다르다. 질의 언어가 SQL과 다르기 떄문에 사용에 익숙해지기까지 다소 어려움이 있을 수 있음. 종류 MongoDB 도큐먼트 지향 데이터 베이스이다.\nbson 데이터 구조로 저장 문서를 기본 저장 단위로 이용하면서 내장 문서와 배열을 이용하여 복잡한 계층구조를 하나의 레코드로 표현한다. 스키마가 없다. 필드 추가 제거는 자유로우며 필요할 때 마다 자유자재로 변경 가능하다. RDBMS 보다 매우 빠르다. 조인과 트랜잭션을 지원하지 않으며 여러 제약조건에 대한 처리도 없다. →버전에 따라 다름 Redis(REmote DIctionary Server) 메모리 기반의 \u0026ldquo;Key-Value\u0026rdquo; 구조 데이터 관리 시스템이며, 모든 데이터를 메모리에 저장하고 조회하기에 빠른 Read, Write 속도를 보장하는 비 관계형 데이터베이스이다.\nString, set, Sorted Set, Hash, List 데이터 형식을 지원한다. Redis는 빠른 오픈 소스인 메모리 키-값 데이터 구조 스토어이며, 다양한 인메모리 데이터 구조 집합을 제공하므로 사용자 정의 어플리케이션을 손쉽게 생성 할 수 있다.\n특징 영속성을 지원하는 인메모리 데이터 저장소\n읽기 성능 증대를 위한 서버측 복제 지원\nRedis가 실행중인 서버가 충돌하는 경우 장애 조치 처리와 함께 더 높은 읽기 성능을 지원하기 위해 슬레이브가 마스터에 연결하고 전체 데이터베이스의 초기 복사본을 받는 마스터/ 슬레이브 복제를 지원. 마스터에서 쓰기가 수행되면 슬레이브 데이터 세트를 실시간으로 업데이트 하기 위해 연결된 모든 슬레이브로 전송됨 쓰기 성능 증대를 위한 클라이언트\n","date":"2023-04-17T19:24:48+09:00","image":"https://codemario318.github.io/post/nosql/nosql_cover_hu705a0f96b7606376fe264778ca77daa9_6424_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/nosql/","title":"NoSQL이란?"},{"content":"Apache 아파치는 클라이언트 요청을 받으면 MPM(Multi Processing Module : 다중처리모듈) 방식으로 처리한다.\n스레드/프로세스 기반 구조 동작 ServerSocket으로 request A가 들어오면 Thread를 할당해준다. Thread는 해당 socket을 가지고 read, write작업 등을 수행한다. 수행 도중 ServerSocket으로 request B가 들어오면, context switching이 일어난다. 새로 들어온 요청에 쓰레드를 배분하고, 또 해당 소켓을 가지고 작업을 수행한다. 아직 마무리되지 않은 A를 처리하기 위해 일정 기간마다 지속적으로 context switching을 반복하고 모든 작업을 마무리 한다. Prefork MPM 실행중인 프로세스를 복제하여 처리하는 방식\n각 프로세스는 한번에 한 연결만 처리하고, 요청량이 많아질수록 프로세스를 복제하여 동작한다.\n프로세스를 복제하는 방식이기 떄문에 메모리가 많이 소비된다\n연결 수 = 프로세스 수\nWorker MPM Prefork 방식은 한개의 프로세스가 한개의 스레드로 처리되지만, Worker 방식은 한개의 프로세스가 여러 쓰레드를 사용하여 처리한다.\n쓰레드를 사용하기 떄문에 Prefork 방식보다 메모리 소모가 적고, 통신량이 많을 때 유리하다.\n문제점 아파치는 접속마다 프로세스 또는 쓰레드를 생성하는 구조이다.\n동시 접속 요청이 많을수록 그만큰 생성 비용이 들고 대용양 요청을 처리할 수 있는 웹 서버로서 한계가 나타난다.\nNginx 한개 또는 고정된 프로세스만 생성하고, 프로세스 내부에서 비동기 방식으로 작업을 처리한다. 따라서 동시 접속 요청이 많아도 프로세스 또는 쓰레드 생성 비용이 존재하지 않는다.\nEvent-Driven 방식 Event-Driven 방식은 Reactor pattern을 사용한다.\nReactor는 이벤트가 들어오면 알맞는 handler로 dispatch 해준다. Handler는 dispatch된 이벤트를 받아서 처리하는 역할을 수행 Reactor pattern\n이벤트 처리(event handling)디자인 패턴으로 하나의 Reactor가 계속 이벤트를 찾고 이벤트가 발생(trigger)하면 해당 이벤트 처리기(event handler)에게 알린다.\nNginx와 Apache의 차이점 컨텐츠의처리 방식 정적 컨텐츠 처리\nApache: 전통적인 파일기반 방식의 정적 컨텐츠 Nginx: 이벤트 처리/비동기식/논블로킹 방식 처리로 인해 정적 컨텐츠 제공시 고속 처리 가능 동적 컨텐츠 처리\nApache: 서버 내에서 처리 기본적으로 유연성과 범용성을 갖추는 방식으로 서버 자체에서 동적 컨텐츠 처리가 가능하다. Nginx: 동적 컨텐츠를 처리하지 않음 동적 웹 페이지 컨텐츠를 가진 모든 요청을 위해 외부 자원과 연계한다. 따라서 최종적으로 동적 컨텐츠가 다시 돌아올 때까지 기다렸다가 클라이언트에게 전달하는 방식을 가지고 있다. OS 지원에 대한 범용성 Apache: 리눅스, BSD, UNIX, WINDOW 역사가 있는 만큼 지원 범위가 다양하기 때문에 일관성 있는 웹 서비스 아키텍쳐를 구현할 수 있다. Nginx: LINUX, BSD, UNIX, WIN(부분 지원) 다양한 운영체제를 지원하지만 아파치 만큼 완벽히 지원하지 않는다. 분산/중앙집중식 구성 방식 Apache: 분산/중앙집중식 구성 채택 .htaccess를 통해 디렉토리별로 추가 구성을 할 수 있다. 단일 기반 뿐만 아니라 분산형 구칙이 가능하므로 대용량 서버 아키텍쳐에서 자원만 충분하다면 여러 웹 서비스를 구현 할 수 있다. Nginx: 중앙집중식 구성 채택 아파치처럼 .htaccess를 지원하지 않는다. 따라서 추가 구성을 할 수 없는 단점이 있다. 하지만 이러한 방식은 가상화, 클라우드, MSA와 같은 아키텍쳐에서는 오히려 경량화와 성능 보장이라는 측면에서 단점이 되지 않을 수도 있다. 모듈 및 확장성/보안 Apache 60개 이상의 다양한 기능과 모듈을 지원하며, 필요에 따라 활성화 또는 비활성 시킬 수 있다. 동적 모듈을 통해 웹 서버의 사용자 지정도 가능하게 할 수 있는 등 다양한 디자인과 확장이 가능하다. 보안을 위해 다양한 Web기반 DDoS 방어 기술을 제공한다. Nginx 다른 코어 모듈을 동적으로 로딩할 수 없도록 되어있다. 옵션을 최소화 해서 태생 부터 성능에 포커싱 했다. 보안에 대한 다양한 기술 문서를 제공하며, 코드 자체가 가볍고 경량화 되어 있어서 보안에 유리한 측면도 있다.https://youngmind.tistory.com/entry/Apache-vs-Nginx ","date":"2023-04-17T19:10:21+09:00","image":"https://codemario318.github.io/post/nginx_vs_apache/web_cover_hu71ff0ea2f7ce80fa0f2ad0c2fcb44a04_52909_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/nginx_vs_apache/","title":"Nginx VS Apache"},{"content":"웹 동작 방식 클라이언트(브라우저)가 HTTP(URL)을 통해 요청을 보내면 HTML, CSS, 이미지와 같은 정적 콘텐츠를 응답으로 보내게 되고 그것을 받은 클라이언트가 해석하여 페이지로 보여준다.\nStatic pages와 Dynamic Pages Static Pages Web Server는 파일 경로의 이름을 받아 경로와 일치하는 file contents를 반환 항상 동일한 페이지를 반환 Ex) image, html, css, javascript 파일과 같이 컴퓨터에 저장되어 있는 파일들 Dynamic Pages 인자의 내용에 맞게 동적인 Contents를 반환 웹 서버에 의해서 실행되는 프로그램을 통해서 만들어진 결과물(WAS)위에서 돌아가는 프로그램 Web Server와 WAS의 차이 Web Server 소프트웨어와 하드웨어로 구분된다.\n하드웨어 Web 서버가 설치되어 있는 컴퓨터 소프트웨어 웹 브라우저 클라이언트로 부터 HTTP 요청을 받아 정적인 컨텐츠(.html, .jpeg, .css 등)을 제공하는 컴퓨터 프로그램 Web Server의 역할 HTTP 프로토콜을 기반으로 하여 클라이언트의 요청을 서비스 하는 기능 담당\n요청에 따라 두 가지 기능 중 적절하게 선택하여 수행\n정적인 컨텐츠 제공 WAS를 거치지 않고 바로 자원을 제공한다 동적인 컨텐츠 제공을 위한 요청 전달 클라이언트의 요청을 WAS에 보내고, WAS가 처리한 결과를 클라이언트에게 전달한다. Web Server의 예 Apache Server Nginx IIS 등 WAS(Web Application Server) DB 조회나 다양한 로직 처리를 요구하는 동적인 컨텐츠를 제공하기위해 만들어진 Application Server\nHTTP를 통해 컴퓨터나 장치에 어플리케이션을 수행해주는 미들웨어이다.\n웹 컨테이너(Web Container), 서블릿 컨테이너(Servlet Container)라고도 불림\nWAS의 역할 WAS = Web Server + Web Container\n웹서버 기능들을 구조적으로 분리하여 처리하고자하는 목적으로 제시됨\n분산 트랜잭션 보안 메시징 쓰레드 처리 등 DB와 서버와 같이 수행됨\nWAS의 주요 기능 프로그램 실행 환경과 DB 접속 기능 제공 여러 개의 트랜잭션 관리 기능 업무를 처리하는 비지니스 로직 수행 WAS가 필요한 이유 웹 페이지는 정적 컨텐츠와 동적 컨텐츠가 모두 존재한다.\n사용자의 요청에 맞게 적절한 동적 컨텐츠를 만들어서 제공해야 한다. 웹 서버만을 이용하게 되면 그에 맞는 결과가 정적 파일로 존재해야 한다. 따라서 WAS를 통해 요청에 맞는 데이터를 DB에서 가져와 비즈니스 로직에 맞게 결과를 만들어 제공함으로 자원을 효율적으로 사용할 수 있다.\nWAS와 Web Server를 분리하는 이유 기능 분리를 통한 서버 부하 방지\nWAS만으로도 웹서비스를 제공 가능하지만 WAS는 DB조회 등 동적인 웹 페이지를 위한 다양한 동작을 하기 때문에 바쁘다. 따라서 웹 서버를 통해 정적인 컨텐츠를 제공하여 부하를 방지한다.\n물리적으로 분리하여 보안 강화\nSSL에 대한 암복호화 처리에 웹서버를 사용한다.\n여러대의 WAS를 연결 가능\nLoad Balancing을 위해 Web Server를 사용 가능하다\nfail over(장애 극복), fail back 처리에 유리 여러대의 서버를 사용하는 대용량 웹 어플리케이션의 경우 웹 서버와 WAS를 분리하여 무중단 운영을 위한 장애 극복에 쉽게 대응할 수 있다. 여러 웹 어플리케이션 서비스 가능\n하나의 웹 서버로 다양한 WAS를 이용하게 만들 수 있다. ","date":"2023-04-17T18:28:10+09:00","image":"https://codemario318.github.io/post/web/web_cover_hu71ff0ea2f7ce80fa0f2ad0c2fcb44a04_52909_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/web/","title":"Web"},{"content":"클로저는 두개의 함수로 만들어진 환경으로 이루어진 특별한 객체의 한 종류이다. 여기서 환경이라 함은 클로저가 생성될 때 그 범위에 있던 여러 지역 변수들이 포함 된 context를 말한다. 이러한 범위는 자바스크립트 코드를 실행하기 위해 발생하는 컴파일 단계에서 결정된다.\n클로저를 통해서 자바스크립트에는 없는 비공개 속성/메소드, 공개 속성/메소드 처럼 구현 할 수 있다.\nLexical Scope 자바스크립트 코드를 실행할 때 컴파일 단계에서 몇가지 일이 일어난다. 그중 하나인 토크나이징과 렉싱이 있다.\n토크나이징 문자열을 나누어 토큰으로 만드는 과정\n1 var num = 5; 위와 같은 구문을 만나게 되면, 아래와 같은 토큰으로 나눈다.\n1 2 3 4 5 var num = 5 ; 렉스타임 토크나이징의 결과물인 토큰을 분석하여 생성된 토큰에 의미를 부여하는 것을 렉싱이라고 하며, 이 과정을 렉스타임이라고 한다.\n렉시컬 스코프 개발자가 코드를 작성할때 변수를 어디에 작성하는가를 바탕으로 렉스타임에 토큰이 분석되며 스코프가 결정된다. 이때 구성된 유효 범위를 렉스컬 스코프라고 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 var a = 10; function foo() { var b = 20; function bar() { var c = 30; colsole.log(a + b + c); // 60 } bar(); } foo(); 위의 코드가 실행될때 스코프 버블은 bar 안쪽에서 부터 시작되어 올라간다.\n코드를 해석하는 과정에서 상위에서 하위로 쌓이는 구조로 해석되기 때문에, scope에 대한 검색은 기본적으로 하위에서 상위는 되지만 상위에서 하위로의 검색은 불가능하다.\n1 2 3 4 5 6 7 8 9 var a = 10; function foo () { var b = 20; console.log(a); // 10 console.log(b); // 20 } console.log(b); // error 정리 컴파일레이션의 렉싱 단계에서 모든 변수들이 어디서 어떻게 선언되었는지 바탕으로 실행 단계에서 스코프를 구성하고, 이렇게 구성되는 스코프가 렉시컬스코프이다.\n클로저 생성하기 내부 함수가 익명 함수로 되어 외부 함수의 반환값으로 사용된다. 내부 함수는 외부 함수의 실행 환경에서 실행된다. 내부 함수에서 사용되는 변수는 외부 함수의 변수 스코프에 있다. 1 2 3 4 5 6 7 8 9 10 11 12 function outer() { var name = `closure`; function inner() { console.log(name); } inner(); } outer(); // console\u0026gt; closure outer함수를 실행시키는 context에는 name이라는 변수가 존재하지 않지만, inner함수가 outer 함수 내부에 선언된 name을 참조하기 때문에, name 변수에 대한 정보를 알 수 없는 outer 변수 외부환경에서도 정상 출력된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 var name = `Warning`; function outer() { var name = `closure`; return function inner() { console.log(name); }; } var callFunc = outer(); callFunc(); // console\u0026gt; closure 위 코드에서 callFunc 를 클로저라고 한다. callFunc 호출에 의해 name 이라는 값이 console에 출력되는데, \u0026ldquo;Warning\u0026ldquo;이 아니라 \u0026ldquo;closure\u0026ldquo;이다. 즉, outer 함수의 context 에 속해있는 변수를 참조하는 것이다. 여기서 outer 함수의 지역변수로 존재하는 name 변수를 free variable(자유변수) 라고 한다.\n이처럼 외부 함수 호출이 종료 되더라도 외부 함수의 지역 변수 및 변수 스코프 객체의 체인 관계를 유지할 수 있는 구조를 클로저라고 한다.\n","date":"2023-04-17T18:11:36+09:00","image":"https://codemario318.github.io/post/js_closure/js_cover_huc1c9a52f33db3f5dad8bb16243cf7e4e_11875_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/js_closure/","title":"Javascript - Closure"},{"content":"Javascript에서 함수의 this 키워드는 다른 언어와 조금 다르게 동작한다. 또한 엄격 모드와 비엄격 모드에서도 일부 차이가 있다.\n대부분의 경우 this의 값은 함수를 호출한 방법에 의해 결정되는데, 실행중에는 할당으로 설정할 수 없고 함수를 호출할 때 마다 다를 수 있다.\nES5는 함수를 어떻게 호출했는지 상관하지 않고 this 값을 설정할 수 있는 bind 메서드를 도입했고, ES2015는 스스로의 this 바인딩을 제공하지 않는 화살표 함수를 추가했다.(렉시컬 컨텍스트안의 this값을 유지함)\n전역 문맥 전역 실행 문맥(global execution context)에서 this는 엄격 모드 여부에 관계 없이 전역 객체를 참조한다.\n1 2 3 4 5 6 7 8 9 // 웹 브라우저에서는 window 객체가 전역 객체 console.log(this === window); // true a = 37; console.log(window.a); // 37 this.b = \u0026#34;MDN\u0026#34;; console.log(window.b) // \u0026#34;MDN\u0026#34; console.log(b) // \u0026#34;MDN\u0026#34; 함수 문맥 함수 내부에서 this의 값은 함수를 호출한 방법에 의해 결정된다.\n단순 호출 엄격 모드가 아닐경우 this의 값이 호출에 의해 설정되지 않으므로, 기본값으로 브라우저에서는 window인 전역 객체를 참조하게 된다.\n1 2 3 4 5 6 7 8 9 function f1() { return this; } // 브라우저 f1() === window; // true // Node.js f1() === global; // true 반면에 엄격 모드에서 this 값은 실행 문맥에 진입하며 설정되는 값을 유지하므로 다음 예시에서 보여지는 것 처럼 this는 undefined로 남아있게 된다.\n1 2 3 4 5 6 function f2(){ \u0026#34;use strict\u0026#34;; // 엄격 모드 참고 return this; } f2() === undefined; // true f2를 객체의 메서드나 속성(예: window.f2())이 아닌 직접 호출했기 때문에 this는 undefined여야 하지만 브라우저에서 엄격 모드를 지원하지 않는다면 window 객체를 반환한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 var prop = 1; var test = { prop: 42, func: function() { var func2 = function() { console.log(this); } console.log(this); func2(); }, }; test.func(); /* \u0026gt; Object { prop: 42, func: function() { var func2 = function() { console.log(this); } console.log(this); func2(); } } \u0026gt; [object Window] // browser \u0026gt; [object global] // node.js */ function 키워드로 선언된 함수가 전역 실행 문맥(global execution context)에서 호출되었기 때문에 this는 엄격 모드 여부에 관계 없이 전역 객체를 참조한다.\nbind 메서드 ECMAScript5는 Function.prototype.bind를 도입했다. f.bind(someObject)를 호출하면 f와 같은 본문(코드)과 범위를 가졌지만 this는 원본 함수를 가진 새로운 함수를 생성한다. 새 함수의 this는 호출 방식과 상관없이 영구적으로bind()의 첫 번째 매개변수로 고정된다.\n1 2 3 4 5 6 7 8 9 10 11 12 function f() { return this.a; } var g = f.bind({a: \u0026#39;azerty\u0026#39;}); console.log(g()); // azerty var h = g.bind({a: \u0026#39;yoo\u0026#39;}); // bind는 한 번만 동작함! console.log(h()); // azerty var o = {a: 37, f: f, g: g, h: h}; console.log(o.a, o.f(), o.g(), o.h()); // 37, 37, azerty, azerty 화살표 함수 화살표 함수에서 this는 자신을 감싼 정적 범위(lexical context)이다. 전역 코드에서는 전역 객체를 가르킨다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 var globalObject = this; var foo = (() =\u0026gt; this); console.log(foo() === globalObject); // true // 객체로서 메서드 호출 var obj = {func: foo}; console.log(obj.func() === globalObject); // true // call을 사용한 this 설정 시도 console.log(foo.call(obj) === globalObject); // true // bind를 사용한 this 설정 시도 foo = foo.bind(obj); console.log(foo() === globalObject); // true 화살표 함수를 call(), bind(), apply()를 사용해 호출할 때 this의 값을 정해주더라도 무시한다. 사용할 매개변수를 정해주는 건 문제 없지만, 첫 번째 매개변수(thisArg)는 null을 지정해야 한다.\n어떤 방법을 사용하든 foo의 this는 생성 시점의 것으로 설정된다.(위 예시에서는 global 객체). 다른 함수 내에서 생성된 화살표 함수에도 동일하게 적용된다. this는 싸여진 렉시컬 컨텍스트로 유지된다.\n1 2 3 4 5 6 7 8 9 10 11 12 var obj = { bar: function() { var x = (() =\u0026gt; this); return x; } }; var fn = obj.bar(); console.log(fn() === obj); // true var fn2 = obj.bar; console.log(fn2()() === window); // true 화살표 함수의 범위는 선언될때 결정되는데, fn은 obj.bar()로 호출된 x를 활용하게 되어 this가 obj 를 의미하게 되고, fn2는 전역 실행 문맥에서 obj.bar 자체를 fn2에 할당 하였기 때문에 window로 설정되었다.\n객체의 메서드를 호출할 때 함수를 어떤 객체의 메서드로 호출하면 this의 값은 그 객체를 사용한다.\n다음 예제에서 o.f()를 실행할 때 o 객체가 함수 내부의 this와 연결된다.\n1 2 3 4 5 6 7 8 9 10 11 12 var o = {prop: 37}; function independent() { return this.prop; } o.f = independent; console.log(o.f()); // logs 37 o.b = {g: independent, prop: 42}; console.log(o.b.g()); // logs 42 this 바인딩은 멤버 대상에 영향을 받는다. 함수를 실행할 때, 객체 o.b의 메소드 g 로서 호출하면 함수 내부의 this는 o.b를 나타낸다.\n객체의 프로토타입 체인에서의 this 메서드가 어떤 객체의 프로토타입 체인 위에 존재한다면, this의 값은 그 객체가 메서드를 가진 것 처럼 설정된다.\n1 2 3 4 5 6 7 8 var o = { f:function() { return this.a + this.b; } }; var p = Object.create(o); p.a = 1; p.b = 4; console.log(p.f()); // 5 이 예제에서, f 속성을 가지고 있지 않은 변수 p가 할당된 객체는, 프로토타입으로 부터 상속받는다. 그러나 그것은 결국 o에서 f 이름을 가진 멤버를 찾는 되는 문제가 되지 않는다 ; p.f를 찾아 참조하기 시작하므로, 함수 내부의 this는 p 처럼 나타나는 객체 값을 취한다. 즉, f는 p의 메소드로서 호출된 이후로, this는 p를 나타낸다. 이것은 JavaScript의 프로토타입 상속의 흥미로운 기능이다.\n접근자와 설정자의 this 함수를 접근자와 설정자에서 호출하더라도 동일하다. 접근자나 설정자로 사용하는 함수의 this는 접근하거나 설정하는 속겅을 가진 객체로 묶인다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 function sum() { return this.a + this.b + this.c; } var o = { a: 1, b: 2, c: 3, get average() { return (this.a + this.b + this.c) / 3; } }; Object.defineProperty(o, \u0026#39;sum\u0026#39;, { get: sum, enumerable: true, configurable: true}); console.log(o.average, o.sum); // 2, 6 생성자로서 함수를 new 키워드와 함께 생성자로 사용하면 this는 새로 생긱 객체에 묶인다.\n","date":"2023-04-17T18:11:36+09:00","image":"https://codemario318.github.io/post/js_this/js_cover_huc1c9a52f33db3f5dad8bb16243cf7e4e_11875_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/js_this/","title":"Javascript - this"},{"content":"hoist라는 단어의 사전적 정의는 \u0026ldquo;끌어올리기\u0026rdquo; 라는 뜻이다. 자바스크립트가 실행될때 선언된 모든 변수 선언부가 코드의 가장 위로 끌어올려진 것 처럼 처리된다.\n호이스트를 통해 변수의 정의가 그 범위에 따라 선언과 할당으로 분리된다. 즉, 변수가 함수 내에서 정의되었을 경우, 선언이 함수 최상위로, 함수 바깥에서 정의되었을 경우, 전역 컨텍스트의 최상위로 변경이 된다.\n1 2 3 4 5 6 7 8 9 function getX() { console.log(x); // undefined var x = 100; console.log(x); // 100 } getX(); 위와 같이 정의 되었을때, 아래와 같이 해석된다.\n1 2 3 4 5 6 7 8 9 10 11 function getX() { var x; console.log(x); x = 100; console.log(x); } getX(); 선언문은 항상 자바스크립트 엔진 구동시 가장 최우선으로 해석하므로 호이스팅 되고, 할당 구문은 런타임 과정에서 이루어지기 때문에 호이스팅 되지 않는다.\n함수의 호이스팅 함수가 자신이 위치한 코드에 상관없이 함수 선언문 형태로 정의한 함수의 유효범위는 전체 코드의 맨 처음부터 시작한다. 함수 선언이 함수 실행 부분보다 뒤에 있더라도 자바스크립트 엔진이 함수 선언을 끌어올리는 것을 의미한다.\n1 2 3 4 5 6 7 foo(); function foo(){ console.log(‘hello’); }; // console\u0026gt; hello foo 함수에 대한 선언을 호이스팅하여 global 객체에 등록시키기 때문에 hello가 제대로 출력된다.\n오류 사례 1 2 3 4 5 6 7 foo(); var foo = function() { console.log(‘hello’); }; // console\u0026gt; Uncaught TypeError: foo is not a function 예제의 함수 표현은 함수 리터럴을 할당하는 구조이기 때문에 호이스팅 되지 않으며 그렇기 때문에 아래와 같이 해석되어 런타임 환경에서 Type Error를 발생시킨다.\n1 2 3 4 5 6 7 8 var foo; foo(); // foo = undefined // console\u0026gt; Uncaught TypeError: foo is not a function foo = function( ) { console.log(‘hello’); }; ","date":"2023-04-17T18:04:35+09:00","image":"https://codemario318.github.io/post/js_hoisting/js_cover_huc1c9a52f33db3f5dad8bb16243cf7e4e_11875_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/js_hoisting/","title":"Javascript - Hoisting"},{"content":"컨테이너는 애플리케이션, 실행 라이브러리, 시스템 도구, 시스템 라이브러리 등을 포함하여 애플리케이션과 그 애플리케이션을 실행하는 환경을 패키징하여 이식성이 뛰어난 소프트웨어 패키지로 만든것이다.\n컨테이너화를 통해 더욱 신속하게 작업을 진행할 수 있다. 효율적으로 소프트웨어를 배포할 수 있다. 매우 높은 수준의 확장성을 확보할 수 있다. 서버 가상화 컴퓨터의 성능을 효율적으로 사용하기 위해 가상화 기술이 등장하였다. 서버 관리자 입장에서 리소스 사용률이 적은 서버들은 낭비라고 생각할 수 있다. 그렇다고 모든 서비스를 한 서버 안에 올린다면 안정성에 문제가 생길 수 있다. 이에 따라 안정성을 높이며 리소스를 최대한 활용할 수 있는 방법으로 고안된 방법이 서버 가상화이다.\n컨테이너 정의 컨테이너는 소프트웨어 서비스를 실행하는 데 필요한 특정 버전의 프로그래밍 언어 런타임 및 라이브러리와 같은 종속 항목과 애플리케이션 코드를 함께 포함하는 경량 패키지이다.\n컨테이너는 운영체제 수준에서 CPU, 메모리, 스토리지, 네트워크 리소스를 쉽게 공유할 수 있게 해주며 컨테이너가 실제로 실행되는 환경에서 애플리케이션을 추상화 할 수 있는 논리 패키징 매커니즘을 제공한다.\n컨테이너의 이점 책임 분리 컨테이너화를 통해 책임을 깔끔하게 분리할 수 있다.\n개발자는 애플리케이션의 로직과 종속 항목에 집중하고, IT 운영팀은 특정 소프트웨어 버전 및 구성과 같은 애플리케이션의 세부 요소 대신 배포 및 관리에 집중할 수 있다.\n워크로드 이동성 컨테이너는 Linux, Windows, Mac 등 운영체제를 가리지 않고, 가상머신 물리적 서버, 개발자 컴퓨터, 데이터 센터, 온프레미스 환경, 퍼블릭 클라우드 등 사실상 어느 환경에서나 구동되므로 개발 및 배포가 크게 쉬워진다.\n애플리케이션 격리 컨테이너는 운영체제 수준에서 CPU, 메모리, 스토리지, 네트워크 리소스를 가상화 하므로 개발자에게 다른 애플리케이션으로부터 논리적으로 격리된 OS 환경을 제공한다.\n컨테이너와 VM의 차이 VM은 기본 하드웨어에 대한 엑세스 권한을 갖는 호스트 운영체제 위에서 Linux또는 Windows 같은 게스트 운영체제를 실행하기 때문에 컨테이너와 비교되는 경우가 많다.\n컨테이너는 가상 머신과 마찬가지로 애필리케이션을 관련 라이브러리 및 종속 항목과 함께 패키지로 묶어 소프트웨어 서비스 구동을 위한 격리 환경을 마련해준다. 그러나 컨테이너를 사용하면 훨씬 작은 단위로 업무를 수행할 수 있어 이점이 훨씬 많다.\nVM 보다 훨씬 더 가볍다. OS 수준에서 가상화되고, VM은 하드웨어 수준에서 가상화된다. OS 커널을 공유하며 VM에서 필요한 것 보다 훨씬 적은 메모리를 사용한다. 컨테이너의 용도 애플리케이션을 실제 구동 환경으로부터 추상화할 수 있는 논리 패키징 메커니즘을 제공함. 이러한 분리를 통해 어떤 환경에서도 컨테이너 기반 애플리케이션을 쉽게 지속적으로 배포할 수 있음.\n민첩한 개발 컨테이너를 사용하면 개발자가 종속 항목과 환경에 미치는 영향을 신경쓰지 않고 훨씬 더 빠르게 개발을 진행할 수 있다.\n효율적인 운영 컨테이너는 경량이며 필요한 컴퓨팅 리소스만 사용하면 된다. 따라서 애플리케이션을 효율적으로 구동할 수 있다.\n폭넓은 구동 환경 컨테이너는 거의 모든 곳에서 구동할 수 있어 환경에 영향 없이 사용할 수 있다.\n컨테이너 기술 Namespaces VM에서는 각 게스트 머신별로 독립적인 공간을 제공하고 서로가 충돌하지 않도록 하는 기능을 갖추고 있다.\n리눅스에서는 이와 동일한 역할을 하는 namespaces 기능을 커널에 내장하고 있다.\nmnt(파일시스템 마운트): 호스트 파일 시스템에 구애받지 않고 독립적으로 파일 시스템을 마운트하거나 언마운트 가능 pid(프로세스): 독립적은 프로세스 공간을 할당 net(네트워크): namespace간 network 충돌 방지(중복 포트 바인딩 등) ipc(SystemV IPC): 프로세스간의 독립적인 통신통로 할당 uts(hostname): 독립적인 hostname 할당 user(UID): 독립적인 사용자 할당 namespaces를 지원하는 리눅스 커널을 사용하고 있다면 다음 명령어를 통해 바로 namespace를 만들어 실행할 수 있다.\n1 $ sudo unshare --fork --pid --mount-proc bash 1 2 3 4 # ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 4.0 0.0 17656 6924 pts/9 S 22:06 0:00 bash root 2 0.0 0.0 30408 1504 pts/9 R+ 22:06 0:00 ps aux PID namespace에 실행한 bash가 PID 1로 할당되어 있고, 바로 다음 실행한 ps aux 명령어가 PID 2를 배정 받았다.\ncgroups - Control Groups cgrups는 자원(resources)에 대한 제어를 가능하게 해주는 리눅스 커널 기능으로 아래와 같은 자원들을 제어할 수 있다.\n메모리 CPU I/O 네트워크 device 노드 (/dev/) 1 2 3 // \u0026#34;dhlee\u0026#34; 유저가 소유하하며, // 메모리를 제어할 그룹 \u0026#34;testgrp\u0026#34; 생성 $ sudo cgcreate -a dhlee -g memory:testgrp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 $ ls -alh /sys/fs/cgroup/memory/testgrp 합계 0 drwxr-xr-x 2 ssut root 0 8월 8 23:19 . dr-xr-xr-x 8 root root 0 7월 7 15:30 .. -rw-r--r-- 1 ssut root 0 8월 8 23:19 cgroup.clone_children --w--w--w- 1 ssut root 0 8월 8 23:19 cgroup.event_control -rw-r--r-- 1 ssut root 0 8월 8 23:19 cgroup.procs -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.failcnt --w------- 1 ssut root 0 8월 8 23:19 memory.force_empty -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.failcnt -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.limit_in_bytes -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.max_usage_in_bytes -r--r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.slabinfo -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.tcp.failcnt -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.tcp.limit_in_bytes -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.tcp.max_usage_in_bytes -r--r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.tcp.usage_in_bytes -r--r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.usage_in_bytes -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.limit_in_bytes -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.max_usage_in_bytes -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.move_charge_at_immigrate -r--r--r-- 1 ssut root 0 8월 8 23:19 memory.numa_stat -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.oom_control ---------- 1 ssut root 0 8월 8 23:19 memory.pressure_level -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.soft_limit_in_bytes -r--r--r-- 1 ssut root 0 8월 8 23:19 memory.stat -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.swappiness -r--r--r-- 1 ssut root 0 8월 8 23:19 memory.usage_in_bytes -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.use_hierarchy -rw-r--r-- 1 ssut root 0 8월 8 23:19 notify_on_release -rw-r--r-- 1 root root 0 8월 8 23:19 tasks /sys/fs/cgroup/*/groupname 경로에 있는 파일을 통해 그룹의 여러 옵션들을 변경 가능\n1 2 // testgrp 최대 메모리 사용량을 2MB로 제한 $ echo 2000000 \u0026gt; /sys/fs/cgroup/memory/testgrp/memory.kmem.limit_in_bytes 정리 LXC, LibContainer, runC 등은 위에서 설명한 cgrups, namespaces를 표준으로 정의해둔 OCI(Open Container Initative) 스펙을 구현한 컨테이너 기술의 구현체이다.\nLXC는 캐노니컬(Canonical)이 지원하고 있는 리눅스 컨테이너 프로젝트로 Docker의 경우 1.8 이전 버전까지 LXC를 이용해 구현해서 사용되었다.\n이후 Docker는 libcontainer → runC(libcontainer의 리팩토링 구현체)로 자체 구현체를 갖게 되었다.\nDocker 도커는 1.11버전부터 위와 같은 구조로 작동한다.\ncontainerd는 OCI 구현체(주로 runC)를 이용해 container를 관리해주는 deamon이다.\n도커 엔진 자체는 이미지, 네트워크, 디스크 등의 리소스 관리 역할을 수행하며, 여기서 도커 엔진과 containerd 각각이 완전히 분리되어 도커 엔진을 재시작 해도, 컨테이너 재시작 없이 사용할 수 있다.\n위와 같이 각각 역할이 분리됨에 따라 도커는 4개의 독립적인 프로세스로 작동하고 있다. (docker, docker-containerd, docker-containerd-shim, docker-runc)\n","date":"2023-04-15T16:30:25+09:00","image":"https://codemario318.github.io/post/docker/0/docker_cover_hue12353db563619e41ee3a11307d3cf25_62602_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/docker/0/","title":"0. 컨테이너와 도커"},{"content":"MySQL 서버는 서버의 상태를 진단할 수 있는 많은 도구들을 지원하지만 이러한 기능은 많은 지식을 필요로 하는 경우가 많다. 로그 파일을 활용하면 MySQL 서버에 대해 깊은 지식이 없어도 서버의 상태나 부하를 일으키는 원인을 쉽게 찾아 해결할 수 있다.\n에러 로그 파일 MySQL이 실행되는 도중 발생하는 에러나 경고 메시지가 출력되는 로그 파일이다. MySQL 설정 파일(my.cnf)에서 log_error라는 이름의 파라미터로 정의된 경로에 생성되는데 설정 파일에 별도로 정의도지 않은 경우 데이터 디렉터리(datadir 파라미터에 설정된 디렉터리)에 .err라는 확장자가 붙은 파일로 생성된다.\nMySQL이 시작하는 과정과 곤련된 정보성 및 에러 메시지 MySQL 설정 파일을 변경하거나 데이터베이스가 비정상적으로 종료된 후 다시 시작하는 경우에는 반드시 MySQL 에러 로그 파일을 통해 설정된 변수의 이름이나 값이 명확하게 설정되고 의도한 대로 적용됐는지 확인해야 한다.\nMySQL 서버가 정상적으로 기동했고(mysqld: ready for commections 메시지 확인), 새로 변경하거나 추가된 파라미터에 대한 에러나 경고성 메시지가 없다면 정상적으로 적용된 것으로 판단하면 된다.\n무시 ignore: 서버는 정상적으로 기동하지만 해당 파라미터가 적용안됨. 실패: 에러 메시지를 출력하고 시작하지 못했다는 메시지가 노출됨 마지막으로 종료할 때 비정상적으로 종료된 경우 나타나는 트랜잭션 복구 메시지 InnoDB의 경우 MySQL 서버가 비정상적으로 종료됐다면 다시 시작되면서 완료하지 못한 트랜잭션을 정리하고 디스크에 기록되지 못한 데이터가 있다면 다시 기록하는데, 이 과정에서 간단한 메시지가 출력된다.\n복구가 불가능한 경우 해당 에러메시지를 출력하고 MySQL은 다시 종료되며, 이 단계에서 발생하는 문제는 해결하기 어려운 문제점 일 때가 많고, innodb_force_recovery 파라미터를 설정하여 재시작 해야 할 수 있다.\n쿼리 처리 도중에 발생하는 문제에 대한 에러 메시지 쿼리 도중 발생하는 문제점은 사전 예방이 어려우며, 에러 로그 파일을 검토하는 과정에서 알게 된다.\n쿼리 실행 도중 발생한 에러나 복제에서 문제가 될 만한 쿼리에 대한 경고 메시지가 에러 로그에 기록되므로 자주 검토하는 것이 숨겨진 문제점을 해결하는데 많은 도움이 될 수 있다.\n비정상적으로 종료된 커넥션 메시지(Aborted connection) 클라이언트 애플리케이션에서 정상적으로 접속 종료를 하지 못하고 프로그램이 종료된 경우 MySQL 서버의 에러 로그 파일에 이런 내용이 기록된다. (네트워크 문제로 인한 겨우 포함)\n이런 메시지가 아주 많이 기록된다면 애플리케이션의 커넥션 종료 로직을 한번 검토해볼 필요가 있다.\nmax_connect_errors 시스템 변숫값이 너무 낮게 설정된 경우 클라이언트 프로그램이 MySQL 서버에 접속하지 못하고 Host 'host_name' is blocked라는 에러가 발생 가능하며, 이러한 경우는 에러가 어떻게 발생하게 됐는지 원인을 확인하고, 문제가 없다면 해당 시스템 변수의 값을 증가시키면 해결된다.\nInnoDB의 모니터링 또는 상태 조회 명령의 결과 메시지 InnoDB 테이블 모니터링이나 락 모니터링, 또는 엔진 상태를 조회하는 명령은 상대적으로 큰 메시지를 로그 파일에 기록한다.\nInnoDB의 모니터링을 활성화 상태로 유지하는 경우에는 에러 로그 파일이 매우 커져서 파일 시스템의 공간을 많이 사용할 수 있으므로, 다시 비활성화하여 에러 로그 파일이 커지지 않게 만들어야 한다.\nMySQL의 종료 메시지 가끔 MySQL이 아무도 모르게 종료돼 있거나 재시작 되는 경우가 있는데, 이러한 경우 에러 로그 파일에서 MySQL이 마지막으로 종료되면서 출력한 메시지를 확는 것이 서버가 종료된 이유를 확인하는 유일한 방법이다.\nReceived SHOUTDOWN from user ... 메시지: 특정 유저가 종료한 경우 없거나 스택 트레이스(16진수 주소값이 잔뜩 출력되는) 메시지: 세그멘테이션 폴트로 비정상 종료된 경우 세그멘테이션 폴트로 종료된 경우 스택 트레이스 내용을 최대한 참조하여 MySQL의 버그와 연관이 있는지 조사 후 버전을 업그레이드 하거나 회피책(WorkAround)를 찾는다. MySQL \u0026ldquo;The Error Log\u0026quot;절을 확인한다. 제너럴 쿼리 로그 파일 MySQL 서버에서 실행되는 쿼리 목륵을 검토하고 싶다면, 쿼리 로그를 활성화하여 실행 쿼리를 쿼리 로그 파일로 기록하게 한 다음, 해당 파일을 검토한다.\n슬로우 쿼리 로그와는 다르게 제너럴 로그는 실행되기 전에 MySQL이 요청을 받으면 바로 기록하기 때문에 쿼리 실행 중에 에러가 발생해도 일단 로그 파일에 기록된다.\n쿼리 로그 파일의 경로는 general_log_file 파라미터에 설정되있으며, 쿼리 로그를 파일이 아닌 테이블에 저장하도록 설정할 수 있으므로 이 경우에는 테이블을 SQL로 조회해 검토해야 한다.\n1 SHOW GLOBAL VARIABLES LIKE \u0026#39;general_log_file\u0026#39;; 슬로우 쿼리 로그 서비스 운영 중에 MySQL 서버의 전체적인 성능 저하를 검사하거나 정기적인 점검을 위한 튜닝이 필요할 때 어떤 쿼리가 문제인지를 판단하는데 많은 도움을 준다.\nlong_query_time: 해당 시스템 변수에 설정한 시간 이상의 시간이 소요된 쿼리가 모두 기록된다. log_output: 슬로우 커리 로그를 파일 또는 테이블에 기록할지 설정할 수 있다. TABLE: mysql DB의 테이블에 제너럴로그나 슬로우 쿼리 로그를 테이블(slow_log, general_log)에 저장한다. FILE: 로그 내용을 디스크의 파일로 저장한다. 로그 파일의 분석이 완료되면 그 결과는 3개의 그룹으로 나뉘어 저장된다.\n슬로우 쿼리 통계 분석 결과의 최상단에 표시되며, 모든 쿼리를 대상으로 슬로우 쿼리 로그의 실행 시간(Exec time), 잠금 대기 시간(Lock time) 등에 대해 평균 및 최소/최대 값을 표시한다.\n실행 빈도 및 누적 실행 시간순 랭킹 각 쿼리별로 응답 시간과 실행 횟수를 보여주는데, pt-query-digest 명령 실행 시 --oorder-by옵션으로 정렬 순서를 변경할 수 있다.\nQuery ID는 실행된 쿼리 문장을 정규화(쿼리에 사용된 리터럴을 제거)해서 만들어진 해시 값을 의미하는데, 같은 모양의 쿼리라면 동일한 Query ID를 가지게 된다.\n쿼리별 실행 횟수 및 누적 실행 시간 상세 정보 Query ID별 쿼리를 쿼리 랭킹에 표시된 순서대로 자세한 내용을 보여준다. 랭킹별 쿼리에서는 대상 테이블에 대해 어떤 쿼리인지만을 표시하는데, 실제 상세한 쿼리 내용은 개별 쿼리의 정보를 확인해보면 된다.\n","date":"2023-04-14T16:31:23+09:00","image":"https://codemario318.github.io/post/real-mysql/4/4/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/4/4/","title":"4.4 MySQL 로그 파일"},{"content":"\n키 캐시 키 버퍼라고도 불리는 MyISAM의 키 캐시는 InnoDB의 버퍼풀과 비슷한 역할을 수행한다. 키 캐시는 인덱스만을 대상으로 작동하며, 인덱스의 디스크 쓰기 작업에 대해서만 부분적으로 버퍼링 역할을 한다.\n키 캐시 효율성 수식\n키 캐시 히트율 = 100 - (Key_reads / Key_read_requests * 100)\nKey_reads: 인덱스를 디스크에서 읽어들인 횟수를 저장하는 상태 변수 Key_read_requests: 키 캐시로부터 인덱스를 읽은 횟수를 저장하는 상태변수 1 SHOW GLOBAL STATUS LIKE \u0026#39;Key%\u0026#39;; 메뉴얼에서는 키 캐시를 이용한 쿼리의 비율(Hit rate)을 99% 로 유지할 것을 권장하며, 99% 미만이라면 키 캐시를 조금 더 크게 설정하는 것이 좋다.\n32비트 운영체제에서는 하나의 키 캐시에 4GB 이상 메모리 공간을 설정할 수 없고, 64비트 운영체제에서는 OS_PER_PROCESS_LIMIT 값에 설정된 크기만큼의 메모리를 할당할 수 있다.\n제한 값 이상의 키 캐시를 할당하고 싶다면 기본 키 캐시 이외에 별도 명명된 키 캐시 공간을 설정해야 하며, 기본 키 캐시 공간을 설정하는 파라미터는 key_buffer_size다.\n1 2 3 key_buffer_size = 4GB kbuf_board.key_buffer_size = 2GB kbuf_comment.key_buffer_size = 2GB 위 같이 설정하면 기본 키 캐시 4GB와 kbuf_board, kbuf_comment라는 이름의 키 캐시가 각각 2GB씩 생성된다.\n기본 키 캐시 영역외 키 캐시 영역은 아무런 설정이 없다면 할당만 해두고 사용하지 않아 추가된 키 캐시는 어떤 인덱스를 캐시할지 MySQL에 알려야 한다.\n1 2 CACHE INDEX db1.board, db2.board IN kbuf_board; CACHE INDEX db1.comment, db2.comment IN kbuf_comment; 운영체제의 캐시 및 버퍼 MyISAM 테이블의 인덱스는 키 캐시를 이용해 디스크를 검색하지 않고도 빠르게 검색될 수 있으나, 디스크 데이터 I/O를 성능을 위한 캐시나 버퍼링 기능은 없다. 따라서 MyISAM 테이블의 데이터 읽기나 쓰기 작업은 항상 운영체제의 디스크 읽기 또는 쓰기 작업으로 요청된다.\n운영체제의 캐시 공간은 남는 메모리를 사용하는 것이 기본 원칙이어서 남는 공간이 없다면 MyISAM 스토리지 엔진 데이터 I/O에 사용될 메모리를 확보할 수 없어 느려진다.\nMyISAM이 주로 사용되는 MySQL에서 일반적으로 키 캐시는 최대 물리 메모리의 40% 이상을 넘지 않게 설정하고, 나머지 메모리 공간은 운영체제가 자체적은 파일 시스템을 위한 캐시 공간을 마련할 수 있게 해주는 것이 좋다.\n데이터 파일과 프라이머리 키(인덱스) 구조 InnoDB 스토리지 엔진을 사용하는 테이블은 프라이머리 키에 의해서 클러스터링되어 저장되지만, MyISAM 테이블은 프라이머리 키에 의한 클러스터링 없이 데이터 파일이 Heap 공간처럼 활용된다. (프라이머리 키 값과 무관하게 INSERT되는 순서대로 데이터 파일에 저장된다.)\nMyISAM 테이블에 저장되는 레크드는 모두 ROWID라는 물리적인 주소값을 가지는데, 프라이머리 키와 세컨더리 인덱스는 모두 데이터 파일에 저장된 레코드의 ROWID 값을 포인터로 가진다.\nROWID는 두가지 방법으로 저장된다.\n고정 길이 ROWID\n자주 사용되지는 않지만 MyISAM 테이블을 생성할 때 MAX_ROWS 옵션을 사용해 명시하면 MySQL 서버는 쵀대로 가질 수 있는 레코드가 한정된 테이블을 생성한다. 레코드 개수가 한정되면 MyISAM 테이블은 ROWID값으로 4바이트 정수를 사용하여 INSERT된 순번으로 ROWID를 사용한다.\n가변 길이 ROWID\nMAX_ROWS 옵션을 사용하지 않으면 MyISAM 테이블의 ROWID는 최대 myisam_data_pointer_size 시스템 변수에 설정된 바이트 수 만큼에 공간을 사용할 수 있다. 기본값은 7이며 최소 2byte 부터 7byte 까지 가변적인 ROWID를 갖게 된다. 첫 바이트는 ROWID의 길이를 저장하는 용도로 사용되고 나머지 공간은 실제 ROWID를 저장하는데 사용한다. 가변적인 ROWID를 가지면 데이터 파일에서 레코드의 위치가 ROWID로 사용되어, 가변 길이 ROWID인 테이블일때 최대 크기 256TB(2**(8(7-1)))가질 수 있다.\n","date":"2023-04-14T15:52:19+09:00","image":"https://codemario318.github.io/post/real-mysql/4/3/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/4/3/","title":"4.3 MyISAM 스토리지 엔진 아키텍처"},{"content":"Double Write Buffer InnoDB 스토리지 엔진의 리두 로그는 공간 낭비를 막기 위해 페이지의 변경된 내용만 기록한다. 이로인해 더티 페이지를 디스크 파일로 플러시할 때 일부만 기록되는 문제가 발생하면 그 페이지의 내용은 복구할 수 없을 가능성이 있다.\n이런 현상을 파셜 페이지 또는 톤 페이지라고 하는데, 하드웨어의 오작동이나 시스템의 비정상 종료 등으로 발생할 수 있다.\n파셜 페이지(Partial-Page)\n데이터 베이스 페이지 중에서 데이터가 일부만 채워진 페이지. 레코드가 페이지의 크기보다 작을 때 발생하며, 레코드가 페이지를 벗어나지 않은 상태에서 페이지의 일부만 사용하게 된다. 톤 페이지(Tone-Page)\n디스크에 기록 중인 페이지의 기록 작업이 중간에 중단되어 발생하는 문제. 페이지 일부가 디스크에 기록되지 않아 데이터 무결성이 손상되는 문제를 일으킬 수 있다. InnoDB 스토리지 엔진은 이러한 문제를 막기 위해 Double-Write 기법을 활용한다.\nInnoDB 스토리지 엔진은 실제 데이터 파일에 변경 내용을 기록하기 전에 기록될 더티 페이지들을 묶어 시스템 테이블 스페이스의 DoubleWrite 버퍼에 기록한다.\n더티 페이지 플러싱 중 오류등으로 서버가 종료되었다면 InnoDB 스토리지 엔진은 재시작 될 때 항상 Double Write 버퍼의 내용을 데이터 파일의 페이지로 복사하게 된다.\nDoubleWrite 기능을 사용할지 여부는 Innodb_doublewrite 시스템 변수로 제어할 수 있다.\n이처럼 DoubleWrite 버퍼는 데이터의 안정성을 위해 사용되는데, HDD처럼 자기 원판이 회전하는 저장 시스템에서는 한 번의 순차 디스크 쓰기를 하는 것으로 부담스럽지 않지만 SSD처럼 랜덤 IO와 순차 IO의 비용이 비슷한 저장 시스템에서는 부담스럽다.\nSSD는 HDD와 다르게 내부적으로 물리적인 섹터 단위로 데이터를 읽고 쓰지 않는다. 따라서 메모리에 복사된 내용이 SSD의 섹터 크기보다 작은 경우에도(순차 디스크 쓰기) 여러번 기록되어야 한다.\n하지만 데이터의 무결성이 매우 중요한 서비스에서는 DoubleWrite의 활성화를 고려하는 것도 좋다. 만약 데이터베이스 서버의 성능을 위해 InnoDB 리두 로그 동기화 설정(innodb_flush_log_at_trx_commit 시스템 변수)을 1이 아닌 값으로 설정했다면, DoubleWrite도 비활성화 하는 것이 좋다.\ninnodb_flush_log_at_trx_commit\n0: 커밋 후 로그 버퍼를 디스크에 즉시 플러시 하지 않고, 로그 버퍼가 일정 수준 채워지거나 데이터베이스 서버가 종료될 때 플러시한다. 데이터 일관성이 보장되지 않을 수 있다. 1(default): 컷밋 후 로그 버퍼를 디스크에 즉시 플러시한다. 데이터 일관성은 보장하지만 디스크 IO가 부하를 발생시킬 수 있다. 2: 커밋 후 로그 버퍼를 디스크에 즉시 플러시 하지 않고, 로그를 별도 파일에 쓴 후 파일을 주기적으로 플러시 한다. 0과 1의 중간 정도로 데이터 일관성과 디스크 IO 부하 감소를 균형있게 유지할 수 있다. 언두 로그 InnoDB 스토리지 엔진은 트랜잭션과 격리 수준을 보장하기 위해 DML로 변경되기 이전 버전의 데이터를 별도로 백업한다. 이렇게 백업된 데이터를 언두 로그(Undo Log)라고 한다.\n트랜잭션 보장\n트랜잭션이 롤백되면 트랜잭션 도중 변경된 데이터를 변경 전 데이터로 복구해야 하는데, 이때 언두 로그에 백업해둔 이전 버전의 데이터를 이용해 복구한다. 격리 수준 보장\n특정 커넥션에서 데이터를 변경하는 도중 다른 커넥션에서 데이터를 조회하면 트랜잭션 격리 수준에 맞게 변경중인 레코드를 읽지 않고 언두 로그에 백업해둔 데이터를 읽어서 반환하기도 한다. 언두 로그 모니터링 언두 로그로 인해 여러가지 성능 이슈가 발생할 수 있어 모니터링이 필요하다.\n대용량 처리 트랜잭션\n1억 건의 레코드가 저장된 100GB 크기의 테이블을 DELETE로 삭제한다고 가정했을때, 언두 로그에 삭제전 값을 저장해야 하므로 언두 로그 공간은 100GB가 된다.\n장시간 활성화된 트랜잭션\n트랜잭션이 완료됐다고 해서 해당 트랜잭션이 생성한 언두 로그를 즉시 삭제할수 없을 수 있다. 먼저 시작된 트랜잭션보다 이후 발생한 트랜잭션이 완료된 경우, 먼저 시작된 완료된 트랜적션이 완료되기 전 까지 언두 로그는 삭제되지 않는다. 이러한 경우 언두 로그의 이력을 필요한 만큼 스캔해야만 필요한 레코드를 찾을 수 있기 때문에 쿼리의 성능이 전반적으로 떨어질 수 있다.\n1 2 3 4 5 6 7 8 9 /* MySQL 모든 버전 */ SHOW ENGING INNODB STATUS \\G /* MySQL 8.0 */ SELECT count FROM information_schema.innodb_metrics WHERE SUBSYSTEM=\u0026#39;transaction\u0026#39; AND NAME=\u0026#39;trx_rseg_history_len\u0026#39; ; MySQL 서버에서 실행되는 INSERT, UPDATE, DELETE 문장이 얼마나 많은 데이터를 변경하느냐에 따라 평상시 언두 로그 건수는 상이할 수 있어, 안정적인 시점의 언두 로그 건수를 확인하고 이를 기중으로 언두 로그의 급증 여부를 모니터링하는 것이 좋다.\nMySQL 서버에서 INSERT 문장으로 인한 언두 로그와 UPDATE, DELETE 문장으로 인한 언두 로그는 별도로 관리된다. UPDATE, DELETE 문장으로 인한 언두 로그는 MVCC와 데이터 복구(롤백 등)에 모두 사용되지만, INSERT 문장으로 인한 언두 로그는 롤백, 데이터 복구만을 위해 사용된다.\n언두 테이블스페이스 관리 언두 로그가 저장되는 공간을 언두 테이블스페이스(Undo Tablespace)라고 한다.\nMySQL 5.6 이전 버전에서는 언두 로그가 모두 시스템 테이블스페이스(ibdata.idb)에 저장되었었지만, 시스템 테이블스페이스의 언두 로그는 MySQL서버가 초기화될 때 생성되기 때문에 확장의 한계가 있었다. 이에 따라 5.6 버전에서는 innodb_undo_tablespaces 시스템 변수가 도입되어 별도 로그 파일을 사용할 수 있게 되었고, 8.0으로 업그레이드되면서 언두 로그는 항상 시스템 테이블스페이스 외부의 별도 로그 파일에만 기록되도록 개선되었다.\n하나의 언두 테이블스페이스는 1~128개의 롤백 세그먼트를 가지며, 롤백 세크먼트는 1개 이상의 언두 슬롯(Undo Slot)을 가진다.\n최대 동시 처리 가능한 트랜잭션의 개수는 다음 수식으로 예측할 수 있다.\n(InnDB 페이지 크기) / 16 * (롤백 세그먼트 개수) * (언두 테이블 스페이스 수)\nInnoDB 기본 설정(innodb_undo_tablespace=2, innodb_rollback_segments=128)을 사용한다면 131,072개 정도의 트랜잭션이 동시에 처리 가능해진다. 일반적인 서비스에서 이 정도까지 동시 트랜잭션이 필요하진 않겠지만 기본값으로 해서 크게 문제될 건 없다.\n언두 로그 슬롯이 부족한 경우에는 트래잭션을 시작할 수 없는 심각한 문제가 발생하기 때문에 적절히 정해야 한다.\nMySQL 8.0 부터 CREATE UNDO TABLESPACE나 DROP TABLESPACE같은 명령으로 새로운 언두 테이블 스페이스를 동적으로 추가하고 삭제할 수 있게 개선되었다.\n언두 테이블스페이스 공간을 필요한 만큼만 남기고 불필요하거나 과도하게 할당된 공간을 운영체제로 반납하는 것을 \u0026lsquo;Undo tablespace truncate\u0026rsquo;라고 하며 자동, 수동 두가지 방법이 있다.\n체인지 버퍼 RDBMS에서 레코드가 추가, 변경될 때 데이터 파일을 변경하는 작업뿐 아니라 해당 테이블에 포함된 인덱스를 업데이트하는 작업도 필요하다. 인덱스를 업데이트하는 작업은 랜덤하게 디스크를 읽는 작업이 필요하므로 테이블에 인덱스가 많다면 상당히 많은 자원을 소모하게 된다. 따라서 InnoDB는 변경해야 할 인덱스 페이지가 버퍼풀에 있으면 바로 업데이트를 수행하지만, 그렇지 않고 디스크로부터 읽어와서 업데이트해야 한다면 이를 즉시 실행하지 않고 임시 공간에 저장해 두고 바로 사용자에게 결과를 반환하는 형태로 성능을 향상시키게 되는데, 이때 사용하는 임시 메모리 공간을 체인지 버퍼(Change Buffer)라고 한다.\n사용자에게 결과를 반환하기 전에 반드시 중복 여부를 체크해야 하는 유니크 인덱스는 체인지 버퍼를 사용할 수 없다.\n체인지 버퍼에 임시로 저장된 인덱스 레코드 조각은 이후 백그라운드 스레드에 의해 병합되는데, 이 스레드를 체인지 버퍼 머지 스레드라고 한다.\nMySQL 5.5 이전 버전까지는 INSERT 작업에 대해서만 이러한 버퍼링이 가능했는데, 이후 조금씩 개선되며 INSERT, UPDATE, DELETE로 인해 키를 추가하거나 삭제하는 작업에 대해서도 버퍼링이 될 수 있게 개선되었다.\n또한 innodb_change_buffering이라는 시스템 변수가 새로 도입되어 작업의 종류별로 체인지 버퍼를 활성화할 수 있으며, 체인지 버퍼가 비효일적일 때는 체인지 버퍼를 사용하지 않게 설정할 수 있게 개선되었다.\nall: 모든 인덱스 관련 작업을 버퍼링(inserts + deletes + purges) none: 버퍼링 안함 inserts: 인덱스에 새로운 아이템을 추가하는 작업만 버퍼링 deletes: 인덱스에서 기존 아이템을 삭제하는 작업(삭제됐다는 마킹 작업)만 버퍼링 changes: 인덱스에 추가하고 삭제하는 작업만(inserts + deletes) 버퍼링 purges: 인덱스 아이템을 영구적으로 삭제하는 작업만 버퍼링(백그라운드 작업) 체인지 버퍼는 기본적으로 InnoDB 버퍼풀로 설정된 메모리 공간의 25%까지 활용할 수 있게 설정돼있으며, 필요하다면 50%까지 설정할 수 있다. innodb_change_buffer_max_size 시스템 변수에 비율을 조정하여 바꿀 수 있다.\n리두 로그 및 로그 버퍼 리두 로그는 트랜잭션의 4가지 요소인 ACID 중에서 D(Durable)에 해당하는 영속성과 가장 밀점하게 연관돼 있다. 리두 로그는 하드웨어나 소프트웨어 등 문제로 인해 MySQL 서버가 비정상적으로 종료됐을 때 데이터 파일에 기록되지 못한 데이터를 잃지 않게 해주는 안전장치이다.\n대부분 데이터베이스 서버는 데이터 변경 내용을 로그로 먼저 기록한다. 대부분 DBMS에서 데이터 파일은 쓰기보다 읽기 성능을 고려한 자료 구조를 가지고 있기 때문에 데이터 파일 쓰기는 디스크의 랜덤 액세스가 필요하여 상대적으로 큰 비용이 필요하다.\n이로 인한 성능 저하를 막기 위해 쓰기 비용이 낮은 자료구조인 리두 로그를 가지고 있으며, 비정상 종료가 발생하면 리두 로그의 내용을 이용해 데이터 파일을 다시 서버가 종료되기 직전 상태로 복구한다.\n또한 성능을 위해 리두 로그를 버퍼링 할 수 있는 InnoDB 버퍼풀이나, 리두 로그를 버퍼링할 수 있는 로그 버퍼와 같은 자료 구조도 가지고 있다.\nMySQL 서버가 비정상으로 종료되는 경우 InnoDB 스토리지 엔진의 데이터 파일은 두 가지 일관되지 않은 데이터를 가질 수 있다.\n커밋됐지만 데이터 파일에 기록되지 않은 데이터 롤백됐지만 데이터 파일에 이미 기록된 데이터 리두로그를 활용하여 변경이 커밋, 롤백, 트랜잭션의 실행 중간 상태였는지 확인하고, 적절히 처리한다.\n데이터베이스 서버에서 리두 로그는 트랜잭션이 커밋되면 즉시 디스크로 기록되도록 시스템 변수를 설정하는 것을 권장한다. 그래야만 서버가 비정상적으로 종료되었을때 직전까지의 트랜잭션 커밋 내용이 리두 로그에 기록될 수 있고, 그 리두 로그를 이용해 장애 직전 시점까지 복구가 가능해진다.\n하지만 트랜잭션이 커밋될 때마다 리두 로그를 디스크에 기록하면 부하가 생길 수 있어, InnoDB 스토리지 엔진에서 리두 로그를 어느 주기로 디스크에 동기화할지를 결정하는 innodb_flush_log_trx_commit 시스템 변수를 제공한다.\n0: 1초에 한 번씩 리두 로그를 디스크로 기록하고 동기화를 실행한다. 서버가 비정상 종료되면 최대 1초 동안의 트랜잭션은 커밋됐더라도 데이터는 사라질 수 있다. 1: 매번 트랜잭션이 커밋될 때마다 디스크로 기록되고 동기화까지 수행한다. 2: 트랜잭션이 커밋될 때마다 디스크로 기록은 되지만 실질적인 동기화는 1초에 한번씩 실행된다. 커밋이 되면 변경 내용이 운영체제의 메모리 버퍼로 기롤되는 것이 보장되기 때문에 MySQL 서버가 비정상 종료되더라도 트랜잭션 데이터는 사라지지 않는다. 리두 로그 파일들의 전체 크기는 버버풀의 효율성을 결정하기 때문에 신중히 결정해야한다. 리두 로그 파일의 크기는 innodb_log_file_size 시스템 변수로 결정하며, innodb_log_files_in_group 시스템 뼌수는 리두 로그 파일 개수를 결정한다.\n리두 로그 파일의 전체 크기를 버퍼풀의 크기에 맞게 설정해야 적절히 변경된 내용을 버퍼풀에 모아 한번에 디스크에 기록할 수 있다.\nACID는 데이터베이스에서 트랜잭션의 무결성을 보장하기 위해 꼭 필요한 4가지 요소(기능)을 의미한다.\nA(Atomic): 트랜잭션은 원자성 작업이어야 함. C(Consistent): 일관성 I(Isolated): 격리성 D(Durable): 영속성. 한 번 저장된 데이터는 지속적으로 유지되어야 함. 리두 로그 아카이빙 MySQL 8.0부터 InnoDB 스토리지 엔진의 리두 로그를 아카이빙 할 수 있는 기능이 추가됐다.\n백업 툴이 리두 로그 아카이빙을 사용하려면 먼저 MySQL 서버에서 아카이빙된 리두 로그가 저장될 디렉터리를 innodb_redo_log_archive_dirs 시스템 변수에 설정해야 하며, 디렉터리는 운영체제의 MySQL 서버를 실행하는 유저만 접근이 가능해야 한다.\n1 2 3 4 mkdir /var/log/mysql_redo_archive cd /var/log/mysql_redo_archive mkdir 20230413 chmod 700 20230413 1 SET GLOBAL innodb_redo_log_archive_dirs=\u0026#39;backup:/var/log/mysql_redo_archive\u0026#39;; 디렉터리가 준비되면 리두 로그 아카이빙을 시작하도록 innodb_redo_log_archive_start UDF(사용자 정의 함수)를 실행한다. 해당 UDF는 리두 로그를 아카이빙할 디렉터리에 대한 레이블과 선택적으로 서브 디렉터리 이름 총 두가지의 매개 변수를 받는다.\n1 DO innodb_redo_log_archive_start(\u0026#39;backup\u0026#39;, \u0026#39;20230413\u0026#39;); 리두 아카이빙을 종료할 때는 innodb_redo_log_archive_stop UDF를 실행한다.\n1 DO innodb_redo_log_archive_stop(); innodb_redo_log_archive_start UDF를 실행한 세션의 연결이 끊어지면 InnoDB 스토리지 엔진은 리두 로그 아카이빙을 멈추고 아카이빙 파일도 자동으로 삭제하므로 커넥션을 유지해야 하고, innodb_redo_log_archive_stop UDF를 호출하여 정상적으로 종료돼야 한다.\n리두 로그 활성화 및 비활성화 InnoDB 스토리지 엔진의 리두 로그는 MySQL 서버가 비정상 종료됐을때 데이터 파일에 기록되지 못한 트랜잭션을 복구하기 위해 항상 활성화되어있다. MySQL 서버에서 트랜잭션이 커밋돼도 데이터 파일은 즉시 디스크로 동기화되지 않는 반면, 리두 로그는 항상 디스크로 기록된다.\nMySQL 8.0 버전부터 수동으로 리두 로그를 비활성화 할 수 있어, 대용량 데이터를 한번에 적재하는 경우 사용하여 적재 시간을 단축할 수 있다.\n어댑티브 해시 인덱스 어댑티브 해시 인덱스는 InnoDB 스토리지 엔진에서 사용자가 자주 요청하는 데이터에 대해 자동으로 생성하는 인덱스로, innodb_adaptive_hash_index 시스템 변수를 이용하여 활성화, 비활성화 할 수 있다.\nInnoDB 스토리지 엔진의 대표적인 인덱스는 B-Tree로 데이터는 PK 순으로 정렬되어 관리되고, Secondary Key는 인덱스키 + PK 조합으로 정렬되어 있다. 특정 데이터를 찾기 위해 Secondary Key에서 PK를 찾고, 찾은 PK를 통해 원하는 데이터를 찾는 형태로 처리된다.\nPK 사용시 데이터에 접근되는 비용은 O(logN)이고, Secondary Key를 사용해 데이터에 접근은 PK에 대한 접근도 필요하므로 2 * O(logN)이다.\n따라서 B-Tree 자료구조 특성으로 데이터가 많아진다 하더라도 탐색 비용이 크게 증가하지 않지만, 동시에 많은 스레드에서 탐색 작업이 발생할 경우 Lock 등으로 인해 성능 저하가 발생할 수 있다.\n어댑티브 해시 인덱스는 B-Tree의 검색 시간을 줄여주기 위해 도입된 기능으로, 자주 읽히는 데이터 페이지의 키 값을 이용해 해시 인덱스를 만들고, 필요할 때마다 어댑티브 해시 인덱스를 검색해서 레코드가 저장된 데이터 페이지를 즉시 찾아갈 수 있다.\n구조 해시 인덱스는 인덱스 키 값과 해당 인덱스 키 값이 저장된 데이터 페이지 주소의 쌍으로 관리된다.\n인덱스 키 값:\nB-Tree 인덱스의 고유번호 + B-Tree 인덱스의 실제 키 값 인덱스의 고유번호가 포함되는 이유는 InnoDB 스토리지 엔진에서 어댑티브 해시 인덱스는 하나만 존재하기 때문이다. 데이터 페이지 주소:\n실제 키 값이 저장된 데이터 페이지의 메모리 주소, 버퍼풀에 로딩된 페이지의 주소를 의미 어댑티브 해시 인덱스는 버퍼풀에 올려진 데이터 페이지에 대해서만 괸리되고, 버퍼풀에서 해당 데이터 페이지가 없어지면 어댑티브 해시 인덱스에서도 해당 페이지의 정보는 사라진다.\n성능 어댑티브 해시 인덱스를 활성화 후 처리량은 2배 가까이 늘었음에도 불구하고 CPU 사용량은 오히려 떨어진다.\nInnoDB 내부잠금(세마포어)의 횟수도 획기적으로 줄어든다.\n추가로 MySQL 8.0 부터는 내부 잠금을 줄이기 위해 어댑티브 해시 인덱스의 파티션 기능을 제공하며 innodb_adaptive_hash_index_parts 시스템 변수를 통해 파티션 개수를 변경할 수 있다(기본값 8개).\n어댑티브 해시 인덱스가 성능에 많은 도음이 된다면 파티션 개수를 더 많이 설정하는 것도 도움이 될 수 있다.\n한계 상황에 따라 어댑티브 해시 인덱스가 성능 향상에 크게 도움이 되지 않는 경우도 있다.\n성능 향상에 도움이 되는 경우 디스크의 데이터가 InnoDB 버퍼풀 크기와 비슷한 경우(디스크 읽기가 많지 않은 경우) 동등 조건 검색(동등 비교 및 IN 연산)이 많은 경우 쿼리가 일부 데이터에만 집중 되는 경우 성능 향상에 크게 도움이 되지 않는 경우 디스크 읽기가 많은 경우 특정 패턴의 쿼리가 많은 경우(JOIN, LIKE 패턴 검색) 매우 큰 데이터를 가진 테이블의 레코드를 폭넓게 읽는 경우 어댑티브 해시 인덱스는 데이터 페이지를 메모리(버퍼풀) 내에서 접근하는 것을 더 빠르게 만드는 기능으로 데이터 페이지를 디스크에서 읽어오는 경우가 많은 경우 데이터베이스 서버에서는 큰 도움이 되지 않는다.\n어댑티브 해시 인덱스 또한 메모리를 사용하며, 때로는 상당히 큰 메모리 공간을 사용할 수 있다. 데이터 페이지의 인덱스 키가 해시 인덱스로 만들어져야 하기 때문에 불필요한 경우 제거되어야 한다. 활성화되면 InnoDB 스토리지 엔진이 필수적으로 검색에 활용해야 하기 때문에 불필요한 접근이 발생할 수 있다. 주의할 점 테이블 삭제(DROP), 변경(ALTER)시 해당 테이블이 가진 모든 데이터 페이지의 내용을 어댑티브 해시 인덱스에서 제거해야한다. 이로 인해 테이블이 삭제되거나 스키마가 변경되는 동안 상당히 많은 CPU 자원을 사용하게되어 데이터베이스 서버의 처리 성능이 떨어진다.\n모니터링 MySQL 서버의 상태 값들을 통해 어댑티브 해시 인덱스가 불필요한 오버헤드만 만들고 있는지 확인할 수 있다.\n1 2 3 4 5 6 7 8 SHOW ENGINE INNODB STATUS\\G /* Hash table size 8747, node heap has 1 buffers(s) Hash table size 8747, node heap has 0 buffers(s) ... 1.03 hash searches/s, 2.64 non-hash searches/s /* searches: 쿼리가 처리되기 위해 내부적으로 키 값의 검색이 몇 번 실행되었는지를 의미함\n어댑티브 해시 인덱스의 효율은 검색 횟수가 아니라 해시 인덱스 히트율과 인덱스가 사용 중인 메모리 공간, 서버의 CPU 사용량을 종합해서 판단해야 한다.\n위 실행 쿼리 결과에서는 28% 정도가 어댑티브 해시 인덱스를 이용했다는 것을 알 수 있는데, 서버의 CPU 사용량이 100%에 근접한다면 효율적이라고 볼 수 있다. 하지만 CPU 사용량이 낮고 어댑티브 해시 인덱스의 메모리 사용량이 높다면 비활성화하여 버퍼풀이 더 많은 메모리를 사용할 수 있게 유도하는 것도 좋은 방법이다.\n어댑티브 해시 인덱스의 메모리 사용량은 performance_schema를 이용해서 확인 가능하다.\n1 2 3 4 5 SELECT EVENT_NAME ,CURRENT_NUMBER_OF_BYTES_USED FROM performance_schema.memory_summary_global_by_event_name WHERE EVENT_NAME=\u0026#39;memory/innodb/adaptive hash index\u0026#39; ; MyISAM, MEMORY 스토리지 엔진 비교 MyISAM MySQL 5.5부터는 InnoDB 스토리지 엔진이 기본 스토리지 엔진으로 채택 되었지만, 이전까지는 MyISAM이 기본 스토리지 엔진으로 사용되는 경우가 많았다.\nMySQL 서버의 시스템 테이블의 기본 스토리지 엔진 MySQL 8.0 부터는 MyISAM이 기본 설정되었던 서버의 시스템 테이블(사용자 인증 관련 정보, 복제 관련 정보가 저장된 mysql DB의 테이블) 등 서버의 모든 기능을 InnoDB 스토리지 엔진으로 교체되었다. 전문 검색 및 공간 좌표 검색 기능 제공. InnoDB 스토리지 엔진에서도 전문 검색과 공간 좌표 검색 기능을 모두 지원하도록 개선되었다. 이러한 이유로 MyISAM 스토리지 엔진은 InnoDB 스토리지 엔진으로 대체될 것으로 예상된다.\nMEMORY MEMORY 스토리지 엔진이 메모리라는 이름 때문에 과대 평가를 받는 경우가 있다.\n단일 스레드 처리 성능 단일 스레드 처리 성능은 MEMORY 스토리지 엔진이 빠를 수 있으나, MySQL 서버는 일반적으로 온라인 트랜잭션 처리를 위한 목적으로 사용되어 동시 처리 성능이 매우 중요하다. MEMORY 스토리지 엔진에서 동시에 많은 클라이언트 쿼리 요청이 실행되는 상황이라면 테이블 수준의 잠금으로 인해 InnoDB 스토리지 엔진을 따라갈 수 없다.\n임시 테이블 용도로 활용 MySQL 5.7 버전까지 내부 임시 테이블 용도로 활용되었으나, 가변 길이 타입의 컬럼을 지원하지 않는다는 문제점으로 MySQL 8.0 부터는 TempTable 스토리지 엔진이 대체되어 사용된다.\n이러한 이유로 MEMORY 스토리지 엔진을 선택해서 얻을 수 있는 장점이 없어져, 향후 버전에서는 제거될 것으로 예상된다.\n","date":"2023-04-13T12:39:01+09:00","image":"https://codemario318.github.io/post/real-mysql/4_2_3/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/4_2_3/","title":"4.2 InnoDB 스토리지 엔진 아키텍처(3)"},{"content":"버퍼풀은 InnoDB 스토리지 엔진의 핵심으로 디스크에서 데이터를 읽어 메모리에 보관하고, 필요할 때 메모리에서 데이터를 읽어와 처리하는 역할을 수행한다. 또한 디스크와 메모리 사이에서 데이터 읽기 및 쓰기를 관리하여 데이터 베이스의 성능을 향상시킨다.\n디스크의 데이터 파일이나 인덱스 정보를 메모리에 캐시해 두는 공간이다. 쓰기 작업을 지연시켜 일괄 작업으로 처리할 수 있게 해주는 버퍼 역할도 수행한다. 일반적인 애플리케이션에서는 INSERT, UPDATE, DELETE처럼 데이터를 변경하는 쿼리는 데이터 파일의 흩어져있는 레코드를 변경하기 때문에 랜덤한 디스크 작업을 발생시킨다. 변경을 모아 처리하여 랜덤 디스크 접근 작업 수를 줄일 수 있다.\n버퍼풀의 크기 설정 운영체제와 각 클라이언트 스레드가 사용할 메모리도 충분히 고려하여 설정한다. MySQL 서버 내에서 메모리를 필요로 하는 부분은 크게 없지만 아주 독특한 경우 레코드 버퍼가 상당한 메모리를 사용하기도 한다.\n레코드버퍼\n각 클라이언트 세션에서 테이블의 레코드를 읽고 쓸 때 버퍼로 사용하는 공간으로 커넥션이 많고 사용하는 테이블도 많다면 레코드 버퍼 용도로 사용되는 메모리 공간이 많이 필요할 수 있다.\nMySQL 서버가 사용하는 레코드 버퍼 공간은 별도로 설정할 수 없어, 전체 커넥션 개수와 각 커넥션에서 읽고 쓰는 테이블의 개수에 따라 결정되고, 동적으로 해제되기도 하므로 정확히 필요한 메모리의 크기를 계산할 수 없다.\n버퍼풀 동적 크기 조절 MySQL 5.7 버전부터 InnoDB 버퍼풀의 크기를 동적으로 조절할 수 있게 개선되어 가능하면 InnoDB 버퍼풀의 크기를 적절히 작은 값으로 설정하고 상황을 봐가며 증가시키는 방법이 최적이다.\ninnodb_buffer_pool_size 시스템 변수로 크기를 설정할 수 있으며, 동적으로 버퍼풀의 크기를 확장할 수 있다.\n크리티컬한 변경이므로 가능하며 MySQL 서버가 한가한 시점을 골라 실행한다. 버퍼풀의 크기를 줄이는 작업은 서비스 영향도가 매우 크므로 주의해야한다. 버퍼풀은 내부적으로 128MB 청크 단위로 쪼개어 관리되어 조절된다. 버퍼풀 나누기 InnoDB 버퍼풀은 정통적으로 버퍼풀 전체를 관리하는 잠금(세마포어)으로 인해 내부 잠금 경합을 많이 유발해왔는데, 이런 경함을 줄이기 위해 버퍼풀을 여러개로 쪼개어 관리할 수 있게 개선되었다.\n버퍼풀이 여러 개의 작은 버퍼풀로 쪼개지면서 개별 버퍼풀을 관리하는 잠금 자체도 경합이 분산되는 효과를 얻을 수 있게 된다.\ninnodb_buffer_pool_instances 시스템 변수를 이용해 버퍼풀을 여러개로 분리하여 관리할 수 있다.\n버퍼풀 구조 InnoDB 스토리지 엔진은 버퍼풀이라는 거대한 메모리 공간을 페이지 크기(innodb_page_size 시스템 변수에 설정된)의 조각으로 쪼개어 InnoDB 스토리지 엔진이 데이터를 필요로 할 때 해당 데이터 페이지를 읽어서 각 조각에 저장한다.\n버퍼풀의 페이지 조각을 관리하기 위해 LRU(least Recently Used) 리스트와, 플러시(Flush) 리스트, 프리(Free) 리스트라는 3개의 자료 구조를 관리한다.\nLRU 리스트: 디스크로부터 읽어온 페이지 저장하는 자료구조. 읽어온 페이지를 최대한 오랫동안 버퍼풀의 메모리에 유지하여 디스크 읽기를 최소화 한다.\n플러시 리스트: 디스크로 동기화되지 않은 데이터를 가진 데이터 페이지(더티 페이지)의 변경 시점 기준의 페이지 목록을 관리한다.\n프리리스트: 버퍼풀에서 실제 사용자 데이터로 채워지지 않은 비어있는 페이지들의 목록. 사용자의 쿼리가 새로벡 디스크의 데이터 페이지를 읽어와야 하는 경우 사용된다.\nLRU 리스트 구조 LRU 리스트는, Old 서브리스트 영역은 LRU, New 서브리스트 영역은 MRU(Most Recently Used)가 합쳐진 방식으로 동작한다.\nNew 서브리스트의 Tail과 Old 서브리스트의 Head가 만나는 지점을 MidPoint라 하며 버퍼풀에 새로운 페이지가 들어올 경우 Old 서브리스트의 Head 부분에 저장한다.\nNew 서브리스트와 Old 서브리스트로 나눈 이유는?\n하나의 큐를 사용하여 페이지를 관리할 경우에 Head 또는 Tail에 페이지를 저장하는 방식을 생각해 볼 수 있다. Head에 저장될 경우 새로 관리되는 페이지가 사용되지 않더라도 오랜 시간동안 리스트에 남아 있게되고, Tail에 저장될경우 해당 페이지를 즉시 읽지 않는다면 리스트에 남아있지 않게 되어 의미가 없어질 수 있다.\n이를 위해 두개의 서브리스트로 나누고, 경험적으로 얻은 5/8 지점을 활용한 중간점 삽입 전략을 사용하는 것 같다.\n데이터를 찾는 과정 필요한 레코드가 저장된 데이터 페이지가 버퍼풀에 있는지 검사\nInnoDB 어댑티브 해시 인덱스를 이용해 페이지를 검색 해당 테이블의 인덱스(B-Tree)를 이용해 버퍼풀에서 페이지를 검색 버퍼풀에 이미 데이터 페이지가 있다면 해당 페이지의 포인터를 MRU 방향으로 승급 디스크에서 필요한 데이터 페이지를 버퍼풀에 적재하고, 적재된 페이지에 대한 포인터를 LRU 헤더 부분에 추가\n버퍼풀의 LRU 헤더에 적재된 데이터 페이지가 실제로 읽히면 MRU 헤더 부분으로 이동(Read Ahead와 같이 대량 읽기의 경우 디스크 페이지가 버퍼풀로 적재는 되지만 실제 쿼리에서 사용되지는 않을수도 있으며, 이련 경우는 MRU로 이동되지 않음)\n버퍼풀에 상주하는 데이터 페이지는 사용자 쿼리가 얼마나 최근에 접근했었는지에 따라 나이가 부여되며, 버퍼풀에 상주하는 동안 쿼리에서 오랫동안 사용되지 않으면 오래된 페이지는 버퍼풀에서 제거됨. 버퍼풀의 페이지가 쿼리에 의해 사용되면 나이가 초기회되고 MRU헤더 부분으로 옮겨진다.\n필요한 데이터가 자주 접근됐다면 해당 페이지의 인덱스 키를 어댑티브 해시 인덱스에 추가\n처음 한번 읽힌 데이터 페이지가 이후 자주 사용된다면 버퍼풀의 MRU 영역에서 살아남게 되고, 그렇지 않은 경우 새롭게 읽히는 데이터 페이지에 밀려 결과적으로 버퍼풀에서 제거된다.\n버퍼풀과 리두 로그 버퍼풀은 데이터 베이스 서버의 성능 향상을 위해 데이터 캐시와 쓰기 버퍼링이라는 두가지 용도가 있다. 따라서 메모리가 허용하는 만큼 크게 설정하면 데이터 캐시 공간을 키워 쿼리의 성능 높힐 수 있지만, 쓰기 버퍼링 성능 향상을 위해서는 버퍼풀과 리두 로그의 관계에 대해 이해하는 것이 중요하다.\n리두 로그(Redo Log)란? 리두 로그는 데이터베이스에 대한 모든 변경 내용을 기록하는 파일셋이다. 시스템 장애나 충돌이 발생했을 때 데이터의 내구성과 일관성을 보장하기 위해 사용된다.\n데이터베이스에 변경 사항이 발생하면, 먼저 선로깅(write-ahead logging)프로세스로 리두 로그에 쓰여지고 변경 내용이 리두 로그에 기록되면 데이터베이스에 적용된다.\n리두 로그 파일은 일반적으로 디스크에 저장되며, MySQL은 변경 사항을 순차적으로 기록한다. 리두 로그 파일이 가득 차면 MySQL은 \u0026ldquo;체크포인트(checkpoint)\u0026ldquo;를 수행하여 데이터베이스의 모든 더티 페이지(버퍼풀에서 수정된 페이지)를 디스크에 기록한 후, 로그 파일을 잘라내어 공간을 확보하게 된다.\nMySQL에서 리두 로그는 원형 버퍼 형식으로 저장되며, 채워지면 다음 변경 사항은 순환 방식으로 다음 사용 가능한 리두 로그 파일에 기록된다. 이를 통해 리두 로그에 변경 사항이 지속적으로 기록되어 시스템 충돌이나 장애가 발생하더라도 변경 사항을 손실하지 않도록 보장한다.\n버퍼풀과 리두 로그의 관계 버퍼풀은 디스크에서 읽은 상태로 전혀 변경되지 않은 클린 페이지(Clean Page)와 함께 INSERT, UPDATE, DELETE를 통해 변경된 데이터를 가진 더티 페이지(Duty Page)를 가지고 있다.\n데이터 변경이 발생하면 먼저 리두 로그에 기록되고 리두 로그는 더티 페이지와 대응하게 된다.\n데이터 변경이 반복되면 결국 리두 로그 파일을 기록할 수 없거나 버퍼풀 용량이 부족해지는데, 이를 대응하기 위해 체크포인트를 수행하여 모든 더티페이지를 디스크에 기록한 후, 리두 로그 파일을 잘라내어 공간을 확보한다.\n이러한 방식이 버퍼풀이 쓰기 버퍼의 역할을 수행하게 만들게 되는데, 이에 따라서 리두 로그 파일의 크기가 작을수록 버퍼풀의 크기가 크더라도 대응되는 더티 페이지가 적으므로 버퍼링으로 얻을 수 있는 효과가 적어지고, 리두 로그 파일이 클수록 체크포인트를 통해 디스크에 기록되는 데이터가 많아져 갑자기 많은 디스크 I/O를 발생 시킬 수 있다.\n따라서, 리두 로그 파일의 크기를 적절히 선택해야하며, 어렵다면 버퍼풀의 크기가 100GB 이하의 MySQL 서버에서는 리두 로그 파일의 전체 크기를 대략 5~10GB 수준으로 선택하고 필요할 때마다 조금씩 늘려가며 최적값을 찾는 것이 좋다.\n버퍼풀 플러시(Buffer Pool Flush) 버퍼풀에서 수정된 데이터 페이지를 디스크로 쓰는 과정으로 MySQL 5.6 버전까지는 InnoDB 스토리지 더티 페이지 플러시 기능이 급작스럽게 디스크 기록이 폭증해서 MySQL 서버의 사용자 쿼리 처리 성능에 영향을 받는 등 그다지 부드럽게 처리되지 않았다.\nMySQL 5.7 버전을 거쳐 8.0 버전으로 업그레이드되면서 대부분의 서비스에서는 더티 페이지 프러시에서 예전과 같이 폭증 현상은 발생하지 않았다. 따라서 InnoDB 스토리지 엔진의 더티 페이지 플러시 성능 문제가 발생하지 않는다면 관련 시스템 변수는 조절하지 않아도 괜찮다.\nInnoDB 스토리지 엔진은 버퍼풀에서 아직 디스크로 기록되지 않은 더티 페이지들을 성능상 악영향 없이 디스크에 동기화하기 위해 다음과 같이 2개의 플러시 기능을 백그라운드로 실행한다.\n플러시 리스트(Flush_list) 플러시 LRU 리스트(LRU_list) 플러시 플러시 리스트 플러시 InnoDB 스토리지 엔진은 리두 로그 공간의 재활용을 위해 주기적으로 오래된 리두 로그 엔트리가 사용하는 공간을 비운다. 이때 오래된 리두 로그 공간이 지워지려면 반드시 InnoDB 버퍼풀의 더티 페이지가 먼저 디스크로 동기화 돼야 한다.\n이를 위해 InnoDB 스토리지 엔진은 주기적으로 플러시 리스트(Flush_list) 플러시 함수를 호출하여 플러시 리스트에서 오래전에 변경된 데이터 페이지 순서대로 디스크에 동기화 하는 작업을 수행한다.\n이때 언제부터 얼마나 많은 더티 페이지를 한번에 디스크로 기록하느냐에 따라 사용자의 쿼리 처리가 악영향을 받지 않으면서 부드럽게 처리된다. 리를 위해 InnoDB 스토리지 엔진은 여러 시스템 변수를 제공한다.\ninnodb_page_cleaners InnoDB 스토리지 엔진에서 더티 페이지를 디스크로 동기화하는 스레드를 클리너 스레드(Cleaner Thread)라고 하고, 클리너 스레드의 개수를 조정할 수 있게 해준다.\n설정값이 버퍼풀 인스턴스 개수보다 많은 경우 innodb_buffer_pool_instances 설정값으로 자동으로 변경하여, 하나의 클리너 스레드가 하나의 버퍼풀 인스턴스를 처리하도록 한다.\n시스템 변수의 설정값이 버퍼풀 인스턴스 개수보다 적은 경우 하나의 클리너 스레드가 여러 개의 버퍼풀 인스턴스를 처리하므로, innodb_page_cleaners 설정값은 innodb_buffer_pool_instances 설정 값과 동일하게 설정하는 것이 좋다.\ninnodb_max_dirty_pages_pct InnoDB 버퍼풀은 클린 페이지와 더티 페이지를 함께 가지고 있어 뭏란정 더티 페이지를 그대로 유지할 수 없다. 기본적으로 InnoDB 스토리지 엔진은 전체 버퍼풀이 가진 페이지의 90%까지 더티페이지를 가질 수 있는데, innodb_max_dirty_pct 시스템 변수를 이용해 더티페이지의 비율을 조절할 수 있다.\n일반적으로 버퍼풀이 더티페이지를 많이 가지고 있을수록 디스크 쓰기 작업을 버퍼링함으로써 I/O 작업을 줄일 수 있으므로 기본값으로 유지하는 것이 좋다.\ninnodb_max_dirty_pages_pct_lwm InnoDB 스토리지 엔진은 innodb_io_capacity 시스템 변수에 설정된 값을 기준으로 더티 페이지 쓰기를 실행하는데, 디스크로 기록되는 더티페이지 개수보다 더 많은 더티페이지가 발생하면 버퍼풀에 더티페이지가 계속 증가하게 되고, 지정한 비율이 넘어가면 더티페이지를 디스크로 기록하여 디스크 쓰기 폭발(Dist IO Bust) 현상이 발생할 가능성이 있다.\n이런 문제를 완화하기 위해 innodb_max_dirty_pages_pct_lwm 시스템 설정 변수를 이용해 일정 수준 이상의 더티페이지가 발생하면 조금씩 더티 페이지를 디스크로 기록한다.\n기본값은 10% 정도로, 디스크 쓰기가 많이 발생하고 더티 페이지 비율이 낮은 상태를 유지한다면 높은 값으로 조정할 수 있다.\ninnodb_io_capacity, innodb_io_capacity_max 데이터베이스 서버에서 어느정도의 디스크 IO가 가능한지 설정하는 값이다. innodb_io_capacity는 일반적인 상황에서 디스크가 적절히 처리할 수 있는 수준의 값을 설정하며, innodb_io_capacity_max는 디스크가 최대 성능을 발휘할 때 어느 정도 IO가 가능한지를 설정한다.\n여기서 언급되는 IO는 InnoDB 스토리지 엔진의 백그라운드 스레드가 수행하는 디스크 작업을 의미하며, 대부분 더티페이지 쓰기이다.\n스토리지 엔진은 사용자의 쿼리를 처리하기 위해 디스크를 읽기도 해야하므로 하드웨어 성능에 무조건 맞추는 것은 좋지 않다.\ninnodb_adaptive_flushing, innodb_adaptive_flushing_lwm 어댑티브 플러시를 활성화 하면 InnoDB 스토리지 엔진은 버퍼불의 더티 페이지 비율이나 innodb_io_capacity, innodb_io_capacity_max 설정 값에 의존하지 않고 알고리즘을 사용한다.\n더티 페이지는 리두 로그와 대응하므로, 리두 로그가 어느정도 증가하는지 분석하여 확인할 수 있다. 어댑티브 플러시 알고리즘은 리두 로그의 증가 속도를 분석하여 적절한 수준의 더티 페이지가 버퍼풀에 유지될 수 있도록 디스크 쓰기를 실행한다.\ninnodb_adaptive_flushing는 기본값이 활성이며, innodb_adaptive_flushing_lwm는 어댑티브 플러시 알고리즘 활성을 위한 활성 리두 공간의 하한 비율을 의미한다.\ninnodb_flush_neighbors 더티페이지를 디스크에 기록할 때 디스크에서 근접한 페이지 중 더티페이지가 있다면 InnoDB 스토리지 엔진이 함께 묶어 디스크로 기록하게 해주는 기능을 활성화 할지 결정한다.\n과거에는 HDD의 경우 IO 비용이 높아 최대한 줄이기 위해 만들어졌다.\n데이터 저장을 하드디스크로 하고있다면 1, 2 정도로 활성화 하고, SSD를 사용한다면 기본값인 비활성으로 유지하는 것이 좋다.\nLRU 리스트 플러시 InnoDB 스토리지 엔진은 LRU 리스트에서 사용 빈도가 낮은 데이터 페이지들을 제거하여 새로운 페이지들을 읽어올 공간을 만들어야 하는데, 이를 위해 LRU 리스트 플러시 함수가 사용된다.\n리스트 끝부분 부터 시작하여 최대 innodb_lru_scan_depth 시스템 변수에 설정된 수만큼의 페이지들을 스캔하는데, 이때 더티체이지는 디스크에 동기화하고, 클린 페이지는 즉시 프리 리스트로 페이지를 옮긴다.\nInnoDB 스토리지 엔진은 버퍼풀 인스턴스 별로 최대 innodb_lru_scan_depth 개수만큼 스캔하기 때문에 실질적으로 LRU 리스트의 스캔은 (innodb_buffer_pool_instances * innodb_lru_scan_depth) 수만큼 수행하게 된다.\n버퍼풀 상태 백업 및 복구 InnoDB 서버의 버퍼풀은 쿼리의 성능에 매우 밀접하게 연결돼 있다. 서버 재실행시 버퍼풀에 쿼리들이 사용할 데이터가 없어 성능이 매우 떨어지게 된다.\n디스크의 데이터가 버퍼풀에 적재돼 있는 상태를 위밍업(Warming Up)이라고 표현하는데, 워밍업 상태에 따라 몇십 배 쿼리 처리속도 차이가 발생하게 된다.\nMySQL 5.5 버전은 재실행시 강제 워밍업을 위해 주요 테이블과 인덱스에 대해 풀스캔을 실행하고 서비스를 오픈했었다. 하지만 5.6 버전부터는 버퍼풀 덤프 및 적재 기능이 도입되어 MySQL 서버 셧다운 전 innodb_buffer_pool_dump_now 시스템 변수를 이용해 현재 InnoDB 버퍼풀 상태를 백업할 수 있다.\n1 2 3 4 5 /* 버퍼풀 상태 백업 */ SET GLOBAL innodb_buffer_pool_dump_now=ON; /* 백업된 버퍼풀 상태 복구 */ SET GLOBAL innodb_buffer_pool_load_now=ON; 버퍼풀 백업을 수행하면 데이터 디렉터리에 ib_buffer_pool이라는 파일로 생성되는데, InnoDB 스토리지 엔진이 버퍼풀의 LRU 리스트에서 적재된 데이터 페이지의 메타 정보만 가져와 저장하여, 버퍼풀이 크다고 하더라도 몇십 MB 이하로 작다.\n하지만 버퍼풀로 복구하는 과정에서 각 테이블의 데이터 페이지를 디스크에서 다시 읽어와야 하기 때문에 버퍼풀의 크기에 따라 매우 오래 걸릴 수 있다.\n1 SHOW STATUS LIKE \u0026#39;Innodb_buffer_pool_dump_status\u0026#39;\\G InnoDB의 버퍼풀을 복구하는 작업은 상당히 많은 디스크 읽기를 필요로 하기 때문에, 복구중 서비스 재개하는 것은 좋지 않을 수 있다. 버퍼풀 적재 작업을 중지하려면 innodb_buffer_pool_load_abort 시스템 변수를 통해 중지하여 재개하는 것을 권장한다.\n1 SET GLOBAL innodb_buffer_pool_load_abort=ON; 백업 및 복구 자동화 InnoDB 스토리지 엔진은 innodb_buffer_pool_dump_at_shutdown, innodb_buffer_pool_load_at_shutdown 설정을 MySQL 설정 파일에 넣으면 서버가 셧다운 되기 직전에 버퍼풀의 백업을 실행하고, MySQL 서버가 시작되면 자동으로 백업된 버퍼풀의 상태를 복구할 수 있는 기능을 제공한다.\n버퍼풀의 적재 내용 확인 MySQL 5.6 버전부터 MySQL 서버의 information_schema 데이터베이스의 innodb_buffer_page 테이블을 이용해 InnoDB 버퍼풀의 메모리에 어떤 테이블의 페이지들이 적재돼 있는지 확인할 수 있었다. 하지만 버퍼풀이 큰 경우에는 테이블 조회가 상당히 큰 부하를 일으키면서 서비스 쿼리가 많이 느려지는 문제가 있어, 실제 서비스용으로 사용되는 MySQL 서버에서는 버퍼풀의 상태를 확인하는 것이 거의 불가능했다.\nMySQL 8.0 버전에서는 information_schema 데이터베이스에 innodb_cached_indexes 테이블이 새로 추가되어, 테이블의 인덱스별로 데이터 페이지가 얼마나 InnoDB 버퍼풀에 적재돼 있는지 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 SELECT it.name table_name ,ii.name index_name ,ici.n_cached_pages n_cached_pages FROM information_schema.innodb_tables it JOIN information_schema.innodb_indexes ii ON ii.table_id = it.table_id JOIN information_schema.innodb_cached_indexes ici ON ici.index_id = ii.index_id WHERE it.name=CONCAT(\u0026#39;employees\u0026#39;, \u0026#39;/\u0026#39;, \u0026#39;employees\u0026#39;) ; 아직 MySQL 서버는 개별 인덱스별로 전체 페이지 개수가 몇 개인지는 사용자에게 알려주지 않기 때문에 information_schema의 테이블을 이용해도 테이블의 인덱스별로 페이지가 InnoDB 버퍼풀에 적재된 비율은 확인할 수가 없다.\n","date":"2023-04-12T14:27:41+09:00","image":"https://codemario318.github.io/post/real_mysql_4/2/2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_4/2/2/","title":"4.2 InnoDB 스토리지 엔진 아키텍처(2) - InnoDB 버퍼풀"},{"content":"\nInnoDB는 MySQL에서 사용할 수 있는 스토리지 엔진 중 거의 유일하게 레코드 기반 잠금을 제공하며, 그 때문에 높은 동시성 처리가 가능하고 안정적이며 성능이 뛰어나다.\n프라이머리 키에 의한 클러스터링 InnoDB의 모든 테이블은 기본적으로 프라이머리 키를 기준으로 클러스터링되어 자장된다.\n프라이머리 키 값의 순서대로 디스크에 저장되며, 모든 세컨더리 인덱스는 레코드의 주소 대신 프라이머리 키의 값을 논리적인 주소로 사용한다. 프라이머리 키가 클러스터링 인덱스이기 때문에 프라이머리 키를 이용한 레인지 스캔은 상당히 빨리 처리될 수 있다. 쿼리의 실행계획에서 프라이머리 키는 기본적으로 다른 보조 인덱스에 비해 비중이 높게 설정된다. 외래 키 지원 외래 키에 대한 지원은 InnoDB 스토리지 엔진 레벨에서 지원하는 기능으로 MyISAM이나 MEMORY 테이블에서는 사용할 수 없다.\n외래 키는 데이터베이스 서버 운영의 불편함 때문에 서비스용 데이터베이스에서는 생성하지 않는 경우도 자주 있다. 그렇다 하더라도 개발 환경의 데이터베이스에서는 좋은 가이드 역할을 할 수 있다.\nInnoDB에서 외래 키는 부모 테이블과 자식 테이블 모두 해당 칼럼에 인덱스 생성이 필요함 변경 시에는 반드시 부모 테이블이나 자식 테이블에 데이터가 있는지 체크하는 작업이 필요하므로, 잠금이 여러 테이블로 전파됨 그로인한 데드락이 발생할 때가 많으므로 개발할때도 외래 키의 존재에 주의하는 것이 좋음 수동으로 데이터를 적재하거나 스키마 변경 등의 관리 작업이 실패할 수 있다. 부모 테이블과 자식 테이블의 관계를 명확히 파악해서 순서대로 작업한다면 문제없이 실행될 수 있지만 외래키가 복잡하게 얽힌 경우에는 간단하지 않다.\nforeign_key_checks 시스템 변수를 OFF로 설정하면 외래키 관계에 대한 체크 작업을 일시적으로 멈출 수 있다. 외래키 체크 작업을 일시적으로 멈추면 대략 레코드 적재나 삭제 등의 작업도 부가적인 체크가 필요 없기 때문에 훨씬 빠르게 처리할 수 있다.\n1 2 3 4 5 SET foreign_key_checks=OFF; /* 작업 수행 ... */ SET foreign_key_checks=ON; 외래키 체크를 일시적으로 중지한 상태에서 외래키 관계를 가진 부모 테이블의 레코드를 삭제했다면 반드시 자식 테이블의 레코드도 살제하여 일관성을 맞춰준 후 다시 외래키 체크 기능을 활성화 해야 한다.\nforeign_key_checks가 비활성화되면 외래키 관계의 부모 테이블에 대한 작업도 무시한다.(ON DELETE CASCADE, ON UPDATE CASCADE)\nMVCC - Multi Version Concurrency Control 일반적으로 레코드 레벨의 트랜잭션을 지원하는 DBMS가 제공하는 기능이며, MVCC의 가장 큰 목적은 잠금을 사용하지 않는 일관된 읽기를 제공하는 데 있다.\nInnoDB는 언두 로그(Undo log)를 이용해 이 기능을 구현한다.\n멀티 버전: 하나의 레코드에 대해 여러 개의 버전이 동시에 관리 1 2 3 4 5 6 7 CREATE TABLE member ( m_id INT NOT NULL, m_name VARCHAR(20) NOT NULL, m_area VARCHAR(100) NOT NULL, PRIMARY KEY (m_id), INDEX ix_area (m_area) ); 1 2 INSERT INTO member (m_id, m_name, m_area) VALUES (12, \u0026#39;홍길동\u0026#39;, \u0026#39;서울\u0026#39;); COMMIT; 1 UPDATE member SET m_area=\u0026#39;경기\u0026#39; WHERE m_id=12; UPDATE 문장이 실행되면 커밋 실행 여부와 관계 없이 InnoDB의 버퍼풀은 새로운 값인 ‘경기’로 업데이트 된다. 그리고 디스크의 데이터 파일에는 체크포인트나 InnoDB의 Write 스레드에 의해 새로운 값으로 업데이트돼 있을 수도 있고 아닐 수도 있다.(InnoDB가 ACID를 보장하기 때문에 일반적으로는 InnoDB의 버퍼풀과 데이터 파일은 동일한 상태라고 가정해도 무방함)\n아직 COMMIT이나 ROLLBACK이 되지 않은 상태에서 다른 사용자가 다음 같은 쿼리로 작업 중인 레코드를 조회한다면, MySQL 서버의 시스템 변수(transaction_isolation)에 설정된 격리 수준(Isolation level)에 따라 다르다.\nREAD_UNCOMMITED: InnoDB 버퍼풀이 현재 가지고 있는 변경된 데이터를 읽어서 반환한다. READ_COMMITTED, REPEATABLE_READ, SERIALIZABLE: 아직 커밋되지 않았기 때문에 InnoDB 버퍼풀이나 데이터 파일에 있는 내용 대신 변경되기 이전의 내용을 보관하고 있는 언두 영역의 데이터를 반환한다. 이러한 과정을 DBMS에서는 MVCC라고 표현한다. 즉 하나의 레코드(회원번호가 12인 레코드)에 대해 2개의 버전이 유지되고, 필요에 따라 어느 데이터가 보여지는지 여러 가지 상황에 따라 다르다.\n트랜잭션이 길어지면 언두에서 관리하는 예전 데이터가 삭제되지 못하고 오랫동안 관리되어야 하며, 자연히 언두 영역이 저장되는 시스템 테이블 스페이스의 공간이 많이 늘어나는 상황이 발생할 수 있다.\nUPDATE 쿼리가 실행되면 InnoDB 버퍼 풀은 즉시 새로운 데이터로 변경되고 기존 데이터는 언두영역으로 복사된다.\nCOMMIT: InnoDB는 더 이상의 변경 작업 없이 지금의 상태를 영구적인 데이터로 만들어 버린다. ROLLBACK: 언두 영역에 있는 백업된 데이터를 InnoDB 버퍼 풀로 다시 복구하고, 언두 영역의 내용을 삭제한다. 커밋이 된다고 언두 영역의 백업 데이터가 항상 바로 삭제되지는 않고, 언두 영역을 필요로 하는 트랜잭션이 없을때 삭제된다.\n잠금 없는 일관된 읽기 - Non-Locking Consistent Read InnoDB 스토리지 엔진은 MVCC 기술을 이용해 감금을 걸지 않고 읽기 작업을 수행한다. 잠금을 걸지 않기 때문에 InnoDB에서 읽기 작업은 다른 트랜잭션이 가지고 있는 잠금을 기다리지 않고, 읽기 작업이 가능하다.\n격리수준이 SERIALIZABLE이 아닌 READ_UNCOMMITED나 READ_COMMITED, REPEATEABLE_READ 수준인 경우 INSERT와 연결되지 않은 순수한 읽기(SELECT) 작업은 다른 트랜잭션의 변경 작업과 관계 없이 항상 잠금을 대기하지 않고 바로 실행된다.\n특정 사용자가 레코드를 변경하고 아직 커밋을 수행하지 않았다 하더라도 변경 트랜잭션이 다른 사용자의 SELECT 작업을 방해하지 않는다. 이를 ‘잠금 없는 일관된 읽기’ 라고 표현하며, InnoDB에서는 변경되기 전의 데이터를 읽기 위해 언두 로그를 사용한다.\n오랜 시간 동안 활성 상태인 트랜잭션으로 인해 MySQL 서버가 느려지거나 문제가 발생할 때가 가끔 있는데, 일관된 읽기를 위해 언두 로그를 삭제하지 못하고 계속 유지해야 하기 때문에 발생하는 문제이다.\n따라서 트랜잭션이 시작됐다면 가능한 빨리 롤백이나 커밋을 통해 트랜잭션을 완료하는 것이 좋다.\n자동 데드락 감지 InnoDB 스토리지 엔진은 내부적으로 잠금이 교착 상태에 빠지지 않았는지 체크하기 위해 잠금 대기 목록(Wait-for List)을 그래프 형태로 관리한다. InnoDB 스토리지 엔진은 데드락 감지 스레드를 통해 주기적으로 잠금 대기를 그래프를 검사해 교착 상태에 빠진 트랜잭션들을 찾아서 그중 하나를 강제 종료한다.\n트랜잭션의 언두 로그양이 적은 트랜잭션이 롤백 해도 처리한 내용이 적기 때문에 선택된다.\nInnoDB 스토리지 엔진은 상위 레이어인 MySQL 엔진에서 관리되는 테이블 잠금(LOCK TABLES 명령으로 잠긴 테이블)은 볼 수가 없어 데드락 감지가 불확실 할 수 있는데, innodb_table_locks 시스템 변수를 활성화 하면 InnoDB 스토리지 엔진 내부의 레코드 잠금뿐만 아니라 테이블 레벨의 잠금 까지 감지할 수 있게 된다.\n일반적인 서비스에서는 데드락 감지 스레드가 데드락을 찾아내는 작업은 부담되지 않지만, 동시 처리 스레드가 매우 많아지거나 트랜잭션이 가진 잠금 개수가 많아지면 데드락 감지 스레드가 느려진다.\n데드락 감지 스레드는 잠금 목록을 검사해야 하기 때문에 잠금 상태가 변경되지 않도록 잠금 목록이 저장된 리스트(잠금 테이블)에 새로운 잠금을 걸고 데드락 스레드를 찾게 되는데, 데드락 감시 스레드가 느려지면 서비스 쿼리를 처리중인 스레드는 더는 작업을 진행하지 못하고 대기하며 서비스에 악영항을 미치게 된다. 이렇게 동시 처리 스레드가 매우 많은 경우 데드락 감지 스레드는 더 많은 CPU 자원을 소모할 수도 있다.\ninnodb_deadlock_detect 시스템 변수를 활용하여 데드락 감지 스레드를 비활성화 할 수 있다. 이럴 경우 데드락 상황 발생시 무한정 대기할 수도 있지만, innodb_lock_wait_timeout 시스템 변수를 활성화하면 일정 시간이 지났을 경우 요청 실패하고 에러 메시지를 반환하게 만들 수 있다.\n데드락 감시 스레드가 부담되어 innodb_deadlock_detect를 OFF로 설장해서 비활성화 하는 경우에는 innodb_lock_time_wait_timeout을 기본값인 50초보다 훨씬 낮은 시간으로 변경하여 사용할 것을 권장한다.\n자동화된 장애 복구 InnoDB에는 손실이나 장애로 부터 데이터를 보호하기 위한 여러가지 메커니즘이 탑재돼있다. 그러한 메커니즘을 이용해 MySQL 서버가 시작될 때 완료되지 못한 트랜잭션이나 디스크에 일부만 기록된(Partial write)데이터 페이지 등에 대한 인련의 복구 작업이 자동으로 진행된다.\nInnoDB 스토리지 엔진은 매우 견고해서 데이터 파일이 손상되거나 MySQL 서버가 시작되지 못하는 경우는 거의 발생하지 않지만, 디스크나 하드웨어 이슈로 InnoDB 스토리지 엔진이 자동으로 복구를 못 하는 경우도 발생할 수 있는데, 한번 문제가 생기면 복구하기 쉽지 않다.\nInnoDB 데이터 파일은 기본적으로 서버가 시작될 때 자동 복구를 수행하며, 자동으로 복구될 수 없는 손상이 있다면 서버가 종료된다.\n장애 복구 대응 MySQL 서버의 설정 파일에 innodb_force_recovery 시스템 변수를 설정하여 시작해야 한다.\n6: 로그 파일 손상 1: 테이블의 데이터 파일이 손상 어떤 부분이 문제인지 알 수 없다면 1~6까지 변경하며 재실행 이후 서버가 가동되고 InnoDB 테이블이 인식된다면 mysqldump를 이용해 데이터를 가능한 만큼 백업하고 그 데이터로 MySQL 서버의 DB와 테이블을 다시 생성하는 것이 좋다.\nInnoDB_force_recovery 옵션 1(SRV_FORCE_IGNORE_CORRUPT):\n테이블스페이스의 데이터나 인덱스 페이지에서 손상된 부분이 발견되도 무시하고 서버를 시작한다. \u0026lsquo;Database page corruption on disk or a failed\u0026rsquo; 출력되는 경우가 많다. mysqldump나 SELECT INTO OUTFILE ...를 이용해 덤프하여 데이터베이스를 다시 구축하는 것이 좋다. 2(SRV_FORCE_NO_BACKGROUND):\n백그라운드 스레드 가운데 메인 스레드를 시작하지 않고 MySQL 서버를 시작한다. 메인 스레드가 언두 데이터를 삭제하는 과정에서 장애가 발생했을때 사용 3(SRV_FORCE_NO_TRX_UNDO):\n일반적으로 MySQL 서버는 재실행시 언두 영역의 데이터를 먼저 파일에 적용하고 리두 로그의 내용을 다시 덮어써서 장애 시점의 데이터 상태를 만들어 낸 후, 최종적으로 커밋되지 않은 트랜잭션의 작업을 롤백하지만 3으로 설정시 롤백하지 않고 그대로 나둔다. 커밋되지 않고 종료된 트랜잭션은 계속 그 상태로 남아있게 된다. 백업 후 데이터베이스를 다시 구축하는 것이 좋다. 4(SRV_FORCE_NO_IBUF_MERGE):\nInnoDB는 INSERT, UPDATE, DELETE 등의 데이터 변경으로 인한 인덱스 변경 작업을 상황에 따라 즉시처리 혹은 버퍼에 두고 나중에 처리할 수 있다. 인서트 버퍼를 통해 처리가 될 경우, 비정상 종료시 병합 될지 알 수 없기 때문에, 인서트 버퍼의 손상을 감지하면 에러를 발생시켜 MySQL 서버의 실행을 막는다. 인서트 버퍼의 내용을 무시하고 강제로 MySQL을 실행시킨다. 인서트 버퍼는 실제 데이터와 관련된 부분이 아니라, 인덱스에 관련된 부분이므로 테이블을 텀프한 후 다시 데이터베이스를 구축하면 데이터의 손실 없이 복구할 수 있다. 5(SRV_FORCE_NO_UNDO_LOG_SCAN):\nMySQL 서버가 종료되는 시점에 처리중인 트랜잭션이 있을 경우 별도의 처리 없이 커넥션을 강제로 끊어버리고 종료된다. MySQL 서버가 재실행되면 InnoDB 엔진은 언두 레코드를 이용해 데이터 페이지를 복구하고 리두 로그를 적용해 종료 시점의 상태로 만들고, 커밋되지 않은 트랜잭션에서 변경한 작업은 모두 롤백 처리한다. 이때 InnoDB 스토리지 엔진이 언두 로그를 사용할 수 없다면 에러가 발생하여 MySQL 서버가 실행될 수 없다. 언두 로그를 모두 무시하고 실행한다. MySQL 서버가 종료되던 시점에 커밋되지 않았던 작업도 모두 커밋된 것처럼 처리되어 잘못된 데이터가 남을 수 있다. 데이터를 백업하고, 데이터베이스를 새로 구축해야한다. 6(SRV_FORCE_NO_LOG_REDO):\nInnoDB 스토리지 엔진의 리두 로그가 손상되면 MySQL 서버가 실행되지 못한다. 해당 복구 모드로 실행하면 리두 로그를 무시하고 서버가 실행된다.\n트랜잭션이 커밋됐다 하더라도 리두 로그에만 기록되고 데이터 파일에 기록되지 않은 데이터는 모두 무시되므로 마지막 체크 포인트시점의 데이터만 남게 된다. 기존 InnoDB의 리두 로그는 모두 삭제 또는 백업하고 MySQL 서버를 시작하는 것이 좋다. 데이터를 백업하고 MySQL 서버를 새로 구축하는 것이 좋다. 위 방법을 수행해도 MySQL서버가 시작되지 않으면 백업을 이용해 다시 구축하는 방법밖에 없다. 백업이 있다면 마지막 백업으로 데이터베이스를 다시 구축하고, 바이너리 로그를 사용해 최대한 장애 시점까지의 데이터를 복구할 수도 있다.\n마지막 풀 백업 시점부터 장애 시점까지의 바이너리 로그가 있다면 이용하는 것이 데이터 손실이 더 적을 수 있다.\n백업은 있지만 복제의 바이너리 로그가 없거나 손실되었다면, 마지막 백업 시점가지만 복구할 수 있다.\n","date":"2023-04-11T19:15:11+09:00","image":"https://codemario318.github.io/post/real-mysql/4/2/1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/4/2/1/","title":"4.2 InnoDB 스토리지 엔진 아키텍처(1)"},{"content":"MySQL의 전체 구조 MySQL 서버는 크게 MySQL 엔진과 스토리지 엔진으로 구분할 수 있다.\n사람으로 비유하면 MySQL 엔진은 머리 역할을 담담하고, 스토리지 엔진은 손과 발의 역할을 담당한다.\nMySQL 엔진 MySQL 엔진은 요청된 SQL 문장을 분석하거나 최적화하는 등 DBMS의 두뇌에 해당하는 처리를 수행한다.\n커넥션 핸들러: 클라이언트 요청에 따라 새로운 연결을 생성하고 관리 SQL 파서 및 전처리기: SQL 쿼리를 최적화 및 실행하기 전에 구문 분석 및 전처리를 담당 옵티마이저: 쿼리의 최적화 MySQL은 표준 SQL(ANSI SQL) 문법을 지원하기 때문에 표준 문법에 따라 작성된 쿼리는 타 DBMS와 호환되어 실행될 수 있다.\n스토리지 엔진 스토리지 엔진은 실제 데이터를 디스크 스토리지에 저장하거나 디스크 스토리지로부터 데이터를 읽어오는 역할 수행한다.\nMySQL 서버에서 MySQL엔진은 하나지만 스토리지 엔진은 여러 개를 동시에 사용할 수 있다.\n1 CREATE TABLE test_table (fd1 INT, fd2 INT) ENGINE=INNODB; 위처럼 테이블이 사용할 스토리지 엔진을 지정하면 해당 테이블의 모든 읽기 작업과 변경 작업은 정의된 스토리지 엔진이 처리한다.\n각 스토리지 엔진은 성능 향상을 위해 키 캐시(MyISAM), 버퍼풀(InnoDB) 같은 기능을 내장하고 있다.\n핸들러 API MySQL 엔진의 쿼리 실행기에서 데이터를 쓰거나 읽어야 할 때는 스토리지 엔진에 쓰기 또는 읽기를 요청하는데, 이러한 요청을 핸들러 요청이라고 하며, 사용되는 API를 핸들러 API라고 한다.\nInnoDB 스토리지 엔진 또한 이 핸들러 API를 이용해 MySQL 엔진과 데이터를 주고 받는다.\n핸들러 API를 통해 발생한 작업은 아래 쿼리로 확인 가능하다.\n1 SHOW GLOBAL STATUS LIKE \u0026#39;Handler%\u0026#39;; MySQL 스레딩 구조 MySQL 서버는 프로세스 기반이 아니라 스레드 기반으로 동작한다.\nMySQL 서버에서 실행 중인 스레드 목록은 performance_schema 데이터베이스에 threads 테이블을 통해 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 SELECT thread_id ,name ,type ,processlist_user ,processlist_host FROM performance_schema.threads ORDER BY type,thread_id ; 백그라운드 스레드의 개수는 MySQL 서버의 설정 내용에 따라 가변적일 수 있다. 동일한 스레드가 2개 이상씩 보이는 것은 MySQL 서버의 설정 내용에 의해 여러 스레드가 동일 작업을 병렬로 처리하는 경우이다.\n포그라운드 스레드(클라이언트 스레드) 포그라운드 스레드는 클라이언트 연결 요청을 처리하고 데이터베이스 작업을 수행한다. 이러한 스레드는 쿼리 실행 중에 CPU 및 I/O 리소스를 사용하므로, 성능에 중요한 역할을 한다.\n포그라운드 스레드는 최소한 MySQL 서버에 접속된 클라이언트의 수만큼 존재하며, 주로 각 클라이언트 사용자가 요청하는 쿼리 문장을 처리한다.\n클라이언트 사용자가 작업을 마치고 커넥션을 종료하면, 해당 커넥션을 담당하던 스레드는 다시 스레드 캐시로 돌아간다.\n이때 이미 스레드 캐시에 일정 개수 이상의 대기중인 스레드가 있으면 스레드 캐시에 넣지 않고 스레드를 종료시켜 일정 개수의 스레드만 스레드 캐시에 존재하게 한다.\n스레드 캐시에 유지할 수 있는 최대 스레드 개수는 thread_cache_size 시스템 변수로 설정한다.\n포그라운드 스레드는 데이터를 MySQL 데이터 버퍼나 캐시로 부터 가져오며, 버퍼나 캐시에 없는 경우 직접 디스크의 데이터나 인덱스 파일로부터 데이터를 읽어와서 작업을 처리한다.\nMyISAM: 디스크 쓰기 작업까지 포그라운드 스레드가 처리 InnoDB: 데이터 버퍼나 캐시까지만 포그라운드 스레드가 처리 백그라운드 스레드 MyISAM의 경우 해당 사항이 별로 없지만, InnoDB는 다음과 같이 여러가지 작업이 백그라운드로 처리된다.\n인서트 버퍼(Insert Buffer)를 병합하는 스레드 로그를 디스크로 기록하는 스레드 InnoDB 버퍼풀의 데이터를 디스크에 기록하는 스레드 데이터 버퍼로 읽어 오는 스레드 잠금이나 데드락을 모니터링 하는 스레드 모두 중요한 역할을 수행하지만 로그 스레드와 버퍼의 데이터를 디스크로 내려쓰는 작업을 처리하는 쓰기 스레드(Write thread)가 특히 중요하다.\nMySQL 5.5 버전부터 데이터 쓰기 스레드와 데이터 읽기 스레드의 개수를 2개 이상 지정할 수 있게 됐으며, innodb_write_io_thread, innodb_read_io_threads 시스템 변수로 스레드의 개수를 설정한다.\nInnoDB에서도 데이터를 읽는 작업은 주로 클라이언트 스레드에서 처리되기 때문에 읽기 스레드는 많이 설정할 필요는 없지만, 쓰기 스레드는 아주 많은 작업을 백그라운드로 처리하기 때문에 일반적인 내장 디스크를 사용할때는 2~4 정도, DAS, SAN과 같은 스토리지를 사용할 때는 디스크를 최적으로 사용할 수 있을 만큼 충분히 설정하는 것이 좋다.\n사용자의 요청을 처리하는 도중 데이터의 쓰기 작업은 지연(버퍼링)되어 처리될 수 있지만 데이터의 읽기 작업은 절대 지연될 수 없다. 일반적인 상용 DBMS에는 대부분 쓰기 작업을 버퍼링해서 일괄 처리하는 기능이 있다.\nInnoDB: INSERT, UPDATE, DELETE 쿼리로 데이터가 변경되는 경우 데이터가 디스크의 데이터 파일로 완전히 저장될 때까지 기다리지 않아도 된다. MyISAM: 사용자 스레드가 쓰기 작업까지 함께 처리하도록 설계되어, 일반적인 쿼리는 쓰기 버퍼링 기능을 사용할 수 없다. 메모리 할당 및 사용 구조 글로벌 메모리 영역과 로컬 메모리 영역으로 구분되며, 서버 내에 존재하는 많은 스레드가 공유해서 사용하는 공간인지 여부에 따라 구분된다.\n글로벌 메모리 영역 일반적으로 클라이언트 스레드의 수와 무관하게 하나의 메모리 공간만 할당된다. 필요에 따라 2개 이상의 메모리 공간을 할당받을 수도 있지만 클라이언트의 스레드 수와 무관하며, 생성된 글로벌 영역이 N개라 하더라도 모든 스레드에 의해 공유된다.\n테이블 캐시 InnoDB 버퍼풀 InnoDB 어댑티드 해시 인덱스 InnoDB 리두 로그 버퍼 등이 대표적인 글로벌 메모리 영역이다.\n로컬 메모리 영역 세션 메모리 영역이라고도 표현하며, MySQL 서버상에 존재하는 클라이언트 스레드가 쿼리를 처리하는 데 사용하는 메모리 영역이다.\n정렬 버퍼 조인 버퍼 바이너리 로그 캐시 네트워크 버퍼 MySQL 서버에 클라이언트가 접속하면, 클라이언트 커넥션(세션)으로부터의 요청을 처리하기 위해 스레드를 하나씩 할당하게 되는데, 클라이언트 스레드가 사용하는 메모리 공간이라고 해서 클라이언트 메모리 영역이라고도 한다.\n로컬 메모리는 각 클라이언트 스레드별로 독립적으로 할당되며 절대 공유되어 사용되지 않는다.\n일반적으로 글로벌 메모리 영역의 크기는 주의해서 설정하지만 소트 버퍼와 같은 오컬 메모리 영역은 크게 신경 쓰지 않고 설정하는데, 최악의 경우 MySQL 서버가 메모리 부족으로 멈춰 버릴수도 있으므로 적절한 메모리 공간을 설정하는 것이 중요하다.\n커넥션이 열러있는 동안 계속 할당된 상태로 남아있는 경우: 커넥션 버퍼, 결과 버퍼 쿼리를 실행하는 순간에만 할당: 소트 버퍼, 조인 버퍼 플러그인 스토리지 엔진 모델 MySQL의 독특한 구조 중 대표적인 중 하나가 플러그인 모델이다.\n스토리지 엔진 검색 엔진을 위한 검색어 파서 사용자의 인증을 위한 Native Authentication, Caching SHA-2 Authentication 등 MySQL은 이미 기본적으로 많은 스토리지 엔진을 가지고 있지만, 필요에 의해 직접 스토리지 엔진을 만드는 것도 가능하다.\nMySQL에서 쿼리가 실행되는 과정을 보면 대부분 작업이 MySQL엔진에서 처리되고, 마지막 데이터 읽기, 쓰기 작업만 스토리지 엔진에 의해 처리한다.\nGROUP BY, ORDER BY 등 복잡한 처리는 스토리지 엔진 영역이 아니라 MySQL 엔진의 처리 영역인 쿼리 실행기에서 처리된다.\n스토리지 엔진에 따라 데이터 읽기/쓰기 작업 처리 방식이 크게 달라질 수 있다.\n하나의 쿼리 작업은 여러 하위 작업으로 나뉘는데, 각 하위 작업이 MySQL 엔진 영역에서 처리되는지 스토리지 엔진 영역에서 처리되는지 구분할 줄 알아야 한다.\n1 2 /* 스토리지 엔진 조회 */ SHOW ENGINES; 서버에 포함되지 않은 스토리지 엔진을 사용하려면 MySQL 서버를 다시 빌드해야 한다. 준비만 되어있다면 플러그인 형태로 빌드된 스토리지 엔진 라이브러리를 다운로드해서 끼워넣기만 하면 사용할 수 있다.\n1 2 /* 플러그인 조회 */ SHOW PLUGINS; 컴포넌트 플러그인 아키텍처는 다음과 같은 단점이 있다.\n오직 MySQL 서버와 인테페이스할 수 있고, 플러그인끼리는 통신할 수 없음 MySQL 서버의 변수나 함수를 직접 호출하기 때문에 안전하지 않음(캡슐화 안됨) 플러그인은 상호 의존 관계를 설정할 수 없어 초기화가 어려움 이러한 문제를 개선하기 위해 MySQL 8.0 부터는 기존의 플러그인 아키텍처를 대체하기 위해 컴포넌트 아키텍처가 지원된다.\n예를 들면, MySQL 5.7 버전까지는 비밀번호 검증 기능이 플러그인 형태로 제공됐지만 MySQL8.0의 비밀번호 검증 기능은 컴포넌트로 개선됐다.\n1 2 3 4 5 /* validate_password 설치 */ INSTALL COMPONENT \u0026#39;file://component_validate_password\u0026#39;; /* 설치된 컴포넌트 확인 */ SELECT * FROM mysql.component; 쿼리 실행 구조 쿼리 파서 쿼리 파서는 사용자 요청으로 들어온 쿼리 문장을 토큰(MySQL이 인식할 수 있는 최소 단위의 어휘나 기호)으로 분리해 트리 형태의 구조로 만들어 내는 작업을 의미한다.\n쿼리 문장의 기본 문법 오류는 이 과정에서 발견되고 사용자에게 오류 메시지를 전달하게 된다. 전처리기 파서 과정에서 만들어진 파서 트리를 기반으로 쿼리 문장에 구조적인 문제점이 있는지 확인한다.\n각 토큰을 테이블 이름이나 컬럼 이름, 또는 내장 함수와 같은 개체를 매핑해 해당 객체의 존재 여부와 객체의 접근 권한 등을 확인하는 과정을 수행한다.\n실제 존재하지 않거나 권한상 사용할 수 없는 개체의 토큰(컬럼, 내장 함수)은 이 단계에서 걸러진다. 옵티마이저 사용자의 요청으로 들어온 쿼리 문장을 저렴한 비용으로 가장 빠르게 처리할지를 결정하는 역할을 담당한다.\nDBMS의 두뇌에 비유되며, 옵티마이저가 더 나은 선택을 하도록 유도하는 것이 매우 중요하다.\n실행 엔진 옵티마이저가 두뇌라면 실행 엔진과 핸들러는 손과 발에 비유할 수 있다.\n옵티마이저가 GROUP BY를 처리하기 위해 임시 테이블을 사용하기로 결정했다면 아래 과정을 거칠 수 있다.\n실행 엔진이 핸들러에게 임시 테이블을 만들라고 요청 실행 엔진은 WHERE 절에 일치하는 레코드를 읽어오라고 핸들러에게 요청 읽어온 레코드들을 1번에서 준비한 임시 테이블로 저장하라고 핸들러에게 요청 데이터가 준비된 임시 테이블에서 필요한 방식으로 데이터를 읽어 오라고 핸들러에게 요청 결과를 사용자나 다른 모듈로 넘김 즉, 만들어진 계획대로 각 핸들러에게 요청해서 받은 결과를 또 다른 핸들러 요청의 입력으로 연결하는 역할을 수행한다.\n핸들러(스토리지 엔진) MySQL 서버의 가장 밑단에서 실행 엔진의 요청에 따라 데이터를 디스크로 저장하고 디스크로부터 읽어 오는 역할을 담당한다.\n핸들러는 결국 스토리지 엔진을 의미하며, MyISAM 테이블을 조작하는 경우 핸들러가 MyISAM 스토리지 엔진이 되고, InnoDB 테이블을 조작하는 경우 InnoDB 스토리지 엔진이 된다.\n복제 MySQL 서버에서 복제(Replication)는 매우 중요한 역할을 담당하며, 지금까지 MySQL 서버에서 복제는 많은 발전을 거듭해왔다.(16장)\n쿼리 캐시 쿼리 캐시는 빠른 응답을 필요로 하는 웹 기반의 응용 프로그램에서 매우 중요한 역할을 담당했다. 쿼리 캐시는 SQL의 실행 결과를 메모리에 캐시하고, 동일 SQL 쿼리가 실행되면 테이블을 읽지 않고 즉시 결과를 반환하기 때문에 매우 빠른 성능을 보였다.\n하지만 쿼리 캐시는 테이블의 데이터가 변경되면 캐시에 저장된 결과 중에서 변경된 테이블과 관련된 테이블과 관련된 것들은 모두 삭제(Invalidate)해야 하므로, 심각한 동시 처리 성능 저하를 유발한다. 또한 MySQL 서버가 발전하면서 성능이 개선되는 과정에서 쿼리 캐시는 계속된 동시 처리 성능 저하와 많은 버그의 원인이 되기도 했다.\n다수의 클라이언트가 동시에 같은 쿼리를 실행하는 경우 쿼리 캐시 락(query cache lock)이 발생 가능하다. 이는 쿼리 캐시에 새로운 결과를 저장하거나 기존 결과를 반환하기 위해 필요한 락(lock)으로, 동시 처리가 많은 시스템에서는 쿼리 캐시를 사용하지 않는 것이 더 나은 성능을 보일 수 있다.\nMySQL 5.6 이하 버전에서는 쿼리 캐시가 InnoDB 또는 NDB Cluster 스토리지 엔진을 사용하는 경우에만 동작하는데 MyISAM 스토리지 엔진을 사용하는 경우에도 쿼리 캐시를 켜면 쿼리 결과가 무한정 캐시될 수 있는 버그가 있었다. 이러한 버그는 시스템의 부하를 높일 뿐만 아니라, 캐시 메모리의 공간을 차지해 다른 쿼리의 실행에 영향을 미칠 수 있다.\n이러한 이유로 MySQL 8.0으로 올라오면서 완전히 제거되고, 관련 시스템 변수도 모두 제거되었다.\n스레드 풀 MySQL 서버 엔터프라이즈 에디션은 스레드풀 기능을 제공하지만 커뮤니티 에디션은 지원하지 않는다. 따라서 Percona Server 플러그인에서 제공하는 스레드풀 기능을 살펴본다.\n스레드풀은 내부적으로 사용자의 요청을 처리하는 스레드 개수를 줄여서 동시 처리되는 요청이 많다 하더라도 MySQL 서버의 CPU가 제한된 개수의 스레드 처리에만 집중할 수 있게 하여 서버의 자원 소모를 줄이는것이 목적이다.\n하지만 스레드풀이 실제 서비스에서 눈에띄는 성능 향상을 보여준 경우는 드물다.\n실행 중인 스레드들을 CPU가 최대한 잘 처리해낼 수 있는 수준으로 줄여서 빨리 처리하게 하는 기능으므로 스케줄링 과정에서 CPU 시간을 제대로 확보하지 못하는 경우 쿼리 처리가 더 느려지는 사례도 발생할 수 있다.\n제한된 수의 스레드만으로 CPU가 처리하도록 적절히 유도하면 CPU의 프로세서 친화도(Processor affinity)도 높히고 불필요한 컨텍스트 스위치를 줄여 오버헤드를 낮출 수 있다.\n스레드 그룹 개수 Percona Server의 스레드 풀은 기본적으로 CPU 코어의 개수만큼 스레드 그룹을 생성하며 일반적으로 CPU 코어의 개수와 맞추는것이 CPU 프로세서 친화도를 높이는 데 좋다.\nMySQL 서버가 처리해야할 요청이 생기면 스레드풀로 처리를 이관하는데, 이미 스레드풀이 처리중인 작업이 있는 경우 시스템 변수에 설정된 개수만큼 추가로 더 받아들여서 처리한다. 너무 많으면 스케줄링해야 할 수레드가 많아져 비효율적으로 작동할 수 있다.\n타이머 스레드 스레드 그룹의 모든 스레드가 일을 처리하고 있다면 스레드 풀은 해당 스레드 그룹에 새로운 작업 스레드를 추가할지, 기존 작업 스레드가 처리를 완료할 때가지 기다릴지 여부를 판단해야 한다.\n주기적으로 스레드 그룹의 상태를 체크해서 thread_pool_stall_limit 시스템 변수에 정의된 시간에 작업을 끝내지 못했다면 새로운 스레드를 생성해 스레드 그룹에 추가한다.\n모든 스레드 그룹의 스레드가 작업을 수행중이라면 시스템 변수에 설정된 개수를 넘어설 수 없어 대기해야 한다.\n응답 시간이 아주 민감한 서비스라면 시스템 변수를 적절히 낮춰 설정해야하며, 0에 가까운 값으로 설정하는 것은 좋지 않고 이런 경우는 스레드풀을 사용하지 않는 것이 좋을 수 있다.\n우선순위 큐 선순위 큐와 후순위 큐를 이용해 특정 트랜잭션이나 쿼리를 우선적으로 처리할 수 있는 기능도 제공한다. 먼저 시작된 트랜잭션 내에 속한 SQL을 빨리 처리해주면 해당 트랜잭션이 가지고 있던 잠금이 빨리 해제되고 잠금 경합을 낮춰 전체적인 처리 성능을 향상시킬 수 있다.\n트랜잭션 지원 메타데이터 데이터베이스 서버에서 테이블의 구조 정보와 스토어드 프로그램 등의 정보를 데이터 딕셔너리 또는 메타데이터라고 하는데, MySQL 서버는 5.7 버전까지 테이블의 구조를 FRM 파일에 저장하고 일부 스토어드 프로그램 또한 파일 기반으로 관리 되었다.\n이러한 파일 기반의 메타데이터는 생성 및 변경 작업이 트랜잭션을 지원하지 않기 때문에 테이블의 생성 또는 변경 도중에 MySQL 서버가 비정상적으로 종료되면 일관되지 않은 상태로 남게되는 문제가 있었다.\n이에따라 8버전 부터는 테이블의 구조 정보나 스토어드 프로그램의 코드 관련 정보를 모두 InnoDB의 테이블에 저장하도록 개선되었다.\n","date":"2023-04-10T00:00:00Z","image":"https://codemario318.github.io/post/real-mysql/4/1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real-mysql/4/1/","title":"4.1 MySQL 엔진 아키텍처"}]