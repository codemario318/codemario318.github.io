[{"content":"문제 설명 고객의 약관 동의를 얻어서 수집된 1~n번으로 분류되는 개인정보 n개가 있습니다. 약관 종류는 여러 가지 있으며 각 약관마다 개인정보 보관 유효기간이 정해져 있습니다. 당신은 각 개인정보가 어떤 약관으로 수집됐는지 알고 있습니다. 수집된 개인정보는 유효기간 전까지만 보관 가능하며, 유효기간이 지났다면 반드시 파기해야 합니다.\n예를 들어, A라는 약관의 유효기간이 12 달이고, 2021년 1월 5일에 수집된 개인정보가 A약관으로 수집되었다면 해당 개인정보는 2022년 1월 4일까지 보관 가능하며 2022년 1월 5일부터 파기해야 할 개인정보입니다. 당신은 오늘 날짜로 파기해야 할 개인정보 번호들을 구하려 합니다.\n모든 달은 28일까지 있다고 가정합니다.\n오늘 날짜를 의미하는 문자열 today, 약관의 유효기간을 담은 1차원 문자열 배열 terms와 수집된 개인정보의 정보를 담은 1차원 문자열 배열 privacies가 매개변수로 주어집니다. 이때 파기해야 할 개인정보의 번호를 오름차순으로 1차원 정수 배열에 담아 return 하도록 solution 함수를 완성해 주세요.\n제한사항 today는 YYYY.MM.DD 형태로 오늘 날짜를 나타냅니다. 1 ≤ terms의 길이 ≤ 20 terms의 원소는 \u0026ldquo;약관 종류 유효기간\u0026rdquo; 형태의 약관 종류와 유효기간을 공백 하나로 구분한 문자열입니다. 약관 종류는 A~Z중 알파벳 대문자 하나이며, terms 배열에서 약관 종류는 중복되지 않습니다. 유효기간은 개인정보를 보관할 수 있는 달 수를 나타내는 정수이며, 1 이상 100 이하입니다. 1 ≤ privacies의 길이 ≤ 100 privacies[i]는 i+1번 개인정보의 수집 일자와 약관 종류를 나타냅니다. privacies의 원소는 \u0026ldquo;날짜 약관 종류\u0026rdquo; 형태의 날짜와 약관 종류를 공백 하나로 구분한 문자열입니다. 날짜는 YYYY.MM.DD 형태의 개인정보가 수집된 날짜를 나타내며, today 이전의 날짜만 주어집니다. privacies의 약관 종류는 항상 terms에 나타난 약관 종류만 주어집니다. today와 privacies에 등장하는 날짜의 YYYY는 연도, MM은 월, DD는 일을 나타내며 점(.) 하나로 구분되어 있습니다. 2000 ≤ YYYY ≤ 2022 1 ≤ MM ≤ 12 MM이 한 자릿수인 경우 앞에 0이 붙습니다. 1 ≤ DD ≤ 28 DD가 한 자릿수인 경우 앞에 0이 붙습니다. 파기해야 할 개인정보가 하나 이상 존재하는 입력만 주어집니다. 분석 1번 문제는 문자열 처리, 기본 자료구조 등을 활용한 단순 구현 문제였습니다. 대략적인 흐름은 아래와 같습니다.\ntoday를 비교할 수 있는 형태로 변환 terms의 약관에 따라 접근 할 수 있는 자료 구조로 저장 privacies의 원소 별 개인정보 수집 일자를 계산할 수 있는 형태로 변환하고, 약관에 따라 만료일 계산 비교할 수 있는 형태로 변환된 today와 비교하여 만료 여부 확인 today와 privacies의 약관에 따라 계산된 개인 정보 수집 일자를 비교하는 처리가 필요했습니다. 따라서, 이 문제의 중요한 포인트는 문자열로 받은 날짜에 대한 처리였습니다.\nterms의 유효기간은 \u0026ldquo;월\u0026rdquo; 단위로만 입력을 받기 때문에 언어의 날짜 계산 툴을 활용해도 문제는 없을 것이라 생각되었습니다. 하지만 파이썬은 날짜에 \u0026ldquo;월\u0026quot;을 더하기 위해서 dateutil을 사용해야 했는데, 이는 기본 라이브러리가 아니기 때문에 활용할 수 없었습니다.\n다행히도 제한사항에 따라 월별 일수가 28일로 고정되어 있었기 때문에 날짜를 계산 처리는 여렵지 않다고 생각되었고, 필요한 기능을 직접 구현하기로 결정하였습니다.\n처음에는 년, 월, 일을 가진 클래스를 구상했었는데 일, 월의 경우는 더해질 경우 올림이 발생하므로 처리가 조금 번거로웠습니다. 그리고, 날짜 비교를 위해 년, 월, 일 모두 비교해야 하므로 비교 처리도 비교적 복잡해 보였습니다.\n문제에서 요구하는 결과는 해당 개인정보의 파기 여부를 확인하는 것 이었기 때문에, 단순 비교를 위해서 타임스탬프를 응용 하기로 했습니다.\n결과 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 from typing import Tuple from datetime import datetime DATE_FORMAT = \u0026#34;%Y.%m.%d\u0026#34; YEAR = 2022 MONTH = 12 DAY = 28 class CustomTimeStamp: def __init__(self, year: int, month: int, day:int) -\u0026gt; None: self.total = 0 self.add(year, month, day) def __ge__(self, customDate: object) -\u0026gt; bool: return self.total \u0026gt;= customDate.total def add(self, year: int = 0, month: int = 0, day: int = 0) -\u0026gt; None: self.__addYearToDays(year) self.__addMonthToDays(month) self.__addDay(day) def __addYearToDays(self, year: int) -\u0026gt; None: self.total += year * MONTH * DAY def __addMonthToDays(self, month: int) -\u0026gt; None: self.total += month * DAY def __addDay(self, day: int) -\u0026gt; None: self.total += day def strToCustomDate(date: str, format: str = DATE_FORMAT) -\u0026gt; object: formatDatetime = datetime.strptime(date, format) year, month, day = int(formatDatetime.year), int(formatDatetime.month), int(formatDatetime.day) return CustomTimeStamp(year, month, day) def solution(today, terms, privacies): answer = [] today = CustomTimeStamp.strToCustomDate(today) terms = dict(map(getTerm, terms)) for i, privacy in enumerate(privacies, 1): date, termType = privacy.split() month = terms[termType] expireDate = CustomTimeStamp.strToCustomDate(date) expireDate.add(month=month) if today \u0026gt;= expireDate: answer.append(i) return answer def getTerm(term: str) -\u0026gt; Tuple[str, int]: termType, strMonth = term.split() return termType, int(strMonth) CustomTimeStamp 앞서 설명드린 내용 처럼 제약사항에 따른 총 일수를 이용해 비교하는 클래스를 구현하였습니다. 유닉스 타임스탬프와 달리 입력으로 받는 년, 월, 일 만 이용해서 총 일수를 계산하여 비교적 간단히 구현할 수 있었습니다.\n__ge__ __ge__ 매직 메소드를 이용해 \u0026gt;= 연산자 오버로딩을 하여 조금 더 깔끔하게 구현될 수 있도록 해봤습니다.\nadd 인스턴스 변수인 total을 __addYearToDays, __addMonthToDays, __addDay 를 이용하여 일 수를 더합니다. 내외부에서 모두 사용되기 때문에 공통화를 위해 구현하였습니다.\n__addYearToDays, __addMonthToDays, __addDay 상수를 통해 일수를 계산하여 total에 저장합니다. 타임스탬프 계산 기준이 바뀔 경우 로직 변경이 필요할 수 있기 때문에 별도로 분리해서 구현해봤습니다.\nstrToCustomDate 입력을 편하게 처리하기 위한 정적 메소드로 datetime 패키지를 이용하여 문자열로 입력받은 일자를 년, 월, 일로 바꾸어 CustomTimeStamp의 인스턴스를 반환하는 기능을 구현했습니다.\nsolution privacies를 순회하며 CustomTimeStamp를 통해 만료일을 계산하고 오늘 날짜와 비교하는 방식으로 구현할 수 있었습니다.\n끝으로 요구사항만 만족하도록 구현했다면 구조가 훨씬 단순해졌을텐데, 구현하다보니 욕심이 생겨 일을 크게 만들어 버렸네요.\n하지만 입력되는 날짜 형식이 변경된다던가, 월 외에도 년, 일에 대해 날짜를 계산해야하는 처리가 추가되었다던가, 날짜 제한이 바뀌어 계산 처리가 바뀌어야 한다던가, 시간, 분, 초 처럼 처리되어야 하는 다른 단위가 추가된다던가 등 여러 변경사항이 있더라도 조금 더 대응하기 쉬워졌지 않았을까 생각해봅니다.\n도움이 되었기를 바라며, 끝까지 읽어주셔서 감사합니다 :)\n","date":"2023-05-16T16:51:44+09:00","image":"https://codemario318.github.io/post/ps/1/cover_hue4c9dbe3b27e9dda3cee8b23cf5bd024_526378_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/ps/1/","title":"[2023 카카오 공채 문제 풀이] 1. 개인정보 수집 유효기간"},{"content":"함수 기반 인덱스 일반적인 인덱스는 컬럼의 값 일부(걸럽의 값 앞부분) 또는 전체에 대해서만 인덱스 생성이 허용되지만, 컬럼의 값을 변형해서 만들어진 값에 대해 인덱스를 구축해야 하는 경우도 있는데, 이러한 경우 함수 기반 인덱스를 활용할 수 있다.\nMySQL 서버의 함수 기반 인덱스는 인덱싱할 값을 계산하는 과정의 차이만 있을 뿐, 실제 인덱스의 내부적인 구조 및 유지 관리 방법은 B-Tree 인덱스와 동일하다.\n가상 컬럼을 이용한 인덱스 1 2 3 4 5 6 7 CREATE TABLE user ( user_id BIGINT, first_name VARCHAR(10), last_name VARCHAR(10), PRIMARY KEY (user_id) ); 이전 버전의 MySQL은 해당 테이블에서 first_name과 last_name을 합쳐 검색해야 하는 요건이 생겼다면 full_name이라는 컬럼을 추가하고 모든 레코드에 대해 해당 컬럼을 업데이트 하는 작업을 거처야 인덱스를 생성 할 수 있었다.\n하지만 MySQL 8.0 버전 부터는 가상 컬럼을 추가하고 해당 가상 컬럼에 인덱스를 생성할 수 있게 됐다.\n1 2 3 ALTER TABLE user ADD full_name VARCHAR(30) AS (CONCAT(first_name, \u0026#39; \u0026#39;, last_name)) VIRTUAL, ADD INDEX ix_fullname (full_name); 가상 컬럼이 VIRTUAL, STORED 옵션 중 어떤 옵션으로 생성됐든 관계 없이 해당 가상 컬럼에 인덱스를 생성할 수 있다. 가상 컬럼은 테이블에 새로운 컬럼을 추가하는 것과 같은 효과를 내기 때문에 실제 테이블의 구조가 변경된다는 단점이 있다.\n함수를 이용한 인덱스 가상 컬럼은 MySQL 5.7 버전에서도 사용할 수 있었지만 함수를 직접 인덱스 생성 구문에 사용할 수는 없었다. 하지만 MySQL 8.0 버전 부터는 테이블 구조를 변경하지 않고, 함수를 직접 사용하는 인덱스를 생성할 수 있게 됐다.\n1 2 3 4 5 6 7 8 CREATE TABLE user ( user_id BIGINT, first_name VARCHAR(10), last_name VARCHAR(10), PRIMARY KEY (user_id), INDEX ix_fullname ((CONCAT(first_name, \u0026#39; \u0026#39;, last_name))) ); 함수를 직접 사용하는 인덱스는 테이블의 구조는 변경하지 않고, 계산된 결괏값의 검색을 빠르게 만들어준다. 함수 기반 인덱스를 제대로 활용하려면 반드시 조건절에 함수 기반 인덱스에 명시된 표현식이 그대로 사용돼야 한다. 함수 생성 시 명시된 표현식과 쿼리의 WHERE 조건절에 사용된 표현식이 다르다면 결과가 같다고 하더라도 MySQL 옵티마이저는 다른 표현식으로 간주해서 함수 기반 인덱스를 사용하지 못한다.\n가상 컬럼과 함수를 직접 이용하는 인덱스는 사용법과 문법에서 조금 차이가 있지만, 내부적으로 동일한 구현 방법을 사용하므로 어떤 방법을 사용하더라도 둘의 성능 차이는 발생하지 않는다.\n멀티 밸류 인덱스 전문 검색 인덱스를 제외한 모든 인덱스는 레코드 1건이 1개의 인덱스 키 값을 가지는 1:1 관계이다. 하지만 멀티 밸류 인덱스는 하나의 데이터 레코드가 여러 개의 키 값을 가질 수 있는 형태의 인덱스다. 일반적인 RDBMS를 기준으로 생각하면 이러한 인덱스는 정규화에 위배되는 형태이지만, 최근 RDBMS들이 JSON 데이터 타입을 지원하기 시작하면서 JSON의 배열 타입의 필드에 저장된 원소들에 대한 인덱스 요건이 발생한 것이다.\nJSON 포맷으로 데이터를 저장하는 MongoDB는 처음부터 이런 형태의 인덱스를 지원하고 있었지만 MySQL 서버는 멀티 밸류 인덱스에 대한 지원 없이 JSON 타입의 컬럼만 지원했다. 하지만 배열 형태에 대한 인덱스 생성이 되지 않아 MongoDB의 기능과 많이 비교되곤 했다.\nMySQL 8.0 버전으로 업그레이드되면서 JSON 관리 기능은 MongoDB에 비해서도 부족함이 없는 상태로 발전했다.\n1 2 3 4 5 6 7 8 9 10 CREATE TABLE user ( user_id BIGINT AUTO_INCREMENT PRIMARY KEY, first_name VARCHAR(10), last_name VARCHAR(10), credit_info JSON, INDEX imx_creditscores ((CAST(credit_info-\u0026gt;\u0026#39;$.credit_scores\u0026#39; AS UNSIGNED ARRAY))) ); INSERT INTO user VALUES (1, \u0026#39;mario\u0026#39;, \u0026#39;lee\u0026#39;, \u0026#39;{\u0026#34;credit_scores\u0026#34;:[360, 353, 351]}\u0026#39;); 멀티 밸류 인덱스를 활용하기 위해서는 일반적인 조건 방식을 사용하면 안 되고, 반드시 다음 함수들을 이용해서 검색해야 옵티마이저가 인덱스를 활용한 실행 계획을 수립한다.\nMEMBER OF() JSON_CONTAINS() JSON_OVERLAPS() 1 SELECT * FROM user WHERE 360 MEMBER OF(credit_info-\u0026gt;\u0026#39;$.credit_scores\u0026#39;); MySQL 서버의 Worklog에는 DECIMAL, INTEGER, DATETIME, VARCHAR/CHAR 타입에 대해 멀티 밸류 인덱스를 지원한다고 명시돼 있지만 MySQL 8.0.21 버전에서는 VARCHAR/CHAR 타입에 대해서는 지원하지 않는다. 하지만 곧 VARCHAR/CHAR타입의 배열 형태 CAST와 멀티 밸류 인덱스가 지원될 것으로 예상된다.\n클러스터링 인덱스 MySQL 서버에서 클러스터링은 테이블의 레코드를 비슷한 것(프라이머리 키를 기준으로)들끼리 묶어서 저장하는 형태로 구현되는데, 이는 주로 비슷한 값들을 동시에 조회하는 경우가 많다는 점에서 착안되었다. MySQL에서 클러스터링 인덱스는 InnoDB 스토리지 엔진에서만 지원한다.\n클러스터링 인덱스 클러스터링 인덱스는 테이블의 프라이머리 키에 대해서만 적용되는 내용이다. 즉 프라이머리 키 값이 비슷한 레코드끼리 묶어서 저장하는 것을 클러스터링 인덱스라고 표현한다.\n따라서 프라이머리 키 값에 의해 레코드에 저장 위치가 결정되며, 프라이머리 키 값이 변경된다면 그 레코드의 물리적인 저장 위치가 바뀌어야 한다는 것을 의미한다.\n프라이머리 키 값으로 클러스터링 된 테이블은 프라이머리 키 값 자체에 대한 의존도가 상당히 크기 때문에 신중히 프라이머리 키를 결정해야한다.\n클러스터링 인덱스는 프라이커리 키 값에 의해 레코드의 저장 위치가 결정되므로 인덱스 알고리즘이라기 보다는 테이블 레코드의 저장 방식이라고 볼 수 있다, 그래서 클러스터링 인덱스와 클러스터링 테이블은 동의어로 사용되기도 한다. 일반적으로 InnoDB와 같이 항상 클러스터링 인덱스로 저장되는 테이블은 프라이머리 키 기반 검색이 매우 빠르며, 대신 레코드의 저장이나 프라이머리 키의 변경이 상대적으로 느리다.\n클러스터링 인덱스 구조를 보면 클러스터링 테이블의 구조 자체는 일반 B-Tree와 비슷하다. 하지만 세컨더리 인덱스를 위한 B-Tree의 리프 노드와는 달리 클러스터링 인겟스의 리프 노드에는 레코드의 모든 컬럼이 같이 저장돼 있음을 알 수 있다.\n1 UPDATE tb_test SET emp_no=10002 WHERE emp_no=10007; 프라이머리 키가 없는 InnoDB 테이블에서는 InnoDB 스토리지 엔진이 다음 우선순위대로 프라이머리 키를 대체할 컬럼을 선택한다.\n프라이머리 키가 있다면 기본적으로 프라이머리 키를 클러스터링 키로 선택 NOT NULL옵션의 유니크 인덱스 중에서 첫 번째 인덱스를 클러스터링 키로 선택 자동으로 유니크한 값을 가지도록 증가되는 컬럼을 내부적으로 추가한 후, 클러스터링 키로 선택 InnoDB 스토리지 엔진이 적절한 클러스터링 키 후보를 찾지 못하는 경우 InnoDB 스토리지 엔진이 내부적으로 레코드의 일련번호 컬럼을 생성하는데, 자동으로 추가된 프라이머리 키는 사용자에게 노출되지 않으며, 쿼리 문장에 명시적으로 사용할 수 없다. 즉 프라이머리 키나 유니크 인덱스가 전혀 없는 InnoDB 테이블에서는 아무 의미 없는 숫자 값으로 클러스터링되어 사용자에게 아무런 혜택도 주지 않는다.\n따라서 InnoDB 테이블에서 클러스터링 인덱스는 테이블당 단 하나만 가질수 있는 엄청난 혜택이므로 가능하다면 프라이머리 키를 명시적으로 생성하는 것이 좋다.\n세컨더리 인덱스에 미치는 영향 MyISAM이나 MEMORY 테이블 같은 클러스터링되지 않은 테이블은 INSERT될 때 처음 저장된 공간에서 절대 이동하지 않는다. 데이터 레코드가 저장된 주소는 내부적인 레코드 아이디(ROWID) 역할을 하며, 프라이머리 키나 세컨더리 인덱스의 각 키는 그 주소를 이용해 실제 데이터를 찾아오기 때문에 프라이머리 키와 세컨더리 인덱스는 구조적으로 아무런 차이가 없다.\nInnoDB는 클러스터링 키 값이 변경될 때마다 데이터 레코드의 주소가 변경되고 그때마다 해당 테이블의 모든 인덱스에 저장된 주솟값을 변경해야 한다. 이런 오버헤드를 제거하기 위해 InnoDB 테이블(클러스터링 테이블)의 모든 세컨더리 인덱스는 해당 레코드가 저장된 주소가 아닌 프라이머리 키 값을 저장하도록 구현돼 있다.\nInnoDB가 MyISAM보다 조금 더 복잡하게 처리되지만, InnoDB테이블에서 프라이머리 키는 더 큰 장점을 제공하기 때문에 성능 저하에 대해 걱정하지 않아도 된다.\n클러스터링 인덱스의 장점과 단점 장점\n프라이머리 키로 검색할 때 처리 성능이 매우 빠름(특히, 프라이머리 키를 범위 검색하는 경우 매우 빠름) 테이블의 모든 세컨더리 인덱스가 프라이머리 키를 가지고 있기 때문에 인덱스만으로 처리될 수 잇는 경우가 많음(커버링 인덱스) 단점\n테이블의 모든 세컨더리 인덱스가 클러스터링 키를 갖기 때문에 클러시터링 키 값의 크기가 클 경우 전체적으로 인덱스의 크기가 커짐 세컨더리 인덱스를 통해 검색할 때 프라이머리 키로 다시 한번 검색해야 하므로 처리 성능이 느림 INSERT할 때 프라이머리 키에 의해 레코드의 저장 위치가 결정되기 때문에 처리 성능이 느림 프라이머리 키를 변경할 때 레코드를 DELETE하고 INSERT 하는 작업이 필요하기 때문에 처리 성능이 느림 일반적으로 웹 서비스와 같은 온라인 트랜잭션 환경에서는 쓰기와 읽기 비율이 2:8, 1:9 정도이기 때문에 조금 느린 쓰기를 감수하고 읽기를 빠르게 유지하는 것은 매우 중요하다.\n클러스터링 테이블 사용 시 주의사항 클러스터링 인덱스 키의 크기 클러스터링 테이블의 경우 모든 세컨더리 인덱스가 프라이머리 키(클러스터링 키) 값을 포함한다. 그래서 프라이머리 키의 크기가 커지면 세컨더리 인덱스도 자동으로 크기가 커진다. 하지만 일반적으로 테이블에 세컨더리 인덱스가 4~5개 정도 생성된다는 것을 고려하며 ㄴ세컨더리 인덱스 크기는 급격히 증가한다.\n프라이머리 키의 크기 레코드당 증하가하는 인덱스 크기 100만 건 저장 시 증가하는 인덱스 크기 10바이트 10 바이트 * 5 = 50 바이트 50바이트 * 1,000,000 = 47MB 50바이트 50 바이트 * 5 = 250 바이트 250바이트 * 1,000,000 = 238MB 레코드 한 건을 생각하면 커 보이지 않을 수 있으나 레코드 건수가 100만 건만 돼도 인덱스의 크기가 거의 190MB나 증가한다. 또한 인덱스가 커질수록 같은 성능을 내기 위해 그만큼의 메모리가 더 필요해지므로 InnoDB 테이블의 프라이머리 키는 신중하게 선택해야 한다.\n프라이머리 키는 AUTO-INCREMENT 보다는 업무적인 컬럼으로 생성(가능하면) InnoDB의 프라이머리 키는 클러스터링 키로 사용되며, 이 값에 의해 레코드의 위치가 결정된다. 즉 프라이머리 키로 검색하는 경우 클러스터링 되지않은 테이블에 비해 매우 빠르게 처리될 수 있음을 의미한다. 프라이머리 키는 의미만큼이나 중요한 역할을 하기 때문에 대부분 검색에서 상당히 빈번하게 사용되는 것이 일반적이다. 그러므로 설령 컬럼의 크기가 크더라도 업무적으로 해당 레코드를 대표할 수 있다면 그 컬럼을 프라이머리 키로 설정하는 것이 좋다.\n프라이머리 키는 반드시 명시할 것 가능하면 AUTO_INCREMENT컬럼을 이용해서라도 프라이머리 키는 생성하는 것을 권장한다. InnoDB에서 프라이머리 키를 정의하지 않으면 스토리지 엔진이 내부적으로 일련번호 컬럼을 추가하지만 사용자는 전혀 접근할 수 없으므로AUTO_INCREMENT 컬럼을 생성하고 프라이머리 키로 설정하여 사용자가 활용할 수 있는 값으로 사용하는 것이 좋다. 또한 ROW 기반 복제나 InnoDB Cluster에서는 모든 테이블이 프라이머리 키를 가져야만 정상적인 복제 성능을 보장하기도 하므로 프라이머리 키는 꼭 생성해야 한다.\nAUTO-INCREMENT 컬럼을 인조 식별자로 사용할 경우 여러 개의 컬럼이 복합으로 프라이머리 키가 만들어지는 경우 프라이머리 키의 크기가 길어질 때가 가끔 있는데, 프라이머리 키의 크기가 길어도 세컨더리 인덱스가 필요하지 않다면 그대로 프라이머리 키를 사용하는 것이 좋다. 세컨더리 인덱스도 필요하고 프라이머리 키의 크기도 길다면 AUTO_INCREMENT컬럼을 추가하고 이를 프라이머리 키로 설정한다.\n이렇게 프라이머리 키를 대체하기 위해 인위적으로 추가된 프라이머리 키를 인조 식별자(Surrogate key)라고 한다. 그리고 로그 테입르과 같이 조회보다는 INSERT 위주의 테이블들은 AUTO_INCREMENT를 이용한 인조 식별자를 프라이머리 키로 설정하는 것이 성능 향상에 도움이 된다.\n유니크 인덱스 유니크는 인덱스라기보다는 제약 조건에 가깝다. 테이블이나 인덱스에 같은 값이 2개 이상 저장될 수 없음을 의미하는데, MySQL에서는 인덱스 없이 유니크 제약만 설정할 방법이 없다.\n유니크 인덱스에서 NULL도 저장될 수 있는데, NULL은 특정 값이 아니므로 2개 이상 저장될 수 있다. MySQL에서 프라이머리 키는 기본적으로 NULL을 허용하지 않는 유니크 속성이 자동으로 부여된다.\n유니크 인덱스와 일반 세컨더리 인덱스의 비교 유니크 인덱스와 유니크하지 않은 일반 세컨더리 인덱스는 인덱스 구조상 아무런 차이점이 없다. 하지만 읽기와 쓰기에 대해 성능적으로 다른 부분이 있다.\n인덱스 읽기 많은 사람들이 유니크 인덱스가 빠르다고 생각하지만, 유니크하지 않은 세컨더리 인덱스에서 한 번 더 해야 하는 작업은 디스크 읽기가 아니라 CPU에서 컬럼 값을 비교하는 작업이기 때문에 성능상 영향이 거의 없다.\n유니크하지 않은 세컨더리 인덱스는 중복된 값이 허용되므로 읽어야 할 레코드가 많아서 느린것이지, 인덱스 자체의 특성 때문에 느린 것이 아니다.\n하나의 값을 검색하는 경우, 유니크 인덱스와 일반 세컨더리 인덱스는 사용되는 실행 계획이 다르다. 하지만 이는 인덱스의 성격이 유니크한지 아닌지에 따른 차이일 뿐 큰 차이는 없다.\n인덱스 쓰기 새로운 레코드가 INSERT 되거나 인덱스 컬럼의 값이 변경되는 경우에는 인덱스 쓰기 작업이 필요하다. 그런데 유니크 인덱스의 키 값을 쓸 때는 중복된 값이 있는지 없는지 체크하는 과정이 한 단계 더 필요하다. 그래서 유니크하지 않은 세컨더리 인덱스의 쓰기보다 느리다.\nMySQL에서는 유니크 인덱스에서 중복된 값을 체크할 때는 읽기 잠금을 사용하고, 쓰기를 할 때는 쓰기 잠금을 사용하는데, 이 과정에서 데드락이 아주 빈번히 발생한다.\n또한 InnoDB 스토리지 엔진에는 인덱스 키의 저장을 버퍼링 하기 위해 체인지 버퍼(Change Buffer)를 사용하여 인덱스의 저장이나 변경 작업이 상당히 빨리 처리되지만, 유니크 인덱스는 반드시 중복 체크를 해야하므로 작업 자체를 버퍼링 하지 못한다. 이 때문에 유니크 인덱스는 일반 세컨더리 인덱스보다 변경 작업이 더 느리다.\n유니크 인덱스 사용시 주의사항 꼭 필요한 경우라면 유니크 인덱스를 생성하는 것은 당연하나, 성능이 좋아질 것으로 생각하고 불필요하게 유니크 인덱스를 생성하지는 않는 것이 좋다. 하나의 테이블에서 같은 컬럼에 유니크 인덱스와 일반 인덱스를 각각 중복해서 생성해 둔 경우가 가끔 있는데, 같은 역할을 하므로 중복해서 인ㄷ게스를 생성할 필요는 없다. 똑같은 컬럼에 대해 프라이머리 키와 유니크 인덱스를 동일하게 생성하는 경우도 있는데 이 또한 중복이다. 결론적으로 유일성이 꼭 보장되어야 하는 컬럼에 대해서는 유니크 인덱스를 생성하되, 꼭 필요하지 않다면 유니크 인덱스 보다는 유니크하지 않은 세컨더리 인덱스를 생성하는 방법도 한 번씩 고려해보자.\n외래키 MySQL에서 외래키는 InnoDB 스토리지 엔진에서만 생성할 수 있으며, 외래키 제약이 설정되면 자동으로 연관되는 테이블의 컬럼에 인덱스까지 생성된다. 외래키가 제거되지 않은 상태에서는 자동으로 생성된 인덱스를 삭제할 수 없다.\nInnoDB의 외래키 관리에는 중요한 두 가지 특징이 있다.\n테이블 변경(쓰기 잠금)이 발생하는 경우에만 잠금 경합(잠금 대기)이 발생한다. 외래키와 연관되지 않은 컬럼의 변경은 최대한 잠금 경함(잠금 대기)를 발생시키지 않는다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 CREATE TABLE tb_parant ( id INT NOT NULL , fd VARCHAR(100) NOT NULL, PRIMARY KEY (id) ) ENGINE=InnoDB; CREATE TABLE tb_child ( id INT NOT NULL, pid INT DEFAULT NULL, fd VARCHAR(100) DEFAULT NULL, PRIMARY KEY (id), KEY ix_parentid (pid), CONSTRAINT child_ibfk_1 FOREIGN KEY (pid) REFERENCES tb_parant (id) ON DELETE CASCADE ) ENGINE=InnoDB; INSERT INTO tb_parant VALUES (1, \u0026#39;p1\u0026#39;), (2, \u0026#39;p2\u0026#39;); INSERT INTO tb_child VALUES (100, 1, \u0026#39;c100\u0026#39;); 자식 테이블의 변경이 대기하는 경우 작업 번호 커넥션-1 커넥션-2 1 BEGIN; 2 UPDATE tb_parent SET fb=\u0026lsquo;changed-2\u0026rsquo; WHERE id=2; 3 BEGIN; 4 UPDATE tb_child SET pid=2 WHERE id=100; 5 ROLLBACK; 6 Query OK, 1 row affected 2번 커넥션 는 부모 테이블의 변경 작업이 완료될 때까지 대기한다. 1번 커넥션에서 트랜잭션이 종료하면 2번 커넥션의 대기중이던 작업이 즉시 처리되는 것을 확인할 수 있다. 즉 자식 테이블의 외래 키 컬럼의 변경은 부모 테이블의 확인이 필요한데, 이 상태에서 부모 테이블의 해당 레코드가 쓰기 잠금이 걸려있으면 해당 쓰기 잠금이 해제될 때까지 기다리게 된다.\n자식 테이블의 외래키가 아닌 컬럼의 변경은 외래키로 인한 잠금 확장이 발생하지 않는다.\n부모 테이블의 변경 작업이 대기하는 경우 작업 번호 커넥션-1 커넥션-2 1 BEGIN; 2 UPDATE tb_child SET fb=\u0026lsquo;changed-100\u0026rsquo; WHERE id=100; 3 BEGIN; 4 DELETE FROM tb_parent WHERE id=1; 5 ROLLBACK; 6 Query OK, 1 row affected 1번 커넥션에서 부모 키를 참조하는 자식 테이블의 레코드를 변경하면 tb_child 테이블의 레코드에 대해 쓰기 잠금을 획득한다. 이 상태에서 2번 커넥션이 부모 테이블에서 해당 부모 레코드를 삭제하는 경우 이 쿼리는 자식 테이블의 레코드에 대한 쓰기 잠금이 해제될 때까지 기대려야 한다. 이는 자식 테이블이 생성될 때 적용된 외래키의 특성(ON DELETE CASCADE) 때문에 부모 레코드가 삭제되면 자식 레코드도 동시에 삭제되는 식으로 작동하기 때문이다.\n데이터베이스에서 외래 키를 물리적으로 생성하려면 이러한 현상으로 인한 잠금 경합까지 고려해 모델링을 진행하는 것이 좋다.\n물리적으로 외래키를 생성하면 자식 테이블에 레코드가 추가되는 경우 해당 참조키가 부모 테이블에 있는지 확인하는데, 물리적인 외래키의 고려 사항은 이러한 체크 작업이 아니라 이러한 체크를 위해 연관 테이블에 읽기 잠금을 걸어야 한다는 점이다.\n이렇게 잠금이 다른 테이블로 확장되면 그만큼 전체적으로 쿼리의 동시 처리에 영향을 미친다.\n","date":"2023-05-14T19:24:10+09:00","image":"https://codemario318.github.io/post/real_mysql_8_4/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_8_4/","title":"8. 인덱스 (4)"},{"content":"문서의 내용 전체를 인덱스화해서 특정 키워드가 포함된 문서를 검색하는 전문(Full Text) 검색에는 InnoDB나 MyISAM 스토리지 엔진에서 제공하는 일반적인 용도의 B-Tree 인덱스를 사용할 수 없다.\n문서 전체에 대한 분석과 검색을 위한 인덱싱 알고리즘을 전문 검색(Full Text Search) 인덱스라고 하는데, 전문 검색 인덱스는 일반화된 기능의 명칭으로 알고리즘의 이름을 지칭하는 것은 아니다.\n인덱스 알고리즘 전문 검색에서는 문서 본문의 내용에서 사용자가 검색하게 될 키워드를 분석하고, 빠른 검색용으로 사용할 수 있게 이러한 키워드로 인덱스를 구축한다. 키워드의 분석 및 인덱스 구축에는 여러 가지 방법이 있을 수 있다.\n전문 검색 인덱스는 문서의 키워드를 인덱싱하는 기법에 따라 구분될 수 있다.\n어근 분석 알고리즘 n-gram 알고리즘 어근 분석 알고리즘 MySQL 서버의 전문 검색 인덱스는 다음과 같은 두 가지 중요한 과정을 거쳐서 색인 작업이 수행된다.\n불용어(Stop Word)처리 검색에서 별 가치가 없는 단어를 모두 필터링해서 제거하는 작업을 의미 불용어의 개수가 많지 않기 때문에 알고리즘을 구현한 코드에 상수로 정의해 사용하는 경우가 많다. 유연성을 위해 불용어 자체를 데이터베이스화해서 사용자가 추가하거나 삭제할 수 있게 구현하는 경우도 있다. 어근 분석(Stemming) 검색어로 선정된 단어의 뿌리인 원형을 찾는 작업이다. MySQL 서버는 오픈소스 형태소 분석 라이브러리인 MeCab을 플러그인 형태로 사용할 수 있게 지원한다. 한글이나 일본어의 경우 영어와 같이 단어의 변형 자체는 거의 없기 때문에 어근분석보다는 문장의 형태소를 분석해서 명사와 조사를 구분하는 기능이 더 중요한 편이다. n-gram 알고리즘 MeCab을 위한 형태소 분석은 매우 전문적인 전문 검색 알고리즘이어서 만족할 만한 결과를 내기 위해서는 많은 노력과 시간을 필요로 한다. 전문적인 검색 엔진을 고려하는 것이 아니라면 범용적으로 적용하기는 쉽지 않기 때문에 이러한 단점을 보완하기 위한 방법으로 n-gram 알고리즘이 도입되었다.\nn-gram이란 본문을 무조건 몇 글자씩 잘라서 인덱싱하는 방법이다. 형태소 분석보다는 알고리즘이 단순하고 국가별 언어에 대한 이해와 준비 작업이 필요 없는 반면, 만들어진 인덱스의 크기는 상당히 큰 편이다. n은 인덱싱할 키워드의 최소 글자 수를 의미하는데, 일반적으로는 2글자 단위로 키워드를 쪼개서 인덱싱하는 2-gram(Bi-gram) 방식이 많이 사용된다.\n1 To be or not to be. That is the question 각 단어는 띄어쓰기와 마침표를 기준으로 10개의 단어로 구분되고, 2글자씩 중첩해서 토큰으로 분리된다.\n단어 bi-gram 토큰 To To be be or or not no ot to to be be that Th ha at is is the th he question qu ue es st ti io on 각 글자가 중첩되어 토큰화 되기 때문에 Bi-gram 알고리즘에서는 글자수 - 1개의 토큰으로 구분된다. 이렇게 구분된 토큰을 인덱스에 저장한다. 이때 중복된 토큰은 하나의 인덱스 엔트리로 병합되어 저장한다.\nMySQL 서버는 이렇게 생성된 토큰들에 대해서 불용어를 걸러내는 작업을 수행하는데, 이때 불용어와 동일하거나 불용어를 포함하는 경우 걸러서 버린다. 기본적으로 MySQL 서버에 내장된 불용어는 information_schema.innodb_ft_default_stopword 테이블을 통해 확인 가능하다.\n입력 불용어 일치 불용어 포함 출력(최종 인덱스 등록) at O be O be O es et ha O he he he io O is O no no on O or O ot ot qu qu st st Th Th th th ti O To O to O ue ue 전문 검색을 더 빠르게 하기 위해 2단계 인덱싱(프론트엔드와 백엔드 인덱스)과 같은 방법도 있지만 MySQL 서버는 구분된 토큰을 단순한 B-Tree 인덱스에 저장한다.\n불용어 변경 및 삭제 n-gram의 토큰 파싱 및 불용어 처리 예시 결과를 보면 \u0026ldquo;ti\u0026rdquo;, \u0026ldquo;at\u0026rdquo;, \u0026ldquo;ha\u0026rdquo; 같은 토큰들은 \u0026ldquo;a\u0026rdquo;, \u0026ldquo;i\u0026rdquo; 철자가 불용어로 등록돼 있기 때문에 모두 걸러진다. 실제로 이 같은 불용어 처리는 사용자에게 도움이 되기보다는 사용자를 더 혼란스럽게 하는 기능일 수도 있다. 그래서 불용어 처리 자체를 완전히 무시하거나 MySQL 서버에 내장된 불용어 대신 사용자가 직접 불용어를 등록하는 방법을 권장한다.\n전문 검색 인덱스의 불용어 처리 무시\n스토리지 엔진과 관계 없이 MySQL 서버의 모든 전문 검색 인덱스에 대해 불용어를 완전히 제거한다. MySQL 서버의 설정 파일(my.cnf)의 ft_storpword_file 시스템 변수에 빈 문자열을 설정한다. 해당 시스템 변수는 서버가 재시작될 때만 인지하기 때문에 설정 변경시 서버를 재시작해야 반영된다. 사용자 정의 불용어를 적용할 때도 해당 파일 경로를 적용하여 반영할 수 있다. InnoDB 스토리지 엔진을 사용하는 테이블의 전문 검색 인덱스에 대해서만 불용어 처리 무시 innodb_ft_enable_stopword 시스템 변수를 OFF로 설정한다. 해당 시스템 변수는 동적인 시스템 변수이므로 서버가 실행 중인 상태에서도 변경할 수 있다. 사용자 정의 불용어 사용\n불용어 목록을 파일로 저장하고, MySQL 서버 설정파일에서 파일의 경로를 ft_stopword_file 설정에 등록한다. InnoDB 스토리지 엔진을 사용하는 테이블의 전문 검색 엔진에만 사용할 수 있는데, innodb_ft_server_stopword_table 시스템 변수에 불용어 테이블을 설정한다. 이때 불용어 목록을 변경한 이후 전문 검색 인덱스가 생성돼야만 변경된 불용어가 적용된다. 1 2 3 4 5 6 CREATE TABLE my_stopword(value VARCHAR(30)) ENGINE = INNODB; INSERT INTO my_stopword(value) VALUES (\u0026#39;MySQL\u0026#39;); SET GLOBAL innodb_ft_server_stopword_table=\u0026#39;mydb/my_stopword\u0026#39;; ALTER TABLE tb_bi_gram ADD FULLTEXT INDEX fx_title_body(title, body) WITH PARSER ngram; innodb_ft_user_stopword_table 시스템 변수를 이용하는 방법도 있으며, innodb_ft_server_stopword_table와 사용법이 동일하다. 여러 전문 검색 인덱스가 서로 다른 불용어를 사용해야 하는 경우에 활용할 수 있다.\n전문 검색 엔진 인덱스의 가용성 전문 검색 인덱스를 사용하려면 반드시 2가지 조건을 만족해야 한다.\n쿼리 문장이 전문 검색을 위한 문법(MATCH ... AGAINST)을 사용 테이블이 전문 검색 대상 컬럼에 대해서 전문 인덱스 보유 1 2 3 4 5 6 7 CREATE TABLE tb_test ( doc_id INT, doc_body TEXT, PRIMARY KEY (doc_id), FULLTEXT KEY fx_docbody (doc_body) WITH PARSER ngram ) ENGINE=InnoDB; 1 2 3 4 5 6 /* 풀 테이블 스캔으로 처리되는 쿼리 */ SELECT * FROM tb_test WHERE doc_body LIKE \u0026#39;%애플%\u0026#39;; 1 2 3 4 5 6 /* 전문 검색 인덱스로 처리되는 쿼리 */ SELECT * FROM tb_test WHERE MATCH(doc_body) AGAINST (\u0026#39;애플\u0026#39; IN BOOLEAN MODE); 전문 검색 인덱스를 사용하려면 반드시 MATCH ... AGAINST ...구문으로 검색 쿼리를 작성해야 하며, 인덱스를 구성하는 컬럼들은 MATCH절의 괄호 안에 모두 명시되어야 한다.\n","date":"2023-05-14T17:24:10+09:00","image":"https://codemario318.github.io/post/real_mysql_8_3/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_8_3/","title":"8. 인덱스 (3) - 전문 검색 인덱스"},{"content":"B-Tree는 데이터베이스의 인덱싱 알고리즘 가운데 가장 일반적으로 사용되고, 가장 먼저 도입되었지만 아직까지도 가장 범용적으로 사용되는 인덱스 알고리즘이다.\nB-Tree에는 여러 변형된 형대의 알고리즘이 있는데, 일반적으로 DMBS에서는 B+-Tree, B*-Tree가 사용된다.\nB-Tree는 컬럼의 원래 값을 변형시키지 않고 인덱스 구조체 내에서는 항상 정렬된 상태로 유지한다. 전문 검색같은 특수한 요건이 아닌 경우, 대부분 인덱스는 B-Tree를 사용한다.\n구조 및 특성 B-Tree는 트리 구조의 최상위에 하나의 루트 노드가 존재하고 그 하위에 자식 노드가 붙어있는 형태이다. 트리 구조의 가장 하위에 있는 노드를 리프 노드라 하고, 트리 구조에서 루트 노드도 아니고 리프 노드도 아닌 중간 노드를 브랜치 노드라고 한다.\n데이터베이스에서 인덱스와 실제 데이터가 저장된 데이터는 따로 관리되는데, 인덱스의 리프 노드는 항상 실제 데이터 레코드를 찾아가기 위한 주솟값을 가진다.\n인덱스의 키 값은 모두 정렬돼 있지만, 데이터 파일릐 레코드는 정렬돼 있지 않고 임의의 순서로 저장돼 있다. 레코드가 삭제되어 빈 공간이 생기면 그다음 INSERT는 가능한 한 삭제된 공간을 재활용 하도록 DBMS가 설계되기 때문에, 항상 INSERT순서대로 저장되는 것은 아니다.\n인덱스는 테이블의 키 컬럼만 가지고 있으므로 나머지 컬럼을 읽으려면 데이터 파일에서 해당 레코드를 찾아야 한다. 이를 위해 인덱스의 리프 노드는 데이터 파일에 저장된 레코드의 주소를 가진다.\nMyISAM 레코드 주소는 MyISAM 테이블 생성 옵션에 따라 레코드가 테이블에 INSERT된 순번이거나 데이터 파일 내의 위치다. (ROWID) 세컨더리 인덱스가 물리적인 주소를 가진다. InnoDB 프라이머리 키가 ROWID 역할을 한다 프라이머리 키를 주소처럼 사용하기 때문에 논리적인 주소를 가진다고 볼 수 있다. 따라서 인덱스를 통해 레코드를 읽을때는 데이터 파일을 바로 찾아가지 못하고, 인덱스에 저장되어있는 프라이머리 키 인덱스의 리프 페이지에 저장돼 있는 레코드를 읽기 위해서는 반드시 프라이머리 키를 저장하고 있는 B-Tree를 다시 한번 검색해야 한다. 이러한 특징으로 인해 InnoDB 스토리지 엔진을 사용하는 테이블의 성능이 떨어질 것 처럼 보이지만 각각 장단점을 가지고 있다.\nB-Tree 인덱스 키 추가 및 삭제 테이블의 레코드를 저장하거나 변경하는 경우 인덱스 키 추가나 삭제 작업이 발생한다. 이에따라 주의해야 할 사항이 있다.\n인덱스 키 추가 새로운 키 값이 B-Tree에 저장될 때는 저장될 키 값을 이용해 B-Tree상의 적절한 위치를 검색해야 한다. 저장될 위치가 결정되면 레코드의 키 값과 대상 레코드의 주소 정보를 B-Tree의 리프 노드에 저장한다.\n리프 노드가 꽉 차서 더는 저장할 수 없을 때는 리프 노드가 분리돼야 하는데, 이는 상위 브랜치 노드까지 처리 범위가 넓어진다. 이러한 작업 탓이 B-Tree는 상대적으로 쓰기 작업(새로운 키를 추가)에 비용이 많이 드는 것으로 알려졌다.\n인덱스 추가로 인해 INSERT, UPDATE 문장이 어떤 영향을 받을지 예상해보려면 테이블의 컬럼 수, 컬럼 크기, 인덱스 컬럼의 특성 등을 확인해야 한다. 대략적으로 테이블의 레코드를 추가하는 작업 비용을 1이라고 가정하면, 해당 테이블의 인덱스에 키를 추가하는 작업 비용을 1.5 정도로 예측한다. 테이블에 인덱스가 3개가 있다면 5.5(1.5 * 3 + 1) 정도로 예측한다. 중요한 것은 이 비용의 대부분이 디스크로부터 인덱스 페이지를 읽고 쓰기를 해야해서 걸리는 시간이다.\nMyISAM이나 MEMORY 스토리지 엔진을 사용하는 테이블에서는 INSERT 문장이 실행되면 즉시 새로운 키 값을 B-Tree 인덱스에 변경하지만, InnoDB 스토리지 엔진은 필요하다면 인덱스 키 추가 작업을 지연시켜 나중에 처리할 수 있다. 하지만 프라이머리 키나 유니크 인덱스의 경우 중복 체크가 필요하기 때문에 즉시 B-Tree에 추가하거나 삭제한다.\n인덱스 키 삭제 B-Tree의 키 값이 삭제되는 경우는 상당히 간단하다. 해당 키 값이 저장된 B-Tree의 리프 노드를 찾아서 그냥 삭제 마크만 하면 작업이 완료된다. 삭제 마킹된 인덱스 키 공간은 계속 그대로 방치하거나 재사용할 수 있다.\n인덱스 키 삭제로 인한 마킹 작업 또한 디스크 쓰기가 필요하며, MySQL 5.5 이상 버전의 InnoDB 스토리지 엔진에서는 버퍼링 되어 지연 처리될 수 있다. 처리가 지연된 인덱스 키 삭제 또한 사용자에게는 특별한 악영향 없이 MySQL 서버가 내부적으로 처리하므로 특별히 걱정할 것은 없다.\nMyISAM이나 MEMORY 스토리지 엔진의 테이블에서는 체인지 버퍼와 같은 기능이 없으므로 인덱스 키 삭제가 완료된 후 쿼리 실행이 완료된다.\n인덱스 키 변경 인덱스의 키 값은 값에 따라 저장될 리프 노드의 위치가 결정되므로 B-Tree의 키 값이 변경되는 경우에는 단순히 인덱스상의 키 값만 변경하는 것은 불가능하다. 따라서 B-Tree의 키 값 변경 작업은 먼저 키 값을 삭제한 후, 다시 새로운 키 값을 추가하는 형태로 처리된다.\n결국 인덱스 키 값을 변경하는 작업은 기존 인덱스 키 값을 삭제한 후 새로운 인덱스 키 값을 추가하는 작업으로 처리되고 InnoDB 스토리지 엔진을 사용하는 테이블에 대해서는 이 작업 모두 체인지 버퍼를 활용해 지연 처리 될 수 있다.\n인덱스 키 검색 인덱스를 검색하는 작업은 B-Tree의 루트 노드부터 시작해 브랜치 노드를 거쳐 최종 리프 노드까지 이동하면서 비교 작업을 수행하는데, 이 과정을 트리 탐색이라고 한다.\n인덱스 트리 탐색은 SELECT뿐만 아니라 UPDATE, DELETE를 처리하기 위해 해당 레코드를 검색해야 할 때도 사용된다.\nB-Tree 인덱스를 이용한 검색은 100% 일치 또는 값의 앞부분만 일치하는 경우에 사용할 수 있다.\n부등호 비교 조건에서도 인덱스를 활용할 수 있지만, 인덱스를 구성하는 키 값의 뒷부분만 검색하는 용도로는 사용할 수 없다. 인덱스를 키 값에 변형이 가해진 후 비교되는 경우 빠른 검색 기능을 활용할 수 없다.(변형된 값은 인덱스에 존재하는 값이 아니므로) 함수나 연산을 수행한 결과로 정렬 또는 검색 InnoDB 테이블에서 지원하는 레코드 잠금이나 넥스트 키락이 검색을 수행한 인덱스를 잠근 후 테이블의 레코드를 잠그는 방식으로 구현돼있어, UPDATE, DELETE 문장이 실행될 때 테이블에 적절히 사용할 수 있는 인덱스가 없으면 불필요하게 많은 레코드를 잠그게 된다.\nB-Tree 인덱스 사용에 영향을 미치는 요소 컬럼 크기 레코드의 건수 유니크한 인덱스 키 값의 개수 인덱스 키 값의 크기 InnoDB 스토리지 엔진은 디스크에 데이터를 저장하는 가장 기본 단위를 페이지 또는 블록이라고 하며, 디스크의 모든 읽기 및 쓰기 작업의 최소 작업 단위가 된다. 또한 페이지는 InnoDB 스토리지 엔진의 버퍼풀에서 데이터를 버퍼링하는 기본 단위이기도 하다. 인덱스도 결국 페이지 단위로 관리되며, 루트와 브랜치 리프 노드를 구분한 기준이 페이지 단위이다.\n일반적으로 DBMS의 B-Tree는 자식 노드의 개수가 가변적인 구조다. 인덱스의 페이지 크기와 키 값의 크기에 따라 자식 노드의 최대 개수가 결정된다. MySQL 5.7 버전부터는 InnoDB 스토리지 엔진의 페이지 크기를 innodb_page_size 시스템 변수를 이용해 4KB ~ 64KB 사이의 값을 결정 할 수 있으며, 기본값은 16KB 이다.\n인덱스 페이지는 인덱스 키 값과 해당하는 주솟값을 인덱스 페이지에 지정하게 되며, 페이지 크기가 변하지 않았을 경우 인덱스 키 값이 커지면 하나의 페이지에 저장되는 레코드의 개수는 줄어들게 된다. 키 크기가 커서 한 페이지에 300개의 레코드가 인덱스 페이지에 저장된다고 가정하면 SELECT 쿼리를 통해 500개의 데이터를 조회해야 한다고 했을때 최소한 2번 읽기 작업이 발생하게 된다.\n따라서, 인덱스를 구성하는 키 값의 크기가 커지면 디스크로 부터 읽어와야 하는 횟수가 늘어날 수 있고, 그만큼 느려질 수 있다. 또한 인덱스 키 값의 길이가 길어진다는 것은 전체적인 인덱스의 크기가 커진다는 것을 의미하기 때문에, 무한정 인덱스를 캐시해둘 수 없는 InnoDB의 버퍼풀, MyISAM의 키 캐시에 저장할 수 있는 공간이 부족해져 메모리 효율이 떨어질 수 있다.\nB-Tree 깊이 인덱스의 깊이는 상당히 중요하지만 제어할 방법은 없다.\nB-Tree의 깊이는 MySQL에서 값을 검색할 때 몇 번이나 랜덤하게 디스크를 읽어야 하는지와 직결되는 문제다. 인덱스 키 값의 크기가 커질수록 하나의 인덱스 페이지가 담을 수 있는 인덱스 키 값의 개수가 적어지고, 이에 따라 같은 개수의 인덱스 키값을 저장하게 되면 더 깊어지게된다. 따라서 디스크 읽기가 더 많이 필요하게 된다.\n인ㄷ게스 키 값의 크기는 가능한 작게 만드는 것이 좋지만, 실제로 아무리 대용량 데이터베이스라도 B-Tree의 깊이가 5단계 이상까지 깊어지는 경우는 흔하지 않다.\n선택도(기수성) 인덱스에서 선택도(Selectivity) 또는 기수성(Cardinality)은 거의 같은 의미로 사용되며, 모든 인덱스 키 값 가운데 유니크한 값의 수를 의미한다.\n인덱스 키 값 가운데 중복된 값이 많아질수록 기수성은 낮아지고 동시에 선택도 또한 떨어지게 된다. 인덱스는 선택도가 높을수록 검색 대상이 줄어들기 때문에 그만큼 빠르게 처리된다.\n선택도가 좋지 않다고 하더라도 정렬이나 그루핑과 같은 작업을 위해 인덱스를 만드는 것이 훨씬 나은 경우도 많다. 인덱스가 항상 검색에만 사용되는 것은 아니므로 여러 가지 용도를 고려해 적절히 인덱스를 설계해야한다.\n선택도가 낮은 인덱스를 처리하면 MySQL 서버는 불필요한 데이터를 더 많이 읽어오게 된다.\n읽어야 하는 레코드의 건수 인덱스를 통해 테이블의 레코드를 읽는 것은 인덱스를 거치지 않고 바로 테이블의 레코드를 읽는 것 보다 높은 비용이 드는 작업이다. 따라서 인덱스를 이용한 읽기의 손익 분기점이 얼마인지를 판단할 필요가 있다.\n일반적인 DBMS의 옵티마이저에서는 인덱스를 통해 레코드 1건을 읽는 것이 테이블에서 직접 레코드 1건을 읽는 것보다 4~5배 정도 비용이 더 많이 드는 작업인 것으로 예측한다. 즉 인덱스를 통해 읽어야 할 레코드의 건수가 전체 테이블 레코드의 20%~25%를 넘어서면 인덱스를 이용하지 않고 테이블을 모두 직접 읽어서 필요한 레코드만 가려내는 방식으로 처리하는 것이 효율적이다.\n많은 레코드를 읽을 때는 강제로 인덱스를 사용하도록 힌트를 추가해도 성능상 얻을 수 있는 이점이 없다.\nB-Tree 인덱스를 통한 데이터 읽기 어떤 경우에 인덱스를 사용하게 유도할지, 또는 사용하지 못하게 할지 판단하려면 MySQL이 어떻게 인덱스를 이용해서 레코드를 읽어 내는지 알아야 한다.\n인덱스 레인지 스캔 1 2 3 SELECT * FROM employees WHERE first_name BETWEEN \u0026#39;Ebbe\u0026#39; AND \u0026#39;Gad\u0026#39;; 인덱스 레인지 스캔은 검색해야 할 인덱스의 범위가 결정됐을 대 사용하는 방식이다. 검색하려는 값의 수나 검색 결과 레코드 건수와 관계 없이 레인지 스캔이라고 표현한다.\n루트 노드에서부터 비교를 시작해 브랜치 노드를 거치고 최종적으로 리프 노드까지 찾아 들어가야만 비로소 필요한 레코드의 시작 지점을 찾을 수 있다. 이처럼 차례대로 죽 읽는 것을 스캔이라고 표현하며, 스캔하다가 리프 노드의 끝까지 읽으면 리프 노드의 구간은 실제 스캔하는 범위를 표현한다.\nB-Tree 인덱스에서 루트와 브랜치 노드를 이용해 스캔 시작 위치를 검색하고, 그 지점부터 필요한 방향으로 인덱스를 읽어 나간다. 인덱스 자체 정렬 특성으로 인해 어떤 방식으로 스캔하든 관계없이, 해당 인덱스를 구성하는 컬럼의 정순 또는 역순으로 정렬된 상태로 레코드를 가져온다.\n인덱스의 리프 노드에서 검색 조건에 일치하는 건들은 데이터 파일에서 레코드를 읽어오는 과정이 필요하다. 이때 리프노드에 저장된 레코드 주소로 데이터 파일의 레코드를 읽어오는데, 레코드 한 건 단위로 랜덤 I/O가 한 번 씩 일어난다. 이러한 이유 때문에 인덱스를 통해 데이터 레코드를 읽는 작업은 비용이 많이 드는 작업으로 분류된다.\n인덱스에서 조건을 만족하는 값이 저장된 위치를 찾는다. 1번에서 탐색된 위치부터 필요한 만큼 인덱스를 차례대로 쭉 읽는다.(인덱스 스캔) 읽어 들인 인덱스 키와 레코드 주소를 이용해 레코드가 저장된 페이지를 가져오고, 최종 레코드를 읽어온다. 쿼리가 필요로 하는 데이터에 따라 3번 과정은 필요하지 않을 수도 있는데, 이를 커버링 인덱스라고 한다. 커버링 인덱스로 처리되는 쿼리는 디스크의 레코드를 읽지 않아도 되기 때문에 랜덤 읽기가 상당히 줄어들고 그만큼 빨라진다.\n1 SHOW STATUS LIKE \u0026#39;Handler_%\u0026#39;; 위 쿼리를 통해 읽은 레코드 건수를 조회할 수 있으나, 실제 인덱스만 읽었는지 인덱스를 통해 테이블의 레코드를 읽었는지(3번)은 구분할 수 없다.\nHandler_read_key: 1번 단계가 실행된 횟수 Handler_read_next: 인덱스 정순으로 읽은 레코드 건수 Handler_read_prev: 인덱스 역순으로 읽은 레코드 건수 Handler_read_first: 첫 번째 레코드를 읽은 횟수 (MIN) Handler_read_last: 마지막 레코드를 읽은 횟수 (MAX) 인덱스 풀 스캔 인덱스 레인지 스캔과는 달리 인덱스의 처음부터 끝까지 모두 읽는 방식이다. 대표적으로 쿼리의 조건절에 사용된 컬럼이 인덱스의 첫 번째 컬럼이 아닌 경우 인덱스 풀 스캔 방식이 사용된다.\n인덱스 리프 노드의 제일 앞 또는 제일 뒤로 이동한다. 인덱스의 리프 노드를 연결하는 링크드 리스트를 따라서 처음부터 끝까지 스캔한다. 인덱스 풀 스캔은 인덱스에 포함된 컬럼만으로 쿼리를 처리할 수 있는 경우 테이블의 레코드를 읽을 필요가 없어 테이블 풀 스캔보다는 효율적이다. 인덱스의 전체 크기는 테이블 자체의 크기보다는 훨씬 작으므로 더 적은 디스크 I/O로 처리할 수 있다.\n루스 인덱스 스캔 오라클 DBMS의 인덱스 스킵 스캔비슷하게 처리되는 방법으로 MySQL 5.7 버전 까지는 기능이 많이 제한적이었지만, MySQL 8.0 버전부터는 다른 상용 DBMS에서 지원하는 인덱스 스킵 스캔과 같은 최적화를 조금씩 지원하기 시작했다.\n인덱스 레인지 스캔과 인덱스 풀 스캔은 루스 인덱스 스캔과는 상반된 의미로 타이트 인덱스 스캔으로 분류한다.\n루스 인덱스 스캔이란 말 그대로 느슨하게 또는 듬성듬성하게 인덱스를 읽는 것을 의미한다. 인덱스 레인지 스캔과 비슷하게 작동하지만 중간에 필요치 않은 인덱스 키 값은 무시하고 다음으로 넘어가는 형태로 처리한다. 일반적으로 GROUP BY 또는 집합 함수 중 MIN(), MAX() 함수에 대해 최적화를 하는 경우 사용된다.\n1 2 3 4 5 6 SELECT dept_no ,MIN(emp_no) FROM dept_emp WHERE dep_no BETWEEN \u0026#39;d002\u0026#39; AND \u0026#39;d004\u0026#39; GROUP BY dept_no ; 인덱스에서 WHERE조건을 만족하는 범위 전체를 다 스캔할 필요가 없다는 것을 옵티마이저는 알고 있기 때문에 조건에 만족하지 않는 레코드는 무시하고 다음 레코드로 이동한다. 루스 인덱스 스캔을 사용하려면 여러가지 조건을 만족해야 한다.\n인덱스 스킵 스캔 데이터베이스 서버에서 인덱스의 핵심은 값이 정렬돼 있다는 것이며, 이로 인해 인덱스를 구성하는 컬럼의 순서가 매우 중요하다.\n1 2 3 ALTER TABLE employees ADD INDEX ix_gender_birthdate (gender, birth_date) ; 인덱스를 사용하려면 WHERE조건절에 gender 컬럼에 대한 비교 조건이 필수이다.\n1 2 3 4 5 6 7 8 9 /* 인덱스 X */ SELECT * FROM employees WHERE birth_date \u0026gt;= \u0026#39;1965-02-01\u0026#39;; /* 인덱스 O */ SELECT * FROM employees WHERE gender = \u0026#39;M\u0026#39; AND birth_date \u0026gt;= \u0026#39;1965-02-01\u0026#39; ; 따라서 위 두 쿼리중 gender 컬럼과 birth_date 컬럼의 조건을 모두 가진 두 번째 쿼리는 인덱스를 효율적으로 사용할 수 있지만, gender 컬럼에 대한 비교 조건이 없는 첫 번째 쿼리는 인덱스를 사용할 수 없어 birth_date 컬럼부터 시작하는 인덱스를 생성해야만 했다.\nMySQL 8.0 버전부터는 옵티마이저가 gender 컬럼을 건너 뛰어서 birth_date 컬럼만으로도 인덱스 검색이 가능하게 해주는 인덱스 스킵 스캔 최적화 기능이 도입됐다.\n1 2 3 4 5 6 SET optimizer_switch=\u0026#39;skip_scan=on\u0026#39;; SELECT * FROM employees WHERE birth_date \u0026gt;= \u0026#39;1965-02-01\u0026#39; ; 1 2 3 4 5 6 7 8 9 10 11 SELECT * FROM employees WHERE gender = \u0026#39;M\u0026#39; AND birth_date \u0026gt;= \u0026#39;1965-02-01\u0026#39; ; SELECT * FROM employees WHERE gender = \u0026#39;F\u0026#39; AND birth_date \u0026gt;= \u0026#39;1965-02-01\u0026#39; ; 위의 쿼리는 아래의 쿼리로 나눠 실행한 것과 비슷한 형태로 최적화를 실행한다.\n인덱스 스킵 스캔은 새롭게 도입된 기능이어서 아직 다음과 같은 단점이 있다.\nWHERE 조건절에 조건이 없는 인덱스의 선행 컬럼의 유니크한 값의 개수가 적어야함 쿼리가 인덱스에 존재하는 컬럼만으로 처리 가능해야함(커버링 인덱스) 다중 컬럼 인덱스 실제 서비스용 데이터베이스에서는 2개 이상의 컬럼을 포함하는 인덱스가 더 많이 사용된다. 두개 이상의 컬럼으로 구성된 인덱스를 다중 컬럼 인덱스(복합 컬럼 인덱스, Concatenated Index)라고 한다.\n인덱스의 두 번째 컬럼은 첫 번째 컬럼에 의존해서 정렬돼있다. 즉 두 번째 컬럼의 정렬은 첫 번째 컬럼이 똑같은 레코드에서만 의미가 있다. 위의 예제에서 emp_no 값의 정렬 순서가 빠르다고 하더라도 dept_no 컬럼의 정렬 순서가 늦다면 인덱스의 두쪽에 위치한다. 따라서 다중 컬럼 인덱스에서는 인덱스 내에서 각 컬럼의 위치(순서)가 상당히 중요하여 신중히 결정해야 한다.\nB-Tree 인덱스의 정렬 및 스캔 방향 인덱스를 생성할 때 설정한 정렬 규칙에 따라 인덱스의 키 값은 항상 오름차순이거나 내림차순으로 정렬되어 저장된다. 하지만 읽을때는 반대로도 가능하며, 인덱스를 어느 방향으로 읽을지는 쿼리에 따라 옵티마이저가 실시간으로 만들어 내는 실행 계획에 따라 결정된다.\n인덱스의 정렬 MySQL 5.7 버전까지는 컬럼 단위로 정렬 순서를 혼합해서 인덱스를 생성할 수 없었지만, 8.0 버전부터는 순서를 혼합한 인덱스도 생성할 수 있게 되었다.\n1 CREATE INDEX ix_teamname_userscore ON employees (team_name ASC, user_score DESC); 인덱스 스캔 방향 인덱스는 항상 오름차순으로만 정렬돼 있지만 인덱스를 최솟값부터 읽으면 오름차순으로 값을 가져올 수 있고, 최댓값부터 거꾸로 읽으면 내림차순으로 값을 가져올 수 있다는 것을 MySQL 옵티마이저는 이미 알고 있다. 즉 인덱스 생성 시점에 오름차순 또는 내림차순으로 정렬이 결정되지만 쿼리가 그 인덱스를 사용하는 시점에 인덱스를 읽는 방향에 따라 오름차순 또는 내림차순 정렬 효과를 얻을 수 있다.\nORDER BY 처리나 MIN, MAX 함수등의 최적화가 필요한 경우에도 인덱스의 읽기 방향을 전환해서 사용하도록 실행 계획을 만들어낸다.\n내림차순 인덱스 MySQL 서버는 실제 내림차순인지 오름차순인지 관계없이 인덱스를 읽는 순서만 변경해서 해결할 수 있지만, 2개 이상의 컬럼으로 구성된 복합 인덱스에서 각각의 컬럼이 내림차순과 오름차순이 혼합된 경우는 내림차순 인덱스로만 해결될 수 있다.\n실제 역순 정려 쿼리가 정순 정렬 쿼리보다 더 시간이 많이 걸린다. MySQL 서버의 InnoDB 스토리지 엔진에서 정순 스캔과 역순 스캔은 페이지 간의 양방향 연결 고리를 통해 전진하느냐 후진하느냐의 차이만 있지만, 실제 내부적으로는 InnoDB에서 인덱스 역순 스캔이 더 느린 이유는 2가지가 있다.\n페이지 잠금이 인덱스 정순 스캔에 적합한 구조 페이지 내에서 인덱스 레코드가 단방향으로만 연결된 구조 쿼리가 많은 레코드를 조회하면서 빈번하게 실행된다면 오름차순 인덱스보다는 내림차순 인덱스가 더 효율적이다. 또한 많은 쿼리가 인덱스의 한쪽만을 집중적으로 읽어 특정 페이지 잠금이 병목이 될 것으로 예상된다면 쿼리에서 자주 사용되는 정렬 순서대로 인덱스를 생성하는 것이 잠금 병목 현상은 완화하는 데 도움이 될 수 있다.\nB-Tree 인덱스의 가용성과 효율성 쿼리의 WHERE 조건이나 GROUP BY, ORDER BY 절이 어떤 경우에 인덱스를 사용할 수 있고 어떤 방식으로 사용할 수 있는지 식별할 수 있어야 인덱스를 최적으로 생성할 수 있다.\n비교 조건의 종류와 효율성 다중 컬럼 인덱스에서 각 컬럼의 순서와 그 컬럼에 사용된 조건이 동등 비교인지 아니면 범위 조건인지에 따라 인덱스 컬럼 활용 형태가 달라지며, 효율 또한 다르다.\n1 2 3 4 5 SELECT * FROM dept_emp WHERE dept_no=\u0026#39;d002\u0026#39; AND emp_no \u0026gt;= 10114 ; A: INDEX (dept_no, emp_no) dept_no='d002' AND emp_no \u0026gt;=10114 인 레코드들을 찾고 dept_no가 \u0026lsquo;d002\u0026rsquo;가 아닐 때까지 인덱스를 읽는다. 조건을 만족하는 레코드를 찾는데 필요한 비교 작업만 수행하므로 효율적으로 인덱스를 활용했다. B: INDEX (emp_no, dept_no) emp_no \u0026gt;=10114 AND dept_no='d002' 인 레코드들을 찾고 dept_no가 모든 레코드가 \u0026lsquo;d002\u0026rsquo;인지 비교한다. 인덱스를 통해 읽은 레코드가 나머지 조건에 맞는지 비교하면서 취사선택하는 작업을 필터링이라고 하며, 케이스 B 인덱스에서는 다중 컬럼 인덱스의 정렬 방식으로 인해 최종적인 조건을 만족하는 레코드를 찾기 위해 더 큰 범위의 데이터를 가져와 비교했다.\n공식적인 명칭은 아니지만 케이스 A 인덱스에서의 도 조건과 같이 작업의 범위를 결정하는 조건을 작업 범위 결정 조건이라 하고 케이스 B 인덱스같이 비교 작업의 범위를 줄이지 못하고 거름종이 역할만 하는 조건을 필터링 조건, 체크 조건이라고 표현한다.\n작업 범위를 결정하는 조건은 많으면 많을수록 쿼리의 처리 성능을 높이지만 체크 조건은 많다고 해서 쿼리의 처리 성능을 높히지는 못한다.\n인덱스의 가용성 B-Tree 인덱스의 특징은 왼쪽 값에 기준해서 오른쪽 값이 정렬된다. 하나의 컬럼 내에서 뿐만 아니라 다우 컬럼 인덱스의 컬럼에 대해서도 함께 적용된다.\nA: INDEX (first_name) B: INDEX (dept_no, emp_no) 인덱스 키 값의 이런 정렬 특성은 빠른 검색의 전제 조건이다. 즉 하나의 컬럼으로 검색해도 값의 왼쪽 부분이 없으면 인덱스 레인지 스캔 방식의 검색이 불가능하다. 또한 다중 컬럼 인덱스에서도 왼쪽 컬럼의 값을 모르면 인덱스 레인지 스캔을 사용할 수 없다.\n1 2 3 4 SELECT * FROM employees WHERE first_name LIKE \u0026#39;%mer\u0026#39; ; first_name 컬럼에 저장된 값의 왼쪽부터 한 글자씩 비교해 가면서 일치하는 레코드를 찾아야 하는데, LIKE 조건으로 왼쪽 부분이 고정되지 않았기 때문에 인덱스 레인지 스캔 방식으로 인덱스를 사용할 수 없다.\n1 2 3 4 SELECT * FROM dept_emp WHERE emp_no \u0026gt;= 10144 ; 인덱스가 dept_no 기준으로 생성되었기 때문에 dept_no 조건 없이 검색하면 인덱스를 효율적으로 사용할 수 없다.\n가용성과 효율성 판단 기본적으로 B-Tree 인덱스의 특성상 다음 조건에서는 사용할 수 없다. (작업 범위 결정 조거능로 사용할 수 없다.)\nNOT-EQUAL로 비교된 경우(\u0026lt;\u0026gt;,NOT IN, NOT BETWEEN, IS NOT NULL) LIKE '%??' 형태로 문자열 패턴이 비교된 경우 스토어드 함수나 다른 연산자로 인덱스 컬럼이변형된 후 비교된 경우 WHERE SUBSTRING(column, 1, 1) = 'X WHERE DAYOFMONTH(column) = 1 NOT-DETERMINISTIC 속성의 스토어드 함수가 비교 조건에 사용된 경우 WHERE column = deterministic_function() 데이터 타입이 서로 다른 비교(인덱스 컬럼의 타입을 변환해야 비교가 가능한 경우) 문자열 데이터 타입의 콜레이션이 다른 경우 다른 일반적은 DBMS에서는 NULL 값이 인덱스에 저장되지 않지만 MySQL 에서는 NULL 값도 인덱스에 저장된다. 다음과 같은 WHERE 조건도 작업 범위 결정 조건으로 인덱스를 사용한다. (WHERE column IS NULL)\n다중 컬럼 인덱스 1 INDEX ix_test ( column_1, .. column_n ) 작업 범위 결정 조건으로 인덱스를 사용하지 못하는 경우 column_1 컬럼에 대한 조건이 없는 경우 column_1 컬럼의 비교 조건이 위의 인덱스 사용 불가 조건 중 하나인 경우 작업 범위 결정 조건으로 사용하는 경우 column_1 ~ column_(i-1)컬럼까지 동등 비교 형태(=, IN) column_i 컬럼에 대해 다음 연산자 중 하나로 비교 동등 비교 크다 작다 형태 LIKE로 좌특 일치 패턴 적업 범위 결정 조건으로 인덱스를 사용하는 쿼리 패턴은 이 밖에도 상당히 많지만, 대표적인 것을 기억해 두면 좀 더 효율적인 쿼리를 쉽게 작성할 수 있다.\n","date":"2023-05-07T17:24:10+09:00","image":"https://codemario318.github.io/post/real_mysql_8_2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_8_2/","title":"8. 인덱스 (2) - B-Tree 인덱스"},{"content":"인덱스는 데이터베이스 쿼리의 성능을 언급하면서 뻬놓을 수 없는 부분이다. 각 인덱스의 특성과 차이는 상당히 중요하며, 물리 수준의 모델링을 할 때도 중요한 요소가 된다.\nMySQL 8.0 버전까지 업그레이드되어 오면서 다른 상용 RDBMS에서 제공하는 많은 기능을 지원하게 됐으며, 기존의 MyISAM 스토리지 엔진에서만 제공하던 전문 검색이나 위치 기반 검색 기능도 모두 InnoDB 스토리지 엔진에서 사용할 수 있게 개선되었다.\n하지만 아무리 MySQL 서버의 옵티마이저가 발전하고 성능이 개선됐다고 해도 여전히 관리자의 역할은 매우 중요하며, 인덱스에 대한 기본 지식은 지금도 앞으로도 개발자나 관리자에게 매우 중요하고 쿼리 튜닝의 기본이 될 것이다.\n디스크 읽기 방식 컴퓨터의 CPU나 메모리처럼 전기적 특성을 띤 장치의 성능은 짧은 시간동안 매우 빠른 속도로 발전했으나 디스크 같은 기계식 장치의 성능은 상당히 제한적으로 발전했다. 최근에는 자기 디스크 원판에 의존하는 하드 디스크보다 SSD 드라이브가 많이 활용되고 있지만, 여전히 데이터 저장 매체는 컴퓨터에서 가장 느린 부분이라는 사실에는 변함이 없다.\n데이터베이스나 쿼리 튜닝에 어느정도 지식을 사용자가 절감하고 있듯이 데이터베이스의 성능 튜닝은 어떻게 디스크 I/O를 줄이느냐가 관권일 때가 많다.\nHDD와 SSD 데이터베이스 서버에서는 항상 디스크 장치가 병목이 된다. SSD는 기존 하드 디스크 드라이브에서 데이터 저장용 원판을 제거하는 대신 플래시 메모리를 장착하고 있다. 따라서 원판을 물리적으로 회전시킬 필요가 없으므로 빠르게 데이터를 읽고 쓸 수 있다.\n플래시 메모리를 사용하는 SSD는 하드 디스크 드라이브보다 용량은 적지만 1000배 가량 빠른 성능을 보여주기 때문에 요즘은 DBMS용 서버는 SSD를 채택하고 있다.\n디스크의 헤더를 움직이지 않고 한 번에 많은 데이터를 읽는 순차 I/O 에서는 SSD가 압도적으로 빠르진 않지만 데이터베이스 서버에서는 대부분 랜덤 I/O가 발생하여 대부분의 상황에서 SSD 성능이 우수하다.\n랜덤 I/O와 순차 I/O 하드디스크 기준으로 랜덤 I/O는 그만큼 디스크 헤드를 자주 움직여야 한다는 뜻이며, SSD도 랜덤 I/O가 순차 I/O에 비해 스루풋이 떨어진다. 데이터베이스 대부분의 작업은 이러한 작은 데이터를 빈번히 읽고 쓰기 때문에 MySQL 서버에는 그룹 커밋이나 바이너리 로그 버퍼 도는 InnoDB 로그 버퍼 등의 기능으로 대응하고 있다.\n쿼리를 튜닝해서 랜덤 I/O를 순차 I/O로 바꿔 실행할 방법은 그다지 많지 않고, 일반적으로 쿼리 튜닝은 랜덤 I/O 자체를 줄여주는 것이 목적이다.\n랜덤 I/O를 줄인다는 것은 쿼리를 처리하는 데 꼭 필요한 데이터만 읽도록 쿼리를 개선하는 것을 의미\n인덱스 레인지 스캔은 데이터를 읽기 위해 주로 랜덤 I/O 사용하고 풀 테이블 스캔은 순차 I/O 사용한다. 따라서 큰 테이블의 레코드 대부분을 읽는 작업에서는 인덱스를 사용하지 않고 풀 테이블 스캔을 사용하도록 유도할 때도 있다. 이런 형태는 온라인 서비스에서는 거의 활용되지 않으며 주로 데이터웨어하우스나 통계 작업에서 많이 활용된다.\n인덱스란? 책의 맨 끝에 있는 색인으로 설명된다. 색인으로 찾는 페이지 번호는 데이터 파일에 저장된 레코드의 주소에 비유될 수 있다.\nDBMS도 데이터베이스 테이블의 모든 데이터를 검색해서 원하는 결과를 가져오려면 시간이 오래 걸리기 때문에, 컬럼의 값과 해당 레코드가 저장된 주소를 키와 값의 쌍으로 삼아 인덱스를 만들고, 빠른 검색을 위해 칼럼의 값을 기준으로 정렬해놓는다.\n자료구조로 비유하면, SortedList, ArrayList를 예로 들 수 있다.\nSortedList: DBMS의 인덱스와 같은 자료 구조. 저장된 값을 항상 정렬된 상태로 유지한다. 데이터가 저장될 때마다 항상 값을 정렬해야 하므로 저장하는 과정이 복잡하고 느리다. 이미 정렬돼 있어 원하는 값을 아주 빨리 찾아올 수 있다. INSERT, UPDATE는 느려지지만 SELECT는 매우 빠르다. ArrayList: 데이터 파일과 같은 자료구조. 저장된 순서대로 별도 정렬 없이 그대로 저장한다. 인덱스 추가시 주의사항 결론적으로 DBMS에서 인덱스는 데이터의 저장 성능을 희생하고 그 대신 데이터의 읽기 속도를 높이는 기능이다. 따라서 다음을 고려하여 인덱스 추가를 결정한다.\n데이터의 저장 속도를 어느정도까지 희생할 수 있는지 읽기 속도를 얼마나 더 빠르게 만들어야 하는지 SELECT쿼리 문장의 WHERE조건절에 사용되는 컬럼이라고 해서 전부 인덱스로 생성하면 데이터 저장 성능이 떨어지고 인덱스의 크기가 비대해져 오히려 역효과만 불러올 수 있다.\n인덱스 분류 인덱스는 데이터를 관리하는 방식(알고리즘)과 중복 값의 허용 여부 등에 따라 여러가지로 나눠볼 수 있다.\n역할 별 분류\n인덱스를 역할 별로 구분해 본다면 프라이머리키와 보조키(Secondary Index, Secondary Key)로 구분할 수 있다.\n프라이머리 키 레코드를 대표하는 컬럼의 값으로 만들어진 인덱스. 테이블에서 해당 레코드를 식별할 수 있는 기준값이 되기 때문에 이를 식별자라고도 부른다. NULL값과 중복값을 허용하지 않는다. 세컨더리 인덱스 프라이머리 키를 제외한 나머지 모든 인덱스. 유니크 인덱스는 프라이머리 키와 성격이 비슷하고 프라이머리 키를 대체해서 사용할 수 있어 대체키로도 불린다. 데이터 저장 방식(알고리즘) 분류\n대표적으로 B-Tree 인덱스와 Hash 인덱스로 구분할 수 있다. 최근에는 Fractal-Tree 인덱스나 로그 기반의 Merge-Tree 인덱스와 같은 알고리즘을 사용하는 DBMS도 개발되고 있다.\nB-Tree 알고리즘 가장 일반적으로 사용되는 인덱스 알고리즘으로, 상당히 오래전에 도입된 만큼 성숙해진 상태이다. 컬럼의 값을 변형하지 않고 원래의 값을 이용해 인덱싱하는 알고리즘이다. Hash 인덱스 알고리즘 컬럼의 값으로 해시값을 계산해서 인덱싱하는 알고리즘으로, 매우 빠른 검색을 지원한다. 값을 변형해서 인덱싱하므로 전방(Prefix)일치와 같이 값의 일부만 검색하거나 범위 검색할 때에는 해시 인덱스를 사용할 수 없다. 메모리 기반의 데이터베이스에서 많이 사용된다. 데이터 중복 허용 여부로 분류\n데이터의 중복 허용 여부로 분류하면 유니크 인덱스와 유니크하지 않은 인덱스로 구분할 수 있다.\n인덱스가 유니크한지 아닌지는 단순히 같은 값이 1개만 존재하는지 1개 이상 존재할 수 있는지를 의미하지만, 실제 DBMS의 쿼리를 실행할 때 유니크 인덱스에 대해 동등 조건으로 검색한다는 것은 1건의 레코드만 찾으면 더 찾지 않아도 된다는 것을 옵티마이저에게 알려주는 효과를 내기 때문에 옵티마이저에게는 상당히 중요하다.\n","date":"2023-05-07T16:24:10+09:00","image":"https://codemario318.github.io/post/real_mysql_8_1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_8_1/","title":"8. 인덱스 (1)"},{"content":"데이터 암호화는 MySQL 5.7 버전부터 지원되기 시작했으며, 처음에는 데이터 파일(테이블스페이스)에서만 암호화 기능이 제공 되었으나 MySQL 8.0으로 업그레이드 리두 로그나 언두 로그, 복제를 위한 바이너리 로그 등도 모두 암호화 기능을 지원하기 시작했다.\n데이터 암호화 여부는 보안 감사에서 필수적으로 언급되는 부분이며, 핀테크 서비스처럼 중요한 정보를 저장하는 서비스에서는 응용 프로그램에서 암호화한 데이터를 데이터베이스 서버에서 다시 암호화하는 이중 암호화 방법을 선택하기도 한다.\n응용 프로그램의 암호화는 주로 중요 정보를 가진 칼럼 단위로 암호화를 수행하며, 데이터베이스 수준에서는 테입르 단위로 암호화를 적용한다.\nMySQL 서버의 데이터 암호화 MySQL 서버의 암호화 기능은 데이터베이스 서버와 디스크 사이의 데이터 읽고 쓰기 지점에서 암호화 또는 복호화를 수행한다. 즉 MySQL 서버(InnoDB 스토리지 엔진)의 I/O 레이어에서만 데이터의 암호화 및 복호화 과정이 실행되므로디스크 입출력 이외의 부분에서는 암호화 처리가 전혀 필요치 않다.\nMySQL 서버가 사용자의 쿼리를 처리하는 과정에서 테이블의 데이터가 암호화돼 있는지 여부를 식별할 필요가 없으며, 암호화된 테이블도 그렇지 않은 테이블과 동일한 처리 과정을 거친다.\n데이터 암호화 기능이 활성화돼 있다고 하더라도 MySQL 내부와 사용자의 입장에서는 아무런 차이가 없기 때문에 이러한 암호화 방식을 가리켜 TDE(Transparent Data Encryption)이라고 한다.\n2단계 키 관리 MySQL 서버의 TDE에서 암호화 키는 키링(KeyRing) 플러그인에 의해 관리되며, MySQL 8.0 버전에서 지왼되는 키링 플러그인은 다음과 같다.\nkeyring_file File-Based 플러그인 keyring_encrypted_file Keyring 플러그인 keyring_okv KMIP 플러그인 keyring_aws Amazon Web Services keyring 플러그인 MySQL커뮤니티 에디션에서는 keyring_file 플러그인만 사용 가능하고, 나머지 플러그인은 모두 엔터프라이즈 에디션에서만 사용 가능하다.\n다양한 플러그인이 제공되지만 마스터 키를 관리하는 방법만 다를 뿐 MySQL 서버 내부적으로 작동하는 방식은 모두 동일하다. MySQL 서버의 키링 플러그인은 2단계(2-Tier) 키 관리 방식을 사용한다.\nMySQL 서버의 데이터 암호화는 마스터 키(master key)와 테이블스페이스 키(tablespace key)라는 두 가지 종류의 키를 가지고 있는데, 테이블스페이스 키는 프라이빗 키(private key)라고도 한다.\nHasicorp Vault 같은 외부 키 관리 솔루션(KMS, Key Management Service) 또는 디스크의 파일(Keyring_file 또는 keyring_encrypted_file 플러그인 사용시)에서 마스터 키를 가져오고, 암호화된 테이블이 생성될 때마다 해당 테이블을 위한 임의의 테이블스페이스 키를 발급한다. 마스터 키를 이용해 테이블 스페이스키를 암호화해서 각 테이블의 데이터 파일 헤더에 저장한다. 이렇게 생성된 테이블스프에스 키는 테이블이 삭제되지 않는 이상 절대 변경되지 않지만, 테이블 스페이스키는 절대 MySQL 서버 외부로 노출되지 않기 때문에 테이블스페이스 키를 주기적으로 변경하지 않아도 보안상 취약점이 되지는 않는다.\n하지만 마스터 키는 외부의 파일을 이용하기 때문에 노출될 가능성이 있어 주기적으로 변경해야 한다.\n1 ALTER INSTANCE ROTATE INNODB MASTER KEY; 마스터키를 변경하면 MySQL서버는 기존의 마스터 키를 이용해 각 테이블의 테이블스페이스 키를 복호화한 다음 새로운 마스터 키로 다시 암호화화한다. 마스터 키가 변경되는 동안 MySQL 서버의 테이블스페이스 키 자체와 데이터 파일의 데이터는 전혀 변경되지 않는다.\nMySQL 서버에서 이렇게 2단계 암호화 방식을 사용하는 이유는 암호화 키 변경으로 인한 과도한 시스템 부하를 피하기 위해서다.\n테이블스페이스 키가 변경된다면 MySQL 서버는 데이터 파일의 모든 데이터를 다시 복호화했다가 다시 암호화해야 하므로, 키를 변경할 때마다 매우 큰 작업을 수행해야 하며, 이에따라 사용자 쿼리를 처리하는 데도 상당한 영향을 미치게 된다.\nMySQL 서버의 TDE에서 지원되는 암호화 알고리즘은 AES 256bit이며, 이외의 알고리즘은 지원되지 않는다.\n테이블스페이스 키는 AES-256(Electronic CodeBook) 알고리즘을 이용해 암호화 되고, 실제 데이터 파일은 AES-256 CBC(Cipher Block Chaining) 알고리즘을 이용해 암호화 된다. 암호화 성능 MySQL 서버의 암호화는 TDE 방식이기 때문에 디스크로부터 한 번 읽은 데이터 페이지는 복호화되어 InnoDB 버퍼풀에 적재된다. 따라서 데이터 페이지가 한 번 메모리에 적재되면 암호화되지 않은 테이블과 동일한 성능을 보인다.\n쿼리가 InnoDB 버퍼풀에 존재하지 않는 데이터 페이지를 읽어야 하는 경우에는 복호화 과정을 거치기 때문에 복호화 시간동안 쿼리 처리가 지연될 수 있다. 암호화된 테이블이 변경되면 다시 디스크로 동기화될 때 암호화돼야 하기 때문에 디스크에 저장할 때도 추가로 시간이 더 걸린다. 데이터 페이지 저장은 사용자의 쿼리를 처리하는 스레드가 아는 MySQL 서버으 백그라운드 스레드가 수행하기 때문에 실제 사용자 쿼리가 지연되는 것은 아니다. UPDATE, DELETE 명령 또한 변경하고자 하는 레코드를 InnoDB 버퍼풀로 읽어와야 하기 대문에 새롭게 디스크에서 읽어야 하는 데이터 페이지의 개수에 따라서 복호화 지연이 발생할 수 있다. AES 암호화 알고리즘은 암호화하고자 하는 평문의 길이가 짧은 경우 암호화 키의 크기에 따라 암호화된 결과의 용량이 더 커질수도 있지만, 이미 데이터 페이지는 암호화 키보다 훨씬 크기 때문에 암호화 결과가 평문의 결과와 동일한 크기의 암호문을 반환한다. 따라서 TDE를 적용한다고 해도 데이터 파일의 크기는 암호화되지 않은 테입르과 동일한 크기를 가진다. 즉 암호화한다고 해서 InnoDB 버퍼풀의 효율이 달라지거나 메모리 사용 효율이 떨어지는 현상은 발생하지 않는다.\n같은 테이블에 대해 암호화와 압축이 동시에 적용되면 MySQL 서버는 압축을 먼저 실행하고 암호화를 적용한다.\n일반적으로 암호화된 결과문은 아주 랜덤한 바이트의 배열을 가지게 되는데, 이는 암축률을 상당히 떨어뜨린다. 암호화된 테이블의 데이터 페이지는 복호화된 상태로 InnoDB 버퍼풀에 저장되지만, 압축된 데이터 페이지는 압축 또는 압축 해제의 모든 상태로 InnoDB 버퍼풀에 전재할 수 있다. 암호화와 복제 MySQL 서버의 복제에서 레플리카 서버는 소스 서버의 모든 사용자 데이터를 동기화할 때 TDE를 이용한 암호화 사용 시 마스터 키와 테이블스페이스 키는 제외된다.\nMySQL 서버에서 기본적으로 모든 노드는 각자의 마스터 키를 할당해야 한다. 데이터베이스 서버의 로컬 디렉터리에 마스터 키를 관리하는 경우에는 소스 서버와 레플리카 서버는 서로 다른 마스터 키를 갖도록 설정해야 한다. 마스터 키 자체가 레플리카로 복제되지 않기 때문에 테이블스페이스 키 또한 레플리카로 복제되지 않는다.\n결국 소스 서버와 레플리카 서버는 서로 각자의 마스터 키와 테이블 스페이스 키를 관리하기 때문에 복제 멤버들의 데이터 파일은 암호화 되기 전의 값이 동일하더라도 실제 암호화된 데이터가 저장된 데이터 파일의 내용은 완전히 달라진다.\n복제 소스 서버의 마스터 키를 변경할 때는 ALERT INSTANCE ROTATE INNODB MASTER KEY 명령을 실행하는데, 이때 명령 자체는 레플리카 서버로 복제되지만 실제 소스 서버의 마스터 키 자체가 레플리카 서버로 전다로디는 것은 아니다. 그래서 마스터 키 로테이션을 실행하면 소스 서버와 레플리카 서버가 각각 서로 다른 마스터 키를 새로 발급받는다.\nMySQL 서버의 백업에서 TDE의 키링(Keyr Ring)파일을 백업하지 않는 경우가 있는데, 이 경우 키링 파일을 찾지 못하면 데이터를 복구할 수 없게 된다. 키링 파일을 데이터 백업과 별도로 백업한다면 마스터 키 로테이션 명령으로 TDE의 마스터 키가 엊네 변경됐는지까지 기억하고 있어야 한다.\nKeyring_file 플러그인 설치 MySQL 서버의 데이터 암호화 기능인 TDE의 암호화 키 관리는 플러그인 방식을 제공된다.\nKeyring_file플러그인은 테이블스페이스 키를 암호화하기 위한 마스터 키를 디스크의 파일로 관리하는데, 이때 마스터 키는 평문으로 디스크에 저장된다. 즉 마스터키가 저장된 파일이 외부에 노출된다면 데이터 암호화는 무용지물이 된다.\nkeyring_file플러그인은 마스터 키를 암호화하지 않은 상태의 평문으로 로컬 디스크에 저장하기 때문에 보안 요건을 충족시켜주지 않을 수 있다. 그럼에도 keyring_file 플러그인을 사용하고자 한다면 MySQL 서버가 시작될 때만 키링 파일을 다른 서버로부터 다운로드해서 로컬 디스크에 저장한 후 MySQL 서버를 시작하는 방법을 고려할 수 있다. MySQL 서버가 시작되면 마스터 키를 메모리에 캐시하기 때문에 로컬 디스크의 키링 파일을 삭제해도 문제는 전혀 없다. Percona Server는 HashiCorp Vault를 연동하는 키 관리 플러그인을 오픈소스로 제공하므로 함께 검토해보는 것을 권장한다.\nTDE 플러그인의 경우 MySQL 서버가 시작되는 단계에서도 가장 빨리 초기화돼야 한다.\n1 2 early-plugin-load = keyring_file.so keyring_file_data = /very/secure/directory/tde_master.key 그래서 다음과 같이 MySQL 서버의 설정 파일(my.cnf)에서 early-plugin-load 시스템 변수에 keyring_file 플러그인을 위한 라이브러리를 명시하면 된다. 그리고 keyring_file 플러그인이 마스터 키를 저장할 키링 파일의 경로를 keyring_file_data 설정에 명시하면 된다.\n설정 파일이 준비되면 MySQL 서버 시작시 자동으로 keyring_file플러그인이 초기화된다.\n1 SHOW PLUGINS; 초기화와 동시에 지정한 경로에 빈 파일을 생성한다. 데이터 암호화 기능을 사용하는 테이블을 생성하거나 마스터 로테이션을 실행하면 키링 파일의 마스터 키가 초기화된다.\n테이블 암호화 키링 플러그인은 마스터 키를 생성하고 관리하는 부분까지만 담당하기 때문에 어떤 키링 플러그인을 사용하든 관계 없이 암호화된 테이블을 생성하고 활용하는 방법은 모두 동일하다.\n테이블 생성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 CREATE TABLE tab_encrypted ( id INT, data VARCHAR(100), PRIMARY KEY (id) ) ENCRYPTION=\u0026#39;Y\u0026#39;; INSERT INTO tab_encrypted VALUES (1, \u0026#39;test\u0026#39;); SELECT * FROM tab_encrypted; /** +--+----+ |id|data| +--+----+ | 1|test| +--+----+ */ MySQL 서버에서 암호화된 테이블만 검색할 때는 information_schema의 TABLES 뷰를 이용한다.\n1 2 3 4 5 SELECT table_schema ,table_name ,create_options FROM information_schema.tables WHERE table_name=\u0026#39;tab_encrypted\u0026#39;; 테이블을 생성할 때마다 옵션을 설정하면 실수로 암호화 적용을 잊어버릴 수도 있으므로 MySQL 서버의 모든 테이블에 암호화를 적용하고자 한다면 default_table_encryption 시스템 변수를 ON으로 설정하면 ENCRYPTION 옵션을 별도로 설정하지 않아도 암호화된 테이블로 생성된다.\n응용 프로그램 암호화와의 비교 응용 프로그램에서 직접 암호화해서 MySQL 서버에 저장하는 경우도 있는데, 이 경우 저장되는 칼럼의 값이 이미 암호화된 것인지 여부를 MySQL 서버는 인지하지 못한다. 그래서 응용 프로그램에서 암호화된 컬럼은 인덱스를 생성하더라도 인덱스의 기능을 100% 활용할 수 없다.\n응용 프로그램에서 직접 암호화하지 않고 MySQL 서버의 암호화 기능(TDE)을 사용한다면 MySQL 서버는 인덱스 관련된 작업을 모두 처리한 후 최종 디스크에 데이터 페이지를 저장할 때만 암호화 하기 때문에 제약이 줄어든다.\n응용 프로그램에서의 암호화 기능은 서비스의 요건과 성능을 고려해서 선택해야 하고, MySQL 서버의 암호화 기능과 혼합해서 사용한다면 더 안전한 서비스를 구축할 수 있을 것이다.\n테이블스페이스 이동 MySQL 서버의 데이터베이스 관리자라면 테이블스페이스만 이동하는 기능을 자주 사용하게 되는데, 테이블을 다른 서버로 복사해야 하는 경우 또는 특정 테이블의 데이터 파일만 백업했다가 복구하는 경우라면 테이블스페이스 이동(Export \u0026amp; Import) 기능이 레코드르 덤프했다가 복구하는 방식보다 훨씬 효율적이고 빠르다.\n그런데 TDE가 적용되어 암호화된 테이블의 원본 MySQL 서버와 목적지 MySQL 서버의 암호화 키(마스터 키)가 다르기 때문에 FLUSH TABLES 명령으로 테입르스페이스를 익스포트 할 수 있다.\n1 FLUSH TABLES souce_table FOR EXPORT; MySQL 서버는 source_table의 저장되지 않은 변경을 모드 디스크로 기록하고, 더이상 접근할 수 없게 잠금을 건다. 그와 동시에 source_table의 구조를 source_table.cfg 파일로 기록한다. 암호화된 테이블의 테이블스페이스 키를 기존 마스터 키로 복호화한 후, 임시로 발급한 마스터 키를 이용해 다시 암호화해서 데이터 파일의 헤더 부분에 저장한다. 따라서 암호화된 테이블의 경우 테이블스페이스 이동 기능을 사용할 때는 반드시 데이터 파일과 임시 마스터 키가 저장된 *.cfp 파일을 함께 복사해야 한다. *.cfg파일은 단순히 테이블의 구조만 가지고 있기 때문에 파일이 없어져도 경고만 발생하지만, *.cfp 파일이 없어지면 복구가 불가능해진다.\n언두 로그 및 리두 로그 암호화 테이블의 암호화를 적용하더라도 디스크로 저장되는 데이터만 암호화되고 MySQL 서버의 메모리에 존재하는 데이터는 복호화된 평문으로 관리되며, 이 평문 데이터가 테이블의 데이터 파일 이외의 디스크 파일로 기록되는 경우에는 여전히 평문으로 저장된다.\n그래서 테이블 암호화를 적용해도 리두 로그나 언두 로그, 그리고 복제를 위한 바이너리 로그에는 평문으로 저장되는 것이다. MySQL 8.0.16 버전 부터는 innodb_undo_log_encrypt, innodb_redo_log_encrypt 시스템 변수를 이용해 언두 로그와 리두 로그를를 암호화 된 상태로 저장할 수 있게 개선되었다.\nMySQL 서버는 리두 로그나 언두 로그를 평문으로 저장하다가 암호화가 활성화되면 그때부터 생성되는 리두 로그와 언두 로그만 암호화해서 저장한다. 반대로 리두 로그와 언두 로그가 암호화되는 상태에서 암호화를 비활성화하면 그때부터 저장되는 로그만 평문으로 저장한다.\n따라서 리두 로그와 언두 로그는 암호화를 활성화 했다가 비활성화 한다고 해서 즉시 암호화에 사용된 키가 불필요해지는 것이 아니다. 특히 언두 로그의 경우 암호화를 비활성화 한다고 하더라도 새로 생성되는 언두 로그는 평문으로 저장되겠지만 기존 언두 로그는 여전히 암호화된 상태로 남아있어 상황에 따라 계속해서 암호화키가 필요할 수 있다.\n바이너리 로그 암호화 테이블 암호화가 적용돼도 바이너리 로그와 릴레이 로그 파일 또한 리두 로그나 언두 로그처럼 평문을 저장한다. 일반적으로 언두 로그와 리두 로그는 기맂 않은 시간동안의 데이터만 가지기 때문에 보안에 민감하지 않을 수 있지만 바이너리 로그 파일의 암호화는 상황에 따라 중요도가 높아질 수 있다.\n바이너리 로그는 의도적으로 상당히 긴 시간동안 보관할 수도 있다. 증분 백업(Incremental Backup)을 위해 바이너리 로그를 보관하기도 한다. 바이너리 로그와 릴레이 로그 파일 암호화 기능은 디스크에 저장된 로그 파일에 대한 암호화만 담당하고, MySQL 서버의 메모리 내부 또는 소스 서버와 레플리카 서버 간의 네트워크 구간에서 로그 데이터를 암호화하지는 않는다. 복제 멤버 간의 네트워크 구간에서도 바이너리 로그를 암호화하고자 한다면 MySQL 복제를 위한 계정이 SSL을 사용하도록 설정한다.\n바이너리 로그 암호화 키 관리 바이너리 로그와 릴레이 로그 파일 데이터의 암호화를 위해서도 MySQL 서버는 2단계 키 관리 방식을 사용한다.\n바이너리 로그와릴레이 로그 파일의 데이터는 파일 키(File Key)로 암호홰해서 디스크로 저장하고, 파일 키는 \u0026ldquo;바이너리 로그 암호화 키\u0026quot;로 암호화 해서 각 바이너리 로그와 릴레이 로그 파일의 헤더에 저장된다.\n즉 \u0026ldquo;바이너리 로그 암호화 키\u0026quot;는 테이블 암호화의 마스터 키와 동일한 역할을 하며, 파일 키는 바이너리 로그와 릴레이 로그 파일 단위로 자동으로 생성되어 해당 로그 파일의 데이터 암호화에만 사용된다.\n바이너리 로그 암호화 키 변경 1 ALTER INSTANCE ROTATE BINLOG MASTER KEY; 바이너리 로그 암호화 키가 변경되면 다음 과정을 거친다.\n증가된 시퀀스 번호와 함께 새로운 바이너리 로그 암호화 키 발금 후 키링 파일에 저장 바이너리 로그 파일과 릴레이 로그 파일 스위치(새로운 로그 파일로 로테이션) 새로 생성되는 바이너리 로그와 릴레이 로그 파일의 암호화를 위해 파일 키를 생성하고, 파일 키는 바이너리 로그 파일 키로 암호화해서 각 로그 파일에 저장 기조 ㄴ바이너리 로그와 릴레이 로그 파일의 파일 키를 읽어서 새로운 바이너리 로그 파일 키로 암호화해서 다시 저장(암호화되지 않은 로그 파일은 무시) 모든 바이너리 로그와 릴레이 로그 파일리 새로운 바이너리 로그 암호화 키로 다시 암호화됐다면 기조 ㄴ바이너리 로그 암호화 키를 키링 파일에서 제거 4번 과정은 상당히 시간이 걸릴 수 있는데, 이를 위해 키링 파일에서 \u0026ldquo;바이너리 로그 암호화 키\u0026quot;는 내부적으로 버전 관리가 이뤄진다.\nmysqlbinlog 도구 활용 MySQL 서버에서는 트랜잭션의 내용을 추적하거나 백업 복구를 위해 암호화된 바이너리 로그를 평문으로 복호화할 일이 자주 발생한다. 하지만 한 번 바이너리 로그 파일이 암호화되면 바이너리 로그 암호화 키가 없으면 복호화할 수 없다.\n그런데 바이너리 로그 암호화 키는 MySQL 서버만 가지고 있어서 복호화가 불가능하다. mysqlbinlog 도구를 이용해 암호화된 바이너리 로그 파일을 직접 열어볼 수 없다는 에러 메시지를 출력하게 된다.\n바이너리 로그 암호화 키는 그 바이너리 로그나 릴레이 로그 파일을 생성한 MySQL 서버만 가지고 있기 때문에 MySQL 서버와 관게없이 mysqlbinlog 도구만으로는 복호화할 방법이 없다. 그래서 예전처럼 다른 서버로 복사하거나 바이너리 로그 파일을 백업하는 것은 소용없어졌다.\n바이너리 로그 파일의 내용을 확인할 방법은 MySQL 서버를 통해 가져오는 방법이 유일하다.\n1 mysqlbinlog --read-from-remote-server -uroot -p -vvv mysql-bin.000011 ","date":"2023-05-01T16:24:10+09:00","image":"https://codemario318.github.io/post/real_mysql_7/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_7/","title":"7. 데이터 암호화"},{"content":"MySQL 서버에서 디스크에 저장된 데이터 파일의 크기는 일반저긍로 쿼리의 처리 성능과도 직결되지만 백업 및 복구 시간과도 밀접하게 연결된다.\n디스크의 데이터 파일이 크면 클수록 쿼리를 처리하기 위해서 더 많은 데이터 페이지를 InnoDB 버퍼풀로 읽어야 할 수 있다. 새로운 페이지가 버퍼풀로 적재되기 때문에 그만큼 더티 페이지가 더 자주 디스크로 기록돼야 한다. 데이터 파일이 크면 클수록 백업 시간이 오래 걸리며, 복구하는 데도 그만큼의 시간이 걸린다. 그만큼 저장 공간이 필요하기 때문에 비용 문제도 있을 수 있다. 많은 DBMS가 이러한 문제점을 해결하기 위해 데이터 압축 알고리즘을 제공하면, MySQL 서버에서 사용 가능한 압축 방식은 크게 테이블 압축과 페이지 압축의 두 가지 종류로 구분할 수 있다.\n페이지 압축 페이지 압축은 MySQL 서버가 디스크에 저장하는 시점에 데이터 페이지가 압축되어 저장되고, 반대로 MySQL 서버가 디스크에서 데이터 페이지를 읽어올 때 압축이 해제되어, 버퍼풀에 데이터 페이지가 적재되면 InnoDB 스토리지 엔진은 압축이 해제된 상태로만 데이터 페이지를 관리한다. 이에 따라 서버의 내부 코드에서는 압축 여부와 관계없이 투명(Transparent)하게 작동하여 Transparent Page Compression로 불리기도 한다.\n16KB 데이터 페이지를 압축한 결과가 용량이 얼마나 될지 예측이 불가능한데 적어도 하나의 테이블은 동일한 크기의 페이지(블록)로 통일돼야 한다. 따라서 페이지 압축 기능은 운영체제별로 특정 버전의 파일 시스템에서만 지원되는 펀치홀(Punch hole)이라는 방식을 사용한다.\n펀치홀이란?\n운영체제에서 제공하는 파일 시스템 인터페이스 일부로, 파일 내용에서 일부 데이터 블록을 삭제하여 디스크 공간을 확보하는 기능이다.\n이전에는 파일 전체를 복사하고 일부분을 수정하는 드으이 방법으로 파일을 수정해야 했으나 시간과 디스크 공간을 많이 소모하게 되어 펀치홀이 개발되었다.\n펀치홀은 일부 운영체제에서만 지원되는 기능이며(리눅스), 대용량 파일 시스템에 사용되어 성능을 향상시키는데 도움을 준다.\n16KB 페이지를 압축(압축 결과를 7KB로 가정) MySQL 서버는 디스크에 압축된 결과 7KB를 기록(이때 MySQL 서버는 압축 데이터 7KB에 9KB 빈 데이터를 기록) 디스크에 데이터를 기록한 후, 7KB 이후의 공간 9KB에 대해 펀치 홀(Punch-hole) 생성 파일 시스템은 7KB만 남기고 나머지 디스크의 9KB 공간은 다시 운영체제로 반납 운영체제(파일 시스템)의 블록 사이즈가 512바이트인 경우, MySQL 서버는 특정 테이블에 대해 16KB 크기의 페이지를 유지하면서도 압축된 다양한 크기의 데이터 페이지를 디스크에 저장하고 압축된 만큼의 공간을 절약할 수 있다.\n문제점\n펀치홀 기능은 운영체제뿐만 아니라 하드웨어 자체에서도 지원을 해야 사용 가능하다. 아직 파일 시스템 관련 명령어(유틸리티)가 펀치홀을 지원하지 못한다. MySQL 서버의 데이터 파일은 해당 서버에만 머무는 것이 아니라 백업했다가 복구하는 과정에서 데이터 파일 복사 과정이 실행되고, 그 외에도 많은 파일 관련 유틸리티들을 활용한다. 이러한 이유로 실제 페이지 압축은 많이 사용되지 않는 상태이다.\n1 2 3 4 5 6 /* 테이블 생성시 */ CREATE TABLE t1 (c1 INT) COMPRESSION=\u0026#34;zlib\u0026#34;; /* 테이블 변경 시 */ ALTER TABLE t1 COMPRESSION=\u0026#34;zlib\u0026#34;; OPTIMAIZE TABLE t1; 테이블 압축 테이블 압축은 운영체제나 하드웨어에 대한 제약 없이 사용할 수 있기 때문에 일반적으로 더 활용도가 높은편이다. 테이블 압축은 우선 디스크의 데이터 파일의 크기를 줄일 수 있기 때문에 그만큼의 이득은 있지만, 내부적인 처리 과정과 버퍼풀에서 처리 방식으로 인해 몇가지 단점이 존재한다.\n버퍼풀 공간 활용률이 낮음 쿼리 처리 성능이 낮음 빈번한 데이터 변경시 압축률 떨어짐 압축 테이블 생성 테이블 압축을 사용하기 위해 압축을 사용하려는 테이블이 별도의 테이블 스페이스를 사용해야 한다.\n이를 위해서는 innodb_file_per_table 시스템 변수가 ON으로 설정된 상태에서 테이블이 생성돼야 한다. 테이블 압축을 사용하는 테이블들은 테이블을 생성할 때 ROW_FORMAT=COMPRESSED 옵션을 명시해야 한다. KEY_BLOCK_SIZE 옵션을 이용해 압축된 페이지의 타깃 크기(목표 크기, 2n(n \u0026gt;= 2))를 명시해야 한다. InnoDB 스토리지 엔진의 페이지 크기가(innodb_page_size)가 16KB 라면 KEY_BLOCK_SIZE는 4KB 또는 8KB만 설정할 수 있다. 페이지 크기(innodb_page_size)가 32KB or 64KB인 경우에는 테이블 압축을 적용할 수 없다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SET GLOBAL innodb_file_per_table=ON; /** ROW_FORMAT 옵션과 KEY_BLOCK_SIZE 옵션을 모두 명시 **/ CREATE TABLE compressed_table ( c1 INT PRIMARY KEY ) ROW_FORMAT=COMPRESSED KEY_BLOCK=8; /** KEY_BLOCK_SIZE 옵션을 모두 명시 ROW_FORMAT 옵션이 생략되면 자동으로 ROW_FORMAT=COMPRESSED 옵션이 추가되어 생성 **/ CREATE TABLE compressed_table ( c1 INT PRIMARY KEY ) ROW_FORMAT=COMPRESSED KEY_BLOCK=8; innodb_file_per_table 시스템 변수가 0 인 상태에서 제너럴 테이블스페이스(General Tablespace)에 생성되는 테이블도 테이블 압축을 사용할 수 있으나, 제너럴 테이블스페이스의 FILE_BLOCK_SIZE에 의해 압축을 사용하지 못할 수 있다.\n테이블 압축 동작 방식 현재 InnoDB스토리지 엔진의 데이터 페이지 크기가 16KB, KEY_BLOCk_SIZE가 8로 설정되었다면,\n16KB의 데이터 페이지를 압축 압축된 결과가 8KB 이하이면 그대로 디스크에 저장 압축된 결과가 8KB를 초과하면 원본 페이지를 스플릿(split)해서 2개의 페이지에 8KB씩 저장 나뉜 페이지 각각에 대해 \u0026ldquo;1\u0026rdquo; 단계를 반복 실행 테이블 압축 방식에서 가장 중요한 것은 원본 데이터 페이지의 압축 결과가 목표 크기(KEY_BLOCK_SIZE)보다 작거나 같을 때까지 반복해서 페이지를 스플릿하는 것이다.\n따라서 목표 크기가 잘못 설정되면 MySQL 서버의 처리 성능이 급격히 떨어질 수 있다.\nKEY_BLOCK_SIZE 결정 테이블 압축에서 가장 중요한 부분은 압축된 결과가 어느 정도가 될지 예측해서 KEY_BLOCK_SIZE를 결정하는 것이다.\n따라서 테이블 압축을 적용하기 전에 먼저 KEY_BLOCK_SIZE를 4KB 또는 8KB로 테이블을 생성하여 샘플 데이터를 저장해 보고 적절한지 판단하는 것이 좋다.\n이때 샘플 데이터는 많으면 많을수록 더 정확한 테스트가 가능한데, 최소한 테이블 데이터 페이지가 10개 정도는 생성되도록 테스트 데이터를 INSERT해보는 것이 좋다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 USE employees; CREATE TABLE employees_comp4k ( emp_no int NOT NULL, /** ... */ first_name varchar(14) NOT NULL, hire_date date NOT NULL, PRIMARY KEY (emp_no), KEY ix_firstname (first_name), KEY ix_hiredate (hire_date) ) ROW_FORMAT=COMRESSED KEY_BLOCK_SIZE=4; /** 테스트 실행 전 innodb_cmp_per_index_enabled 시스템 변수를 ON으로 변경해야 인덱습려로 압축 실행 횟수와 성공 횟수가 기록된다. */ SET GLOBAL innodb_cmp_per_index_enabled=ON; INSERT INTO employees_comp4k SELECT * FROM employees; SELECT table_name ,index_name ,compress_ops ,compress_ops_ok (compress_ops - compress_ops_ok) / compress_ops * 100 AS compression_failure_pct FROM information_schema.INNODB_CMP_PER_INDEX; 결과\n\u0026hellip; index_name \u0026hellip; compression_failure_pct PRIMARY 27.6737 ix_firstname 8.0168 ix_hiredate 13.4561 실패율이 높다는 의미는 압축 결과가 4KB를 초과하여 데이터 페이지를 스플릿해서 다시 압축을 많이 했다는 뜻이다. 일반적으로 압축 실패율은 3~5%미만으로 유지하는 것이 좋으며 이에 맞추어 KEY_BLOCk_SIZE를 적절히 조절해야 한다.\nKEY_BLOCk_SIZE를 크게 설정하였는데도 압출 실패율이 높게 나타난다면, InnoDB 버퍼풀에서 디스크로 기록되기 전에 압축하는 과정에 꽤 오랜 시간이 걸릴 것으로 예측 가능하다. 성능에 민감한 서비스라면 압축을 적용하지 않는 것이 좋을 수 있다.\n압축 실패율이 높다고 해서 무조건 압축을 사용하지 말아야 한다는 것을 의미하지는 않는다.\nINSERT만 되는 로그 테이블은 실패율이 높다고 하더라도 데이터 파일의 크기가 큰 폭으로 줄어든다면 큰 손해는 아닐 수 있다. 반대로 실패율이 낮다고 해서 무조건 압축을 적용하는 것도 좋지 않을 수 있다.\n알고리즘 실행에 적지 않은 CPU 자원을 소비하므로, 테이블의 데이터가 빈번하게 조회되고 변경되는 경우 압축을 적용하지 않는 것이 좋을 수 있다. 압축된 페이지의 버퍼풀 적재 및 사용 InnoDB 스토리지 엔진은 압축된 테이블의 데이터 페이지를 버퍼풀에 적재하면 압축된 상태와 압축이 해제된 생태 2개 버전을 관리한다. 따라서 InnoDB 스토리지 엔진은 디스크에서 읽은 상태 그대로의 데이터 페이지 목록을 관리하는 LRU 리스트와 압축된 페이지들의 압축 해제 버전인 Unzip_LRU 리스트를 별도로 관리하게 된다.\n결국 InnoDB 스토리지 엔진은 압축된 테이블에 대해서는 버퍼풀의 공간을 이주응로 사용함으로써 메모리를 낭비하게 된다. 압축된 페이지에서 데이터를 읽거나 변경하기 위해서 압축을 해제해야하므로 CPU를 상대적으로 많이 소모한다. 이러한 단점을 보완하기 위해 Unzip_LRU 리스트를 별도로 관리하고 있다가 MySQL 서버로 유입되는 요청 패턴에 따라 적절히 다음과 같은 처리를 수행한다.\nInnoDB 버퍼풀 공간이 필요한 경우에는 LRU 리스트에서 원본 데이터 페이지(압축)는 유지하고, Unzip_LRU 리스트에서 압축 해제된 버전은 제거해서 버퍼풀의 공간을 확보한다. 압축된 데이터 페이지가 자주 사용되는 경우에는 Unzip_LRU 리스트에 압축 해제된 페이지를 계속 유지하면서 압축 및 압축 해제 작업을 최소화한다. 압축된 데이터 페이지가 사용되지 않아서 LRU 리스트에서 제거되는 경우에는 Unzip_LRU 리스트에서도 함께 제거된다. InnoDB 스토리지 엔진은 버퍼풀에서 압축 해제된 버전의 데이터 페이지를 적절한 수준으로 유지하기 위해 다음과 같은 어댑티브 알고리즘을 사용한다.\nCPU 사용량이 높은 서버에서는 가능하면 압축과 압축 헤제를 피하기 위해 Unzip_LRU의 비율을 높에서 유지한다. Disk IO 사용량이 높은 서버에서는 가능하면 Unzip_LRU 리스트의 비율을 낮춰 InnoDB 버퍼풀 공간을 더 확보하도록 작동한다. 테이블 압축 관련 설정 테입르 압축을 사용할 때 연관된 시스템 변수가 몇가지 있는데, 모두 페이지의 압축 실패율을 낮추기 위해 필요한 튜닝 포인트를 제공한다.\ninnodb_cmp_index_enable\n테이블 압축이 사용된 테이블의 모든 인덱스별로 압축 성공 및 압축 실행 횟수를 수집하도록 설정한다. 비활성화시 테이블 단위의 압축 성공 및 실행 횟수만 수집 테이블 단위 수집된 정보: infoamtion_schema.INNODB_CMP테이블에 기록 인덱스 단위 수집된 정보: infoamtion_schema.INNODB_CMP_PER_INDEX테이블에 기록 innodb_compression_level\nInnoDB의 데이터 압축은 zlib알고리즘만 지원하는데, 시스템 변수를 이용해 압축률을 설정할 수 있다(0~9). 값이 커질수록 느려지고, 작아진다. 기본값은 6 innodb_compression_failure_threshold_pct\n테이블 단위로 압축 실패율이 시스템 설정값 보다 커지면 압축을 실행하기 전 원본 데이터 페이지 끝에 의도적으로 일정 크기의 빈 공간을 추가한다. 추가된 빈 공간은 압축률을 높여서 압축 결과가 KEY_BLOCK_SIZE보다 작아지게 만드는 효과를 낸다. 추가하는 빈 공간을 패딩이라고 하며, 패딩 공간은 실패율이 높아질수록 계속 증가된 크기를 가진다. innodb_log_compressed_pages\nMySQL 서버가 비정상적으로 종료됐다가 다시 시작되는 경우 압축 알고리즘의 버전 차이가 있더라도 복구 과정이 실패하지 않도록 InnoDB 스토리지 엔진은 압축된 데이터 페이지를 그대로 리두 로그에 기록한다. 압축 알고리즘을 덥그레이드 할 대 도움이 되지만, 데이터 페이지를 통째로 로그에 저장하는 것은 리두 로그의 증가량에 상당한 영향을 미칠수도 있다. 압축을 적용한 후 리두 로그 용량이 매우 빠르게 증가한다건아 버퍼풀로부터 더티 페이지가 한꺼번에 많이 기록되는 패턴으로 바뀌었다면, 해당 변수를 OFF로 설정하여 모니터링 해보는 것이 좋다. 기본값은 ON, 가능하면 기본값 상태를 유지하자. ","date":"2023-05-01T14:12:10+09:00","image":"https://codemario318.github.io/post/real_mysql_6/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_6/","title":"6. 데이터 압축"},{"content":"안녕하세요, 마리오입니다 :)\nAuth 영역 개발 처음은 스프링부트 프로젝트 구성입니다. 스프링 공식 문서에 있는 가이드를 참고하였습니다.\n새 프로젝트 만들기 가이드 문서에 나온대로 Spring Initializr를 사용하여 프로젝트를 생성해보겠습니다. IntelliJ에도 통합되어 있어 IDE에서 명령줄이나 웹 UI를 사용하지 않고도 새 프로젝트를 만들고 가져올 수 있다고 하네요.\nhttps://start.spring.io/#!language=kotlin\u0026amp;type=gradle-project-kotlin링크에 접속하여 언어를 Kotlin, 빌드 도구를 Gradle - Kotlin으로 설정하겠습니다.\n그 외 설정은 starter.spring.io의 기본 설정으로 유지하고, Dependencies만 필요한것을 추가하여 사용하겠습니다.\nSpring Web Spring Data JPA H2 Database Spring Boot DevTools Spring Security Redis Persistence with JPA Kotlin에서는 무분별한 상속을 막기 위해 기본적으로 모든 클래스는 기본 변경자가 final로 처리됩니다. Hibernate의는 이러한 규칙을 강요하지않아 final entity를 받더라도 에러가 발생하지는 않지만, JPA와 관련된 클래스와 프로퍼티들은 반드시 open 되어야 합니다(KT-28525).\n이에 따라 final로 해석되는 Kotlin Class를 그대로 활용할 경우 lazy fetching에 활용되는 Proxy가 정상적으로 동작하지 못해 eager fetching으로 처리되어 데이터 조회시 성능 이슈를 발생 시킬 수 있습니다.\n이러한 문제는 allopen 플러그인을 통해 해결 가능하므로, 플러그인 관련 코드를 build.gradle.kts 추가해주겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 plugins { // ... kotlin(\u0026#34;plugin.allopen\u0026#34;) version \u0026#34;1.7.22\u0026#34; } // ... allOpen { annotation(\u0026#34;jakarta.persistence.Entity\u0026#34;) annotation(\u0026#34;jakarta.persistence.Embeddable\u0026#34;) annotation(\u0026#34;jakarta.persistence.MappedSuperclass\u0026#34;) } kapt kapt(Kotlin Annotation Processing Tool)는 Kotlin에서 어노테이션 프로세서(annotation processor)를 실행하고 처리하는 도구로써 Kotlin으로 어노테이션을 작성하고 실행하기 위해 사용됩니다.\nKotlin에서는 애플리케이션 프로퍼티를 사용할 때 읽기 전용 프로퍼티 사용을 권장하는데, 커스텀 프로퍼티는 IDE에서 인식하지 못하기 때문에 메타데이터 생성을 위해 spring-boot-configuration-processor 종속성을 이용하여 Kapt를 구성해야한다고 하네요.\n마찬가지로 build.gradle.kts에 관련 설정을 추가하겠습니다.\n1 2 3 4 5 6 7 8 9 plugins { //... kotlin(\u0026#34;kapt\u0026#34;) version \u0026#34;1.7.22\u0026#34; } dependencies { //... kapt(\u0026#34;org.springframework.boot:spring-boot-configuration-processor\u0026#34;) } \u0026ldquo;설정 \u0026gt; 플러그인\u0026rdquo; 에서 Spring Boot 플러그인이 활성되어 있는지 확인\n\u0026ldquo;설정 \u0026gt; 빌드, 실행, 배포 \u0026gt; 컴파일러 \u0026gt; 어노테이션 프로세서\u0026rdquo; 에서 어노테이션 처리 활성화 터미널에서 ./gradlew kaptKotlin 명령 실행하여 메타데이터 생성 끝으로 프로젝트를 생성하고 기본적으로 필요한 구성은 모두 끝난 것 같습니다. 다음 차례 부터는 구현이되겠네요!\n끝까지 읽어주셔서 감사합니다 :D\n","date":"2023-04-27T12:30:18+09:00","image":"https://codemario318.github.io/post/toy-mario/0/auth/1/toy_mario_cover_hu223669cbbbe5efa1c70286790dba8a54_16582_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/toy-mario/0/auth/1/","title":"[Toy Mario: Auth] 1. 스프링부트 프로젝트 구성하기"},{"content":"안녕하세요, 마리오입니다 :)\n프로젝트 첫 걸음으로 Auth 영역을 만들어 보겠습니다.\n기술 검토 인증 관련 기능을 별도 API 서버로 분리하여 구현하기 위해 JWT(JSON Web Token)인증 방식이 가장 적절하다고 판단하였고, JWT인증 구현 관련 정보가 가장 많은 Spring Boot를 사용하려고 합니다.\n언어는 Java, Kotlin중 선택할 수 있었는데요!\n상대적으로 Java가 더 많은 정보들이 있었지만, 17 버전 이후부터는 Kotlin과 큰 차이가 없어서 평소에 공부하고 싶었던 Kotlin으로 결정하였습니다.\n기능 명세 Auth 영역에서 필수적이라고 생각되는 기능들은 아래와 같습니다. 회원 기능을 구현하는 방식은 다양하게 존재하지만, 저는 이메일을 ID로 사용하는 방식으로 구현해보려고 합니다.\n회원가입 로그인 로그아웃 비밀번호 재설정 회원탈퇴 메일 발송 ID를 이메일로 활용하기 때문에, 사용자가 자신의 이메일을 사용했는지 검증하는 처리가 필요합니다. 비밀번호 재설정에서도 필요하겠네요.\n이러한 이유로 이메일 검증을 위해 입력한 이메일 주소로 인증번호를 보내서 확인하는 처리가 필요할 것 같습니다.\n메일 발송도 여러 방식으로 처리 가능한데요. 저는 이메일 발송 인터페이스를 구현하고 처음에는 스프링에서 제공하는 라이브러리를 활용하여 구현하고, 이후에 별도 서버로 분리하여 메시지큐를 활용해 발송하는 방식으로 구현해보겠습니다.\n인증 번호 관리 사용자가 입력한 인증 번호를 확인하기 위해 발급한 인증 번호 관리가 필요한데 인증 번호는 영속성을 유지할 필요가 없습니다.\n따라서 일정 시간만 유지하는 것이 기능 이슈가 적을 것으로 생각되기 때문에 Redis를 활용해보겠습니다.\nAccess Token Only 앞서 말씀 드렸던 것 처럼 JWT를 활용하여 인증을 구현할 예정인데요. 이를 위해 Spring Security를 활용할 예정입니다.\n로그인 유지 기간이 오랜 시간은 필요하지 않을 것 같아 Refresh Token은 사용하지 않겠습니다.\nJWT Blacklist JWT를 이용해 인증 처리를 하게 되면 토큰이 만료되기 전까지 유효한 토큰으로 인식되는데, 로그아웃을 했을 경우 해당 토큰을 더 이상 사용하지 못하게 해야합니다.\n저는 인증번호에서도 사용한 Redis를 통해 JWT Blacklist를 구현하여 로그아웃을 구현하겠습니다.\n끝으로 사실 스프링을 제대로 써본적이 없고, kotlin도 처음이어서 걱정이 앞서네요 \u0026hellip; 그래도 최선을 다해서 해보겠습니다! 끝까지 읽어주셔서 감사합니다 :D\n","date":"2023-04-26T13:49:18+09:00","image":"https://codemario318.github.io/post/toy-mario/0/auth/0/toy_mario_cover_hu223669cbbbe5efa1c70286790dba8a54_16582_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/toy-mario/0/auth/0/","title":"[Toy Mario: Auth] 0. 시작"},{"content":"안녕하세요, 마리오입니다 :)\n일을 쉬는 동안 평소에 생각만 하고 있던 토이 프로젝트를 구현해보려 합니다. 그리고 프로젝트를 수행하는 과정에서 했던 고민과 학습 내용들을 글로 남겨보겠습니다.\n구조 사용자의 데이터를 받아 학습된 인공지능 모델을 통해 나온 결과를 제공하는 재미 위주의 서비스를 만들어 볼 예정입니다. 대략적인 흐름을 살펴보면 아래와 같습니다.\n처음엔 이미지 데이터를 이용하는 서비스만 생각했는데, 이후에도 다른 인공지능 모델들도 활용하고 싶었습니다. 그래서 모델마다 하나의 서비스를 가지고 모델이 추가될때마다 확장 가능할 수 있게 구성해봤습니다.\n시작할때는 데이터만 받는 방식으로 간단하게 만들어 보려고 했는데 고민하다보니 처음보다 복잡해졌네요.\n대략적인 구조는 이렇지만 아직 공부가 부족하기 때문에 프로젝트 진행에 따라 바뀔수도 있을 것 같습니다 :D\nAuth 영역 처음에 고려했던 부분은 아니지만 사용자가 자신이 사용했던 서비스의 결과를 다시 확인한다던가, 공유 한다던가 할때 더 많은 기능을 제공할 수 있을 것 같아 추가하게 되었습니다.\n이메일 정도만 활용해서 간단한 회원 기능을 구성할 예정입니다.\nService 영역 클라이언트의 요청 처리를 담당하게 될 영역입니다.\n사용자가 보낸 데이터를 받고, 인증된 사용자에게 Data API 서버를 통해 모델의 결과를 받아와서 다시 클라이언트에게 제공하는 역할을 담당합니다.\nData 영역 Service API 서버를 통해 받은 데이터를 처리하는 서버입니다. 데이터 관련 역할을 분리하기 위해서 구성해봤습니다.\n데이터 전처리 모델에게 결과 받아오기 데이터 아카이빙 위 3가지 역할을 담당하게 될 것 같은데, 처음 구현할 이미지 데이터를 활용하는 서비스는 데이터 전처리가 클라이언트, Service 영역 에서도 가능하고, 아카이빙도 Service 영역에서 해결하는게 좋을 수도 있을 것 같아 고민이 많은 부분이네요.\nData 영역은 개발 과정에서 역할을 나누게 되어 없어지거나, 여러개 작은 영역으로 분리할수도 있을 것 같네요.\nModel 영역 요청받은 데이터로 모델의 결과를 응답하는 영역입니다. 그리고 해당 모델을 새로 저장된 데이터로 다시 학습 하게 구현해 볼 예정입니다.\n끝으로 고민 하다보니 욕심이 많아져서 이것 저것 많은 기능을 넣게 되었네요. 앞으로 험난한 길을 가게될 것으로 예상됩니다 :(\n앞서 조금씩 언급했던 내용처럼 처음에는 간단한 CV 모델을 활용해서 전체 흐름을 파악해 볼 생각이에요. 느리더라도 꾸준히 수행해서 꼭 완성해보도록 하겠습니다.\n끝까지 읽어주셔서 감사합니다 :D\n","date":"2023-04-26T10:00:01+09:00","image":"https://codemario318.github.io/post/toy-mario/0/1/toy_mario_cover_hu223669cbbbe5efa1c70286790dba8a54_16582_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/toy-mario/0/1/","title":"[Toy Mario] 시작"},{"content":"데이터베이스의 데이터를 영속화하기 위해 볼륨 마운트를 사용할 수 있으며, 볼륨 마운트는 어플리케이션의 데이터를 영구적으로 저장해야 할 때 좋은 선택이 될 수 있다.\n바인드 마운트는 호스트 파일 시스템에서 디렉토리를 컨테이너로 공유하는 다른 유형의 마운트이다. 어플리케이션을 개발할 때 바인드 마운트를 사용하여 소스 코드를 컨테이너로 마운트 할 수 있다. 파일 시스템 변경 사항을 감시하고 그에 따라 대응하는 프로세스를 컨테이너에서 실행할 수 있기 때문에, 컨테이너는 파일을 저장할 때마다 즉시 코드 변경 사항을 인식한다.\n따라서 바인드 마운트와 파일 변경 상항을 감지하고 이에 따라 원하는 동작을 수행할 수 있으며, 이번 장에서는 자동으로 애플리케이션을 다시 시작하는 도구인 nodemon을 활용해본다.\nQuick volume type comparisons Named volumes Bind mounts 호스트 위치 도커가 선택함 사용자가 결정 마운트 예시(\u0026ndash;mount) type=volume,src=my-volume,target=/usr/local/data type=bind,src=/path/to/data,target=/usr/local/data Populates new volume with container contents Yes No Supports Volume Drivers Yes No ","date":"2023-04-24T16:46:25+09:00","image":"https://codemario318.github.io/post/docker_5/docker_cover_hue12353db563619e41ee3a11307d3cf25_62602_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/docker_5/","title":"5. Use bind mounts"},{"content":"컨테이너의 파일 시스템 컨테이너가 실행되면, 파일시스템은 여러 레이어들을 사용한다. 각각의 컨테이너는 파일을 생성, 업데이트, 삭제하기 위한 자신만의 \u0026ldquo;스크래치 공간\u0026quot;을 가지는데, 다른 컨테이너에서 같은 이미지를 사용한다고 해도 어떤 변경 사항에 대해서도 다른 컨테이너에서 활용할 수 없다.\nScratch image?\n빈 베이스 이미지로, 최소한의 컨테이너 이미지를 만들기 위해 사용된다. scratch를 기본 이미지로 사용하면 컨테이너에는 파일 시스템 내용이 없으므로 앱이 필요로 하는 모든 파일을 이미지의 일부로 포함해야 한다.\nscratch space는 사용 중인 이미지에 기반하여 각 컨테이너마다 생성되는 파일 시스템이다.\n컨테이너 볼륨 컨테이너는 파일을 생성, 업데이트 및 삭제할 수 있지만 컨테이너를 제거하면 해당 변경 사항이 모두 손실되며 Docker는 해당 컨테이너의 모든 변경 사항을 격리한다. 이때, 볼륨을 사용하면 모든 변경 사항을 반영할 수 있다.\n볼륨은 컨테이너의 특정 파일 시스템 경로를 호스트 머신과 연결하는 기능을 제공한다. 컨테이너에서 디렉토리를 마운트하면 해당 디렉토리의 변경 사항을 호스트 머신에서도 볼 수 있다. 컨테이너 재시작에서 동일한 디렉토리를 마운트하면 동일한 파일을 확인할 수 있다.\n예제 Todo 앱 데이터 유지하기 todo앱은 컨테이너 파일 시스템에 위치한 /etc/todos/todo.db 파일에 데이터를 저장한다(SQLite).\n데이터베이스 관련 파일을 호스트에 지속적으로 유지하고 다음 컨테이너에서 사용할 수 있도록 만들 수 있어야한다. 볼륨을 생성하고 데이터가 저장된 디렉토리에 연결(마운트)하여 컨테이너가 todo.db 파일에 쓰기를 수행하면 데이터는 볼륨에서 호스트로 유지된다.\n볼륨 마운트는 일종의 불투명한 데이터 버킷이며, Docker는 디스크 상의 저장 위치를 포함하여 볼륨 전체를 관리한다.\n볼륨 생성 1 2 3 docker volume create todo-db # todo-db 기존 컨테이너 중지 및 삭제: 볼륨을 사용하지 않은 상태에서 실행된 컨테이너 이므로\u0026hellip; 1 docker rm -f \u0026lt;id\u0026gt; --mount 옵션을 추가하여 볼륨 마운트를 지정하여 앱 컨테이너 실행 1 2 3 docker run -dp 3000:3000 --mount type=volume,src=todo-db,target=/etc/todos getting-started # ba3740f92525240162cff2cd3dac9420c048bff2087482cc30bd8411ce9ba198 앱을 열고 데이터 todo 리스트 추가 컨테이너 삭제 후 3번 명령어 실행 후 데이터 확인 Dive into the volume docker volume inspect 명령을 사용하면 Docker가 볼륨을 사용할 때 데이터를 어디에 저장하고 있는지 등 여러 정보들을 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 docker volume inspect todo-db [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2023-04-24T07:37:34Z\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: {}, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/todo-db/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;todo-db\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] Mountpoint가 디스크상의 실제 위치를 의미하며, 대부분의 컴퓨터에서 호스트가 이 디렉토리에 접근하기 위해 root 권한이 필요하다.\n","date":"2023-04-24T14:45:25+09:00","image":"https://codemario318.github.io/post/docker_4/docker_cover_hue12353db563619e41ee3a11307d3cf25_62602_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/docker_4/","title":"4. Persist the DB"},{"content":"만들어진 Docker 이미지는 Docker 레지스트리를 활용하여 공유할 수 있다. 기본 레지스트리는 Docker Hub이다.\nDocker ID\nDocker ID는 Docker Hub에 접근할 수 있는 권한을 제공한다. Docker Hub는 세계에서 가장 큰 라이브러리, 커뮤니티로 무료로 Docker ID를 만들어 사용할 수 있다.\nrepo 만들기 Docker Hub에 이미지를 push하려면 먼저 Docker Hub에 Repository를 생성해야 한다.\nDocker Hub 회원가입 및 로그인 \u0026ldquo;Create Repository\u0026rdquo; 버튼 클릭 repo 이름 설정 및 public 설정 선택 Create 버튼 클릭 이미지 푸시하기 1 2 3 4 5 docker push codemario318/getting-started # Using default tag: latest # The push refers to repository [docker.io/codemario318/getting-started] # An image does not exist locally with the tag: codemario318/getting-started 이미지를 docker push 명령으로 로컬에 존재하는 이미지를 푸시하기 위해서 tag이 필요하다. tag가 존재하지 않는다면 명령을 실행해도 이미지를 찾을 수 없기 때문에 오류가 발생하며, 빌드된 이미지에 다른 이름을 지정해여 태그를 붙여야 한다.\n1 docker login -u YOUR-USER-NAME 1 2 3 4 5 docker image ls # REPOSITORY TAG IMAGE ID CREATED SIZE # codemario318/getting-started latest 8b52580feb5e 2 days ago 262MB # getting-started latest 8b52580feb5e 2 days ago 262MB 1 2 3 4 5 6 7 8 9 10 11 12 docker push codemario318/getting-started # Using default tag: latest # The push refers to repository [docker.io/codemario318/getting-started] # 90b7c400bc47: Pushed # 0cf2f64e7aa4: Pushed # 9e6afbbf5565: Pushed # d9f41532a73b: Mounted from library/node # 6c0a2592426a: Mounted from library/node # 1984e605c08a: Mounted from library/node # 26cbea5cba74: Mounted from library/node # latest: digest: sha256:c11adf2600f6c2da46bfb1a8f74664a766bb69f11badcd3ed16a2de75a464c1a size: 1788 Docker Hub에서 값을 복사하고 있다면, 이미지 이름 부분에서 tagname을 추가하지 않아도 된다. 태그를 지정하지 않으면 Docker는 latest라는 이름의 태그를 사용한다.\n새 인스턴스에서 이미지 실행하기 이미지가 빌드되고 레지스트리에 푸시되었으므로, 컨테이너 이미지를 보유하지 않은 새 인스턴스에서 앱을 실행해도 같은 결과가 나온다.\nPlay with Docker 사용시 주의사항\nPlay with Docker는 amd64 플랫폼을 사용하므로, Apple silicon 기반의 ARM 기반 Mac을 사용하는 경우 이미지를 Play with Docker와 호환되도록 다시 빌드하고 새 이미지를 레포지토리에 푸시해야 한다.\n1 2 3 4 5 6 7 8 9 10 docker build --platform linux/amd64 -t YOUR-USER-NAME/getting-started . docker image ls # REPOSITORY TAG IMAGE ID CREATED SIZE # codemario318/getting-started latest 769987ace2c3 About a minute ago 265MB # getting-started latest 8b52580feb5e 2 days ago 262MB # codemario318/getting-started \u0026lt;none\u0026gt; 8b52580feb5e 2 days ago 262MB docker push codemario/getting-started Play with Docker에서 새 인스턴스를 추가한 후 명령을 실행한다.\n정상적으로 동작하는 것을 확인할 수 있다.\n","date":"2023-04-24T09:50:25+09:00","image":"https://codemario318.github.io/post/docker_3/docker_cover_hue12353db563619e41ee3a11307d3cf25_62602_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/docker_3/","title":"3. Share the application"},{"content":"애플리케이션과 컨테이너 이미지를 업데이트 및 삭제할 수 있다.\n소스 코드 업데이트 하기 1 2 3 4 ... - \u0026lt;p className=\u0026#34;text-center\u0026#34;\u0026gt;No items yet! Add one above!\u0026lt;/p\u0026gt; + \u0026lt;p className=\u0026#34;text-center\u0026#34;\u0026gt;You have no todo items yet! Add one above!\u0026lt;/p\u0026gt; ... 1 docker build -t getting-started . 1 2 3 4 docker run -dp 3000:3000 getting-started docker: Error response from daemon: driver failed programming external connectivity on endpoint laughing_burnell (2b5285b3ebfa65b51a44116c620e56b96b787874c71b25c8643b6a9ee137cb49): Bind for 0.0.0.0:3000 failed: port is already allocated. src/static/js/app.js의 56 라인의 소스 코드를 수정한다.\n이전과 마찬가지로 build 후 run 명령을 실행하면 기존 컨테이너가 이미 호스트의 포트 3000을 사용하고 있어 오류가 발생한다. 이 문제를 해결하려면 이전 컨테이너를 제거해야 한다.\n이전 컨테이너 지우기 컨테이너를 제거하려면 먼저 중지되어야 한다. 그 후 CLI 또는 Docker Desktop을 사용하여 제거할 수 있다.\nCLI를 통해 컨테이너를 중지하고, 삭제하기 위해서 컨테이너 ID가 필요한데 docker ps 명령을 통해 확인할 수 있다.\n1 2 3 4 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2b5285b3ebfa getting-started \u0026#34;docker-entrypoint.s…\u0026#34; 35 minutes ago Up 35 minutes 0.0.0.0:3000-\u0026gt;3000/tcp youthful_yonath 1 docker stop \u0026lt;the-container-id\u0026gt; 1 docker rm \u0026lt;the-container-id\u0026gt; force 플래그를 docker rm 명령에 추가하여 컨테이너를 중지하고 제거할 수도 있다.\n1 docker rm -f \u0026lt;the-container-id\u0026gt; 업데이트된 앱 컨테이너 실행하기 1 2 docker run -dp 3000:3000 getting-started 9732df7c2474fea5223bcd682bbd1a3b7f18cc1f965330bd666478d10a7a73cb 이전과 같은 docker run 명령으로 업데이트된 애플리케이션이 실행된 것을 확인할 수 있다.\n","date":"2023-04-24T09:19:25+09:00","image":"https://codemario318.github.io/post/docker_2/docker_cover_hue12353db563619e41ee3a11307d3cf25_62602_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/docker_2/","title":"2. Update the application"},{"content":"트랜잭션의 격리 수준(isolation level)이란 여러 트랜잭션이 동시에 처리될 때 특정 트랜잭션이 다른 트랜잭션에서 변경하거나 조회하는 데이터를 볼 수 있게 허용할지 말지를 결정한다.\nREAD UNCOMMITTED\nDIRTY READ라고도 하며 일반적인 데이터베이스에서는 거의 사용하지 않는다. READ COMMITTED REPEATABLE READ SERIALIZABLE\n동시성이 중요한 데이터베이스에서는 거의 사용되지 않는다. 4개의 격리 수준에서 뒤로 갈수록 각 트랜잭션 간의 데이터 고립 정도가 높아진다. 격리 수준이 높아질수록 MySQL 서버의 처리 성능이 많이 떨어질 것으로 생각하지만, SERIALIZABLE 격리 수준이 아니라면 크게 성능의 개선이나 저하는 발생하지 않는다.\nDIRTY READ NON-REPEATABLE READ PHANTOM READ READ UNCOMMITTED 발생 발생 발생 READ COMMITTED 없음 발생 발생 REPEATABLE READ 없음 없음 발생(InnoDB X) SERIALIZABLE 없음 없음 없음 SQL-92, SQL-99 표준에 따르면 REPEATABLE READ 격리 수준에서는 PHANTOM READ가 발생할 수 있지만, InnoDB에서는 독특한 특성으로 인해 발생하지 않는다.\n일반적인 온라인 서비스 용도의 데이터베이스는 READ COMMITED와 REPEATABLE READ 중 하나를 사용한다.\n오라클: READ COMMITED MySQL: REPEATABLE READ READ UNCOMMITTED READ UNCOMMITTED 격리 수준에서는 각 트랜잭션에서 변경 내용이 COMMIT, ROLLBACK 여부에 상관 없이 다른 트랜잭션에서 보인다.\nA가 emp_no가 500000이고 first_name이 Lara인 새로운 사원을 INSERT 하는데, B가 변경된 내용을 커밋하기도 전에 emp_no=500000인 사원을 검색해도 조회 가능하다.\n이에 따라 A의 INSERT가 ROLLBACK 되어도, B는 조회한 내용이 정상적인 사원이라고 생각하여 처리하게 된다.\n이처럼 어떤 트랜잭션에서 처리한 작업이 완료되지 않았는데도 다른 트랜잭션에서 볼 수 있는 현상을 더티 리드라고 하며, 더티 리드가 허용되는 격리 수준이 READ UNCOMMITTED이다.\n더티 리드 현상은 데이터가 나타났다 사라지는 현상을 초래하므로 개발자와 사용자를 혼란스럽게 만들 수 있다. 더티 리드를 유발하는 READ UNCOMMITTED는 RDBMS 표준에서는 트랜잭션의 격리 수준으로 인정하지 않을 정도로 정합성에 문제가 많으므로 READ COMMITTED 이상의 격리 수준을 사용할 것을 권장한다.\nREAD COMMITTED 오라클 DBMS에서 기본으로 사용되는 격리 수준이며, 온라인 서비스에서 가장 많이 선택되는 격리 수준이다. 어떤 트랜잭션에서 데이터를 변경했더라도 COMMIT이 완료된 데이터만 다른 트랜잭션에서 조회할 수 있기 때문에 더티 리드 현상은 발생하지 않는다.\nA가 emp_no=500000인 사원의 first_name을 \u0026ldquo;Lara\u0026quot;에서 \u0026ldquo;Toto\u0026quot;로 변경했는데, 이때 새로운 값인 \u0026ldquo;Toto\u0026quot;는 employees 테이블에 즉시 기록되고 이전 값인 \u0026ldquo;Lara\u0026quot;는 언두 영역으로 백업된다.\nCOMMIT이 완료 되기 전 B가 emp_no=500000인 사원의 first_name을 조회하면, 언두 영역에 백업된 레코드에서 가져온 \u0026ldquo;Lara\u0026quot;를 조회하게 된다.\nNON-REPEATABLE READ READ COMMITTED 격리 수준에서도 NON-REPEATABLE READ라는 부정합 문제가 있다.\nB가 BEGIN 명령으로 트랜잭션을 시작하고 first_name이 \u0026ldquo;Toto\u0026quot;인 사용자를 검색했는데 일치하는 결과가 없었다. 하지만 A가 사원 번호가 500000인 사원의 이름을 \u0026ldquo;Toto\u0026quot;로 변경하고 커밋을 실행한 후, B가 똑같은 쿼리로 다시 조회하면 1건이 조회된다.\n중요한 포인트는 사용자가 동일 트랜잭션 내에서 같은 쿼리를 활용하여 조회했는데도 불구하고 다른 결과를 조회해 온 것이며, 이는 하나의 트랜잭션 내에서 똑같은 SELECT 쿼리를 실행했을 때는 항상 같은 결과를 가져와야 한다는 REPEATABLE READ 정합성에 어긋나는 것이다.\n이러한 부정합 현상은 일반적인 웹 프로그램에서는 크게 문제되지 않을 수 있지만 하나의 트랜잭션에서 동일 데이터를 여러 번 읽고 변경하는 작업이 금전적인 처리와 연결되면 문제가 될 수도 있다.\n트랜잭션 내에서 실행되는 SELECT 문장과 트랜잭션 없이 실행되는 SELECT 문장의 차이\nREAD COMMITTED 격리 수준에서는 차이가 별로 없으나 REPEATABLE READ 격리 수준에서는 SELECT 쿼리도 트랜잭션 범위 내에서만 작동한다.\n즉 트랜잭션을 시작한 상태에서 SELECT 쿼리를 실행한다면, 다른 트랜잭션에서 데이터를 변경하고 COMMIT 되었다고 하더라도 동일한 결과를 반환한다.\nREPEATABLE READ MySQL의 InnoDB 스토리지 엔진에서 기본으로 사용되는 격리 수준이다. 바이너리 로그를 가진 MySQL 서버에서는 최소 REPEATABLE READ 격리 수준 이상을 사용해야 한다.\nInnoDB 스토리지 엔진의 REPEATABLE READ 격리수준에서는 트랜잭션이 ROLLBACK될 가능성에 대비해 변경되기 전 레코드를 언두 공간에 백업해두고 실제 레코드 값을 변경하고(MVCC) 백업된 이전 데이터를 이용해 동일 트랜잭션 내에서는 동일한 결과를 보여줄 수 있도록 보장하므로 NON-REPEATABLE READ 부정합이 발생하지 않는다.\nREPEATABLE READ와 READ COMMITTED의 차이는 언두 영역에 백업된 레코드의 여러 버전 가운데 몇 번째 이전 버전까지 찾아 들어가야 하느냐에 있다.\nInnoDB의 모든 트랜잭션은 고유한 트랜잭션 번호(순차적으로 증가하는 값)를 가지며, 언두 영역에 백업된 모든 레코드에는 변경을 발생시킨 트랜잭션 번호가 포함되어 있다. 언두 영역에 백업된 데이터는 InnoDB 스토리지 엔진이 불필요하다고 판단하는 시점에 주기적으로 삭제한다.\nREPEATABLE READ 격리 수준에서는 MVCC를 보장하기 위해 실행중인 트랜잭션 가운데 가장 오래된 트랜잭션 번호보다 트랜잭션 번호가 앞선 언두 영역의 데이터는 삭제할 수가 없다. 그렇다고 가장 오래된 트랜잭션 번호 이전의 트랜잭션에 의해 변경된 모든 언두 데이터가 필요한 것은 아니며, 특정 트랜잭션 번호의 구간 내에서 백업된 언두 데이터가 보존 돼야 한다.\n12번 트랜잭션에서 사원 이름을 \u0026ldquo;Toto\u0026quot;로 변경하고 커밋을 수행했다. 하지만 10번 트랜잭션에서 12번 트랜잭션 커밋 전 후로 해당 사원을 조회해도 자신의 트랜잭션 번호보다 작은 트랜잭션에서 변경한 것만 보게 되므로, 이전 값인 \u0026ldquo;Lara\u0026quot;를 받아온다.\n트랜잭션을 시작하고 장시간 종료하지 않으면 언두 영역이 백업된 데이터로 무한정 커질 수도 있으며, 이로 인해 백업된 레코드가 많아지면 MySQL 서버의 처리 성능이 떨어질 수 있다.\nPHANTOM READ REPEATABLE READ 격리 수준에서도 부정합이 발생 가능하다.\nA가 employees 테이블에 INSERT를 실행하는 도중 B가 SELECT ... FOR UPDATE 쿼리로 테이블을 2번 조회하는데 다른 결과를 반환하고 있다.\nSELECT ... FOR UPDATE 쿼리는 SELECT하는 레코드에 쓰기 잠금을 걸어야 하는데, 언두 레코드에는 잠금을 걸 수 없다. 따라서 SELECT ... FOR UPDATE나 SELECT ... LOCK IN SHARE MODE로 조회되는 레코드는 언두 영역의 변경 전 데이터를 가져오는 것이 아니라 현재 레코드의 값을 가져오게 된다.\n이렇게 다른 트랜잭션에서 수행한 변경 작업에 의해 레코드가 보였다 안 보였다 하는 현상을 PHANTOM READ(PHANTOM ROW)라고 한다.\nSERIALIZABLE 가장 단순한 격리 수준이면서 동시에 가장 엄격한 격리 수준이다. 그만큼 동시 처리 성능도 다른 트랜잭션 격리 수준보다 떨어진다.\nInnoDB 테이블에서 기본적으로 순수한 SELECT 작업은 아무런 레코드 잠금도 설정하지 않고 실행(Non-locking consistent read: 잠금이 필요없는 일관된 읽기)되지만 트랜잭션의 격리 수준이 SERIALIZABLE로 설정되면 읽기 작업도 공유 잠금(읽기 잠금)을 획득해야만 하며, 동시에 다른 트랜잭션은 레코드를 변경하지 못하게 된다. 따라서 SERIALIZABLE 격리 수준에서는 일반적인 DBMS에서 일어나는 PHANTOM READ 문제가 발생하지 않는다.\n하지만 InnoDB 스토리지 엔진에서는 갭 락과 넥스트 키 락 덕분에 REPEATABLE READ 격리 수준에서도 PHANTOM READ가 발생하지 않아 굳이 사용할 필요성은 없다.\n엄밀하게는 SELECT ... FOR UPDATE, SELECT ... FOR SHARE 쿼리의 경우 REPEATABLE READ 격리수준에서 PHANTOM READ 현상이 발생할 수 있다.\n하지만 레코드의 변경 이력(언두 레코드)에 잠금을 걸 수는 없기 때문에, 이러한 잠금을 동반한 SELECT 쿼리는 예외적인 상황으로 볼 수 있다.\n","date":"2023-04-22T15:31:10+09:00","image":"https://codemario318.github.io/post/real_mysql_5_3/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_5_3/","title":"5.3 MySQL의 격리 수준"},{"content":"잠금은 동시성을 제어하기 위한 기능으로 하나의 데이터를 여러 커넥션에서 동시에 동일한 자원(레코드나 테이블)을 요청할 경우 순서대로 한 시점에는 하나의 커넥션만 변경할 수 있게 해주는 역할을 한다.\nMySQL 엔진 레벨\n모든 스토리지에 영향을 미친다.\n스토리지 엔진 레벨\n스토리지 엔진 간 상호 영향을 미치지는 않는다.\nMySQL 엔진의 잠금 글로벌 락 글로벌 락은 FLUSH TABLES WITH READ LOCK 명령으로 획득할 수 있으며, MySQL에서 제공하는 잠금 가운데 가장 범위가 크다. 한 세션에서 글로벌 락을 획득하면 다른 세션에서 SELECT를 제외한 대부분의 DDL 문장이나 DML 문장을 실행하는 경우 락이 해제될 때까지 대기 상태로 남는다.\n글로벌 락이 영향을 미치는 범위는 MySQL 서버 전체이며, 작업 대상 테이블이나 데이터베이스가 다르더라도 동일하게 영향을 미친다. 여러 데이터베이스에 존재하는 MyISAM이나 MEMORY 테이블에 대해 mysqldump로 일관된 백업을 받아야 할 때는 글로벌 락을 사용해야 한다.\n글로벌 락을 거는 FLUSH TABLES WITH READ LOCK 명령은 실행과 동시에 서버에 존재하는 모든 테이블을 닫고 잠금을 거는데, 읽기 잠금을 걸기 전에 먼저 테이블을 플러시 해야 하기 때문에 실행 중인 모든 종류의 쿼리가 완료돼야 한다. 명령이 실행되기 전에 테이블이나 레코드에 쓰기 잠금을 거는 SQL이 실행되었다면 해당 테이블의 읽기 잠금을 걸기 위해 먼저 실행된 SQL과 그 트랜잭션이 완료될 때 까지 기다려야 한다.\n장시간 실행되는 쿼리와 FLUSH TABLES WITH READ LOCK 명령이 최악의 케이스로 실행되면 서버의 모든 쿼리가 오랜 시간 실행되지 못하고 대기할 수 있다.\n백업락 MySQL 8부터 InnoDB가 기본 스토리지 엔진으로 채택되었고, InnoDB 스토리지 엔진은 트랜잭션을 지원하기 때문에 일관된 데이터 상태를 위해 모든 데이터 변경 작업을 멈출 필요는 없기 때문에 조금 더 가벼운 글로벌 락의 필요성이 생겼고, 백업 락이 도입되었다.\n1 2 LOCK INSTANCE FOR BACKUP; UNLOCK INSTANCE; 특정 세션에서 백업 락을 획득하면 모든 세션에서 다음과 같이 테이블의 스키마나 사용자의 인증 관련 정보를 변경할 수 없게 되지만, 일반적인 테이블의 데이터 변경은 허용된다.\n데이터베이스 및 테이블 등 모든 객체 생성 및 변경, 삭제 REPAIR TABLE과 OPTIMIZE TABLE 명령 사용자 관리 및 비밀번호 변경 테이블 락 테이블락은 개별 테이블 단위로 설정되는 잠금이며, 명시적 또는 묵시적으로 특정 테이블의 락을 획들할 수 있다. MyISAM뿐 아니라 InnoDB 스토리지 엔진을 사용하는 테이블도 동일하게 설정 가능하다.\n명시적: LOCK TABLES table_name [ READ | WRITE ] 명령\nUNLOCK TABLES 명령으로 해제 특별한 상황이 아니면 애플리케이션에서 사용할 필요가 거의 없다. 묵시적: 테이블에 데이터를 변경하는 쿼리를 실행하면 발생\nMySQL 서버가 데이터가 변경되는 테이블에 잠금을 설정하고, 변경한 후 즉시 잠금을 해제한다. InnoDB 테이블의 경우 스토리지 엔진 차원에서 레코드 기반 잠금을 제공하기 때문에 단순 데이터 변경 쿼리로 인해 묵시적인 테이블 락이 설정되지는 않고, DDL의 경우에만 영향을 미친다. 네임드 락 네임드 락은 GET_LOCK() 함수를 이용해 임의의 문자열에 대해 잠금을 설정할 수 있다. 잠금 대상이 테이블이나 레코드 또는 AUTO_INCREMENT와 같은 데이터베이스 객체가 아니라 사용자가 지정한 문자열(String)에 대해 획득하고 반납하는 잠금이다.\n네임드 락은 자주 사용되지는 않지만, 여러 클라이언트가 상호 동기화를 처리해야 할 때 네임드 락을 이용하면 쉽게 해결할 수 있다.\n많은 레코드에 대해서 복잡한 요건으로 레코드를 변경하는 트랜잭션에 유용하게 사용할 수 있다. 배치 프로그램처럼 한꺼번에 많은 레코드를 변경하는 쿼리는 데드락의 원인이 되곤 하는데, 동일 데이터를 변경하거나 참조하는 프로그램끼리 분류해서 네임드 락을 걸고 쿼리를 실행하면 아주 간단히 해결할 수 있다.\n1 2 3 4 5 6 7 8 -- \u0026#34;mylock\u0026#34; 이라는 문자열에 대해 잠금을 획득 SELECT GET_LOCK(\u0026#39;mylock\u0026#39;, 2); -- \u0026#34;mylock\u0026#34; 이라는 문자열에 대해 잠금 설정 확인 SELECT IS_FREE_LOCK(\u0026#39;mylock\u0026#39;); -- \u0026#34;mylock\u0026#34; 이라는 문자열에 대해 잠금을 반납(해제) SELECT RELEASE_LOCK(\u0026#39;mylock\u0026#39;); MySQL 8 버전부터는 네임드 락을 중첩해서 사용할 수 있게 되었으며, 현재 세션에서 획득한 네임드 락을 한번에 모두 해제하는 기능도 추가되었다.\n1 2 3 4 5 6 7 SELECT GET_LOCK(\u0026#39;mylock_1\u0026#39;, 10); SELECT GET_LOCK(\u0026#39;mylock_2\u0026#39;, 10); SELECT RELEASE_LOCK(\u0026#39;mylock_1\u0026#39;); SELECT RELEASE_LOCK(\u0026#39;mylock_2\u0026#39;); SELECT RELEASE_ALL_LOCKS(); 메타데이터 락 메타데이터 락은 데이터베이스 객체(테이블이나 뷰 등)의 이름이나 구조를 변경하는 경우 획득하는 잠금이다. 명시적으로 획득하거나 해제할 수 없고, 테이블의 이름을 변경하는 경우 자동으로 획득한다.\nREAD TABLE 명령의 경우 원본 이름과 변경될 이름 두 개 모두 한꺼번에 잠금을 설정한다.\n1 RENAME TABLE rank TO rank_backup , rank_new TO rank; 위와 같이 하나의 RENAME TABLE 명령문에 두 개의 RENAME 작업을 한번에 실행하면 실제 애플리케이션에서는 \u0026ldquo;Table not found 'rank'\u0026ldquo;같은 상황을 발생시키지 않고 적용하는 것이 가능하다.\n하지만 이 문장을 나누어 실행하면 아주 짧은 시간이지만 rank 테이블이 존재하지 않는 순간이 생기며, 그 순간에 실행되는 쿼리는 오류를 발생시킨다.\n1 2 RENAME TABLE rank TO rank_backup; RENAME TABLE rank_new TO rank; 메타데이터 잠금과 트랜잭션 동시 활용 때로는 메타데이터 잠금과 InnoDB의 트랜잭션을 동시에 사용해야 하는 경우도 있다.\n1 2 3 4 5 CREATE TABLE access_log ( id BIGINT NOT NULL AUTO_INCREMENT, ... PRIMARY KEY(id) ); 위와 같이 INSERT만 실행되는 로그 테이블이 있을때, 이 테이블의 구조를 변경해야 한다면 Online DDL을 이용하여 변경할 수 도 있지만, 시간이 너무 오래 걸리는 경우 언두 로그의 증가와 Online DDL이 실행되는 동안 누적된 Online DDL 버퍼의 크기 등 고민해야 할 문제가 많다. 더 큰 문제는 MySQL서버의 DDL은 단일 스레드로 작동하기 대문에 상당히 많은 시간이 소모된다.\n이때는 새로운 구조의 테이블을 생성하고 먼저 최근 데이터까지는 프라이머리 키인 id 값을 범위별로 나눠서 여러 개의 스레드로 빠르게 복사하고, 나머지 데이터는 트랜잭션과 테이블 잠금, RENAME TABLE 명령으로 응용 프로그램의 중단 없이 실행할 수 있다.\n이때 남은 데이터를 복사 하는 시간 동안은 테이블의 잠금으로 인해 INSERT를 할 수 없게 되어 가능하면 미리 아주 최근 데이터까지 복사해 둬야 잠금 시간을 최소화하여 서비스에 미치는 영향을 줄일 수 있다.\n1 2 3 4 5 6 7 8 9 10 CREATE TABLE access_log_new ( id BIGINT NOT NULL AUTO_INCREMENT, ... PRIMARY KEY(id) ) KEY_BLOCK_SIZE=4; INSERT INTO access_log_new SELECT * FROM access_log WHERE id \u0026gt;= 0 AND id \u0026lt; 10000; INSERT INTO access_log_new SELECT * FROM access_log WHERE id \u0026gt;= 10000 AND id \u0026lt; 20000; INSERT INTO access_log_new SELECT * FROM access_log WHERE id \u0026gt;= 20000 AND id \u0026lt; 30000; INSERT INTO access_log_new SELECT * FROM access_log WHERE id \u0026gt;= 30000 AND id \u0026lt; 40000; 1 2 3 4 5 6 7 8 9 10 11 12 SET autocommit = 0; LOCK TABLES access_log WRITE, access_log_new WRITE; SELECT MAX(id) as @MAX_ID FROM access_log_new; INSERT INTO access_log_new SELECT * FROM access_log WHERE pk \u0026gt; @MAX_ID; COMMIT; RENAME TABLE access_log TO access_log_old, access_log_new TO access_log; UNLOCK TABLES; DROP TABLE access_log_old; InnoDB 스토리지 엔진 잠금 InnoDB 스토리지 엔진은 MySQL에서 제공하는 잠금과는 별개로 스토리지 엔진 내부에서 레코드 기반의 잠금 방식을 탑재하고 있다. 이러한 레코드 기반 잠금 방식 덕뿐에 MyISAM보다는 훨씬 뛰어난 동시성 처리를 제공할 수 있다.\n하지만 이원화된 잠금 처리로 인해 InnoDB 스토리지 엔진에서 사용되는 잠금에 대한 정보는 MySQL 명령을 이용해 접근하기 까다롭고, 내용도 어셈블리 코드를 보는 것 같아서 이해하기 어려웠다.\nlock_monitor innodb_lock_monitor라는 이름의 InnoDB 테이블을 생성하여 잠금 정보를 덤프하는 방법 SHOW ENGINE INNODB STATUAS 최근 버전에서 InnoDB의 트랜잭션과 잠금, 잠금 대기중인 트랜잭션의 목록을 조회할 수 있는 방법이 도입되었다.\nMySQL 서버의 information_schema 데이터베이스 INNODB_TRX, INNODB_LOCKS, INNODB_LOCK_WAITS 테이블 활용 Performance Schema InnoDB 스토리지 엔진의 내부 잠금(세마포어)에 대한 모니터링 InnoDB 스토리지 엔진의 잠금 InnoDB 스토리지 엔진은 레코드 기반의 잠금 기능을 제공하며, 잠금 정보가 상당히 작은 공간으로 관리되기 때문에 레코드 락이 페이지 락, 테이블 락으로 락 에스컬레이션 되는 경우는 없다.\n레코드 락 뿐만 아니라 레코드와 레코드 사이의 간격을 잠그는 갭 락도 존재한다.\n레코드락 레코드 자체만 잠그는 것을 레코드 락(Record lock, Record only lock)이라고 하며, 다른 상용 DBMS의 레코드 락과 동일한 역할을 한다. 중요한 차이는 InnoDB 스토리지 엔진은 레코드 자체가 아니라 인덱스의 레코드를 잠근다.\n인덱스가 하나도 없는 테이블이더라도 내부적으로 자동 생성된 클러스터 인덱스를 이용해 잠금을 설정한다. 따라서 InnoDB에서는 대부분 보조 인덱스를 이용한 변경 작업은 이어서 설명할 넥스트 키 락(Next key lock) 또는 갭 락(Gap lock)을 사용하지만 프라이머리 키 또는 유니크 인덱스에 의한 변경 작업에서는 갭에 대해서는 잠그지 않고 레코드 자체에 대해서만 락을 건다.\n갭 락 갭 락은 레코드 자체가 아니라 레코드와 바로 인접한 레코드 사이의 간격만을 잠그는 것을 의미한다. 레코드와 레코드 사이의 간격에 새로운 레코드가 생성(INSERT)되는 것을 제어한다.\n갭 락은 그 자체보다는 넥스트 키 락의 일부로 자주 사용된다. 넥스트 키 락 레코드 락과 갭 락을 합쳐 놓은 형태의 잠금을 넥스트 키 락(Next key lock)이라고 한다.\nSTATEMENT 포맷의 바이너리 로그를 사용하는 MySQL 서버에서는 REPEATABLE READ 격리 수준을 사용해야 한다. 또한 innodb_locks_unsafe_for_binlog 시스템 변수가 비활성화되면 변경을 위해 검색하는 레코드에는 넥스트 키 락 방식으로 잠금이 걸린다.\nInnoDB의 갭 락이나 넥스트 키 락은 바이너리 로그에 기록되는 쿼리가 레플리카 서버에서 실행될 때 소스 서버에서 만들어 낸 결과와 동일한 결과를 만들어 내도록 보장하는 것이 주목적이다.\n넥스트 키 락과 갭 락으로 인해 데드락이 발생하거나 다른 트랜잭션을 기다리게 만드는 일이 자주 발생하므로, 바이너리 로그 포맷을 ROW 형태로 바꿔서 넥스트 키 락이나 갭 락을 줄이는 것이 좋다.\n자동 증가 락 MySQL에서는 자동 증가하는 숫자 값을 추출하기 위해 AUTO_INCREMENT라는 컬럼 속성을 제공한다. AUTO_INCREMENT 컬럼이 사용된 테이블에 동시에 여러 레코드가 INSERT 되는 경우, 저장되는 각 레코드는 중복되지 않고 저장된 순서대로 증가하는 일련번호 값을 가져야 한다. 이를 위해 내부적으로 AUTO_INCREMENT 락이라고 하는 테이블 수준의 잠금을 사용한다.\nAUTO_INCREMENT락은 INSERT, REPLACE 쿼리 문장과 같이 새로운 레코드를 저장하는 쿼리에서만 필요하다. InnoDB의 다른 잠금과 달리 트랜잭션과 관계 없이 INSERT나 REPLACE 문장에서 AUTO_INCREMENT 값을 가져오는 순간만 락이 걸렸다가 즉시 해제된다. AUTO_INCREMENT 락은 테이블에 단 하나만 존재하기 대문에 두 개의 INSERT 쿼리가 동시에 실행되는 경우 하나의 쿼리가 락을 걸면 나머지 쿼리는 락을 기다려야 한다. AUTO_INCREMENT 컬럼에 명시적으로 값을 설정하더라도 자동 증가 락을 걸게 된다. innodb_autoinc_lock_mode\nMySQL 5.0 이하 버전에서는 AUTO_INCREMENT 락을 명시적으로 획득하고 해제하는 방법은 없다. 하지만 아주 짧은 시간동안 걸렸다가 해제되는 잠금이라서 대부분의 경우 문제가 되지 않는다.\nMySQL 5.1 이상 부터는 innodb_autoinc_lock_mode 시스템 변수를 이용해 자동 증가 락의 작동 방식을 변경할 수 있다.\ninnodb_autoinc_lock_mode=0\n모든 INSERT 문장이 자동 증가 락을 사용한다.\ninnodb_autoinc_lock_mode=1\nINSERT하는 쿼리 중에서 MySQL 서버가 INSERT되는 레코드의 건수를 정확히 예측할 수 있을 때는 자동 증가 락을 사용하지 않고, 훨씬 가볍고 빠른 래치(뮤텍스)를 이용해 처리한다.\n개선된 래치는 자동 증가 락과 달리 아주 짧은 시간 동안만 잠금을 걸고 필요한 자동 증가 값을 가져오면 즉시 잠금이 해제된다. 건수를 예측할 수 없을때는 이전 같이 자동 증가 락을 사용한다. 한번에 할당 받은 자동 증가 값이 남아서 사용되지 못하면 폐기하므로 레코드 자동 증가 값은 연속되지 않고 누락된 값이 발생할 수 있다. 하나의 INSERT 문장으로 INSERT 되는 레코드는 연속된 자동 증가 값을 가지게 된다. innodb_autoinc_lock_mode=2\n절대로 자동 증가 락을 걸지 않고 경량화된 래치(뮤텍스)를 사용한다.\n설정에서는 하나의 INSERT 문장으로 INSERT 되는 레코드라 하더라도 연속된 자동 증가값을 보장하지는 않는다. INSERT ... SELECT와 같은 대량 INSERT 문장이 실행되는 중에도 다른 커넥션에서 INSERT를 수행할 수 있으므로 동시 처리 성능이 높아진다. 자동 증가 기능은 유니크한 값이 생성된다는 것만 보장한다. 따라서 소스 서버와 레플리카 서버의 자동 증가 값이 달라질 수 있어 주의해야한다. MySQL 5.7 까지는 기본값이 1이었으나, 8버전 바이너리 로그 포맷이 STATEMENT가 아니라 ROW로 변경되었기 때문에 2로 바뀌었다. ROW 포맷이 아니라 STATEMENT 포맷의 바이너리 로그를 사용한다면 1로 변경해서 사용해야 한다.\n자동 증가 값이 한 번 증가하면 절대 줄어들지 않는 이유는 AUTO_INCREMENT 잠금을 최소화 하기 위해서다. INSERT 쿼리가 실패했더라도 한 번 증가된 AUTO_INCREMENT 값은 다시 줄어들지 않고 그대로 남는다.\n래치(Latch)란?\nDBMS에서 여러 스레드 혹은 프로세스가 동시에 접근할 수 있는 데이터에 대한 접근 제어 기술 중 하나이다.\n래치는 뮤텍스(Mutex)와 유사한 개념이지만, 뮤텍스는 오직 한 스레드 혹은 프로세스만이 해당 자원에 접근할 수 있도록 하는 동기화 기술이고, 래치는 여러 개의 스레드 혹은 프로세스가 읽기만 가능하고 쓰기는 하나의 스레드 혹은 프로세스만 가능하도록 하는 동기화 기술이다.\n래치는 읽기 래치와 쓰기 래치로 구분되며, 읽기 래치는 여러 스레드 혹은 프로세스가 동시에 해당 데이터를 읽을 수 있게 하고, 쓰기 래치는 오직 하나의 스레드 혹은 프로세스만 해당 데이터를 변경할 수 있게 한다.\n래치는 주로 인덱스 구조나 버퍼풀 등에서 사용되며, 동시성을 높이기 위해 적극적으로 활용된다. 그러나 래치를 과도하게 사용하거나 사용 방법이 부적절할 경우 데드락(deadlock) 등의 문제가 발생할 수 있으므로 주의해서 사용해야 한다.\n인덱스와 잠금 InnoDB의 잠금은 레코드를 잠그는 것이 아니라 인덱스를 잠그는 방식으로 처리된다. 즉 변경해야 할 레코드를 찾기 위해 검색한 인덱스의 레코드를 모두 락을 걸어야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 --// KEY ix_firstname (first_name) SELECT COUNT(*) FROM employees WHERE first_name=\u0026#39;Georgi\u0026#39; ; /* +---+ |253| +---+ */ SELECT COUNT(*) FROM employees WHERE first_name=\u0026#39;Georgi\u0026#39; AND last_name=\u0026#39;Klassen\u0026#39; ; /* +---+ | 1| +---+ */ UPDATE employees SET hire_date = NOW() WHERE first_name=\u0026#39;Georgi\u0026#39; AND last_name=\u0026#39;Klassen\u0026#39; ; UPDATE 쿼리가 실행되면 1건의 레코드가 업데이트 되지만, UPDATE 문장의 조건에서 인덱스를 이용할 수 있는 조건은 first_name뿐이며, last_name 컬럼은 인덱스가 없기 때문에 first_name='Georgi'인 레코드 253건의 레코드가 모두 잠긴다.\nUPDATE 문장을 위해 적절히 인덱스가 준비돼 있지 않다면 각 클라이언트 간의 동시성이 상당히 떨어져서 한 세션에서 UPDATE 작업을 하는 중에는 다른 클라이언트는 테이블을 업데이트하지 못하고 기다려야 하는 상황이 발생한다.\n만약 employees 테이블에 인덱스가 하나도 없다면, 테이블을 풀스캔 하면서 UPDATE를 수행하기 때문에 테이블에 있는 모든 레코드를 잠그게 된다.\n레코드 수준의 잠금 확인 및 해제 InnoDB 스토리지 엔진을 사용하는 테이블의 레코드 수준 잠금은 테이블 수준의 잠금보다는 조금 더 복잡하다.\n테이블 잠금에서는 잠금의 대상이 테이블 자체이므로 쉽게 문제의 원인이 발견되고 해결될 수 있으나, 레코드 수준의 잠금은 테이블의 레코드 각각에 잠금이 걸리므로 그 레코드가 자주 사용되지 않는다면 오랜 시간 동안 잠겨진 상태로 남아 있어도 잘 발견되지 않는다.\n예전 버전의 MySQL 서버에서는 레코드 잠금에 대한 메타 정보를 제공하지 않았기 때문에 어려웠지만, 5.1 버전부터는 레코드 잠금과 잠금 대기에 대한 조회가 가능하므로 쿼리 하나만 실행하면 잠금과 잠금 대기 상태를 바로 확인할 수 있다.\nMySQL 5.1 +\ninformation_schema라는 DB에 INNODB_TRX, INNODB_LOCKS, INNODB_LOCK_WAITS 라는 테이블을 통해 확인 MySQL 8.0 +\ninformation_schema의 정보들은 조금씩 제거되고 있다. performance_schema의 data_locks, data_lock_waits로 확인 ","date":"2023-04-22T15:30:10+09:00","image":"https://codemario318.github.io/post/real_mysql_5_2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_5_2/","title":"5.2 잠금"},{"content":"트랜잭션은 작업의 완전성을 보장해 주는 것이다.\n논리적이 작업 셋을 모두 완벽하게 처리하거나, 처리하지 못할 경우에는 원 상태로 복구해서 작업의 일부만 적용되는 현상(Partial update)이 발생하지 않게 만들어주는 기능이다.\n트랜잭션은 꼭 여러 개의 변경 작업을 수행하는 쿼리가 조합됐을 때만 의미있는 개념은 아니다. 트랜잭션은 하나의 논리적인 작업 셋이 100% 적용되거나(COMMIT) 아무것도 적용되지 않아야(ROLLBACK)함을 보장해 주는 것이다.\nMySQL에서의 트랜잭션 MySQL에서 InnoDB와 달리 MyISAM과 MEMORY 스토리지 엔진은 트랜잭션을 제공하지않는다. 처리방식의 차이를 확인하고, 트랜잭션을 사용할 때 주의할 점을 알아야 한다.\n1 2 3 4 5 CREATE TABLE tab_myisam ( fdpk INT NOT NULL, PRIMARY KEY (fdpk) ) ENGINE=MyISAM; INSERT INTO tab_myisam (fdpk) VALUE (3); CREATE TABLE tab_myInnodb ( fdpk INT NOT NULL, PRIMARY KEY (fdpk) ) ENGINE=INNODB; INSERT INTO tab_myInnodb (fdpk) VALUE (3); 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 SET autocommit=ON; INSERT INTO tab_myisam (fdpk) VALUES (1), (2), (3); /* ERROR 1062 (23000): Duplicate entry \u0026#39;3\u0026#39; for key \u0026#39;PRIMARY\u0026#39; +----+ |fdpk| +----+ | 1| +----+ | 2| +----+ | 3| +----+ */ INSERT INTO tab_myinnodb (fdpk) VALUES (1), (2), (3); /* ERROR 1062 (23000): Duplicate entry \u0026#39;3\u0026#39; for key \u0026#39;PRIMARY\u0026#39; +----+ |fdpk| +----+ | 3| +----+ */ 두 테이블의 INSERT 문장이 프라이머리 키 중복 오류로 실패했으나, MyISAM 테이블은 INSERT 문장이 순차적으로 실행되고 즉시 반영된다. 따라서 1과 2를 저장하고 3을 저장할 때 오류가 발생하여 쿼리 실행이 종료되며, 기존에 실행되었던 INSERT로 테이블에 1, 2가 저장되었고 오류로 실패해도 그대로 유지된다. MEMORY 스토리지 엔진도 동일한 방식으로 처리된다.\n반면 InnoDB는 쿼리 중 일부라도 오류가 발생하면 전체를 원 상태로 만든다는 트랜잭션의 원칙대로 INSERT 문장을 실행하기 전 상태로 복구한다.\nMyISAM에서 발생한 이러한 현상을 부분 업데이트(Partial Update)라고 표현하며, 테이블 데이터의 정합성을 맞추기 위해 실패한 쿼리로 인해 남은 레코드를 다시 삭제하는 등 어려운 문제를 만들어 낸다.\n주의사항 트랜잭션 또한 DBMS의 커넥션과 동일하게 트랜잭션의 범위를 최소화 하여, 꼭 필요한 최소의 코드에만 적용하는 것이 좋다.\nAS-IS 처리 시작 데이터베이스 커넥션 생성 트랜잭션 시작 사용자의 로그인 여부 확인 사용자의 글쓰기 내용의 오류 여부 확인 첨부로 업로드된 파일 확인 및 저장 사용자의 입력 내용을 DBMS에 저장 첨부 파일 정보를 DBMS에 저장 저장된 내용 또는 기타 정보를 DBMS에서 조회 게시물 등록에 대한 알림 메일 발송 알림 메일 발송 이력을 DBMS에 저장 트랜잭션 종료(COMMIT) 데이터베이스 커넥션 반납 처리완료 데이터베이스의 커넥션 생성하고 트랜잭션을 시작하는 부분\n실제로 DBMS에 데이터를 저장하는 작업은 5번부터 시작되므로 2,3,4 작업이 아무리 빨리 처리된다고 하더라도 트랜잭션에 포함시킬 필요는 없다. 일반적으로 데이터베이스 커넥션은 개수가 제한적이어서 각 단위 프로그램이 커넥션을 소유하는 시간이 길어질수록 여유 커넥션의 개수는 줄어든다. 이에 따라 커넥션을 기다려야 하는 상황이 발생할 수 있다. 메일 전송이나 FTP 파일 전송 작업 또는 네트워크를 통해 원격 서버와 통신하는 작업\n8번에서 프로그램이 실행되는 동안 통신할 수 없는 상황이 발생한다면 웹 서버뿐 아니라 DBMS 서버까지 위험해지는 상황이 발생하게 될 수 있으므로 트랜잭션 내에서 제거하는 것이 좋다. 작업 단위 분리\n처리 되어야 하는 DBMS 작업을 (5,6), (7), (9) 총 3개로 분리해서 트랜잭션을 만들 수 있고, 7번이 단순 조회라면 트랜잭션을 사용하지 않아도 된다. TO-BE 처리 시작 사용자의 로그인 여부 확인 사용자의 글쓰기 내용의 오류 여부 확인 첨부로 업로드된 파일 확인 및 저장 데이터베이스 커넥션 생성(또는 커넥션 풀에서 가져오기) 트랜잭션 시작 사용자의 입력 내용을 DBMS에 저장 첨부 파일 정보를 DBMS에 저장 트랜잭션 종료(COMMIT) 저장된 내용 또는 기타 정보를 DBMS에서 조회 게시물 등록에 대한 알림 메일 발송 트랜잭션 시작 알림 메일 발송 이력을 DBMS에 저장 트랜잭션 종료(COMMIT) 데이터베이스 커넥션 반납 처리완료 위와 같은 방식으로 개선 될 수 있다.\n","date":"2023-04-22T15:29:10+09:00","image":"https://codemario318.github.io/post/real_mysql_5_1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_5_1/","title":"5.1 트랜잭션"},{"content":"Get the app 1 git clone https://github.com/docker/getting-started.git Build the app’s container image 컨테이너 이미지를 빌드하려면 Dockerfile을 사용해야한다.\nDockerfile? Dockerfile은 파일 확장자가 없는 간단한 텍스트 기반 파일로서 도커 이미지를 만들기 위한 빌드 스크립트로 활용된다. 도커 이미지는 컨테이너를 생성하는 데 사용되며, Dockerfile은 이러한 이미지를 구성하는 데 필요한 모든 정보를 제공한다.\nDockerfile은 도커 이미지를 생성하는 데 필요한 기본 운영 체제 이미지, 필요한 응용 프로그램, 설정 파일 및 다른 종속성 등 모든 구성 요소를 명시하며, 일반적으로 다음과 같은 명령어로 구성된다.\nFROM: 기본이 되는 이미지 RUN: 새로운 레이어에서 실행될 명령어 COPY: 호스트 파일 시스템에서 파일이나 디렉토리를 이미지로 복사 ADD: COPY 명령과 유사하지만, URL에서 파일을 다운로드하거나 tar 파일에서 파일을 추출하는 등의 작업을 수행할 수 있음 WORKDIR: 명령어가 실행될 작업 디렉토리를 설정 ENV: 환경 변수를 설정 EXPOSE: 컨테이너가 사용하는 포트 CMD: 컨테이너가 시작될 때 실행할 명령어를 설정 Dockerfile에 정의된 모든 명령어는 도커 이미지의 각 레이어로 구성된다. 각 레이어는 독립적으로 캐싱되고 이미지를 다시 빌드할 때 이전에 캐싱된 레이어를 재사용하여 빌드 속도를 향상시켜 도커 이미지를 효율적이고 일관적인 방식으로 생성할 수 있다.\nDockerfile 생성 빌드할 어플리케이션이 있는 디렉터리(/getting-started/app)로 이동하여 Dockerfile 생성 후 필요한 내용을 채운다.\n1 2 cd getting-started/app touch Dockerfile 1 2 3 4 5 6 7 8 # syntax=docker/dockerfile:1 FROM node:18-alpine WORKDIR /app COPY . . RUN yarn install --production CMD [\u0026#34;node\u0026#34;, \u0026#34;src/index.js\u0026#34;] EXPOSE 3000 빌드 컨테이너 이미지를 빌드한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 docker build -t getting-started . [+] Building 26.1s (13/13) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 186B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; resolve image config for docker.io/docker/dockerfile:1 2.4s =\u0026gt; docker-image://docker.io/docker/dockerfile:1@sha256:39b85bbfa7536a5feceb7372a0817649ecb2724562a38 3.4s =\u0026gt; =\u0026gt; resolve docker.io/docker/dockerfile:1@sha256:39b85bbfa7536a5feceb7372a0817649ecb2724562a38360f 0.0s =\u0026gt; =\u0026gt; sha256:9d0cd65540a143ce38aa0be7c5e9efeed30d3580d03667f107cd76354f2bee65 10.82MB / 10.82MB 3.2s =\u0026gt; =\u0026gt; sha256:39b85bbfa7536a5feceb7372a0817649ecb2724562a38360f4d6a7782a409b14 8.40kB / 8.40kB 0.0s =\u0026gt; =\u0026gt; sha256:7f44e51970d0422c2cbff3b20b6b5ef861f6244c396a06e1a96f7aa4fa83a4e6 482B / 482B 0.0s =\u0026gt; =\u0026gt; sha256:a28edb2041b8f23c38382d8be273f0239f51ff1f510f98bccc8d0e7f42249e97 2.90kB / 2.90kB 0.0s =\u0026gt; =\u0026gt; extracting sha256:9d0cd65540a143ce38aa0be7c5e9efeed30d3580d03667f107cd76354f2bee65 0.2s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; [internal] load metadata for docker.io/library/node:18-alpine 2.3s =\u0026gt; [internal] load build context 0.1s =\u0026gt; =\u0026gt; transferring context: 4.59MB 0.1s =\u0026gt; [1/4] FROM docker.io/library/node:18-alpine@sha256:ca5d399560a9d239cbfa28eec00417f1505e5e108f3ec6 8.3s =\u0026gt; =\u0026gt; resolve docker.io/library/node:18-alpine@sha256:ca5d399560a9d239cbfa28eec00417f1505e5e108f3ec6 0.0s =\u0026gt; =\u0026gt; sha256:fefc7d195eee885e1f309ca2b5eff078b537b766f6bd949f8eb69fe895088821 2.41MB / 2.41MB 1.8s =\u0026gt; =\u0026gt; sha256:ca5d399560a9d239cbfa28eec00417f1505e5e108f3ec6938d230767eaa81f61 1.43kB / 1.43kB 0.0s =\u0026gt; =\u0026gt; sha256:46f34dda633f5708462f6d6ee7ef829535bd0ee04b82cbe28dcccb5df74b3eb1 1.16kB / 1.16kB 0.0s =\u0026gt; =\u0026gt; sha256:a966e12937d2cdbf1cf501f972673875426566c53820a3fe54c2a15b0ad93639 6.50kB / 6.50kB 0.0s =\u0026gt; =\u0026gt; sha256:c41833b44d910632b415cd89a9cdaa4d62c9725dc56c99a7ddadafd6719960f9 3.26MB / 3.26MB 0.6s =\u0026gt; =\u0026gt; sha256:762c2470eea4dfd0e37925b903f27172a7b89fd8b11bb8cf61554941c3293636 47.28MB / 47.28MB 6.7s =\u0026gt; =\u0026gt; extracting sha256:c41833b44d910632b415cd89a9cdaa4d62c9725dc56c99a7ddadafd6719960f9 0.1s =\u0026gt; =\u0026gt; sha256:06fc22ed341f1d0c400e6972828f8731f3544007cda80ea1d333fe15acf0a28b 448B / 448B 1.2s =\u0026gt; =\u0026gt; extracting sha256:762c2470eea4dfd0e37925b903f27172a7b89fd8b11bb8cf61554941c3293636 1.4s =\u0026gt; =\u0026gt; extracting sha256:fefc7d195eee885e1f309ca2b5eff078b537b766f6bd949f8eb69fe895088821 0.1s =\u0026gt; =\u0026gt; extracting sha256:06fc22ed341f1d0c400e6972828f8731f3544007cda80ea1d333fe15acf0a28b 0.0s =\u0026gt; [2/4] WORKDIR /app 0.1s =\u0026gt; [3/4] COPY . . 0.0s =\u0026gt; [4/4] RUN yarn install --production 8.6s =\u0026gt; exporting to image 0.7s =\u0026gt; =\u0026gt; exporting layers 0.7s =\u0026gt; =\u0026gt; writing image sha256:8b52580feb5e556abf813797e2003cbb30d53dfeee0f865fde0f16de6354c6a1 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/getting-started docker build 명령은 Dockerfile을 사용하여 새 컨테이너 이미지를 빌드한다.\n명령 실행 후 많은 레이어들을 다운로드 하게 되었는데, 이는 도커 파일 작성시 기본이 되는 이미지를 FROM으로 명시했기 때문이다. 하지만 해당 이미지가 존재하지 않았기 때문에 다운로드 하게 된다.\n도커가 이미지를 다운로드한 후에는 Dockerfile에서 지시한대로 애플리케이션을 복사하고 yarn을 사용하여 애플리케이션의 종속성을 설치한다. CMD로 이미지에서 컨테이너를 시작할 때 실행할 기본 명령을 지정할 수 있다.\n명령에 사용된 -t 옵션을 명시하면 이미지에 태그를 설정할 수 있으며, 입력된 getting-started로 이름을 지정하여 컨테이너를 실행할 때 해당 이미지를 참조할 수 있게 된다.\n명령의 마지막에 있는 .는 도커가 현재 디렉토리에서 Dockerfile을 찾아야 한다는 것을 알린다.\nStart an app container 이미지가 있다면 docker run명령을 통해 컨테이너에서 애플리케이션을 실행할 수 있다.\n1 2 3 docker run -dp 3000:3000 getting-started 2b5285b3ebfa65b51a44116c620e56b96b787874c71b25c8643b6a9ee137cb49 -d: 새 컨테이너를 백그라운드 모드로 실행한다. -p 호스트의 포트와 컨테이너의 포트간 매핑을 생성한다. 포트 매핑이 없다면 애플리케이션에 접근할 수 없다. 명령이 정상적으로 수행되었다면 http://localhost:3000 링크를 통해 애플리케이션에 접근할 수 있다.\n컨테이너를 간단히 확인해 보면, getting-started 이미지를 사용하고 있으며 포트 3000으로 실행 중인 컨테이너가 적어도 하나 있어야 하고, 컨테이너를 확인하려면 CLI 또는 Docker Desktop의 그래픽 인터페이스를 사용할 수 있다.\n1 2 3 4 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2b5285b3ebfa getting-started \u0026#34;docker-entrypoint.s…\u0026#34; 3 minutes ago Up 3 minutes 0.0.0.0:3000-\u0026gt;3000/tcp youthful_yonath ","date":"2023-04-21T16:08:25+09:00","image":"https://codemario318.github.io/post/docker_1/docker_cover_hue12353db563619e41ee3a11307d3cf25_62602_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/docker_1/","title":"1. Containerize an application"},{"content":"노드 쿠버네티스는 컨테이너를 파드내에 배치하고 노드에서 실행함으로 워크로드를 구동한다. 노드는 클러스터에 따라 가상 또는 물리적 머신일 수 있다. 각 노드는 컨트롤 플레인에 의해 관리되며 파드를 실행하는데 필요한 서비스를 포함한다.\n일반적으로 클러스터에는 여러개의 노드가 있으며, 학습 또는 리소스가 제한되는 환경에서는 하나만 있을 수도 있다.\n노드의 컴포넌트에는 kubelet, 컨테이너 런타임 그리고 kube-proxy가 포함된다.\n관리 API 서버에 노드를 추가하는 두가지 주요 방법이 있다.\n노드의 kubelet으로 컨트롤 플레인에 자체 등록 사용자(또는 다른 사용자)가 노드 오브젝트를 수동으로 추가 노드 오브젝트 또는 노드의 kubelt으로 자체 등록한 후 컨트롤 플레인은 새 노드 오브젝트가 유효한지 확인한다.\n1 2 3 4 5 6 7 8 9 10 { \u0026#34;kind\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;10.240.79.157\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my-first-k8s-node\u0026#34; } } } 쿠버네티스는 내부적으로 노드 오브젝트를 생성(표시)한다. 쿠버네티스는 kubelet이 노드의 metadata,name필드와 일치하는 API 서버에 등록이 되어있는지 확인한다. 노드가 정상이면(필요한 모든 서비스가 실행중인 경우) 파드를 실행할 수 있게 된다. 그렇지 않으면, 해당 노드는 정상이 될때까지 모든 클러스터 활동에 대해 무시된다.\n💡 쿠버네티스는 유효하지 않은 노드 오브젝트를 유지하고, 노드가 정상적인지 확인한다. 상태 확인을 중지하려면 사용자 또는 컨트롤러에서 노드 오브젝트를 명시적으로 삭제해야 한다.\n노드 오브젝트의 이름은 유효한 DNS 서브 도메인 이름이어야 한다.\n노드 이름 고유성 이름은 노드를 식별한다. 두 노드는 동시에 같은 이름을 가질 수 없다. 쿠버네티스는 또한 같은 이름의 리소스가 동일한 객체라고 가정한다. 노드의 경우, 동일한 이름을 사용하는 인스턴스가 동일한 상태와 노드 레이블과 같은 동일한 속성을 갖는다고 암시적으로 가정한다. 인스턴스가 이름을 변경하지 않고 수정된 경우 이로 인해 불일치가 발생할 수 있다. 노드를 대폭 교체하거나 업데이트 해야 하는 경우, 기존 노드 오브젝트를 먼저 API 서버에서 제거하고 업데이트 후 다시 추가해야 한다.\n노드 상태 kubectl을 사용해서 노드 상태와 기타 세부 정보를 볼수 있다.\n1 kubectl describe node \u0026lt;insert-node-name-here\u0026gt; 주소 주소 필드는 클라우드 제공 사업자 또는 베어메탈 구성에 따라 다양하게 사용된다.\nHostName: 노드의 커널에 의해 알려진 호스트명이다. -hostname-override 파라미터를 통해 치환될 수 있다. ExternalIP: 일반적으로 노드의 IP 주소는 외부로 라우트 가능 (클러스터 외부에서 이용 가능) 하다 . InternalIP: 일반적으로 노드의 IP 주소는 클러스터 내에서만 라우트 가능하다. 컨디션 conditions필드는 모든 Running 노드의 상태를 기술한다.\n노드 컨디션 설명 Ready 노드가 상태 양호하며 파드를 수용할 준비가 되어 있는 경우 True, 노드의 상태가 불량하여 파드를 수용하지 못할 경우 False, 그리고 노드 컨트롤러가 마지막 node-monitor-grace-period (기본값 40 기간 동안 노드로부터 응답을 받지 못한 경우) Unknown DiskPressure 디스크 사이즈 상에 압박이 있는 경우, 즉 디스크 용량이 넉넉치 않은 경우 True, 반대의 경우 False MemoryPressure 노드 메모리 상에 압박이 있는 경우, 즉 노드 메모리가 넉넉치 않은 경우 True, 반대의 경우 False PIDPressure 프로세스 상에 압박이 있는 경우, 즉 노드 상에 많은 프로세스들이 존재하는 경우 True, 반대의 경우 False NetworkUnavailable 노드에 대해 네트워크가 올바르게 구성되지 않은 경우 True, 반대의 경우 False 용량과 할당가능 노드 상에 사용 가능한 리소스를 나타낸다.\nCPU 메모리 스케줄 되어질 수 있는 최대 파드 수 등 용량 블록 필드는 노드에 있는 리소스의 총량을 나타낸다. 할당가능 블록은 일반 파드에서 사용할 수 있는 노드의 리소스 양을 나타낸다.\n정보 노드에 대한 일반적은 정보가 기술된다. kubelet이 노드에서 수집하여 쿠버네티스 API 로 전송한다.\n커널 버전 쿠버네티스 버전 컨테이너 런타임 상세 정보 등 하트비트 쿠버네티스 노드가 보내는 하트비느느 ㄴ클러스터가 개별 노드가 가용한지를 판단할 수 있도록 도움을 주고, 장애가 발견된 경우 조치를 할 수 있게 한다.\n노드의 .status 에 대한 업데이트 kube=-node-lease 네임 스페이스 내의 Lease오브젝트 노드의 .status에 비하면, 리스는 경량의 리소스이다. 큰 규모의 클러스터에서는 리스를 하트비트에 사용하여 업데이트로 인한 성능 영향을 줄일 수 있다.\nkubelet은 노드의 .status 생성과 업데이트 및 관련된 리스의 업데이트를 담당한다.\nkubelet은 상태가 변경되거나 설정된 인터벌보다 오래 업데이트가 없는 경우 노드의 .status를 업데이트한다. 노드의 .status 업데이트에 대한 기본 인터벌은 접근이 불가능한 노드에 대한 타임아웃인 40초 보다 훨씬 긴 5분이다. kubelet은 리스 오브젝트를 (기본 업데이트 인터벌인) 매 10초마다 생성하고 업데이트한다. 리스 업데이트는 노드의 .status 업데이트와는 독립적이다. 만약 리스 업데이트가 실패하면, kubelet은 200밀리초에서 시작하고 7초의 상한을 갖는 지수적 백오프를 사용해서 재시도한다. 노드 컨트롤러 노드 컨트롤러는 노드의 다양한 측면을 관리하는 쿠버네티스 컨트롤 플레인 컴포넌트이다.\n노드 컨트롤러는 노드가 생성되어 유지되는 동안 다양한 역할을 한다.\n등록 시점에 (CIDR 할당이 사용토록 설정된 경우) 노드에 CIDR 블럭을 할당하는 것이다.\n노드 컨트롤러의 내부 노드 리스트를 클라우드 제공사업자의 사용 가능한 머신 리스트 정보를 근거로 최신상태로 유지하는 것이다.\n클라우드 환경에서 동작 중일 경우, 노드상태가 불량할 때마다, 노드 컨트롤러는 해당 노드용 VM이 여전히 사용 가능한지에 대해 클라우드 제공사업자에게 묻는다. 사용 가능하지 않을 경우, 노드 컨트롤러는 노드 리스트로부터 그 노드를 삭제한다. 노드의 동작 상태를 모니터링하는 것이다. 노드가 접근 불가능(unreachable) 상태가 되는 경우, 노드의 .status 필드의 Ready 컨디션을 업데이트한다. 이 경우에는 노드 컨트롤러가 Ready 컨디션을 Unknown으로 설정한다. 노드가 계속 접근 불가능 상태로 남아있는 경우, 해당 노드의 모든 파드에 대해서 API를 이용한 축출을 트리거한다. 기본적으로, 노드 컨트롤러는 노드를 Unknown으로 마킹한 뒤 5분을 기다렸다가 최초의 축출 요청을 시작한다. 기본적으로, 노드 컨트롤러는 5 초마다 각 노드의 상태를 체크한다. 체크 주기는 kube-controller-manager 구성 요소의 --node-monitor-period 플래그를 이용하여 설정할 수 있다.\n축출 빈도 제한 대부분의 경우, 노드 컨트롤러는 초당 --node-eviction-rate(기본값 0.1)로 축출 속도를 제한한다. 이 말은 10초당 1개의 노드를 초과하여 파드 축출을 하지 않는다는 의미가 된다.\n노드 축출 행위는 주어진 가용성 영역 내 하나의 노드가 상태가 불량할 경우 변화한다. 노드 컨트롤러는 영역 내 동시에 상태가 불량한 노드의 퍼센티지가 얼마나 되는지 체크한다(Ready 컨디션은 Unknown 또는 False 값을 가진다).\n상태가 불량한 노드의 비율이 최소 -unhealthy-zone-threshold (기본값 0.55)가 되면 축출 속도가 감소한다. 클러스터가 작으면 (즉 -large-cluster-size-threshold 노드 이하면 - 기본값 50) 축출이 중지된다. 이외의 경우, 축출 속도는 초당 -secondary-node-eviction-rate(기본값 0.01)로 감소된다. 이 정책들이 가용성 영역 단위로 실행되어지는 이유는 나머지가 연결되어 있는 동안 하나의 가용성 영역이 컨트롤 플레인으로부터 분할되어 질 수도 있기 때문이다. 만약 클러스터가 여러 클라우드 제공사업자의 가용성 영역에 걸쳐 있지 않는 이상, 축출 매커니즘은 영역 별 가용성을 고려하지 않는다.\n노드가 가용성 영역들에 걸쳐 퍼져 있는 주된 이유는 하나의 전체 영역이 장애가 발생할 경우 워크로드가 상태 양호한 영역으로 이전될 수 있도록 하기 위해서이다. 그러므로, 하나의 영역 내 모든 노드들이 상태가 불량하면 노드 컨트롤러는 --node-eviction-rate 의 정상 속도로 축출한다.\n모든 영역이 완전히 상태불량(클러스터 내 양호한 노드가 없는 경우)한 경우이다. 이러한 경우, 노드 컨트롤러는 컨트롤 플레인과 노드 간 연결에 문제가 있는 것으로 간주하고 축출을 실행하지 않는다. (중단 이후 일부 노드가 다시 보이는 경우 노드 컨트롤러는 상태가 양호하지 않거나 접근이 불가능한 나머지 노드에서 파드를 축출한다.)\n또한, 노드 컨트롤러는 파드가 테인트를 허용하지 않을 때 NoExecute 테인트 상태의 노드에서 동작하는 파드에 대한 축출 책임을 가지고 있다. 추가로, 노드 컨틀로러는 연결할 수 없거나, 준비되지 않은 노드와 같은 노드 문제에 상응하는 테인트를 추가한다. 이는 스케줄러가 비정상적인 노드에 파드를 배치하지 않게 된다.\n리소스 용량 추적 노드 오브젝트는 노드 리소스 용량에 대한 정보: 예를 들어, 사용 가능한 메모리의 양과 CPU의 수를 추적한다. 노드의 자체 등록은 등록하는 중에 용량을 보고한다. 수동으로 노드를 추가하는 경우 추가할 때 노드의 용량 정보를 설정해야 한다.\n쿠버네티스 스케줄러는 노드 상에 모든 노드에 대해 충분한 리소스가 존재하도록 보장한다. 스케줄러는 노드 상에 컨테이너에 대한 요청의 합이 노드 용량보다 더 크지 않도록 체크한다. 요청의 합은 kubelet에서 관리하는 모든 컨테이너를 포함하지만, 컨테이너 런타임에 의해 직접적으로 시작된 컨 테이너는 제외되고 kubelet의 컨트롤 범위 밖에서 실행되는 모든 프로세스도 제외된다.\n참고: 파드 형태가 아닌 프로세스에 대해 명시적으로 리소스를 확보하려면, 시스템 데몬에 사용할 리소스 예약하기을 본다.\n노드 토폴로지 기능 상태: Kubernetes v1.18 [beta]\nTopologyManager 기능 게이트(feature gate)를 활성화 시켜두면, kubelet이 리소스 할당 결정을 할 때 토폴로지 힌트를 사용할 수 있다. 자세한 내용은 노드의 컨트롤 토폴로지 관리 정책을 본다.\n그레이스풀(Graceful) 노드 셧다운(shutdown) 기능 상태: Kubernetes v1.21 [beta]\nkubelet은 노드 시스템 셧다운을 감지하고 노드에서 실행 중인 파드를 종료하려고 시도한다.\nKubelet은 노드가 종료되는 동안 파드가 일반 파드 종료 프로세스를 따르도록 한다.\n그레이스풀 노드 셧다운 기능은 systemd inhibitor locks를 사용하여 주어진 기간 동안 노드 종료를 지연시키므로 systemd에 의존한다.\n그레이스풀 노드 셧다운은 1.21에서 기본적으로 활성화된 GracefulNodeShutdown 기능 게이트로 제어된다.\n기본적으로, 아래 설명된 두 구성 옵션, shutdownGracePeriod 및 shutdownGracePeriodCriticalPods 는 모두 0으로 설정되어 있으므로, 그레이스풀 노드 셧다운 기능이 활성화되지 않는다. 기능을 활성화하려면, 두 개의 kubelet 구성 설정을 적절하게 구성하고 0이 아닌 값으로 설정해야 한다.\n그레이스풀 셧다운 중에 kubelet은 다음의 두 단계로 파드를 종료한다.\n노드에서 실행 중인 일반 파드를 종료시킨다. 노드에서 실행 중인 중요(critical) 파드를 종료시킨다. 그레이스풀 노드 셧다운 기능은 두 개의 [KubeletConfiguration](https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/) 옵션으로 구성된다.\nshutdownGracePeriod: 노드가 종료를 지연해야 하는 총 기간을 지정한다. 이것은 모든 일반 및 중요 파드의 파드 종료에 필요한 총 유예 기간에 해당한다. shutdownGracePeriodCriticalPods: 노드 종료 중에 중요 파드를 종료하는 데 사용되는 기간을 지정한다. 이 값은 shutdownGracePeriod 보다 작아야 한다. 예를 들어, shutdownGracePeriod=30s, shutdownGracePeriodCriticalPods=10s 인 경우, kubelet은 노드 종료를 30초까지 지연시킨다. 종료하는 동안 처음 20(30-10)초는 일반 파드의 유예 종료에 할당되고, 마지막 10초는 중요 파드의 종료에 할당된다.\n**참고: 그레이스풀 노드 셧다운 과정에서 축출된 파드는 셧다운(shutdown)된 것으로 표시된다. kubectl get pods 명령을 실행하면 축출된 파드의 상태가 Terminated으로 표시된다. 그리고 kubectl describe pod 명령을 실행하면 노드 셧다운으로 인해 파드가 축출되었음을 알 수 있다.\nReason: Terminated Message: Pod was terminated in response to imminent node shutdown.**\n논 그레이스풀 노드 셧다운 기능 상태: Kubernetes v1.24 [alpha]\n전달한 명령이 kubelet에서 사용하는 금지 잠금 메커니즘(inhibitor locks mechanism)을 트리거하지 않거나, 또는 사용자 오류(예: ShutdownGracePeriod 및 ShutdownGracePeriodCriticalPods가 제대로 설정되지 않음)로 인해 kubelet의 노드 셧다운 관리자(Node Shutdown Mananger)가 노드 셧다운 액션을 감지하지 못할 수 있다. 자세한 내용은 위의 그레이스풀 노드 셧다운 섹션을 참조한다.\n노드가 셧다운되었지만 kubelet의 노드 셧다운 관리자가 이를 감지하지 못하면, 스테이트풀셋에 속한 파드는 셧다운된 노드에 \u0026lsquo;종료 중(terminating)\u0026rsquo; 상태로 고착되어 다른 동작 중인 노드로 이전될 수 없다. 이는 셧다운된 노드의 kubelet이 파드를 지울 수 없어서 결국 스테이트풀셋이 동일한 이름으로 새 파드를 만들 수 없기 때문이다. 만약 파드가 사용하던 볼륨이 있다면, 볼륨어태치먼트(VolumeAttachment)도 기존의 셧다운된 노드에서 삭제되지 않아 결국 파드가 사용하던 볼륨이 다른 동작 중인 노드에 연결(attach)될 수 없다. 결과적으로, 스테이트풀셋에서 실행되는 애플리케이션이 제대로 작동하지 않는다. 기존의 셧다운된 노드가 정상으로 돌아오지 못하면, 이러한 파드는 셧다운된 노드에 \u0026lsquo;종료 중(terminating)\u0026rsquo; 상태로 영원히 고착될 것이다.\n위와 같은 상황을 완화하기 위해, 사용자가 node.kubernetes.io/out-of-service 테인트를 NoExecute 또는 NoSchedule 값으로 추가하여 노드를 서비스 불가(out-of-service) 상태로 표시할 수 있다. kube-controller-manager에 NodeOutOfServiceVolumeDetach기능 게이트 가 활성화되어 있고, 노드가 이 테인트에 의해 서비스 불가 상태로 표시되어 있는 경우, 노드에 매치되는 톨러레이션이 없다면 노드 상의 파드는 강제로 삭제될 것이고, 노드 상에서 종료되는 파드에 대한 볼륨 해제(detach) 작업은 즉시 수행될 것이다. 이를 통해 서비스 불가 상태 노드의 파드가 빠르게 다른 노드에서 복구될 수 있다.\n논 그레이스풀 셧다운 과정 동안, 파드는 다음의 두 단계로 종료된다.\n매치되는 out-of-service 톨러레이션이 없는 파드를 강제로 삭제한다. 이러한 파드에 대한 볼륨 해제 작업을 즉시 수행한다. 참고: • node.kubernetes.io/out-of-service 테인트를 추가하기 전에, 노드가 완전한 셧다운 또는 전원 꺼짐 상태에 있는지 (재시작 중인 것은 아닌지) 확인한다. • 사용자가 서비스 불가 상태 테인트를 직접 추가한 것이기 때문에, 파드가 다른 노드로 옮겨졌고 셧다운 상태였던 노드가 복구된 것을 확인했다면 사용자가 서비스 불가 상태 테인트를 수동으로 제거해야 한다.\n파드 우선순위 기반 그레이스풀 노드 셧다운 기능 상태: Kubernetes v1.23 [alpha]\n그레이스풀 노드 셧다운 시 파드 셧다운 순서에 더 많은 유연성을 제공할 수 있도록, 클러스터에 프라이어리티클래스(PriorityClass) 기능이 활성화되어 있으면 그레이스풀 노드 셧다운 과정에서 파드의 프라이어리티클래스가 고려된다. 이 기능으로 그레이스풀 노드 셧다운 시 파드가 종료되는 순서를 클러스터 관리자가 프라이어리티 클래스 기반으로 명시적으로 정할 수 있다.\n위에서 기술된 것처럼, 그레이스풀 노드 셧다운 기능은 파드를 중요하지 않은(non-critical) 파드와 중요한(critical) 파드 2단계(phase)로 구분하여 종료시킨다. 셧다운 시 파드가 종료되는 순서를 명시적으로 더 상세하게 정해야 한다면, 파드 우선순위 기반 그레이스풀 노드 셧다운을 사용할 수 있다.\n그레이스풀 노드 셧다운 과정에서 파드 우선순위가 고려되기 때문에, 그레이스풀 노드 셧다운이 여러 단계로 일어날 수 있으며, 각 단계에서 특정 프라이어리티 클래스의 파드를 종료시킨다. 정확한 단계와 단계별 셧다운 시간은 kubelet에 설정할 수 있다.\n다음과 같이 클러스터에 커스텀 파드 프라이어리티 클래스가 있다고 가정하자.\n파드 프라이어리티 클래스 이름 파드 프라이어리티 클래스 값 custom-class-a 100000 custom-class-b 10000 custom-class-c 1000 regular/unset 0 kubelet 환경 설정 안의 shutdownGracePeriodByPodPriority 설정은 다음과 같을 수 있다.\n파드 프라이어리티 클래스 값 종료 대기 시간 100000 10 seconds 10000 180 seconds 1000 120 seconds 0 60 seconds 이를 나타내는 kubelet 환경 설정 YAML은 다음과 같다.\n1 2 3 4 5 6 7 8 9 **shutdownGracePeriodByPodPriority**: - **priority**: 100000 **shutdownGracePeriodSeconds**: 10 - **priority**: 10000 **shutdownGracePeriodSeconds**: 180 - **priority**: 1000 **shutdownGracePeriodSeconds**: 120 - **priority**: 0 **shutdownGracePeriodSeconds**: 60 위의 표에 의하면 priority 값이 100000 이상인 파드는 종료까지 10초만 주어지며, 10000 이상 ~ 100000 미만이면 180초, 1000 이상 ~ 10000 미만이면 120초가 주어진다. 마지막으로, 다른 모든 파드는 종료까지 60초가 주어질 것이다.\n모든 클래스에 대해 값을 명시할 필요는 없다. 예를 들어, 대신 다음과 같은 구성을 사용할 수도 있다.\n파드 프라이어리티 클래스 값 종료 대기 시간 100000 300 seconds 1000 120 seconds 0 60 seconds 위의 경우, custom-class-b에 속하는 파드와 custom-class-c에 속하는 파드는 동일한 종료 대기 시간을 갖게 될 것이다.\n특정 범위에 해당되는 파드가 없으면, kubelet은 해당 범위에 해당되는 파드를 위해 기다려 주지 않는다. 대신, kubelet은 즉시 다음 프라이어리티 클래스 값 범위로 넘어간다.\n기능이 활성화되어 있지만 환경 설정이 되어 있지 않으면, 순서 지정 동작이 수행되지 않을 것이다.\n이 기능을 사용하려면 GracefulNodeShutdownBasedOnPodPriority 기능 게이트를 활성화해야 하고, kubelet config의 ShutdownGracePeriodByPodPriority를 파드 프라이어리티 클래스 값과 각 값에 대한 종료 대기 시간을 명시하여 지정해야 한다.\n참고: 그레이스풀 노드 셧다운 과정에서 파드 프라이어리티를 고려하는 기능은 쿠버네티스 v1.23에서 알파 기능으로 도입되었다. 쿠버네티스 1.26에서 이 기능은 베타 상태이며 기본적으로 활성화되어 있다.\ngraceful_shutdown_start_time_seconds 및 graceful_shutdown_end_time_seconds 메트릭은 노드 셧다운을 모니터링하기 위해 kubelet 서브시스템에서 방출된다.\n스왑(swap) 메모리 관리 기능 상태: Kubernetes v1.22 [alpha]\n쿠버네티스 1.22 이전에는 노드가 스왑 메모리를 지원하지 않았다. 그리고 kubelet은 노드에서 스왑을 발견하지 못한 경우 시작과 동시에 실패하도록 되어 있었다. 1.22부터는 스왑 메모리 지원을 노드 단위로 활성화할 수 있다.\n노드에서 스왑을 활성화하려면, NodeSwap 기능 게이트가 kubelet에서 활성화되어야 하며, 명령줄 플래그 --fail-swap-on 또는 구성 설정에서 failSwapOn가 false로 지정되어야 한다.\n경고: 메모리 스왑 기능이 활성화되면, 시크릿 오브젝트의 내용과 같은 tmpfs에 기록되었던 쿠버네티스 데이터가 디스크에 스왑될 수 있다.\n사용자는 또한 선택적으로 memorySwap.swapBehavior를 구성할 수 있으며, 이를 통해 노드가 스왑 메모리를 사용하는 방식을 명시한다.\n1 2 memorySwap: swapBehavior: LimitedSwap` swapBehavior에 가용한 구성 옵션은 다음과 같다.\nLimitedSwap: 쿠버네티스 워크로드는 스왑을 사용할 수 있는 만큼으로 제한된다. 쿠버네티스에 의해 관리되지 않는 노드의 워크로드는 여전히 스왑될 수 있다. UnlimitedSwap: 쿠버네티스 워크로드는 요청한 만큼 스왑 메모리를 사용할 수 있으며, 시스템의 최대치까지 사용 가능하다. 만약 memorySwap 구성이 명시되지 않았고 기능 게이트가 활성화되어 있다면, kubelet은 LimitedSwap 설정과 같은 행동을 기본적으로 적용한다.\nLimitedSwap 설정에 대한 행동은 노드가 (\u0026ldquo;cgroups\u0026quot;으로 알려진) 제어 그룹이 v1 또는 v2 중에서 무엇으로 동작하는가에 따라서 결정된다.\ncgroupsv1: 쿠버네티스 워크로드는 메모리와 스왑의 조합을 사용할 수 있다. 파드의 메모리 제한이 설정되어 있다면 가용 상한이 된다. cgroupsv2: 쿠버네티스 워크로드는 스왑 메모리를 사용할 수 없다. 테스트를 지원하고 피드벡을 제공하기 위한 정보는 KEP-2400 및 디자인 제안에서 찾을 수 있다.\n","date":"2023-04-21T14:00:47+09:00","image":"https://codemario318.github.io/post/kubernetes_5/kubernetes_cover_hu15504362c54454204bbf321d3bfe3873_12586_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/kubernetes_5/","title":"쿠버네티스: 5. 클러스터 아키텍처"},{"content":"관리기법 ⛔ 쿠버네티스 오브젝트는 하나의 기법만 사용하여 관리해야 한다. 동일한 오브젝트에 대해 여러 기법을 혼용하면 오동작이 발생할 수 있다.\n관리기법 대상 권장 환경 지원하는 작업자 수 학습 난이도 명령형 커맨드 활성 오브젝트 개발 환경 1+ 낮음 명령형 오브젝트 구성 개별 파일 프로덕션 환경 1 보통 선언형 오브젝트 구성 파일이 있는 디렉터리 프로덕션 환경 1+ 높음 명령형 커맨드 사용자가 클러스터 내 활성 오브젝트를 대상으로 직접 동작시킨다. 실행할 작업을 인수 또는 플래그로 kubectl 커맨드에 지정한다.\n1 kubectl create deployment nginx --image nginx 트레이드 오프 장점\n커맨드는 하나의 동작을 나타내는 단어로 표현됨 클러스터를 수정하기 위해 단 하나의 단계만을 필요로 한다. 단점\n커맨드는 변경 검토 프로세스와 통합되지 않는다. 변경에 관한 감사 추적(audit trail)을 제공하지 않는다. 활성 동작 중인 경우를 제외하고는 레코드의 소스를 제공하지 않는다. 새로운 오브젝트 생성을 위한 템플릿을 제공하지 않는다. 명령형 오브젝트 구성 kubectl 커맨드로 작업, 선택적 플래그, 파일 이름을 지정하여 실행한다.\n","date":"2023-04-21T13:57:47+09:00","image":"https://codemario318.github.io/post/kubernetes_4/kubernetes_cover_hu15504362c54454204bbf321d3bfe3873_12586_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/kubernetes_4/","title":"쿠버네티스: 4. 쿠버네티스 오브젝트 관리"},{"content":" 쿠버네티스 오브젝트는 하나의 의도를 담은 레코드이다.\n쿠버네티스 오브젝트는 쿠버네티스 시스템에서 영속성을 가지는 오브젝트이다. 쿠버네티스는 클러스터의 상태를 나타내기 위해 .yaml로 작성된 오브젝트에 상세 내용을 기술한다.\n어떤 컨테이너화된 애플리케이션이 동작 중인지(어느 노드에서 동작중인지) 해당 어플리케이션이 이용할 수 있는 리소스 해당 애플리케이션이 어떻게 재구동 정책, 업그레이드 내고장성과 같은 것에 동작해야 하는지에 대한 정책 오브젝트를 생성하게 되면, 쿠버네티스 시스템은 오브젝트에 담긴 상태를 보장하기 위해 지속적으로 동작하게 된다.\n오브젝트 명세(spec)과 상태(status) 거의 모든 쿠버네티스 오브젝트는 spec 오브젝트와 status 오브젝트로 구성된다.\nspec spec을 가지는 오브젝트는 오브젝트를 생성할 때 리소스에 원하는 특징(의도한 상태)에 대한 설명을 제공하여 설정한다.\nstatus 쿠버네티스 시스템과 컴포넌트에 의해 제공되고 업데이트된 오브젝트의 현재 상태를 설명한다. 컨트롤 플레인은 모든 오브젝트의 실제 상태를 사용자가 의도한 상태와 일치시키기 위해서 끊임없이 능독적으로 관리한다.\nhttps://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md\n쿠버네티스 오브젝트 기술하기 쿠버네티스에서 오브젝트를 생성할 때, 오브젝트에 대한 기본적인 정보와 더불어, 의도한 상태를 기술한 오브젝트 spec을 제시해야 한다.\n대부분의 경우 정보를 .yaml 파일로 kubectl에 제공한다. kubectl은 API 요청이 이루어질 때, JSON 형식으로 정보를 변환시켜준다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 요구되는 필드 생성하고자 하는 쿠버네티스 오브젝트에 대한 .yaml 파일 내, 다음 필드를 윟나 값들을 설정해 줘야한다.\napiVersion: 해당 오브젝트를 생성하기 위해 사용하고 있는 쿠버네티스 API 버전 kind: 오브젝트의 종류 metadata: 오브젝트를 유일하게 구분지어 줄 데이터(이름 문자열, UID, 선택적인 네임스페이스 등) spec에 대한 정확한 포맷은 모든 쿠버네티스 오브젝트마다다르고, 그 오브젝트 특유의 중첩된 필드를 포함한다.\nhttps://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/\n","date":"2023-04-21T13:53:47+09:00","image":"https://codemario318.github.io/post/kubernetes_3/kubernetes_cover_hu15504362c54454204bbf321d3bfe3873_12586_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/kubernetes_3/","title":"쿠버네티스: 3. 쿠버네티스 오브젝트 이해하기"},{"content":"쿠버네티스 클러스터 컨테이너화 된 애프릴케이션을 실행하는 노드라고 하는 워커 머신의 집합으로, 모든 클러스터는 최소 한 개의 워커 노드를 가진다.\n워커 노드는 애플리케이션의 구성요소인 파드를 호스트하며, 컨트롤 플레인은 워커 노드와 클러스터 내 파드를 관리한다.\n프로덕션 환경에서는 일반적으로 컨트롤 플레인이 여러 컴퓨터에 걸쳐 실행되고, 클러스터는 일반적으로 여러 노드를 실행하므로 내결함성과 고가용성이 제공된다.\n컨트롤 플레인 컴포넌트 컨트롤 플레인 컴포넌트는 클러스터에 관한 전반적인 결정 (스케줄링 등)을 수행하고 클러스터 이벤트를 감지하고 반응한다.\n컨트롤 플레인 컴포넌트는 클러스터 내 어떠한 머신에서도 동작할 수 있으나, 간결성 유지를 위해 구성 스크립트는 보통 동일 머신 상에 모든 컨트롤 플레인 컴포넌트를 구동시키고, 사용자 컨테이너는 해당 머신 상에 동작시키지 않는다.\nkube-apiserver API 서버는 쿠버네티스 API를 노출하는 쿠버네티스 컨트롤 플레인 컴포넌트이다. API 서버는 쿠버네티스 컨트롤 플레인의 프론트 엔드이다.\n쿠버네티스 API서버는 정말 그냥 API서버라구욧\nkube-apiserver는 마스터 노드의 중심에서 모든 클라이언트, 컴포넌트로 부터 오는 요청들을 받는 REST API 서버이다.\nkubectl와 Kubernetes SDK를 이용할 수 있지만, 직접 REST API 호출로 쿠버네티스와 통신할 수 있다.\n쿠버네티스 API를 사용하여 애플리케이션을 작성하는 경우 클라이언트 라이브러리 중 하나를 사용하는 것이 좋다. https://kubernetes.io/ko/docs/reference/using-api/client-libraries/\netcd 모든 클러스터 데이터를 담는 쿠버네티스 뒷단의 저장소로 사용되는 일관성 및 고가용성 키-값 저장소.\nKubernetes 운영을 위한 etcd 기본 동작 원리의 이해\nkubernetes는 기반 스토리지(backing storage)로 etcd를 사용하고 있고, 모든 데이터가 etcd에 보관된다(클러스터에 어떤 노드가 몇 개나 있고 어떤 파드가 어떤 노드에서 동작하고 있는지등 ). 동작중인 클러스터의 etcd 데이터베이스가 유실된다면 컨테이너 뿐만 아니라 클러스터가 사용하는 모든 리소스는 미아가 된다.\n따라서 쿠버네티스 클러스터에서 etcd를 뒷단 저정소로 사용한다면, 이 데이터를 백업하는 계획을 필수적으로 갖추어야 한다.\nkube-scheduler 노드가 배정되지 않은 새로 생성된 파드를 감지하고, 실행할 노드를 선택하는 컨트롤 플레인 컴포넌트.\n아래 항목들에 대해 고려하여 스케줄링이 이루어진다.\n리소스에 대한 개별 및 총체적 요구 사항 하드웨어/소프트웨어/정책적 제약 어피니티(affinity) 및 안티-어피니티(anti-affinity) 명세 데이터 지역성 워크로드간 간섭 데드라인 kube-controller-manager 컨트롤러 프로세스를 실행하는 컨트롤 플레인 컴포넌트.\n각 컨트롤러는 논리적으로 분리된 프로세스이지만, 복잡성을 낮추기 위해 모두 단일 바이너리로 컴파일되고 단일 프로세스 내에서 실행된다.\n노드 컨트롤러: 노드가 다운되었을 때 통지와 대응에 관한 책임을 가진다. 잡 컨트롤러: 일회성 작업을 나타내는 잡 오브젝트를 감시한 다음, 해당 작업을 완료할 때까지 동작하는 파드를 생성한다. 엔드포인트 컨트롤러: 엔드포인트 오브젝트를 채운다.(서비스와 파드를 연결시킨다.) 서비스 어카운트 \u0026amp; 토큰 컨트롤러: 새로운 네임스페이스에 대한 기본 계정과 API 접근 토큰을 생성한다. cloud-controller-manager 클라우드 별 컨트롤 로직을 포함하는 쿠버네티스 컨트롤 플레인 컴포넌트이다.\n클라우드 컨트롤러 매니저를 통해 클러스터를 클라우드 공급자의 API에 연결하고, 해당 클라우드 플랫폼과 상호 작용하는 컴포넌트와 클러스터와만 상호 작용하는 컴포넌트를 구분할 수 있게 해준다.\n클라우드 제공자 전용 컨트롤러만 실행한다. 자신의 사내 또는 PC 내부의 학습 환경에서 쿠버네티스를 실행 중인 경우 클러스터에는 클라우드 컨트롤러 매니저가 없다.\nkube-controller-manager와 마찬가지로 cloud-controller-manager는 논리적으로 독립적인 여러 컨트롤 루프를 단일 프로세스로 실행하는 단일 바이너리로 결합한다. 수평으로 확장(두 개 이상의 복제 실행)해서 성능을 향상시키거나 장애를 견딜 수 있다.\n노드 컨트롤러: 노드가 응답을 멈춘 후 클라우드 상에서 삭제되었는지 판별하기 위해 클라우드 제공 사업자에게 확인하는 것 라우트 컨트롤러: 기본 클라우드 인프라에 경로를 구성하는 것 서비스 컨트롤러: 클라우드 제공 사업자 로드밸런서를 생성, 업데이트 그리고 삭제하는 것 노드 컴포넌트 노드 컴포넌트는 동작 중인 파드를 유지시키고 쿠버네티스 런타임 환경을 제공하며, 모든 노드 상에서 동작한다.\nkubelet 클러스터의 각 노드에서 실행되는 에이전트. kubelet은 파드에서 컨테이너가 확실하게 동작하도록 관리한다.\n다양한 메커니즘을 통해 제공된 파드 스펙(PodSpec)의 집합을 받아 컨테이너가 해당 파드 스펙에 따라 건강하게 동작하는 것을 확실히 한다.\nkubelet은 쿠버네티스를 통해 생성되지 않는 컨테이너는 관리하지 않는다.\nkube-proxy 클러스터의 각 노드에서 실행되는 네트워크 프록시로, 크버네티스의 서비스 개념 구현부이다.\n노드의 네트워크 규칙을 관리한다. 네트워크 규칙이 내부 네트워크 세션이나 클러스터 바깥에서 파드로 네트워크 통신을 하도록 해준다.\n운영체제에 가용한 패킷 필터링 계층이 있는 경우, 이를 사용한다. 그렇지 않으면, kube-proxy는 트래픽 자체를 포워드한다.\n컨테이너 런타임 컨테이너 런타임은 컨테이너 실행을 담당하는 소프트웨어이다.\n쿠버네티스는 containerd, CRI-O와 같은 컨테이너 런타임 및 모든 Kubernetes CRI(컨테이너 런타임 인터페이스) 구현체를 지원한다.\nCRI란?\n클러스터 컴포넌트를 다시 컴파일하지 않아도 Kubelet이 다양한 컨테이너 런타임을 사용할 수 있도록 하는 플러그인 인터페이스이다.\n","date":"2023-04-21T13:42:47+09:00","image":"https://codemario318.github.io/post/kubernetes_2/kubernetes_cover_hu15504362c54454204bbf321d3bfe3873_12586_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/kubernetes_2/","title":"쿠버네티스: 2. 쿠버네티스 컴포넌트"},{"content":"쿠버네티스란? 쿠버네티스는 컨테이너화된 워크로드와 서비스를 관리하기 위한 이식성이 있고, 확장 가능한 오픈소스 플랫폼이다.\n컨테이너화된 워크로드와 서비스를 관리하기 위한 이식성이 있다. 확장가능한 오픈소스 플랫폼이다. 선언적 구성과 자동화를 모두 용이하게 해준다. 크고 빠르게 성장하는 생태계를 가지고 있다. 쿠버네티스 서비서, 기술 지원 및 도구는 어디서나 쉽게 이용할 수 있다. 컨테이너 장점\n기민한 애플리케이션 생성과 배포\nVM 이미지를 사용하는 것에 비해 컨테이너 이미지 생성이 보다 쉽고 효율적임. 지속적인 개발, 통합 및 배포\n안정적이고 주기적으로 컨테이너 이미지를 빌드해서 배포할 수 있고 (이미지의 불변성 덕에) 빠르고 효율적으로 롤백할 수 있다. 개발과 운영의 관심사 분리\n배포 시점이 아닌 빌드/릴리스 시점에 애플리케이션 컨테이너 이미지를 만들기 때문에, 애플리케이션이 인프라스트럭처에서 분리된다. 가시성(observability)\nOS 수준의 정보와 메트릭에 머무르지 않고, 애플리케이션의 헬스와 그 밖의 시그널을 볼 수 있다. 개발, 테스팅 및 운영 환경에 걸친 일관성\n랩탑에서도 클라우드에서와 동일하게 구동된다. 클라우드 및 OS 배포판 간 이식성\nUbuntu, RHEL, CoreOS, 온-프레미스, 주요 퍼블릭 클라우드와 어디에서든 구동된다. 애플리케이션 중심 관리\n가상 하드웨어 상에서 OS를 실행하는 수준에서 논리적인 리소스를 사용하는 OS 상에서 애플리케이션을 실행하는 수준으로 추상화 수준이 높아진다. 느슨하게 커플되고, 분산되고, 유연하며, 자유로운 마이크로서비스 애플리케이션은 단일 목적의 머신에서 모놀리식 스택으로 구동되지 않고 보다 작고 독립적인 단위로 쪼개져서 동적으로 배포되고 관리될 수 있다. 리소스 격리\n애플리케이션 성능을 예측할 수 있다. 리소스 사용량\n고효율 고집적. 쿠버네티스가 왜 필요하고 무엇을 할 수 있나 기존 \u0026ldquo;전통적인 배포\u0026rdquo;, \u0026ldquo;가상화된 배포\u0026quot;를 거치며 \u0026ldquo;컨테이너를 통한 배포\u0026rdquo; 까지 발전해왔다.\n컨테이너를 통한 개발 환경은 애플리케이션을 포장하고 실행하는 좋은 방법이지만, 프로덕션 환경에서는 애플리케이션을 실행하는 컨테이너를 관리하고 가동 중지 시간이 없는지 확인해야 하는 등 여러 작업이 필요하게 된다.\n쿠버네티스는 분산 시스템을 탄력적으로 실행하기 위한 프레임 워크를 제공한다. 애플리케이션의 확장과 장애 조치를 처리하고, 배포 패턴 등을 제공한다. 예를 들어, 쿠버네티스는 시스템의 카나리아 배포를 쉽게 관리 할 수 있다.\n서비스 디스커버리와 로드 밸런싱 쿠버네티스는 DNS 이름을 사용하거나 자체 IP 주소를 사용하여 컨테이너를 노출할 수 있다. 컨테이너에 대한 트래픽이 많으면, 쿠버네티스는 네트워크 트래픽을 로드밸런싱하고 배포하여 배포가 안정적으로 이루어질 수 있다.\n스토리지 오케스트레이션 쿠버네티스를 사용하면 로컬 저장소, 공용 클라우드 공급자 등과 같이 원하는 저장소 시스템을 자동으로 탑재 할 수 있다.\n자동화된 롤아웃과 롤백 쿠버네티스를 사용하여 배포된 컨테이너의 원하는 상태를 서술할 수 있으며 현재 상태를 원하는 상태로 설정한 속도에 따라 변경할 수 있다. 예를 들어 쿠버네티스를 자동화해서 배포용 새 컨테이너를 만들고, 기존 컨테이너를 제거하고, 모든 리소스를 새 컨테이너에 적용할 수 있다.\n자동화된 빈 패킹(bin packing) 컨테이너화된 작업을 실행하는데 사용할 수 있는 쿠버네티스 클러스터 노드를 제공한다. 각 컨테이너가 필요로 하는 CPU와 메모리(RAM)를 쿠버네티스에게 지시한다. 쿠버네티스는 컨테이너를 노드에 맞추어서 리소스를 가장 잘 사용할 수 있도록 해준다.\n자동화된 복구(self-healing) 쿠버네티스는 실패한 컨테이너를 다시 시작하고, 컨테이너를 교체하며, \u0026lsquo;사용자 정의 상태 검사\u0026rsquo;에 응답하지 않는 컨테이너를 죽이고, 서비스 준비가 끝날 때까지 그러한 과정을 클라이언트에 보여주지 않는다.\n시크릿과 구성 관리 쿠버네티스를 사용하면 암호, OAuth 토큰 및 SSH 키와 같은 중요한 정보를 저장하고 관리 할 수 있다. 컨테이너 이미지를 재구성하지 않고 스택 구성에 시크릿을 노출하지 않고도 시크릿 및 애플리케이션 구성을 배포 및 업데이트 할 수 있다.\n","date":"2023-04-21T13:19:47+09:00","image":"https://codemario318.github.io/post/kubernetes_1/kubernetes_cover_hu15504362c54454204bbf321d3bfe3873_12586_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/kubernetes_1/","title":"쿠버네티스: 1. 쿠버네티스란 무엇인가?"},{"content":"JUnit 프레임워크 ComparisonCompactor.java 모듈 개선하기 생성자 두 번째 인수와 세 번째 인수를 비교한다. comapct 함수에 문자열을 넣으면 결과 메시지 앞에 문자열이 추가된다. 비교 후 다른 문자열을 “[]”를 써서 강조한다. 첫 번째 인수는 다른 문자열을 출력할 때 “[]” 앞 뒤로 출력할 문자열 개수이다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 package junit.framework; public class ComparisonCompactor { private static final String ELLIPSIS = \u0026#34;...\u0026#34;; private static final String DELTA_END = \u0026#34;]\u0026#34;; private static final String DELTA_START = \u0026#34;[\u0026#34;; private int fContextLength; private String fExpected; private String fActual; private int fPrefix; private int fSuffix; public ComparisonCompactor(int contextLength, String expected, String actual) { fContextLength = contextLength; fExpected = expected; fActual = actual; } public String compact(String message) { if (fExpected == null || fActual == null || areStringsEqual()) return Assert.format(message, fExpected, fActual); findCommonPrefix(); findCommonSuffix(); String expected = compactString(fExpected); String actual = compactString(fActual); return Assert.format(message, expected, actual); } private String compactString(String source) { String result = DELTA_START + source.substring(fPrefix, source.length() - fSuffix + 1) + DELTA_END; if (fPrefix \u0026gt; 0) result = computeCommonPrefix() + result; if (fSuffix \u0026gt; 0) result = result + computeCommonSuffix(); return result; } private void findCommonPrefix() { fPrefix = 0; int end = Math.min(fExpected.length(), fActual.length()); for (; fPrefix \u0026lt; end; fPrefix++) { if (fExpected.charAt(fPrefix) != fActual.charAt(fPrefix)) break; } } private void findCommonSuffix() { int expectedSuffix = fExpected.length() - 1; int actualSuffix = fActual.length() - 1; for (; actualSuffix \u0026gt;= fPrefix \u0026amp;\u0026amp; expectedSuffix \u0026gt;= fPrefix; actualSuffix--, expectedSuffix--) { if (fExpected.charAt(expectedSuffix) != fActual.charAt(actualSuffix)) break; } fSuffix = fExpected.length() - expectedSuffix; } private String computeCommonPrefix() { return (fPrefix \u0026gt; fContextLength ? ELLIPSIS : \u0026#34;\u0026#34;) + fExpected.substring(Math.max(0, fPrefix - fContextLength), fPrefix); } private String computeCommonSuffix() { int end = Math.min(fExpected.length() - fSuffix + 1 + fContextLength, fExpected.length()); return fExpected.substring(fExpected.length() - fSuffix + 1, end) + (fExpected.length() - fSuffix + 1 \u0026lt; fExpected.length() - fContextLength ? ELLIPSIS : \u0026#34;\u0026#34;); } private boolean areStringsEqual() { return fExpected.equals(fActual); } } 멤버 변수 앞에 붙인 접두어 오늘날 사용하는 개발 환경에서는 변수 이름에 범위를 명시할 필요가 없다.\n접두어 f는 중복되는 정보다.\n1 2 3 4 5 private int contextLength; private String expected; private String actual; private int prefix; private int suffix; 캡슐화되지 않은 조건문 1 2 3 4 5 6 7 8 9 10 11 public String compact(String message) { if (expected == null || actual == null || areStringsEqual()) { return Assert.format(message, expected, actual); } findCommonPrefix(); findCommonSuffix(); String expected = compactString(this.expected); String actual = compactString(this.actual); return Assert.format(message, expected, actual); } 의도를 명확히 표현하려면 조건문을 캡슐화 하는것이 좋다. 조건문을 메서드로 뽑아내 적절한 이름을 붙인다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public String compact(String message) { if (**shouldNotCompact**()) { return Assert.format(message, expected, actual); } findCommonPrefix(); findCommonSuffix(); String expected = compactString(this.expected); String actual = compactString(this.actual); return Assert.format(message, expected, actual); } private boolean **shouldNotCompact**() { return expected == null || actual == null || areStringsEqual(); } 중복되는 변수 이름 compact 함수에서 사용하는 this.expected와 this.acutal같은 경우 기존 함수에 expected, actual 이라는 지역 변수가 있었는데, 클래스 변수 fExpected, fAcutal에서 f를 제거하여 같은 이름을 사용하게 되었다.\n함수에서 멤버 변수와 이름이 같은 변수를 사용하는 이유를 파악하고 더 서술적(구체적)인 이름으로 바꾼다.\n1 2 String compactExpected = compactString(expected); String compactActual = compactString(actual); 조건문의 부정문 사용 부정문은 긍정문보다 이해하기 약간 더 어렵다. 첫 문장 if를 긍정으로 만들어 조건문을 반전한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public String compact(String message) { if (**canBeCompacted**()) { return Assert.format(message, expected, actual); } findCommonPrefix(); findCommonSuffix(); String expected = compactString(this.expected); String actual = compactString(this.actual); return Assert.format(message, expected, actual); } private boolean **canBeCompacted**() { return expected **!=** null || actual **!=** null || **!**areStringsEqual(); } 동작, 의미를 잘 표현할 수 있는 이름을 사용 compact 함수는 문자열을 앞축하라는 의미가 강하지만 실제로 canBeCompacted가 false면 압축하지 않는다. compact라는 이름을 붙이면 오류 점검이라는 부가 단계(기능)이 숨겨진다. 함수는 단순히 압축된 문자열이 아니라 형식이 갖춰진 문자열을 반환한다. 실제로는 formatCompatedComparison이라는 이름이 적합하다.\n새 이름에 인수를 고려하면 가독성이 훨신 좋아진다.\n1 2 3 public String formatCompactedComparison(String message) { ... } 함수가 한 가지 책임만 가지게 if 문 안에서는 예상 문자열과 실제 문자열을 진짜로 압축한다. 이 부분을 빼내 compactExpectedAndActual이라는 메서드로 만든다.\n형식을 맞추는 작업은 formatCompactedComparison에게 전적으로 맡긴다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ... private String compactExpected; private String compactActual; ... public String formatCompactedComparison(String message) { if (canBeCompacted()) { **compactExpectedAndActual**(); return Assert.format(message, compactExpected, compactActual); } else { return Assert.format(message, expected, actual); } } private **compactExpectedAndActual**() { findCommonPrefix(); findCommonSuffix(); compactExpected = compactString(expected); compactActual = compactString(actual); } compactExpected, compactActual를 멤버 변수로 승격했다.\n사용 방식 일관성 새 함수compactExpectedAndActual에서 마지막 두 줄은 변수를 반환하지만 첫째 줄과 둘째줄은 반환값이 없다. 멤버변수 함수 사용방식이 일관적이지 못하다.\nfindCommonPrefix, findCommonSuffix는 함수 내부에서 멤버변수 prefix, suffix에 값을 할당하지만, compactString은 반환값을 만들게 되고, 멤버 변수에 반환값을 할당한다. findCommonPrefix, findCommonSuffix 를 변경하여 접두어 값과 접미어 값의 위치를 반환하게 바꾼다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 private void compactExpectedAndActual() { prefixIndex = findCommonPrefix(); suffixIndex = findCommonSuffix(); compactExpected = compactString(expected); compactActual = compactString(actual); } private int findCommonPrefix() { int prefixIndex = 0; int end = Math.min(expected.length(), actual.length()); for (; prefixIndex \u0026lt; end; prefixIndex++) { if (expected.charAt(prefixIndex) != actual.charAt(prefixIndex)) break; } return prefixIndex; } private int findCommonSuffix() { int expectedSuffix = expected.length() - 1; int actualSuffix = actual.length() - 1; for (; actualSuffix \u0026gt;= prefixIndex \u0026amp;\u0026amp; expectedSuffix \u0026gt;= prefixIndex; actualSuffix--, expectedSuffix--) { if (expected.charAt(expectedSuffix) != actual.charAt(actualSuffix)) break; } return expected.length() - expectedSuffix; } 숨겨진 시간적인 결합을 노출 findCommonSuffix 를 주의 깊게 살펴보면 숨겨진 시간적인 결함이 존재한다. findCommonSuffix는 findCommonPrefix가 prefixIndex를 계산한다는 사실에 의존하게 된다.\n만약 findCommonPrefix와 findCommonSuffix를 잘못된 순서로 호출하면 문제를 찾기 어렵다.\n따라서 시간 결함을 외부에 노출하고자 findCommonSuffix를 고쳐 prefixIndex를 인수로 넘겼다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 private void compactExpectedAndActual() { prefixIndex = findCommonPrefix(); suffixIndex = findCommonSuffix(prefixIndex); compactExpected = compactString(expected); compactActual = compactString(actual); } private int findCommonSuffix(int prefixIndex) { int expectedSuffix = expected.length() - 1; int actualSuffix = actual.length() - 1; for (; actualSuffix \u0026gt;= prefixIndex \u0026amp;\u0026amp; expectedSuffix \u0026gt;= prefixIndex; actualSuffix--, expectedSuffix--) { if (expected.charAt(expectedSuffix) != actual.charAt(actualSuffix)) break; } return expected.length() - expectedSuffix; } prefixIndex를 인수로 전달하는 방식은 함수 호출 순서를 확실히 명시할 수 있지만, prefixIndex가 필요한 이유를 명확히 설명하지 못한다.\n필요한 이유가 분명히 드러나지 않으므로 다른 개발자가 원래대로 되돌려놓을지도 모른다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 private void compactExpectedAndActual() { **findCommonPrefixAndSuffix**(); compactExpected = compactString(expected); compactActual = compactString(actual); } private void **findCommonPrefixAndSuffix**() { **findCommonPrefix**(); int expectedSuffix = expected.length() - 1; int actualSuffix = actual.length() - 1; for (; actualSuffix \u0026gt;= prefixIndex \u0026amp;\u0026amp; expectedSuffix \u0026gt;= prefixIndex; actualSuffix--, expectedSuffix--) { if (expected.charAt(expectedSuffix) != actual.charAt(actualSuffix)) break; } suffixIndex = expected.length() - expectedSuffix; } private **void** findCommonPrefix() { prefixIndex = 0; int end = Math.min(expected.length(), actual.length()); for (; prefixIndex \u0026lt; end; prefixIndex++) if (expected.charAt(prefixIndex) != actual.charAt(prefixIndex)) break; } findCommonPrefix와 findCommonSuffix를 되돌리고, findCommonSuffix라는 이름을 findCommonPrefixAndSuffix로 바꾼 후 findCommonPrefixAndSuffix에서 가장 먼저 findComonPrefix를 호출한다.\n두 함수를 호출하는 순서가 앞서 고친 코드보다 훨씬 더 분명해진다.\n그리고 PrefixAndSuffix함수를 정리한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 private void findCommonPrefixAndSuffix() { findCommonPrefix(); int suffixLength = 1; for (; !suffixOverlapsPrefix(suffixLength); suffixLength++) { if (charFromEnd(expected, suffixLength) != charFromEnd(actual, suffixLength)) break; } suffixIndex = suffixLength; } private char charFromEnd(String s, int i) { return s.charAt(s.length() - i); } private boolean suffixOverlapsPrefix(int suffixLength) { return actual.length() - suffixLength \u0026lt; prefixLength || expected.length() - suffixLength \u0026lt; prefixLength; } 적절한 이름으로 바꾸기 코드를 정리하니 suffixIndex가 index가 아닌 실제로는 접미어 길이(length)를 의미한다. 이름이 적절치 않다. prefixIndex도 마찬가지이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 public class ComparisonCompactor { ... private int **suffixLength**; ... private void findCommonPrefixAndSuffix() { findCommonPrefix(); **suffixLength** = 0; for (; !suffixOverlapsPrefix(suffixLength); suffixLength++) { if (charFromEnd(expected, suffixLength) != charFromEnd(actual, suffixLength)) break; } } private char charFromEnd(String s, int i) { return s.charAt(s.length() - i - 1); } private boolean suffixOverlapsPrefix(int suffixLength) { return actual.length() - suffixLength \u0026lt;= prefixLength || expected.length() - suffixLength \u0026lt;= prefixLength; } ... private String compactString(String source) { String result = DELTA_START + source.substring(prefixLength, source.length() - **suffixLength**) + DELTA_END; if (prefixLength \u0026gt; 0) result = computeCommonPrefix() + result; if (**suffixLength** \u0026gt; 0) result = result + computeCommonSuffix(); return result; } ... private String computeCommonSuffix() { int end = Math.min(expected.length() - **suffixLength** + contextLength, expected.length()); return expected.substring(expected.length() - **suffixLength**, end) + (expected.length() - **suffixLength** \u0026lt; expected.length() - contextLength ? ELLIPSIS : \u0026#34;\u0026#34;); } } computeCommonSuffix에서 +1을 없애고 charFromEnd에 -1을 추가하고 suffixOverlapsPrefix에 ≤ 를 사용했다. 그 후 suffixLength로 바꿔 가독성을 크게 높힐 수 있었다.\n불필요한 코드 제거 +1를 제거하던 중 compactString에서 다음 행을 발견했다.\n1 if (suffixlength \u0026gt; 0) suffixLength가 1씩 감소했으므로 당연히 연산자를 ≥로 고쳐야 하지만 지금 상황에서는 ≥는 나올 수 없다.\n코드를 좀 더 분석해보면 if 문은 길이가 0인 접미어를 걸러내어 suffixIndex가 언제나 1 이상이므로 if 문 자체가 필요 없다.\ncompactString에 있는 if문이 모두 필요 없어 보여 제거후 테스트를 하니 모두 통과했다. 불필요한 if문을 제거하여 더 깔끔하게 만든다.\n1 2 3 4 5 6 7 private String compactString(String source) { return computeCommonPrefix() + DELTA_START + source.substring(prefixLength, source.length() - suffixLength) + DELTA_END + computeCommonSuffix(); } 최종 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 package junit.framework; public class ComparisonCompactor { private static final String ELLIPSIS = \u0026#34;...\u0026#34;; private static final String DELTA_END = \u0026#34;]\u0026#34;; private static final String DELTA_START = \u0026#34;[\u0026#34;; private int contextLength; private String expected; private String actual; private int prefixLength; private int suffixLength; public ComparisonCompactor( int contextLength, String expected, String actual) { this.contextLength = contextLength; this.expected = expected; this.actual = actual; } public String formatCompactedComparison(String message) { String compactExpected = expected; String compactActual = actual; if (shouldBeCompacted()) { findCommonPrefixAndSuffix(); compactExpected = compact(expected); compactActual = compact(actual); } return Assert.format(message, compactExpected, compactActual); } private boolean shouldBeCompacted() { return !shouldNotBeCompacted(); } private boolean shouldNotBeCompacted() { return expected == null || actual == null || expected.equals(actual); } private void findCommonPrefixAndSuffix() { findCommonPrefix(); suffixLength = 0; for (; !suffixOverlapsPrefix(); suffixLength++) { if (charFromEnd(expected, suffixLength) != charFromEnd(actual, suffixLength)) break; } } private char charFromEnd(String s, int i) { return s.charAt(s.length() - i - 1); } private boolean suffixOverlapsPrefix() { return actual.length() - suffixLength \u0026lt;= prefixLength || expected.length() - suffixLength \u0026lt;= prefixLength; } private void findCommonPrefix() { prefixLength = 0; int end = Math.min(expected.length(), actual.length()); for (; prefixLength \u0026lt; end; prefixLength++) if (expected.charAt(prefixLength) != actual.charAt(prefixLength)) break; } private String compact(String s) { return new StringBuilder() .append(startingEllipsis()) .append(startingContext()) .append(DELTA_START) .append(delta(s)) .append(DELTA_END) .append(endingContext()) .append(endingEllipsis()) .toString(); } private String startingEllipsis() { return prefixLength \u0026gt; contextLength ? ELLIPSIS : \u0026#34;\u0026#34;; } private String startingContext() { int contextStart = Math.max(0, prefixLength - contextLength); int contextEnd = prefixLength; return expected.substring(contextStart, contextEnd); } private String delta(String s) { int deltaStart = prefixLength; int deltaEnd = s.length() - suffixLength; return s.substring(deltaStart, deltaEnd); } private String endingContext() { int contextStart = expected.length() - suffixLength; int contextEnd = Math.min(contextStart + contextLength, expected.length()); return expected.substring(contextStart, contextEnd); } private String endingEllipsis() { return (suffixLength \u0026gt; contextLength ? ELLIPSIS : \u0026#34;\u0026#34;); } } 코드를 살펴보면 초반에 내렸던 결정 일부를 번복했다.\n처음 추출했던 메서드 몇 개를 formatCompactedComparison에다 도로 집어넣었다. shouldNotBeCompacted 조건을 되돌렸다. 코드를 리펙터링 하다보면 원래 했던 변경을 되돌리는 경우가 흔하다.\n리펙터링은 코드가 어느 수준에 이를때까지 수많은 시행착오를 반복하는 작업이다.\n","date":"2023-04-21T11:11:13+09:00","image":"https://codemario318.github.io/post/clean_code_15/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean_code_15/","title":"클린코드: 15. JUnit 들여다보기"},{"content":" Heruistics(발견법): 경험, 학습 등으로 습득한 직감을 이용해 현실적으로 만족할 만한 수준의 해답을 찾는 것\n리펙토링(마틴 파울러)에서 언급된 내용과 저자의 경험을 합쳐 깨끗하지 못한 코드를 발견하는 방법에 대하여 설명했다.\n아래에 해당된다면 깨끗한 코드인지 고민해서 확인해볼 필요가 있다.\n주석 C1: 부적절한 정보 주석은 코드와 설계에 기술적인 설명을 부연하는 수단이다.\n다른 시스템에 저장할 정보는 주석으로 적절하지 못하다.\n소스 코드 관리 시스템 버그 추적 시스템 이슈 추적 시스템 기타 기록 관리 프로그램 등 예를 들어, 변경 이력은 과도한 정보로 코드를 번잡하게 만든다.\nC2: 쓸모없는 주석 쓸모 없는 주석은 일단 들어가고 나면 업데이트를 주저하기 때문에 변해가는 코드에서 쉽게 멀어지고 코드를 그릇된 방향으로 이끌 여지가 있다.\n오래된 주석, 엉뚱한 주석, 잘못된 주석은 더 이상 쓸모가 없다.\n주석은 빨리 낡는다. 따라서 쓸모 없어질 주석은 달지 않는 편이 좋다. 또한 쓸모 없어진 주석은 재빨리 삭제하는 편이 가장 좋다.\nC3: 중복된 주석 주석은 코드만으로 다하지 못하는 설명을 부연하는 역할을 해야한다.\n서술적인 이름 등 깨끗한 코드로 설명할 수 있는 내용으로 주석을 만들어 중복되는 설명을 만드는 주석은 사용하지 않는다.\nC4: 성의없는 주석 꼭 필요한 주석은 잘 작성해야할 의무가 있다.\n단어를 신중하게 선택한다. 문법과 구두점을 올바로 사용한다. 주절대지 않는다. 당연한 소리를 반복하지 않는다. 간결하고 명료하게 작성한다. C5: 주석 처리된 코드 주석 처리된 코드는 만들어지면 코드의 중요도나 의미와 관계없이, (얼마나 오래된 코드인지, 중요한 코드인지) 누군가가 사용할수도 있을 것이라는 생각 때문에삭제하기 꺼려진다.\n결국 이렇게 남겨진 코드는 읽는 사람을 혼동하게 하는 등 코드를 오염시킨다.\n주석으로 처리된 코드를 발견하면 즉각 지워라! 누군가 필요로 한다면 버전관리 시스템으로 확인할 수 있다.\n주석으로 처리된 코드는 대부분 당장 필요한 경우가 없어서 이전에 필요한 사람도 오랜 기간 후에 소수의 사람일 가능성이 높다. 따라서 주석으로 남겨 코드를 오염시켜 사람들을 혼동시키는 것 보다. 버전관리 시스템을 통해 필요한 사람들이 찾게 남기는 것이 더 올바른 선택이다.\n환경 E1: 여러 단계로 빌드해야 한다. 빌드는 간단히 한 단계로 끝나야 한다. 기타 시스템에 필요한 파일을 찾느라 여기저기 뒤적을 필요가 없어야 한다. 한 명령으로 전체를 체크아웃해서 한 명령으로 빌드할 수 있어야 한다.\nE2: 여러 단계로 테스트해야 한다. 테스트 자체가 귀찮은 행위이다. 따라서 간단하고 빠르고 결과가 명료해야 자주 자주 활용될 수 있고, 이를 통해 코드의 품질, 테스트를 유지할 수 있다.\n모든 단위테스트는 한 명령으로 돌려야 한다. IDE에서 버튼 하나로 모든 테스트를 돌린다면 가장 이상적이다. 아무리 열악한 환경이라도 셸에서 명령 하나로 가능해야 한다.\n함수 F1: 너무 많은 인수 함수에서 인수 개수는 적을수록 고민할 사항들이 적어진다. 따라서 가능하다면 인수가 없는게 좋고 적에 만드는 것이 좋다.\n인수가 4개 이상일 경우 인수가 정말로 필요한지 고민해봐야 한다.\nF2: 출력 인수 출력 인수: 함수 입력으로 사용된 인수가 출력이 되는 경우\n일반적으로 독자는 출력이 아닌 입력으로 생각한다. 함수에서 어떠한 상태를 변경해야 한다면, 변경 대상을 인수로 넣어 사용하는 것이 아니라, 객체의 상태를 표현하게 만들고 함수 내부에서 this 같은 키워드를 통해 객체의 상태를 변경해야 한다.\nF3: 플래그 인수 Boolean 인수는 함수가 여러 기능(참일때 이거 아닐때 이거)을 수행한다는 명백한 증거다. 플래그 인수는 혼란을 초래하므로 피해야한다.\nF4: 죽은 함수 아무도 호출하지 않는 함수는 삭제한다.\n일반 G1: 한 소스 파일에 여러 언어를 사용한다. 오늘날 프로그래밍 환경은 한 소스 파일 내에서 다양한 언어를 지원한다.\n이상적으로 소스 파일 하나에 언어 하나만 사용하는 방식이 가장 좋으나, 현실적으로 여러 언어가 불가피하다.\n하지만 소스 파일에서 언어 수와 범위를 최대한 줄이도록 애써야 한다.\nG2: 당연한 동작을 구현하지 않는다. 최소 놀람 원칙(The Principle of Least Surprise): \u0026ldquo;필요한 기능에 크나큰 깜짝 놀래킬만한 요소가 있다면 해당 기능을 다시 설계할 필요가 있을 수 있다”\n함수나 클래스는 다른 프로그래머가 당연하게 여길만한 동작과 기능을 제공해야 한다.\nG3: 경계를 올바르게 처리하지 않는다. 코드를 올바르게 처리하는것은 당연하지만 복잡하다는 것을 관과하고 직관에 의존하여 만든다. 이러한 과정에서 모든 경계와 구석진 곳에서 증명하지 않는다.\n부지런함을 대신할 지름길은 없다. 모든 경계조건, 모든 구석진 곳, 모든 예외는 우아하고 직관적인 알고리즘을 좌초시킬 암초다.\n스스로의 직관에 의존하지 않고, 모든 경계조건을 찾아내 테스트 하는 테스트 케이스를 만들어야 한다.\nG4: 안전 절차 무시. 실패하는 테스트 케이스를 제껴두고 나중으로 미루는 태도는 신용카드가 공짜 돈이라는 생각만큼 위험하다.\nG5: 중복 DRY(Don’t Reapeat Yourself): 익스트림 프로그래밍의 핵심 규칙중 하나로 선언한 후 “한 번, 단 한번만(Once, and only once)” 론 제프리스는 이 규칙을 “모든 테스트를 통과한다” 규칙 다음으로 중요하게 꼽았다.\n코드에서 중복을 발견할 때마다 추상화할 기회로 간주하라. 중복된 코드르 하위 루틴이나 다른 클래스로 분리하는 등 추상화로 중복을 정리하면 설계 언어의 어휘가 늘어난다.\nG6: 추상화 수준이 올바르지 못하다. 기초 클래스는 구현 정보를 몰라야 한다. (구현에 관계없이 의도한 결과만 받아오고, 이름을 통해 결과물을 잘 표현해야 한다.)\n추상화는 저차원 상세 개념에서 고차원 일반 개념을 분리한다.\n추상화로 개념을 분리할 때는 모든 저차원 개념을 파생 클래스에 넣고, 모든 고차원 개념은 기초 클래스에 넣는다.\n세부 구현과 관련한 상수, 변수, 유틸리티 함수는 기초 클래스에 넣으면 안된다. 1 2 3 4 5 6 7 public interface Stack { Object pop() throw EmptyException; void push(Object o) throws FullException; double percentFull(); class EmptyEception extends Exception {} class FullException extends Exception {} } percentFulll 함수는 추상화 수준이 올바르지 못함 꽉 찬 정도를 의미하는 결과물을 반환하는데, 경우에 따라서 (ex. 크기가 무한대) 꽉 찬 정도를 알아낼 수 없다. 그러므로 BoundedStack 같은 파생 인터페이스에 넣어야 마땅하다. stack의 크기가 논리적으로 무한한 경우 0을 반환하면 된다고 하지만 물리적으로 무한할 수 없다. 1 stack.percentFull() \u0026lt; 50.0; 위 코드의 경우 스택 크기를 확인할때 OutOfMemoryException 예외가 발생할 여지가 있다. 또한 0을 반환하면 거짓된 정보를 반환하게 된다. 일반적으로 stack을 구현할 때 크기를 저장할 변수를 만듦. push 할때 크기값을 증가시키므로 한계를 넘어서는 갯수를 저장할 때 예외가 발생시킬수 있음 percentFull을 구현한다고 하더라도, 무한한 경우를 검사해서 예외를 발생시킬 수 있음 G7: 기초 클래스가 파생 클래스에 의존한다. 개념을 기초 클래스와 파생 클래스로 나누는 가장 흔한 이유는 고차원 기초 클래스 개념을 저차원 파생 클래스 개념으로부터 분리해 독립성을 보장하기 위해서다.\n따라서 기초 클래스는 파생 클래스를 사용한다면 뭔가 문제가 있다는 말이다.\n일반적으로는 기초 클래스와 파생 클래스를 다른 파일로 배포하는 편이 좋다.\n독립적인 개별 컴포넌트 단위로 시스템을 배치할 수 있다. 컴포넌트를 변경한다면 해당 컴포넌트만 다시 배치하면 된다. 즉, 변경이 시스템에 미치는 영향이 아주 작아지므로 현장에서 시스템을 유지보수하기가 한결 수월하게 된다.\nG8: 과도한 정보 클래스나 모듈 인터페이스에 노출할 함수를 제한하여 필요한 기능만 활용하도록 한다.\n자료와 함수를 최대한 숨기고 필요한 기능만 깐깐하게 공개하여 결합도를 높힐 여지를 만들지 마라\n잘 정의된 모듈은 인터페이스가 아주 작다. 작은 인터페이스로도 많은 동작을 구현할 수 있다.\n반면 부식하게 정의된 모듈은 인터페이스가 구질구질하다. 그래서 간단한 동작 하나에도 온갖 인터페이스가 필요하다.\n잘 정의된 인터페이스는 많은 함수를 제공하지 않는다. 함수를 적게 제공하기 때문에 (결합도를 높힐 여지가 적으므로) 결합도가 자연스럽게 낮아진다.\n클래스가 제공하는 메서드 수는 작을수록 좋다. 함수가 아는 변수 수도 작을수록 좋다. 클래스에 들어있는 인스턴스 변수 수도 작을수록 좋다. G9: 죽은 코드 죽은 코드란 실행되지 않는 코드를 가르킨다. 죽은 코드를 발견하면 장례식을 치뤄줘라!\n불가능한 조건을 확인하는 if 문 thorw 문이 없는 try 문에서 catch 블록 아무도 호출하지 않는 유틸리티 함수 switch-case 문에서 불가능한 case 조건 등 죽은 코드는 설계가 변해도 제대로 수정되지 않기 때문에 새로운 규칙이나 표기법을 따르지 않아 일관성을 해치거나 레거시로 남게되고, 다른 코드들도 같이 오염시킬 여지를 준다.\nG10: 수직 분리 변수와 함수는 사용되는 위치에 가깝게 정의한다.\n지역 변수는 처음으로 사용하기 직전에 선언하며 수직으로 가까운 곳에 위치해야 한다. 비공개 함수는 처음으로 호출함 직후에 정의한다. G11: 일관성 부족 간단한 일관성만으로도 코드를 읽고 수정하기 쉬워진다.\n어떤 개념을 특정 방식으로 구현했다면 유사한 개념도 같은 방식으로 구현한다.\n최소 놀람 원칙에도 부합한다. 표기법은 신중하게 선택하고, 선택된 표기법을 준수해야 한다.\nG12: 잡동사니 비어 있는 기본 생성자 아무도 사용하지 않는 변수 아무도 호출하지 않는 함수 정보를 제공하지 못하는 주석 코드만 복잡하게 만들 뿐이므로 제거한다.\nG13: 인위적인 결합 함수, 상수, 변수를 선언할 때는 시간을 들여 올바른 위치를 고민한다.\n서로 무관한 개념을 인위적으로 결합하지 않는다.\nEnum 일반적인 enum은 특정 클래스에 속할 이유가 없다. enum이 클래스에 속하면 사용하는 코드가 특정 클래스를 알아야 한다. 범용 static 변수 일반적으로 인위적인 결합은 직접적인 상호작용이 없는 두 모듈 사이에서 일어난다. 뚜렷한 목적이 없이 변수, 상수, 함수를 당장 편한 위치에(잘못된) 넣어버려 발생한다.\nG14: 기능 욕심 기능 욕심은 한 클래스의 내부를 다른 클래스에 노출하게 되므로, 별다른 문제가 없다면, 제거하는 것이 좋다.\n클래스 메서드는 자기 클래스의 변수와 함수에 관심을 가져야지 다른 클래스의 변수와 함수에 관심을 가져서는 안된다.\n메서드가 다른 객체의 참조자와 변경자를 사용해 그 객체 내용을 조작한다면 메서드가 그 객체 클래스의 범위를 욕심내는 탓이다.\n1 2 3 4 5 6 7 8 9 10 11 public class HourlyPayCalculator { public Money calculateWeeklyPay(HourlyEmployee e) { int tenthRate = e.getTenthRate().getPennies(); int tenthsWorked = e.getTenthsWorked(); int straightTime = Math.min(400, tenthsWorked); int overTime = Math.max(0, tenthsWorked - straightTime); int straightPay = straightTime * tenthRate; int overtimePay = (int)Math.round(overTime * tenthRate * 1.5); return new Money(straightPay + overtimePay); } } calculateWeeklyPay 메서드가 HourlyEmployee 객체에서 많은 정보를 가져와 처리한다. calculateWeeklyPay 메서드는 HourlyEmployee 클래스의 범위를 욕심낸다고 볼 수 있다. 자신이 HourlyEmployee 클래스에 속하는 것 처럼 구현되어 있다.\nHourlyEmployee 클래스에서 적어도 tenthRate, straightTime, overTime을 반환하는 것이 적합해보임\nG15: 선택자 인수 선택자 인수는 목적을 기억하기 어려울 뿐 아니라 각 선택자 인수가 여러 함수를 하나로 조합한다. 선택자 인수는 큰 함수를 작은 함수 여럿으로 쪼개지 않으려는 게으름의 소산이다.\nflag 변수들을 의미한다.\n부울 인수, enum, int 등 함수 동작을 제어하려는(분기하려는) 인수는 하나같이 바람직 하지 않다. 일반적으로 인수를 넘겨 동작을 선택하는 대신 새로운 함수를 만드는 편이 좋다.\n💡 유사한 개념을 통합해 코드를 줄이기 위해 사용되는 경우를 종종 봐왔는데, 이런 경우 유사한 개념을 분리하고 각각 별도 구현하여 분리한 기능을 호출하여 사용되는 것이 올바른 선택인 것 같다.\nG16: 모호한 의도 코드를 짤 때는 의도를 최대한 분명히 밝혀야 한다.\n행을 바꾸지 않고 표현한 수식 헝가리식 표기법(변수 및 함수 인자 이름 앞에 데이터 타입을 명시하는 코딩 규칙) 매직 넘버 등 독자에게 의도를 분명히 표현하도록 시간을 투자해야한다.\nG17: 잘못 지운(전가한) 책임 (MisPlaced responsibility) 최소 놀람 원칙을 적용하여 기능을 적절한 위치에 배치해야 한다.\n독자에게 직관적인 위치가 아니라 개발자에게 편한 함수에 배치하는 것을 비꼬고있다.\n다른 위치에 배치될 경우 함수 이름을 잘 지어 확실하게 표현해야한다.\nG18: 부적절한 static 함수 반드시 static 함수로 정의해야겠다면 오버라이딩할 가능성이 없는지 꼼꼼히 살핀다.\nstatic 함수는 오버라이딩 할 가능성이 없어야 한다.\n1 HourlyPayCalculator.calculatePay(employee, overtimeRate); 위 메서드는 특정 객체와 관련이 없으면서 모든 정보를 인수에서 가져오기 때문에 static 함수로 정의해도 괜찮아 보이지만, 수당을 계산하는 기능이 여러개로 분리될 가능성이 있어(오버라이딩 될 수 있어) 적합하지 않다. 따라서 Employee 클래스에 속하는 인스턴스 함수여야 한다.\n일반적으로 static 함수보다 인스턴스 함수가 좋다. 조금이라도 의심스럽다면 인스턴스 함수로 정의한다.\nG19: 서술적 변수 계산을 몇 단계로 나누고 중간값에 좋은 변수 이름만 붙여도 읽기 쉬운 코드로 바뀐다.\n프로그램 가독성을 높이는 가장 효과적인 방법 중 하나가 계산을 여러 단계로 나누고 중간 값으로 서술적인 변수 이름을 사용하는 방법이다.\n[Book] 켄트벡의 구현 패턴 - gpeegpee/learn-java Wiki\n1 2 3 4 5 6 7 Matcher match = headerPattern.matcher(line); if (match.find()) { String key = match.group(1); String value = match.group(2); headers.put(key.toLowerCase(), value); } 서술적인 변수 이름을 사용하여 match를 이용하여 찾은 첫번째 그룹이 키(key)이고, 두번째로 일치하는 그룹이 값(value)이라는 사실이 명확히 들어난다.\n서술적인 변수 이름은 많이 써도 괜찮다.\nG20: 이름과 기능이 일치하는 함수 1 Date newDate = date.add(5); 위 함수는 5일을 더하는 함수인가? 5주 혹은 5시간? date 인스턴스를 변경하는 함수인가? 아니면 예전 date 인스턴스는 두고 새로운 Date를 반환하는 함수인지 알 수 없다.\ndate 인스턴스에 5일을 더해 date 인스턴스를 변경하는 함수라면 addDaysTo 혹은 increaseByDays date 인스턴스를 변경하지 않으면서 5일 뒤인 새 날짜를 반환 daysLater, daysSince 이름만으로 분명하지 않기에 구현을 살피거나 문서를 뒤적여야 한다면 더 좋은 이름으로 바꾸거나 아니면 더 좋은 이름을 붙이기 쉽도록 기능을 정리해야 한다.\nG21: 알고리즘을 이해하라 코드가 돌아간다는 사실을 아는 것과 돌아가기 위한 알고리즘이 올바르다는 사실을 아는 것은 다르다.\n대다수 괴상한 코드는 사람들이 알고리즘을 충분히 이해하지 않은 채 코드를 구현한 탓이다.\n구현이 끝났다고 선언하기 전에 함수가 돌아가는 방식을 확실히 이해하는지 확인하라. 테스트 케이스를 모두 통과한다는 사실만으로 부족하다.\n알고리즘이 올바르다는 사실을 확인하고 이해하려면 기능이 뻔히 보일 정도로 함수를 깔끔하고 명확하게 재구성하는 방법이 최고다.\nG22: 논리적 의존성은 물리적으로 드러내라 한 모듈이 다른 모듈에 의존한다면 의존하는 모든 정보를 명시적으로 요청하여 물리적인 의존성이 드러낸다.\n의존성의 드러나있지 않다면 코드를 한눈에 파악할 수 없어 잘못 구현하거나, 아니더라도 코드를 분석을 해야 하기때문에 유지보수에 어려움이 생긴다.\nG23: if-else 혹은 switch/case 문보다 다형성을 사용하라 대다수 개발자가 switch문을 사용하는 이유는 그 상황에서 가장 올바른 선택이기보다는 당장 손쉬운 선택이기 때문이다. 유형보다 함수가 더 쉽게 변하는 경우는 긱히 드물다. 저자는 ‘switch 문 하나’ 규칙을 따른다. 선택 유형 하나에는 switch문을 한번만 사용한다. 같은 선택을 수행하는 다른 코드에서는 다형성 객체를 생성해 switch 문을 대신한다.\nG24: 표준 표기법을 따르라 팀은 업계 표준에 기반한 구현 표준을 따라야 한다.\n구현 표준\n표준을 설명하는 문서는 코드 자체로 충분해야 하며 업계 표준을 따라르기 때문에 별도 문서를 만들 필요는 없어야 한다.\n인스턴스 변수 이름을 선언하는 위치 클래스, 메서드, 변수 이름을 정하는 방법 괄호 넣는 위치 G25: 매직 숫자는 명명된 상수로 교체하라 일반적으로 코드에서 숫자를 사용하지 않는것이 좋다. 사용되는 숫자는 명명된 상수 뒤로 숨겨라\n매직 숫자라는 용어는 단순히 숫자만 의미하지 않는다. 의미가 분명하지 않은 토큰을 모두 가르킨다. G26: 정확하라 코드에서 뭔가를 결정할 때는 정확히 결정한다(들어맞게). 결정을 내리는 이유와 예외를 처리할 방법을 분명히 알아야 한다.\n호출하는 함수가 null을 반환할 가능성이 있다면 null을 항상 점검한다. 조회 결과가 하나뿐이라 짐작한다면 하나인지 확실히 확인한다.\n갱신 가능성이 희박하다고 잠금과 트랜잭션 관리를 건너뛰는 행동은 아무리 잘 봐줘도 게으름이다.\nList로 선언할 변수를 ArrayList로 선언하는 행동은 지나친 제약이다.\n코드에서 모호성과 부정확은 의견차나 게으름의 결과다.\nG27: 관례보다 구조를 사용하라 설계 결정을 강제할 때는 규칙보다 관례를 사용한다. 더 나아가 구조 자체로 강제하면 더 좋다.\nenum을 활용한 switch-case 보다 추상 메서드가 있는 기초 클래스가 더 좋다. switch-case문을 매번 똑같이 구현하게 강제하기는 어렵지만, 파생 클래스는 추상 메서드를 모두 구현하지 않으면 안되기 때문이다.\nG28: 조건을 캡슐화하라 부울 논리는 이해하기 어렵다. 조건의 의도를 분명히 밝히는 함수로 표현하라.\nG29: 부정 조건을 피하라 부정 조건은 긍정 조건보다 이해하기 어렵다. 가능하면 긍정 조건으로 표현한다.\nG30: 함수는 한가지만 해야한다 함수를 짜다보면 함수 안에 여러 단락을 이어, 일련의 작업을 수행하고픈 유혹에 빠진다. 이런 함수는 한가지만 수행하는 함수가 아닐 가능성이 높다. 한 가지만 수행하는 좀 더 작은 함수 여럿으로 나눠야 한다.\nG31: 숨겨진 시간적인 결합 때로는 시간적인 결합이 필요하다. 하지만 시간적인 결합을 숨겨서는 안된다. 함수를 만들때 함수 인수를 적절히 배치하여 함수가 호출되는 순서를 명백히 드러낸다.\n함수 인수를 통해 각 함수가 내놓는 결과를 사용하게 구현하면 일종의 연결 소자 역할을 하게 되어 시간적인 결합을 노출하기 쉽다.\nG32: 일관성을 유지하라 코드 구조를 잡을 때는 이유를 고민하라. 그리고 그 이유를 코드 구조로 명백히 표현하라.\n구조에 일관성이 없어 보인다면 남들이 맘대로 바꿔도 괜찮다고 생각한다. 반면 시스템 전반에 걸쳐 구조가 일관성이 있다면 남들도 일관성을 따르고 보존한다.\nG33: 경계 조건을 캡슐화하라 경계 조건은 빼먹거나 놓치기 쉽다. 따라서 경계 조건을 코드 여기저기세서 처리하지 않고 한 곳에서 별도로 처리한다.\nG34: 함수는 추상화 수준을 한 단계만 내려가야 한다 함수 내 모든 문장은 추상화 수준이 동일해야 한다. 그리고 추상화 수준은 함수 이름이 의미하는 작업보다 한 단계만 낮아야 한다.\n개념은 아주 간단하지만 인간은 추강화 수준을 뒤섞는 능력이 너무나도 뛰어나 따르기 어렵다. 1 2 3 4 5 6 7 8 9 10 11 public String render() throws Exception { StringBuffer html = new StringBuffer(\u0026#34;\u0026lt;hr\u0026#34;); if (size \u0026gt; 0) { html.append(\u0026#34; size=\\\u0026#34;\u0026#34;).append(size + 1).append(\u0026#34;\\\u0026#34;\u0026#34;); } html.append(\u0026#34;\u0026gt;\u0026#34;); return html.toString(); } 위 함수는 추상화 수준이 두개 이상 섞여있다.\n수평선에 크기가 있다는 개념 hr 태그 자체의 문법 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public String render() throws Exception { HtmlTag hr = new HtmlTag(\u0026#34;hr\u0026#34;); if (extraDashes \u0026gt; 0 ) { hr.addAttribute(\u0026#34;size\u0026#34;, hrSize(extraDashes)); } return hr.html(); } private String hrSize(int height) { int hrSize = height + 1; return String.format(\u0026#34;d\u0026#34;, hrSize); } G35: 설정 정보는 최상위 단계에 둬라 추상화 최상위 단계에 뒤야 할 기본값 상수나 설정 관련 상수를 저차운 함수에 숨겨서는 안된다. 대신 고차원 함수에서 저차원 함수를 호출할 때 인수로 넘긴다.\nG36: 추이적 탐색을 피하라 일반적으로 한 모듈은 주변 모듈을 모를수록 좋다. 좀 더 구체적으로 A가 B를 사용하고, B가 C를 사용한다 하더라도 A가 C를 알아야 할 필요는 없다는 뜻이다.\nex. a.getB().getC().doSometing(); 이를 “디미터의 법칙”, “부끄럼 타는 코드 작성”이라 부른다.\n요지는 자신이 직접 사용하는 모듈만 알아야 한다는 뜻이다. 내가 아는 모듈이 연이어 자신이 아는 모듈을 따라가며 시스템 전체를 휘저을 필요가 없다.\nJava J1: 긴 import 목록을 피하고 와일드카드를 사용하라. J2: 상수는 상속하지 않는다. 상수를 상속하여 사용하는 것은 언어의 범위 규칙을 속이는 행위이다. 대신 static import를 사용하라\nJ3: 상수 VS enum enum을 사용하면 public static final int라는 옛날 기교를 더 이상 사용할 필요가 없다. int는 코드에서 의미를 잃어버리기도 한다.\n반면 enum은 이름이 부여된 열거체이므로 의미를 잃어버리지 않는다.\n메서드와 필드도 사용할 수 있다. int보다 훨씬 더 유연하고 서술적이다.\n이름 N1: 서술적인 이름을 사용하라 소프트웨어의 가독성은 90% 이름이 결정한다. 그러므로 시간을 들여 현명한 이름을 선택하고 유효한 상태로 유지한다.\n서술적인 이름을 신중히 고른다. 소프트웨어가 진화하면 의미도 변하므로 선택한 이름이 적합한지 자주 되돌아본다. N2: 적절한 추상화 수준에서 이름을 선택하라 구현을 드러내는 이름은 피하는 것이 좋다. 작업 대상 클래스나 함수가 위치하는 추상화 수준을 반영하는 이름을 선택하라.\nN3: 가능하다면 표준 명명법을 사용하라 기존 명명법을 사용하는 이름은 이해하기 더 쉽다.\n디자인 패턴을 활용하면 클래스 이름에 패턴 이름을 사용한다. 자바에서 객체를 문자열로 변환하는 함수는 toString이라는 이름을 쓴다. 등 이름에 기존 관례가 있다면 관례를 따르는 것이 좋다.\n프로젝트에 유효한 의미가 담긴 이름을 많이 사용할수록 독자가 코드를 이해하기 쉬워진다.\nN4: 명확한 이름 함수나 변수의 목적을 명확히 밝히는 이름을 선택한다. 광범위하거나 모호한 이름을 사용하지 않는다.\nN5: 긴 범위는 긴 이름을 사용하라 이름 길이는 범위 길이에 비례해야 한다. 범위가 작으면 아주 짧은 이름을 사용해도 괜찮다. 하지만 범위가 길어지면 긴 이름을 사용한다.\nN6: 인코딩을 피하라 이름에 유형 정보나 범위 정보를 넣어서는 안 된다.\nN7: 이름으로 부수 효과를 설명하라 함수, 변수, 클래스가 하는 일을 모두 기술하는 이름을 사용한다. 이름에 부수 효과를 숨기지 않는다.\n테스트 T1: 불충분한 테스트 테스트 케이스는 잠재적으로 깨질 만한 부분을 보두 테스트해야 한다. 테스트 케이스가 확인하지 않는 조건이나 검증하지 않는 계산이 있다면 그 테스트는 불완전하다.\nT2: 커버리지 도구를 사용하라 커버리지 도구는 테스트가 빠뜨리는 공백을 알려준다. 도구를 사용하며 ㄴ테스트가 불충분한 모듈, 클래스, 함수를 찾기가 쉬워진다.\n상위 15 개 코드 커버리지 도구 (Java, JavaScript, C ++, C #, PHP 용) - 다른\nT3: 사소한 테스트를 건너뛰지 마라 사소한 테스트는 짜기 쉽다. 사소한 테스트가 제공하는 문서적 가치는 구현에 드는 비용을 넘어선다.\nT4: 무시한 테스트는 모호함을 뜻한다 때로는 요구사항이 불분명하기에 프로그램이 돌아가는 방식을 확신하기 어렵다. 불분명한 요구사항은 테스트 케이스를 주석으로 처리하거나 테스트 케이스에 @Ignore를 붙여 표현한다.\n선택 기준은 모호함이 존재하는 테스트 케이스가 컴파일이 가능한지 불가능한지에 달려있다.\nT5: 경계 조건을 테스트하라 경계 조건은 각별히 신경 써서 테스트한다. 알고리즘의 중앙 조건은 올바로 짜놓고 경계 조건에서 실수하는 경우가 흔하기 때문\nT6: 버그 주변은 철저히 테스트하라 버그는 서로 모이는 경향이 있다. 한 함수에서 버그를 발견했다면 그 함수를 철저히 테스트하는 편이 좋다.\nT7: 실패 패턴을 살펴라 때로는 테스트 케이스가 실패하는 패턴으로 문제를 진단할 수 있다.\n합리적인 순서로 정렬된 꼼꼼한 테스트 케이스는 실패 패턴을 드러낸다.\nT8: 테스트 커버리지 패턴을 살펴라 통과하는 테스트가 실행하거나 실행하지 않는 코드를 살펴보면 실패하는 테스트 케이스의 실패 원인이 드러난다.\nT9: 테스트는 빨라야한다 느린 테스트 케이스는 실행하지 않게 된다. 일정이 촉박하면 느린 테스트 케이스를 제일 먼저 건너 뛴다.\n결론 휴리스틱 목록을 익힌다고 소프트웨어 장인이 되지는 못한다. 전문가 정신과 장인 정신은 가치에서 나온다. 그 가치에 기반한 규율과 절제가 필요하다.\n","date":"2023-04-21T11:11:13+09:00","image":"https://codemario318.github.io/post/clean_code_17/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean_code_17/","title":"클린코드: 17. 냄새와 휴리스틱"},{"content":" 프로그래밍은 과학보다 공예에 가깝다. 깨끗한 코드를 짜려면 먼저 지저분한 코드를 짠 뒤에 정리해야 한다.\nArgs 구현 명령행 인수 구문을 분석하기 위한 클래스인 Args 클래스 개선을 예시로 확인해본다.\nMarshalling(마샬링): 객체의 메모리에서 표현방식을 저장 또는 전송에 적합한 다른 데이터 형식으로 변환하는 과정이다.\nArgs: 1차 초안 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 import java.text.ParseException; import java.util.*; public class Args { private String schema; private String[] args; private boolean valid = true; private Set\u0026lt;Character\u0026gt; unexpectedArguments = new TreeSet\u0026lt;Character\u0026gt;(); private Map\u0026lt;Character, Boolean\u0026gt; booleanArgs = new HashMap\u0026lt;Character, Boolean\u0026gt;(); private Map\u0026lt;Character, String\u0026gt; stringArgs = new HashMap\u0026lt;Character, String\u0026gt;(); private Map\u0026lt;Character, Integer\u0026gt; intArgs = new HashMap\u0026lt;Character, Integer\u0026gt;(); private Set\u0026lt;Character\u0026gt; argsFound = new HashSet\u0026lt;Character\u0026gt;(); private int currentArgument; private char errorArgumentId = \u0026#39;\\0\u0026#39;; private String errorParameter = \u0026#34;TILT\u0026#34;; private ErrorCode errorCode = ErrorCode.OK; private enum ErrorCode { OK, MISSING_STRING, MISSING_INTEGER, INVALID_INTEGER, UNEXPECTED_ARGUMENT } public Args(String schema, String[] args) throws ParseException { this.schema = schema; this.args = args; valid = parse(); } private boolean parse() throws ParseException { if (schema.length() == 0 \u0026amp;\u0026amp; args.length == 0) return true; parseSchema(); try { parseArguments(); } catch (ArgsException e) { } return valid; } private boolean parseSchema() throws ParseException { for (String element : schema.split(\u0026#34;,\u0026#34;)) { if (element.length() \u0026gt; 0) { String trimmedElement = element.trim(); parseSchemaElement(trimmedElement); } } return true; } private void parseSchemaElement(String element) throws ParseException { char elementId = element.charAt(0); String elementTail = element.substring(1); validateSchemaElementId(elementId); if (isBooleanSchemaElement(elementTail)) parseBooleanSchemaElement(elementId); else if (isStringSchemaElement(elementTail)) parseStringSchemaElement(elementId); else if (isIntegerSchemaElement(elementTail)) { parseIntegerSchemaElement(elementId); } else { throw new ParseException( String.format(\u0026#34;Argument: %c has invalid format: %s.\u0026#34;, elementId, elementTail), 0); } } private void validateSchemaElementId(char elementId) throws ParseException { if (!Character.isLetter(elementId)) { throw new ParseException( \u0026#34;Bad character:\u0026#34; + elementId + \u0026#34;in Args format: \u0026#34; + schema, 0); } } private void parseBooleanSchemaElement(char elementId) { booleanArgs.put(elementId, false); } private void parseIntegerSchemaElement(char elementId) { intArgs.put(elementId, 0); } private void parseStringSchemaElement(char elementId) { stringArgs.put(elementId, \u0026#34;\u0026#34;); } private boolean isStringSchemaElement(String elementTail) { return elementTail.equals(\u0026#34;*\u0026#34;); } private boolean isBooleanSchemaElement(String elementTail) { return elementTail.length() == 0; } private boolean isIntegerSchemaElement(String elementTail) { return elementTail.equals(\u0026#34;#\u0026#34;); } private boolean parseArguments() throws ArgsException { for (currentArgument = 0; currentArgument \u0026lt; args.length; currentArgument++) { String arg = args[currentArgument]; parseArgument(arg); } return true; } private void parseArgument(String arg) throws ArgsException { if (arg.startsWith(\u0026#34;-\u0026#34;)) parseElements(arg); } private void parseElements(String arg) throws ArgsException { for (int i = 1; i \u0026lt; arg.length(); i++) parseElement(arg.charAt(i)); } private void parseElement(char argChar) throws ArgsException { if (setArgument(argChar)) argsFound.add(argChar); else { unexpectedArguments.add(argChar); errorCode = ErrorCode.UNEXPECTED_ARGUMENT; valid = false; } } private boolean setArgument(char argChar) throws ArgsException { if (isBooleanArg(argChar)) setBooleanArg(argChar, true); else if (isStringArg(argChar)) setStringArg(argChar); else if (isIntArg(argChar)) setIntArg(argChar); else return false; return true; } private boolean isIntArg(char argChar) { return intArgs.containsKey(argChar); } private void setIntArg(char argChar) throws ArgsException { currentArgument++; String parameter = null; try { parameter = args[currentArgument]; intArgs.put(argChar, new Integer(parameter)); } catch (ArrayIndexOutOfBoundsException e) { valid = false; errorArgumentId = argChar; errorCode = ErrorCode.MISSING_INTEGER; throw new ArgsException(); } catch (NumberFormatException e) { valid = false; errorArgumentId = argChar; errorParameter = parameter; errorCode = ErrorCode.INVALID_INTEGER; throw new ArgsException(); } } private void setStringArg(char argChar) throws ArgsException { currentArgument++; try { stringArgs.put(argChar, args[currentArgument]); } catch (ArrayIndexOutOfBoundsException e) { valid = false; errorArgumentId = argChar; errorCode = ErrorCode.MISSING_STRING; throw new ArgsException(); } } private boolean isStringArg(char argChar) { return stringArgs.containsKey(argChar); } private void setBooleanArg(char argChar, boolean value) { booleanArgs.put(argChar, value); } private boolean isBooleanArg(char argChar) { return booleanArgs.containsKey(argChar); } public int cardinality() { return argsFound.size(); } public String usage() { if (schema.length() \u0026gt; 0) return \u0026#34;-[\u0026#34; + schema + \u0026#34;]\u0026#34;; else return \u0026#34;\u0026#34;; } public String errorMessage() throws Exception { switch (errorCode) { case OK: throw new Exception(\u0026#34;TILT: Should not get here.\u0026#34;); case UNEXPECTED_ARGUMENT: return unexpectedArgumentMessage(); case MISSING_STRING: return String.format(\u0026#34;Could not find string parameter for -%c.\u0026#34;, errorArgumentId); case INVALID_INTEGER: return String.format(\u0026#34;Argument -%c expects an integer but was \u0026#39;%s\u0026#39;.\u0026#34;, errorArgumentId, errorParameter); case MISSING_INTEGER: return String.format(\u0026#34;Could not find integer parameter for -%c.\u0026#34;, errorArgumentId); } return \u0026#34;\u0026#34;; } private String unexpectedArgumentMessage() { StringBuffer message = new StringBuffer(\u0026#34;Argument(s) -\u0026#34;); for (char c : unexpectedArguments) { message.append(c); } message.append(\u0026#34; unexpected.\u0026#34;); return message.toString(); } private boolean falseIfNull(Boolean b) { return b != null \u0026amp;\u0026amp; b; } private int zeroIfNull(Integer i) { return i == null ? 0 : i; } private String blankIfNull(String s) { return s == null ? \u0026#34;\u0026#34; : s; } public String getString(char arg) { return blankIfNull(stringArgs.get(arg)); } public int getInt(char arg) { return zeroIfNull(intArgs.get(arg)); } public boolean getBoolean(char arg) { return falseIfNull(booleanArgs.get(arg)); } public boolean has(char arg) { return argsFound.contains(arg); } public boolean isValid() { return valid; } private class ArgsException extends Exception { } } 문제점 인스턴스 변수가 너무 많다. “TILT” 와 같은 알수 없는 문자열이 있다. HashSets, TreeSets, try-catch-catch 블록 등 지저분해 보인다. Boolean만 지원하는 Args.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 package com.objectmentor.utilities.getopts; import java.util.*; public class Args { private String schema; private String[] args; private boolean valid; private Set\u0026lt;Character\u0026gt; unexpectedArguments = new TreeSet\u0026lt;Character\u0026gt;(); private Map\u0026lt;Character, Boolean\u0026gt; booleanArgs = new HashMap\u0026lt;Character, Boolean\u0026gt;(); private int numberOfArguments = 0; public Args(String schema, String[] args) { this.schema = schema; this.args = args; valid = parse(); } public boolean isValid() { return valid; } private boolean parse() { if (schema.length() == 0 \u0026amp;\u0026amp; args.length == 0) return true; parseSchema(); parseArguments(); return unexpectedArguments.size() == 0; } private boolean parseSchema() { for (String element : schema.split(\u0026#34;,\u0026#34;)) { parseSchemaElement(element); } return true; } private void parseSchemaElement(String element) { if (element.length() == 1) { parseBooleanSchemaElement(element); } } private void parseBooleanSchemaElement(String element) { char c = element.charAt(0); if (Character.isLetter(c)) { booleanArgs.put(c, false); } } private boolean parseArguments() { for (String arg : args) parseArgument(arg); return true; } private void parseArgument(String arg) { if (arg.startsWith(\u0026#34;-\u0026#34;)) parseElements(arg); } private void parseElements(String arg) { for (int i = 1; i \u0026lt; arg.length(); i++) parseElement(arg.charAt(i)); } private void parseElement(char argChar) { if (isBoolean(argChar)) { numberOfArguments++; setBooleanArg(argChar, true); } else unexpectedArguments.add(argChar); } private void setBooleanArg(char argChar, boolean value) { booleanArgs.put(argChar, value); } private boolean isBoolean(char argChar) { return booleanArgs.containsKey(argChar); } public int cardinality() { return numberOfArguments; } public String usage() { if (schema.length() \u0026gt; 0) return \u0026#34;-[\u0026#34; + schema + \u0026#34;]\u0026#34;; else return \u0026#34;\u0026#34;; } public String errorMessage() { if (unexpectedArguments.size() \u0026gt; 0) { return unexpectedArgumentMessage(); } else return \u0026#34;\u0026#34;; } private String unexpectedArgumentMessage() { StringBuffer message = new StringBuffer(\u0026#34;Argument(s) -\u0026#34;); for (char c : unexpectedArguments) { message.append(c); } message.append(\u0026#34; unexpected.\u0026#34;); return message.toString(); } public boolean getBoolean(char arg) { return booleanArgs.get(arg); } } 간결하고 단순하며 이해하기도 쉬웠다.\n하지만 코드를 잘 살펴보면 나중에 엉망으로 변해갈 씨앗이 보인다.\nBoolean과 String을 지원하는 Args.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 package com.objectmentor.utilities.getopts; import java.text.ParseException; import java.util.*; public class Args { private String schema; private String[] args; private boolean valid = true; private Set\u0026lt;Character\u0026gt; unexpectedArguments = new TreeSet\u0026lt;Character\u0026gt;(); private Map\u0026lt;Character, Boolean\u0026gt; booleanArgs = new HashMap\u0026lt;Character, Boolean\u0026gt;(); private Map\u0026lt;Character, String\u0026gt; stringArgs = new HashMap\u0026lt;Character, String\u0026gt;(); private Set\u0026lt;Character\u0026gt; argsFound = new HashSet\u0026lt;Character\u0026gt;(); private int currentArgument; private char errorArgument = \u0026#39;\\0\u0026#39;; enum ErrorCode { OK, MISSING_STRING } private ErrorCode errorCode = ErrorCode.OK; public Args(String schema, String[] args) throws ParseException { this.schema = schema; this.args = args; valid = parse(); } private boolean parse() throws ParseException { if (schema.length() == 0 \u0026amp;\u0026amp; args.length == 0) return true; parseSchema(); parseArguments(); return valid; } private boolean parseSchema() throws ParseException { for (String element : schema.split(\u0026#34;,\u0026#34;)) { if (element.length() \u0026gt; 0) { String trimmedElement = element.trim(); parseSchemaElement(trimmedElement); } } return true; } private void parseSchemaElement(String element) throws ParseException { char elementId = element.charAt(0); String elementTail = element.substring(1); validateSchemaElementId(elementId); if (isBooleanSchemaElement(elementTail)) parseBooleanSchemaElement(elementId); else if (isStringSchemaElement(elementTail)) parseStringSchemaElement(elementId); } private void validateSchemaElementId(char elementId) throws ParseException { if (!Character.isLetter(elementId)) { throw new ParseException( \u0026#34;Bad character:\u0026#34; + elementId + \u0026#34;in Args format: \u0026#34; + schema, 0); } } private void parseStringSchemaElement(char elementId) { stringArgs.put(elementId, \u0026#34;\u0026#34;); } private boolean isStringSchemaElement(String elementTail) { return elementTail.equals(\u0026#34;*\u0026#34;); } private boolean isBooleanSchemaElement(String elementTail) { return elementTail.length() == 0; } private void parseBooleanSchemaElement(char elementId) { booleanArgs.put(elementId, false); } // // here! private boolean parseArguments() { for (currentArgument = 0; currentArgument \u0026lt; args.length; currentArgument++) { String arg = args[currentArgument]; parseArgument(arg); } return true; } private void parseArgument(String arg) { if (arg.startsWith(\u0026#34;-\u0026#34;)) parseElements(arg); } private void parseElements(String arg) { for (int i = 1; i \u0026lt; arg.length(); i++) parseElement(arg.charAt(i)); } private void parseElement(char argChar) { if (setArgument(argChar)) argsFound.add(argChar); else { unexpectedArguments.add(argChar); valid = false; } } private boolean setArgument(char argChar) { boolean set = true; if (isBoolean(argChar)) setBooleanArg(argChar, true); else if (isString(argChar)) setStringArg(argChar, \u0026#34;\u0026#34;); else set = false; return set; } private void setStringArg(char argChar, String s) { currentArgument++; try { stringArgs.put(argChar, args[currentArgument]); } catch (ArrayIndexOutOfBoundsException e) { valid = false; errorArgument = argChar; errorCode = ErrorCode.MISSING_STRING; } } private boolean isString(char argChar) { return stringArgs.containsKey(argChar); } // private void setBooleanArg(char argChar, boolean value) { booleanArgs.put(argChar, value); } private boolean isBoolean(char argChar) { return booleanArgs.containsKey(argChar); } public int cardinality() { return argsFound.size(); } public String usage() { if (schema.length() \u0026gt; 0) return \u0026#34;-[\u0026#34; + schema + \u0026#34;]\u0026#34;; else return \u0026#34;\u0026#34;; } public String errorMessage() throws Exception { if (unexpectedArguments.size() \u0026gt; 0) { return unexpectedArgumentMessage(); } else switch (errorCode) { case MISSING_STRING: return String.format(\u0026#34;Could not find string parameter for -%c.\u0026#34;, errorArgument); case OK: throw new Exception(\u0026#34;TILT: Should not get here.\u0026#34;); } return \u0026#34;\u0026#34;; } private String unexpectedArgumentMessage() { StringBuffer message = new StringBuffer(\u0026#34;Argument(s) -\u0026#34;); for (char c : unexpectedArguments) { message.append(c); } message.append(\u0026#34; unexpected.\u0026#34;); return message.toString(); } public boolean getBoolean(char arg) { return falseIfNull(booleanArgs.get(arg)); } private boolean falseIfNull(Boolean b) { return b == null ? false : b; } // here! public String getString(char arg) { return blankIfNull(stringArgs.get(arg)); } private String blankIfNull(String s) { return s == null ? \u0026#34;\u0026#34; : s; } public boolean has(char arg) { return argsFound.contains(arg); } public boolean isValid() { return valid; } } String유형 관련 코드 추가 후 코드가 어지러워졌다. 이후 Integer 인수 유형을 추가하니 코드가 완전히 엉망이 되었다.\n그래서 멈췄다. 인수 유형을 더 추가해야 했는데, 코드가 훨씬 더 나빠질 것이라는 확신으로 리펙터링을 시작했다.\n새 인수 유형을 추가하려면 주요 지점 세곳에다 코드를 추가해야 한다는 사실을 알았다. (parse, get, set)\n인수 유형에 해당하는 HashMap을 선택하기 위해 스키마 요소의 구문 분석 명령행 인수에서 인수 유형을 분석해 진짜 유형으로 변환. getXXX메서드를 구현해 호출자에게 진짜 유형을 반환 💡 인수 유형은 다양하지만 모두가 유사한 메서드를 제공하므로 클래스 하나가 적합하다 판단했다. 그래서 ArgumentMarshaler라는 개념이 탄생했다.\n점진적으로 개선하다. 프로그램을 망치는 가장 좋은 방법 중 하나는 개선이라는 이름 아래 구조를 크게 뒤집는 행위다. 개선 전과 똑같은 결과를 만들게 하는 것이 어렵기 때문이다.\n그래서 테스트 주도 개발이라는 기법을 사용하여, 변경 후에도 시스템이 변경 전과 똑같이 돌아가는 것을 확인한다.\nArgumentMarshaler 각 인수 유형을 처리하는 코드를 Args내부에 선언된 ArgumentMarshaler 클래스에 옮긴 후, 이후 ArgumentMarshaler 파생 클래스를 만들어 분리했다.\n프로그램 구조를 조금씩 변경하는 동안에도 시스템의 정상 동작을 유지하기 쉬워지기 때문 아래 코드를 Args.java끝에 추가했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 private class ArgumentMarshaler { private boolean booleanValue = false; public void setBoolean(boolean value) { booleanValue = value; } public boolean getBoolean() { return booleanValue; } } private class BooleanArgumentMarshaler extends ArgumentMarshaler { } private class StringArgumentMarshaler extends ArgumentMarshaler { } private class IntegerArgumentMarshaler extends ArgumentMarshaler { } 처음 Boolean 인수를 저장하는 HashMap에서 Boolean 인수 유형을 ArgumentMarshaler유형으로 바꿨다. 1 private Map\u0026lt;Character, **ArgumentMarshaler**\u0026gt; booleanArgs = new HashMap\u0026lt;Character, **ArgumentMarshaler**\u0026gt;(); 그 후 깨진 부분에 변경을 반영했다. 1 2 3 4 5 6 7 8 9 10 11 private void parseBooleanSchemaElement(char elementId) { booleanArgs.put(elementId, **new BooleanArgumentMarshaler()**); } ... private void setBooleanArg(char argChar, boolean value) { booleanArgs.**get**(argChar).**setBoolean**(value); } ... public boolean getBoolean(char arg) { return falseIfNull(booleanArgs.get(arg).**getBoolean**()); } 새 인수 유형을 추가하려면 세 곳 (parse, get, set)을 변경해야 했는데, 정확히 해당 영역에서 변경이 이루어졌다.\nbooleanArgs가 ArgumentMarshaler 를 값으로 사용하게 변경되었으므로 위 getBoolean에서 사용하는 기존 falseIfNull이 NullPointerException을 막을 수 없었다.\n1 2 3 4 5 6 7 8 9 10 11 12 ... private Map\u0026lt;Character, Boolean\u0026gt; booleanArgs = new HashMap\u0026lt;Character, Boolean\u0026gt;(); ... private class ArgumentMarshaler { ... public boolean getBoolean(char arg) { Args.ArgumentMarshaler am = booleanArgs.get(arg); return am != null \u0026amp;\u0026amp; am.getBoolean(); } } ArgumentMarshaler.getBoolean 에서 null을 검사하게 구현하고 기존 falseIfNull을 제거했다.\nString 인수 추가 Boolean 인수를 추가하는 과정과 유사하게 String 인수를 추가했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 private Map\u0026lt;Character, **ArgumentMarshaler**\u0026gt; stringArgs = new HashMap\u0026lt;Character, **ArgumentMarshaler**\u0026gt;(); private void parseStringSchemaElement(char elementId) { stringArgs.put(elementId, **new StringArgumentMarshaler()**); } // ... private void setStringArg(char argChar) throws ArgsException { currentArgument++; try { stringArgs.**get**(argChar).**setString**(args[currentArgument]); } catch (ArrayIndexOutOfBoundsException e) { valid = false; errorArgumentId = argChar; errorCode = ErrorCode.MISSING_STRING; throw new ArgsException(); } } // ... public String getString(char arg) { **Args.ArgumentMarshaler am** = stringArgs.get(arg); return am == null ? \u0026#34;\u0026#34; : am.getString(); } // ... private class ArgumentMarshaler { private String stringValue; public void setString(String s) { stringValue = s; } public String getString() { return stringValue == null ? \u0026#34;\u0026#34; : stringValue; } } 테스트 케이스가 하나라도 실패하면 다음 변경으로 넘어가기 전에 오류를 수정했다.\n마찬가지로 int 인수 기능도 추가했다.\n추상클래스를 이용한 분리 Boolean, String, Integer 인수 처리에 대한 모든 기능을 Args의 ArgumentMarshaler로 옮겼으므로 미리 선언해둔 파생 클래스로 분산한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 private **abstract** class ArgumentMarshaler { **protected** boolean booleanValue = false; private String stringValue; private int integerValue; public void setBoolean(boolean value) { booleanValue = value; } public boolean getBoolean() { return booleanValue; } public void setString(String s) { stringValue = s; } public String getString() { return stringValue == null ? \u0026#34;\u0026#34; : stringValue; } public void setInteger(int i) { integerValue = i; } public int getInteger() { return integerValue; } **public abstract void set(String s);** } 점진적으로 한단계씩 개선하기 위해 ArgumentMarshaler를 추상 클래스로 바꾸고 하나씩 옮긴다.\n1 2 3 4 5 private class BooleanArgumentMarshaler extends ArgumentMarshaler { public void set(String s) { booleanValue = true; } } ArgumentMarshaler를 상속받은 유형 별 클래스에 set메서드를 구현하고, 기존 ArgumentMarshaler.setXXX을 set으로 대체한다.\n→ BooleanArgumentMarshaler.set 에서 s를 사용하지 않음에도 선언한 이유는 ArgumentMarshaler 를 상속받는 다른 클래스들이 String 인자를 받아 사용하기 때문\nget 1 2 3 4 5 private abstract class ArgumentMarshaler { protected boolean booleanValue = false; ... **public abstract void get();** } get을 추상 메서드로 만든 후 로직을 추가한다.\n1 2 3 4 5 6 7 8 9 10 private class BooleanArgumentMarshaler extends ArgumentMarshaler { **private boolean booleanValue = false;** public void set(String s) { booleanValue = true; } **public Object get() { return booleanValue; }** } 그 후 ArgumentMarshaler 에 protected로 선언되어있던 변수를 xxxArgumentMarshaler의 private변수로 선언한다.\nArgumentMarshaler 검증 1 2 3 4 5 private abstract class ArgumentMarshaler { ... **public abstract Object get(); public abstract void set(String s) throws ArgsException;** } set을 통해 ArgumentMarshaler를 상속받은 클래스의 private변수인 xxxValue에 값을 할당하게 되는데 자료형이 다를경우 오류가 발생한다.\nArgs 통합 xxxArgs를 통합하기 위한 첫걸음으로 marshalers 를 추가했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class Args { // ... private Map\u0026lt;Character, ArgumentMarshaler\u0026gt; booleanArgs = new HashMap\u0026lt;Character, ArgumentMarshaler\u0026gt;(); private Map\u0026lt;Character, ArgumentMarshaler\u0026gt; stringArgs = new HashMap\u0026lt;Character, ArgumentMarshaler\u0026gt;(); private Map\u0026lt;Character, ArgumentMarshaler\u0026gt; intArgs = new HashMap\u0026lt;Character, ArgumentMarshaler\u0026gt;(); **private Map\u0026lt;Character, ArgumentMarshaler\u0026gt; marshalers = new HashMap\u0026lt;Character, ArgumentMarshaler\u0026gt;();** // ... private void parseBooleanSchemaElement(char elementId) { ArgumentMarshaler m = new BooleanArgumentMarshaler(); booleanArgs.put(elementId, m); **marshalers.put(elementId, m);** } private void parseIntegerSchemaElement(char elementId) { ArgumentMarshaler m = new IntegerArgumentMarshaler(); intArgs.put(elementId, m); **marshalers.put(elementId, m);** } private void parseStringSchemaElement(char elementId) { ArgumentMarshaler m = new StringArgumentMarshaler(); stringArgs.put(elementId, m); **marshalers.put(elementId, m);** } } 그 후 parseXXX 메서드에 marshalers.put을 수행하여 테스트를 통과하지 않는지 살폈다. 테스트는 실패없이 돌아갔다.\n1 2 3 private boolean isBooleanArg(char argChar) { return booleanArgs.containsKey(argChar); } 그 후 Args.isBooleanArgs를 아래로 수정했다.\n1 2 3 4 private boolean isBooleanArg(char argChar) { ArgumentMarshaler m = marshalers.get(argChar); return m instanceof BooleanArgumentMarshaler; } 수정 후 테스트를 모두 통과하여 다른 항목들에 대해서도 동일한 수정을 했다. 변경후에도 테스트를 통과하여 marshalers.get을 호출하는 코드를 통합했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 private boolean setArgument(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); if (isBooleanArg(m)) setBooleanArg(argChar); else if (isStringArg(m)) setStringArg(argChar); else if (isIntArg(m)) setIntArg(argChar); else return false; return true; } private boolean isIntArg(ArgumentMarshaler m) { return m instanceof IntegerArgumentMarshaler; } private boolean isStringArg(ArgumentMarshaler m) { return m instanceof StringArgumentMarshaler; } private boolean isBooleanArg(ArgumentMarshaler m) { return m instanceof BooleanArgumentMarshaler; } 다음으로 set함수에서 기존 HashMap을 marshalers HashMap으로 교체한다.\n1 2 3 4 5 6 private void setBooleanArg(ArgumentMarshaler m) { try { m.set(\u0026#34;true\u0026#34;); // was: booleanArgs.get(argChar).set(\u0026#34;true\u0026#34;); } catch (ArgsException e) { } } 각 setXXX의 형식을 통합하여 예외처리를 동일하게 적용할 수 있었다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 private boolean setArgument(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); try { if (m instanceof BooleanArgumentMarshaler) setBooleanArg(m); else if (m instanceof StringArgumentMarshaler) setStringArg(m); else if (m instanceof IntegerArgumentMarshaler) setIntArg(m); else return false; } catch (ArgsException e) { valid = false; errorArgumentId = argChar; throw e; } return true; } private void setIntArg(ArgumentMarshaler m) throws ArgsException { currentArgument++; String parameter = null; try { parameter = args[currentArgument]; m.set(parameter); } catch (ArrayIndexOutOfBoundsException e) { errorCode = ErrorCode.MISSING_INTEGER; throw new ArgsException(); } catch (ArgsException e) { errorParameter = parameter; errorCode = ErrorCode.INVALID_INTEGER; throw e; } } private void setStringArg(ArgumentMarshaler m) throws ArgsException { currentArgument++; try { m.set(args[currentArgument]); } catch (ArrayIndexOutOfBoundsException e) { errorCode = ErrorCode.MISSING_STRING; throw new ArgsException(); } } HashMap 통합 ArgumentMarshaler를 이용하여 만든 유형별 HashMap을 하나의 HashMap으로 통합한다.\n1 2 3 4 5 6 7 8 9 10 public boolean getBoolean(char arg) { Args.ArgumentMarshaler am = marshalers.get(arg); boolean b = false; try { b = am != null \u0026amp;\u0026amp; (Boolean) am.get(); } catch (ClassCastException e) { b = false; } return b; } 이에따라 다른 인수유형도 아래와 같이 변경 가능했고,\n1 2 3 4 5 private void parseBooleanSchemaElement(char elementId) { ArgumentMarshaler m = new BooleanArgumentMarshaler(); ~~booleanArgs.put(elementId, m);~~ marshalers.put(elementId, m); } 결과적으로 하나의 Hashmap으로 통합 가능했다.\nsetArgument 유형을 일일이 확인하는 코드를 없애고 ArgumentMarshaler.set만으로 만든다.\nsetXXXArg를 각각 ArgumentMarshaler 로 내려야 하는데, setIntArgs는 args와 currentArgument라는 인스턴스 변수 두개가 쓰인다.\n해당 메서드를 내리려면 args와 currentArgument를 인수로 넘겨야 하므로(인수가 많아지므로) 코드가 지저분해진다.\n따라서 args를 list로 변환 후 Iterator를 set 함수로 전달한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 public class Args { private String schema; private String[] args; private boolean valid = true; private Set\u0026lt;Character\u0026gt; unexpectedArguments = new TreeSet\u0026lt;Character\u0026gt;(); private Map\u0026lt;Character, ArgumentMarshaler\u0026gt; marshalers = new HashMap\u0026lt;Character, ArgumentMarshaler\u0026gt;(); private Set\u0026lt;Character\u0026gt; argsFound = new HashSet\u0026lt;Character\u0026gt;(); **private Iterator\u0026lt;String\u0026gt; currentArgument;** private char errorArgumentId = \u0026#39;\\0\u0026#39;; private String errorParameter = \u0026#34;TILT\u0026#34;; private ErrorCode errorCode = ErrorCode.OK; **private List\u0026lt;String\u0026gt; argsList;** private enum ErrorCode { OK, MISSING_STRING, MISSING_INTEGER, INVALID_INTEGER, UNEXPECTED_ARGUMENT } public Args(String schema, String[] args) throws ParseException { this.schema = schema; **argsList = Arrays.asList(args);** valid = parse(); } private boolean parse() throws ParseException { if (schema.length() == 0 \u0026amp;\u0026amp; **argsList.size()** == 0) return true; parseSchema(); try { parseArguments(); } catch (ArgsException e) { } return valid; } // --- private boolean parseArguments() throws ArgsException { for (currentArgument = **argsList.iterator()**; currentArgument.**hasNext()**;) { String arg = currentArgument.**next()**; parseArgument(arg); } return true; } // --- private void setIntArg(ArgumentMarshaler m) throws ArgsException { String parameter = null; try { parameter = currentArgument.**next()**; m.set(parameter); } catch (**NoSuchElementException** e) { errorCode = ErrorCode.MISSING_INTEGER; throw new ArgsException(); } catch (ArgsException e) { errorParameter = parameter; errorCode = ErrorCode.INVALID_INTEGER; throw e; } } private void setStringArg(ArgumentMarshaler m) throws ArgsException { try { m.set(currentArgument.**next()**); } catch (**NoSuchElementException** e) { errorCode = ErrorCode.MISSING_STRING; throw new ArgsException(); } } } 따라서 args를 list로 변환 후 Iterator를 set 함수로 전달하는 변경 후에 테스트를 모두 통과했다. set 함수를 적절한 파생 클래스로 내려도 괜찮아져 처음으로 setArgument를 아래로 변경했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 private boolean setArgument(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); if (m == null) return false; try { if (m instanceof BooleanArgumentMarshaler) setBooleanArg(m); else if (m instanceof StringArgumentMarshaler) setStringArg(m); else if (m instanceof IntegerArgumentMarshaler) setIntArg(m); ~~else return false;~~ } catch (ArgsException e) { valid = false; errorArgumentId = argChar; throw e; } return true; } 연쇄적인 if-else 구문을 완전히 제거하기 위해 제거하고 오류 코드를 따로 꺼냈다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 private boolean setArgument(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); if (m == null) return false; try { if (m instanceof BooleanArgumentMarshaler) setBooleanArg(m, currentArgument); else if (m instanceof StringArgumentMarshaler) setStringArg(m); else if (m instanceof IntegerArgumentMarshaler) setIntArg(m); } catch (ArgsException e) { valid = false; errorArgumentId = argChar; throw e; } return true; } // --- private void setBooleanArg(ArgumentMarshaler m, Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { ~~try {~~ m.set(\u0026#34;true\u0026#34;); } ~~catch (ArgsException e) { }~~ } 단계적으로 조금씩 변경하며 매번 테스트를 돌려야 하므로 코드를 옮기고 제거하는 일이 많아진다.\n위와 마찬가지로 iterator를 인자로 받는 이유는 추상 메서드로 호출하기 위해서다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 private abstract class ArgumentMarshaler { public abstract void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException; public abstract void set(String s) throws ArgsException; public abstract Object get(); } private class BooleanArgumentMarshaler extends ArgumentMarshaler { private boolean booleanValue = false; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { booleanValue = true; } public void set(String s) { booleanValue = true; } public Object get() { return booleanValue; } } private class StringArgumentMarshaler extends ArgumentMarshaler { private String stringValue = \u0026#34;\u0026#34;; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { } public void set(String s) { stringValue = s; } public Object get() { return stringValue; } } private class IntegerArgumentMarshaler extends ArgumentMarshaler { private int intValue = 0; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { } public void set(String s) throws ArgsException { try { intValue = Integer.parseInt(s); } catch (NumberFormatException e) { throw new ArgsException(); } } public Object get() { return intValue; } } 새로운 추상 메서드를 추가하고 각각 변경한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 private boolean setArgument(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); if (m == null) return false; try { if (m instanceof BooleanArgumentMarshaler) m.set(currentArgument); else if (m instanceof StringArgumentMarshaler) m.set(currentArgument); else if (m instanceof IntegerArgumentMarshaler) m.set(currentArgument); } catch (ArgsException e) { valid = false; errorArgumentId = argChar; throw e; } return true; } private class StringArgumentMarshaler extends ArgumentMarshaler { private String stringValue = \u0026#34;\u0026#34;; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { try { stringValue = currentArgument.next(); } catch (NoSuchElementException e) { errorCode = ErrorCode.MISSING_STRING; throw new ArgsException(); } } public void set(String s) { } public Object get() { return stringValue; } } private class IntegerArgumentMarshaler extends ArgumentMarshaler { private int intValue = 0; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { String parameter = null; try { parameter = currentArgument.next(); set(parameter); } catch (NoSuchElementException e) { errorCode = ErrorCode.MISSING_INTEGER; throw new ArgsException(); } catch (ArgsException e) { errorParameter = parameter; errorCode = ErrorCode.INVALID_INTEGER; throw e; } } public void set(String s) throws ArgsException { try { intValue = Integer.parseInt(s); } catch (NumberFormatException e) { throw new ArgsException(); } } public Object get() { return intValue; } } setArgument가 테스트에 통과하면 m.set이 정상 동작을 의미하므로 통합할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 private boolean setArgument(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); if (m == null) return false; try { m.set(currentArgument); return true; } catch (ArgsException e) { valid = false; errorArgumentId = argChar; throw e; } } 마지막으로 ArgumentMarshaler를 인터페이스로 변경하면 완료된다.\n1 2 3 4 private interface ArgumentMarshaler { void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException; Object get(); } 결론 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 package com.objectmentor.utilities.args; import static com.objectmentor.utilities.args.ArgsException.ErrorCode.*; import java.util.*; public class Args { private Map\u0026lt;Character, ArgumentMarshaler\u0026gt; marshalers; private Set\u0026lt;Character\u0026gt; argsFound; private ListIterator\u0026lt;String\u0026gt; currentArgument; public Args(String schema, String[] args) throws ArgsException { marshalers = new HashMap\u0026lt;Character, ArgumentMarshaler\u0026gt;(); argsFound = new HashSet\u0026lt;Character\u0026gt;(); parseSchema(schema); parseArgumentStrings(Arrays.asList(args)); } private void parseSchema(String schema) throws ArgsException { for (String element : schema.split(\u0026#34;,\u0026#34;)) if (element.length() \u0026gt; 0) parseSchemaElement(element.trim()); } private void parseSchemaElement(String element) throws ArgsException { char elementId = element.charAt(0); String elementTail = element.substring(1); validateSchemaElementId(elementId); if (elementTail.length() == 0) marshalers.put(elementId, new BooleanArgumentMarshaler()); else if (elementTail.equals(\u0026#34;*\u0026#34;)) marshalers.put(elementId, new StringArgumentMarshaler()); else if (elementTail.equals(\u0026#34;#\u0026#34;)) marshalers.put(elementId, new IntegerArgumentMarshaler()); else if (elementTail.equals(\u0026#34;##\u0026#34;)) marshalers.put(elementId, new DoubleArgumentMarshaler()); else if (elementTail.equals(\u0026#34;[*]\u0026#34;)) marshalers.put(elementId, new StringArrayArgumentMarshaler()); else throw new ArgsException(INVALID_ARGUMENT_FORMAT, elementId, elementTail); } private void validateSchemaElementId(char elementId) throws ArgsException { if (!Character.isLetter(elementId)) throw new ArgsException(INVALID_ARGUMENT_NAME, elementId, null); } private void parseArgumentStrings(List\u0026lt;String\u0026gt; argsList) throws ArgsException { for (currentArgument = argsList.listIterator(); currentArgument.hasNext();) { String argString = currentArgument.next(); if (argString.startsWith(\u0026#34;-\u0026#34;)) { parseArgumentCharacters(argString.substring(1)); } else { currentArgument.previous(); break; } } } private void parseArgumentCharacters(String argChars) throws ArgsException { for (int i = 0; i \u0026lt; argChars.length(); i++) parseArgumentCharacter(argChars.charAt(i)); } private void parseArgumentCharacter(char argChar) throws ArgsException { ArgumentMarshaler m = marshalers.get(argChar); if (m == null) { throw new ArgsException(UNEXPECTED_ARGUMENT, argChar, null); } else { argsFound.add(argChar); try { m.set(currentArgument); } catch (ArgsException e) { e.setErrorArgumentId(argChar); throw e; } } } public boolean has(char arg) { return argsFound.contains(arg); } public int nextArgument() { return currentArgument.nextIndex(); } public boolean getBoolean(char arg) { return BooleanArgumentMarshaler.getValue(marshalers.get(arg)); } public String getString(char arg) { return StringArgumentMarshaler.getValue(marshalers.get(arg)); } public int getInt(char arg) { return IntegerArgumentMarshaler.getValue(marshalers.get(arg)); } public double getDouble(char arg) { return DoubleArgumentMarshaler.getValue(marshalers.get(arg)); } public String[] getStringArray(char arg) { return StringArrayArgumentMarshaler.getValue(marshalers.get(arg)); } } 1 2 3 public interface ArgumentMarshaler { void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class BooleanArgumentMarshaler implements ArgumentMarshaler { private boolean booleanValue = false; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { booleanValue = true; } public static boolean getValue(ArgumentMarshaler am) { if (am != null \u0026amp;\u0026amp; am instanceof BooleanArgumentMarshaler) return ((BooleanArgumentMarshaler) am).booleanValue; else return false; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import static com.objectmentor.utilities.args.ArgsException.ErrorCode.*; public class StringArgumentMarshaler implements ArgumentMarshaler { private String stringValue = \u0026#34;\u0026#34;; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { try { stringValue = currentArgument.next(); } catch (NoSuchElementException e) { throw new ArgsException(MISSING_STRING); } } public static String getValue(ArgumentMarshaler am) { if (am != null \u0026amp;\u0026amp; am instanceof StringArgumentMarshaler) return ((StringArgumentMarshaler) am).stringValue; else return \u0026#34;\u0026#34;; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import static com.objectmentor.utilities.args.ArgsException.ErrorCode.*; public class IntegerArgumentMarshaler implements ArgumentMarshaler { private int intValue = 0; public void set(Iterator\u0026lt;String\u0026gt; currentArgument) throws ArgsException { String parameter = null; try { parameter = currentArgument.next(); intValue = Integer.parseInt(parameter); } catch (NoSuchElementException e) { throw new ArgsException(MISSING_INTEGER); } catch (NumberFormatException e) { throw new ArgsException(INVALID_INTEGER, parameter); } } public static int getValue(ArgumentMarshaler am) { if (am != null \u0026amp;\u0026amp; am instanceof IntegerArgumentMarshaler) return ((IntegerArgumentMarshaler) am).intValue; else return 0; } } 리펙토링을 안전하게 수행하기 위해 조금씩 변경하는 과정을 거쳤다. 결과적으로 절차적인 코드가 객체지향적으로 리펙토링 되었다.\n규모가 있는 코드를 한번에 변경하는 것은 매우 어렵다. 따라서 테스트를 기반으로 조금씩 변경하는 과정을 거쳤다. 테스트는 매우 중요하다!\n","date":"2023-04-21T11:00:13+09:00","image":"https://codemario318.github.io/post/clean_code_14/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean_code_14/","title":"클린코드: 14. 점진적인 개선"},{"content":" 창발성: 떠오름 현상, 創發性, emergent property, emergence\n하위 계층에서 없는 특성이나 행동이 상위 계층에서 자발적으로 돌연히 출현하는 현상, 불시에 솟아나는 특성(wikipedia) 남이 모르거나 하지 아니한 것을 처음으로 또는 새롭게 밝혀내거나 이루어 내는 성질(naver dict) 창발적 설계로 깔끔한 코드를 구현하자 간단한 기본 규칙들(하위 계층)을 지키게 되면 자연스럽게 좋은 코드(상위 계층)가 만들어진다(발생한다).\n단순한 설계 규칙 모든 테스트를 실행한다. 중복을 없앤다. 프로그래머의 의도를 표현한다. 크래스와 메서드 수를 최소로 줄인다. 위 4가지 규칙을 지키면 자연스럽게 깨끗한 코드가 만들어 진다.\n코드 구조와 설계를 파악하기 쉬워짐 이에 따라 SRP, DIP 같은 원칙을 적용하기 쉬워짐 모든 테스트를 실행하라 설계는 의도한 대로 돌아가는 시스템을 내놓아야 한다. 완벽한 설계를 따라 시스템을 만들었다고 하더라도 검증이 불가능하다면 완벽한 설계라고 확언할 수 없다.\n이러한 의도한 대로 돌아가는 시스템을 검증하는 방법은 테스트이며, 의도한 대로 돌아가는 시스템은 모든 테스트 케이스를 통과하는 시스템이다.\n따라서 테스트가 불가능한 시스템은 검증이 불가능한 시스템이고, 검증이 불가능하다면 어디서 문제가 발생할 지 예측할 수 없다.\n테스트가 가능한 시스템을 만들려고 애쓰면 설계 품질이 더불어 높아진다.\n크기가 작고 목적 하나만 수행하는 클래스가 나온다. SRP를 준수하는 클래스는 테스트가 훨씬 더 쉽다.\n테스트 케이스가 많을수록 개발자는 테스트가 용이하게 코드를 작성하게 된다. 따라서 철저한 테스트가 가능한 시스템을 만들면 더 나은 설계가 얻어진다\n결합도가 높으면 테스트 케이스를 작성하기 어렵다. 테스트 케이스를 많이 작성할수록 개발자는 DIP와 같은 원칙을 적용하고, 의존성 주입, 인터페이스, 추상화 등과 같은 도구를 사용해 결합도를 낮춘다.\n“테스트 케이스를 만들고 계속 돌려라\u0026ldquo;라는 간단하고 단순한 규칙을 따르면 시스템은 낮은 결합도와 높은 응집력이라는, 객체 지향 방법론이 지향하는 목표를 저절로 달성한다.\n2~4. 리펙터링 테스트 케이스를 모두 작업했다면 코드와 클래스를 정리해도 괜찮다. 코드를 점직적으로 리팩터링 해나간다.\n테스트 케이스로 인해 코드를 정리하면서 시스템이 깨질까 걱정할 필요가 없다.\n리팩터링 단계에서는 소프트웨어 설계 품질을 높이는 기법이라면 무엇이든 적용해도 괜찮다.\n응집도를 높이기 결합도를 낮추기 관심사를 분리하기 시스템 관심사를 모듈로 나누기 함수와 클래스 크기를 줄이기 더 나은 이름을 선택하기 다양한 기법을 동원한다.\n이 단계는 단순한 설계 규칙 중 나머지 3개를 적용해 중복을 제거하고, 프로그래머의 의도를 표현하고, 클래스와 메서드 수를 최소로 줄이는 단계이다.\n중복을 없애라 중복은 추가 작업, 추가 위험, 불필요한 복잡도를 뜻한다. 따라서 우수한 설계에서 중복은 커다란 적이다.\n똑같은 코드 비슷한 코드 구현 중복 비슷한 코드는 더 비슷하게 고쳐주면 리팩터링이 쉬워진다. 구현 중복도 중복의 한 형태이다.\nSet 클래스 예시: 구현 중복 1 2 int size() {} boolean isEmpty() {} Set 클래스에서 위와 같은 메서드가 있다고 가정할 때, 각 메서드를 따로 구현하는 방법도 있다. isEmpty는 부울 값을 반환하고, size는 개수를 반환한다.\n1 2 3 boolean isEmpty() { return 0 == size(); } 여기서 isEmpty를 별도로 구현하지 않고, size를 이용하면 코드를 중복해서 구현할 필요가 없어진다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public void scaleToOneDimension(float desiredDimension, float imageDimension) { if (Math.abs(desiredDimension - imageDimension) \u0026lt; errorThreshold) { return; } float scalingFactor = desiredDimension / imageDimension; scalingFactor = (float)(Math.floor(scalingFactor * 100) * 0.01f); RenderedOp newImage = ImageUtilities.getScaledImage(image, scalingFactor, scalingFactor); image.dispose(); System.gc(); image = newImage; } public synchronized void rotate(int degrees) { RenderedOp newImage = ImageUtilities.getRotatedImage(image, degrees); image.dispose(); System.gc(); image = newImage; } scaleToOneDimension 메서드와 rotate 메서드를 살펴보면 일부 코드가 동일하다. 다음과 같이 코드를 정리해 중복을 제거한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public void scaleToOneDimension(float desiredDimension, float imageDimension) { if (Math.abs(desiredDimension - imageDimension) \u0026lt; errorThreshold) { return; } float scalingFactor = desiredDimension / imageDimension; scalingFactor = (float)(Math.floor(scalingFactor * 100) * 0.01f); replaceImage(ImageUtilities.getScaledImage(image, scalingFactor, scalingFactor)); } public synchronized void rotate(int degrees) { replaceImage(ImageUtilities.getrotatedImage(image, degrees)); } public void replaceImage(RenderedOp newImage) { image.dispose(); System.gc(); image = newImage; } 코드 3줄을 공통 replaceImage 메서드로 뽑아서 중복을 제거했다. 새 메서드로 뽑아내니 기존 클래스가 SRP를 위반하므로, replaceImage를 적합한 다른 클래스로 옮기면 된다.\n이러한 공통 코드 분리를 통해 메서드의 가시성이 높아졌다. 따라서 다른 팀원이 새 메서드를 좀 더 추상화해 다른 맥락에서 재사용할 기회를 포착할지도 모른다.\n이러한 소규모 재사용은 시스템 복잡도를 극적으로 줄여준다. 소규모 재사용을 제대로 익혀야 대규모 재사용이 가능하다.\nTEMPLATE METHOD 패턴 [Design Pattern] 템플릿 메서드 패턴이란 - Heee\u0026rsquo;s Development Blog\n고차원 중복을 제거할 목적으로 자주 사용하는 기법으로 어떤 작업을 처리하는 일부분을 서브 클래스로 캡슐화하여 전체 일을 수행하는 구조는 바꾸지 않으면서 특정 단계에서 수행하는 내역을 바꾸는 패턴\n전체적으로 동일하면서 부분적으로는 다른 구문으로 구성된 메서드의 코드 중복을 최소화 할때 유용함 동일한 기능을 상위 클래스에서 정의하면서 확장/변화가 필요한 부분만 서브 클래스에서 구현할 수 있도록 한다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public class VacationPolicy { public void accrueUSDivisionVacation() { // 지금까지 근무한 시간을 바탕으로 휴가 일수를 계산하는 코드 // ... // 휴가 일수가 미국 최소 법정 일수를 만족하는지 확인하는 코드 // ... // 휴가 일수를 급여 대장에 적용하는 코드 // ... } public void accfrueEUDivisionVacation() { // 지금까지 근무한 시간을 바탕으로 휴가 일수를 계산하는 코드 // ... // 휴가 일수가 유럽연합 최소 법정 일수를 만족하는지 확인하는 코드 // ... // 휴가 일수를 급여 대장에 적용하는 코드 // ... } } 위 코드에서 최소 법정 일수를 계산하는 코드만 제외하면 두 메서드는 거의 동일하다. 최소 법정 일수를 계산하는 알고리즘은 직원 유형에 따라 조금 변한다.\n이러한 구조에서 Template Method 패턴을 적용해 중복을 제거한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 abstract public class Vacation Policy { public void accrueVacation() { calculateBaseVacationHours(); alterForLegalMinimums(); applyToPayroll(); } private void calculateBaseVacationHours() { // 지금까지 근무한 시간을 바탕으로 휴가 일수를 계산하는 코드 // ... }; abstract protected void alterForLegalMinimums(); private void applyToPayroll() { // 휴가 일수를 급여 대장에 적용하는 코드 // ... }; } public class USVacationPolicy extends VacationPolicy { @override protected void alterForLegalMinimums() { // 미국 최소 법정 일수 사용; } } public class EUVacationPolicy extends VacationPolicy { @override protected void alterForLegalMinimums() { // 유럽연합 최소 법정 일수 사용; } } 하위 클래스는 중복되지 않는 정보만 제공해 accrueVacation 알고리즘에서 빠진 구멍(alterForLegalMinimums)을 메운다.\n표현하라 코드르 짜는동안 문제에 푹 빠져 코드를 구석구석 이해하게 되어, 자신만 이해하는 코드를 짜기가 쉽다. 이런 코드는 이후 유지보수할 사람이 코드를 짜는 사람만큼이나 문제를 깊이 이해할 가능성은 희박하다.\n소프트웨어 프로젝트 비용 중 대다수는 장기적인 유지보수에 들어가게 되는데, 코드를 변경하면서 버그 여지를 주지 않으려면 유지보수 개발자가 시스템을 제대로 이해해야 한다.\n하지만 시스템이 점차 복잡해지면서 유지보수 개발자가 시스템을 이해하느라 보내는 시간은 점점 늘어나고 동시에 코드를 오해할 가능성도 점점 커진다.\n따라서 이해하기 쉬운 코드를 짜야하고, 코드에서 개발자의 의도를 분명히 표현하고 명백하게 짤수록 다른 사람이 코드르 이해하기 쉬워진다.\n표현력을 높이는 방법 좋은 이름을 선택한다. 이름과 기능이 완전히 딴판인 클래스나 함수로 혼란을 주면 안된다. 함수와 클래스 크기를 가능한 줄인다. 작은 클래스와 작은 함수는 이름 짓기도 쉽고, 구현하기도 쉽고, 이해하기도 쉽다. 표준 명칭을 사용한다. 디자인 패턴은 의사소통과 표현력 강화가 주요 목적이다. 클래스가 COMMAND나 VISITOR와 같은 표준 패턴을 사용한다면 클래스 이름에 패턴 이름을 넣어준다. 단위 테스트 케이스를 꼼꼼히 작성한다. 테스트 케이스는 소위 ‘예제로 보여주는 문서’다. 잘 만든 테스트 케이스를 읽어보면 클래스 기능이 한눈에 들어온다. 표현력을 높이는 방법은 노력이다. 함수와 클래스에 조금 더 시간을 투자하자. 더 나은 이름을 선택하고, 큰 함수를 작은 함수 여럿으로 나누고, 자신의 작품에 조금만 더 주의를 기울이자!\n나중에 코드를 읽을 사람은 바로 자신일 가능성이 높다.\n클래스와 메소드 수를 최소로 줄여라 중복을 제거하고, 의도를 표현하고, SRP를 준수한다는 기본적인 개념도 극단으로 치달으면 득보다 실이 많아진다.\n클래스와 메서드 크기를 줄이자고 조그만 클래스와 메서드를 수없이 만드는 사례도 없지 않다. 그래서 이 규칙은 함수와 클래스 수를 가능한 줄이라고 제안한다.\n때로는 무의미하고 독단적인 전책 탓에 클래스 수와 메서드 수가 늘어나기도 한다.\n클래스마다 무조건 인터페이스를 생성하라고 요구하는 구현 표준 자료 클래스와 동작 클래스는 무조건 분리해야 한다. 가능한 독단적인 견해는 멀리하고 실용적인 방식을 택한다.\n목표는 함수와 클래스 크기를 작게 유지하면서 시스템 크기도 작게 유지하는 데 있다.\n이 규칙은 간단한 설계 규칙 네 개중 가장 우선순위가 낮다. 즉, 틀래스와 함수 수를 줄이는 작업도 중요하지만, 테스트 케이스를 만들고 중복을 제거하고 의도를 표현하는 작업이 더 중요하다는 뜻이다.\n결론 경험을 대신할 단순한 개발 기법은 없다. 하지만 소개하는 기법은 저자들이 수십 년 동안 쌓은 경험의 정수다.\n💡 단순한 설계 규칙을 따른다면 우수한 기법과 원칙을 단번에 활용할 수 있다.\n","date":"2023-04-21T10:52:13+09:00","image":"https://codemario318.github.io/post/clean_code_12/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean_code_12/","title":"클린코드: 12. 창발성"},{"content":"코드의 표현력과 그 코드로 이루어진 함수에 아무리 신경 쓸지라도 좀 더 차원 높은 단계까지 신경쓰지 않으면 깨끗한 코드를 얻기는 어렵다.\n클래스 체계 표준 자바 관례에 의하면 아래 순서로 선언하게 된다.\npublic static 상수 private static 변수 비공개 인스턴스 변수 공개 변수가 필요한 경우는 거의 없다 공개 함수 비공개 함수 자신을 호출하는 공개 함수 직후에 넣는다. 즉 추상화 단계가 순차적으로 내려간다. 그래서 프로그램은 신문 기사처럼 읽힌다.\n캡슐화 변수와 유틸리티 함수는 가능한 공개하지 않는 편이 낫지만 반드시 숨겨야한다는 법칙도 없다. 때로는 변수나 유틸리티 함수를 protected로 선언해 테스트 코드에 접근을 허용하기도 한다.\n같은 패키지 안에서 테스트 코드가 함수를 호출하거나 변수를 사용해야 한다면 그 함수나 변수를 protected로 선언하거나 패키지 전체로 공개한다.\n하지만 그 전에 비공개 상태를 유지할 방법을 강구해야 한다.\n💡 캡슐화를 풀어주는 결정은 언제나 최후의 수단이다.\n클래스는 작아야 한다. 클래스를 만들 때 첫 번째 규칙은 크기다. 클래스는 작아야 한다. 클래스를 설계할 때도, 함수와 마찬가지로, 작게가 기본 규칙이다.\n얼마나 작아야 할까? 함수는 물리적인 행 수로 크기를 측정했지만 클래스는 맡은 책임 수로 측정한다.\n1 2 3 4 5 6 7 public class SuperDashboard extends JFrame implements MetaDataUser { public Component getLastFocusedCompnent(); public void setLastFocused(Component lastFocused); public int getMajorVersionNumber(); public int getMinorVersionNumber(); public int getBuildNumber(); } 메서드 개수를 5개 정도로 줄인 클래스이다. 하지만 메서드 수가 적음에도 불구하고 책임이 너무 많다.\n클래스 이름 클래스 이름은 해당 클래스 책임을 기술해야 한다. 실제로 클래스 작명은 크기를 줄이는 첫 번째 관문이다.\n간결한 이름이 떠오르지 않는다면 대부분 클래스 크기가 너무 커서(책임이 많아서) 그렇다. 예를 들어 클래스 이름에 Processor, Manager, Super 등과 같이 모호한 단어가 있다면 클래스가 여러 책임을 떠안고있다는 뜻이다.\n클래스 설명 클래스 설명은 만일(if), 그리고(and), (하)며(or), 하지만(but)을 사용하지 않고서 25단어 내외로 가능해야 한다.\n위 SuperDashboard 의 설명을 만들면\n“마지막으로 포커스를 얻었던 컴포넌트에 접근하는 방법을 제공하며, 버전과 빌드 번호를 추적하는 메커니즘을 제공한다.”\n인데 “~하며”가 포함되므로 책임이 많다고 생각할 수 있다.\n단일 책임 원칙 - SRP: Single Responsibility Principle 단일 책임 원칙은 클래스나 모듈을 변경할 이유가 단 하나뿐이어야 한다는 원칙이다.\nSRP는 ‘책임’이라는 개념을 정의하며 적절한 클래스 크기를 제시한다. 클래스는 책임, 즉 변경할 이유가 하나여야 한다는 의미다.\n겉보기에 작아보이는 위 SuperDashboard는 변경할 이유가 2가지다.\n소프트웨어 버전 정보를 추적한다. 버전 정보는 소프트웨어를 출시할 때마다 달라진다. 자바 스윙 컴포넌트를 관리한다. SuperDashboard는 최상위 GUI 윈도의 스윙 표현인 JFrame에서 파생한 클래스다. 즉, 스윙 코드를 변경할 때마다 버전 번호가 달라진다. 책임(변경할 이유)을 파악하려 애쓰다 보면 코드를 추상화 하기도 쉬워진다.\n1 2 3 4 5 public class Version { public int getMajorVersionNumber(); public int getMinorVersionNumber(); public int getBuildNumber(); } 버전 정보를 다루는 메서드 세 개를 따로 빼내 version이라는 독자적인 클래스를 만든다.\nSRP는 객체 지향 설계에서 더욱 중요한 개념이다. 또한 이해하고 지키기 수월한 개념이기도 하다. 하지만 이상하게도 SRP는 클래스 설계자가 가장 무시하는 규칙 중 하나다.\n소프트웨어를 돌아가게 만드는 활동과 소프트웨어를 깨끗하게 만드는 활동은 완전히 별개다.\n개발자는 자잘한 책임 클래스가 많아지면 큰 그림을 이해하기 어려워진다고 우려한다. 큰 그림을 이해하려면 이 클래스 저 클래스를 수없이 넘나들어야 한다고 걱정한다.\n하지만 작은 클래스가 많은 시스템이든 큰 클래스가 몇 개뿐인 시스템이든 익힐 내용은 그 양이 비슷하다.\n클래스를 작게 유지하는 행위는 코드를 체계적으로 정리하는 과정으로, 단일 책임을 가진 클래스를 만들고 이름을 잘 붙인다면 자연스럽게 코드가 정리된다.\n책임이 많은 클래스는 큰 공구함에 여러 공구를 마구 때려넣는 행동과 비슷하다. 응집도 클래스는 인스턴스 변수 수가 적어야 한다. 각 클래스 메서드는 클래스 인스턴스 변수를 하나 이상 사용해야 한다.\n일반적으로 메서드가 변수를 더 많이 사용할수록 메서드와 클래스는 응집도가 더 높다. 모든 인스턴스 변수를 메서드마다 사용하는 클래스는 응집도가 가장 높다.\n일반적으로 이처럼 응집도가 가장 높은 클래스는 가능하지도 바람직하지도 않다. 그렇지만 대부분 응집도가 높은 클래스를 선호한다. 응집도가 높다는 말은 클래스에 속한 메서드와 변수가 서로 의존하며 논리적인 단위로 묶인다는 의미다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class Stack { private int topOfStack = 0; List\u0026lt;Integer\u0026gt; elements = new LinkedList\u0026lt;Integer\u0026gt;(); public int size() { return topOfStack; } public void push(int element) { topOfStack++; elements.add(element); } public int pop() throws PoppedWhenEmpty { if (topOfStack == 0) { throw new PoppedWhenEmpty(); } int element = elements.get(--topOfStack); elements.remove(topOfStack); return element; } } 위 클래스는 응집도가 아주 높다. size()를 제외한 다른 두 메서드는 두 변수를 모두 사용한다.\n함수를 작게, 매개변수 목록을 짧게라는 전략을 따르다 보면 때때로 몇몇 메서드만이 사용하는 인스턴스 변수가 아주 많아진다.\n응집도가 높아지도록 변수와 메서드를 적절히 분리해 새로운 클래스 두세 개로 쪼개준다.\n응집도를 유지하면 작은 클래스 여럿이 나온다. 큰 함수를 작은 함수 여럿으로 나누기만 해도 클래스 수가 많아진다.\n예를 들어 변수가 아주 많은 큰 함수 하나가 있다. 큰 함수 일부를 작은 함수 하나로 빼내고 싶은데, 빼내려는 코드가 큰 함수에 정의된 변수 넷을 사용한다. 그렇다면 변수 네 개를 새 함수에 인수로 넘겨야 옳은가?\n전혀 아니다. 만약 네 변수를 클래스 인스턴스 변수로 승격한다면 새 함수는 인수가 필요없다. 그만큼 함수를 쪼개기 쉬워진다.\n이렇게 하면 클래스가 응집력을 잃는다. 몇몇 함수만 사용하는 인스턴스 변수가 점점 더 늘어나기 때문이다.\n몇몇 함수가 몇몇 변수만 사용한다면 독자적인 클래스로 분리하면 된다. 응집력을 잃는다면 쪼개라\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 package literatePrimes; public class PrintPrimes { public static void main(String[] args) { final int M = 1000; final int RR = 50; final int CC = 4; final int WW = 10; final int ORDMAX = 30; int P[] = new int[M+1]; int PAGENUMBER; int PAGEOFFSET; int ROWOFFSET; int C; int J; int K; boolean JPRIME; int ORD; int SQUARE; int N; int MULT[] = new int[ORDMAX+1]; J = 1; K = 1; P[1] = 2; ORD = 2; SQUARE = 9; while (K \u0026lt; M) { do { J = J+2; if (J == SQUARE) { ORD = ORD + 1; SQUARE = P[ORD] * P[ORD]; MULT[ORD-1] = J; } N = 2; JPRIME = true; while (N \u0026lt; ORD \u0026amp;\u0026amp; JPRIME) { while (MULT[N] \u0026lt; J) { MULT[N] = MULT[N] + P[N] + P[N]; } if (MULT[N] == J) { JPRIME = false; } N = N + 1; } } while (!JPRIME); K = K + 1; P[K] = J; } { PAGENUMBER = 1; PAGEOFFSET = 1; while (PAGEOFFSET \u0026lt;= M) { System.out.println(\u0026#34;The First \u0026#34; + M + \u0026#34; Prime Numbers --- Page \u0026#34; + PAGENUMBER); System.out.println(\u0026#34;\u0026#34;); for (ROWOFFSET = PAGEOFFSET; ROWOFFSET \u0026lt; PAGEOFFSET + RR; ROWOFFSET++) { for (C = 0; C \u0026lt; CC; C++) { if (ROWOFFSET + C * RR \u0026lt;= M) { System.out.println(\u0026#34;%10d\u0026#34;, P[ROWOFFSET + C * RR]); } } System.out.println(\u0026#34;\u0026#34;); } System.out.println(\u0026#34;\\f\u0026#34;); PAGENUMBER = PAGENUMBER + 1; PAGEOFFSET = PAGEOFFSET + RR * CC; } } } } 함수가 하나뿐인 위 프로그램은 엉망이다.\n들여쓰기 심함 이해하기 힘든 변수 이름 구조가 빽빽함 → 여러함수로 나눠야한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package literatePrimes; public class PrimePrinter { public static void main(String[] args) { final int NUMBER_OF_PRIMES = 1000; int[] primes = PrimeGenerator.generate(NUMBER_OF_PRIMES); final int ROWS_PER_PAGE = 50; final int COLUMNS_PER_PAGE = 4; RowColumnPagePrinter tablePrinter = new RowColumnPagePrinter(ROWS_PER_PAGE, COLUMNS_PER_PAGE, \u0026#34;The First \u0026#34; + NUMBER_OF_PRIMES + \u0026#34; Prime Numbers\u0026#34;); tablePrinter.print(primes); } } ... 가장 먼저 눈에 띄는 변화가 프로그램이 길어졌다. 길이가 늘어난 이유는 여러가지다.\n리팩터링한 프로그램은 좀 더 길고 서술적인 변수 이름을 사용한다. 리팩터링한 프로그램은 코드에 주석을 추가하는 수단으로 함수 선언과 클래스 선언을 활용한다. 가독성을 높이고자 공백을 추가하고 형식을 맞추었다. PrimePrinter 클래스는 main 함수 하나만 포함하며 실행 환경을 책임진다. 호출 방식이 달라지면 클래스도 바뀐다. 예를들어 SOAP 서비스로 바꾸려면 PrimePrinter 클래스를 고친다.\nPrimeGenerator 클래스는 소수 목록을 생성하는 방법을 안다. 코드를 살펴보면 알겠지만, 객체로 인스턴스화하는 클래스가 아니다. 단순히 변수를 선언하고 감추려고 사용하는 유용한 공간일 뿐이다.\n소수를 계산하는 알고리즘이 바뀐다면 PrimeGenerator 클래스를 고친다. 변경하기 쉬운 클래스 대다수 시스템은 지속적인 변경이 가해진다. 그리고 뭔가 변경할 때마다 시스템이 의도대로 동작하지 않을 위험이 따른다. 깨끗한 시스템은 클래스를 체계적으로 정리해 변경에 수반하는 위험을 낮춘다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class Sql { public Sql(String table, Column[] columns); public String create(); public String insert(Object[] fields); public String selectAll(); public String findByKey(String keyColumn, String keyValue); public String select(Column column, String pattern); public String select(Criteria criteria); public String preparedInsert(); private String columnList(Column[] columns); private String valuesList(Object[] fields, final Column[] columns); private String selectWithCriteria(String criteria); private String placeholderList(Column[] columns); } update를 아직 지원하지 않는 코드다. 지원할 시점이 오면 클래스에 손대어 코쳐야 한다. 문제는 코드에 손대면 위험이 생긴다는 사실이다. 어떤 변경이든 클래스에 손대면 다른 코드를 망가뜨릴 잠정적인 위험이 존재한다.\n새로운 SQL문을 지원하려면 반드시 Sql 클래스에 손대야 한다. 또한 기존 SQL문 하나를 수정할 때도 반드시 Sql 클래스에 손대야 한다. 이렇듯 변경할 이유가 두 가지이므로 Sql 클래스는 SRP를 위반한다.\n단순히 구조적인 관점에서도 Sql은 SRP를 위반한다. selectWithCriteria라는 비공개 메서드가 있는데, 이 메서드는 select문을 처리할 때만 사용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 abstract public class Sql { public Sql(String table, Column[] columns) abstract public String generate(); } public class CreateSql extends Sql { public CreateSql(String table, Column[] columns) @Override public String generate(); } public class SelectSql extends Sql { public SelectSql(String table, Column[] columns) @Override public String generate(); } public class InsertSql extends Sql { public InsertSql(String table, Column[] columns) @Override public String generate(); } public class SelectWithCriteraSql extends Sql { public SelectWithCriteraSql(String table, Column[] columns) @Override public String generate(); } public class SelectWithMatchSql extends Sql { public SelectWithMatchSql(String table, Column[] columns, Criteria criteria) @Override public String generate(); } public class FindByKeySql extends Sql { public FindByKeySql(String table, Column[] columns, Column column, String pattern) @Override public String generate(); } public class CreateSql extends Sql { public CreateSql(String table, Column[] columns, String keyColumn, String keyValue) @Override public String generate(); } public class PreparedInsertSql extends Sql { public PreparedInsertSql(String table, Column[] columns) @Override public String generate(); private String placeholderList(Column[] columns); } public class Where extends Sql { public Where(String criteria) @Override public String generate(); } public class ColumnList extends Sql { public ColumnList(Column[] columns) @Override public String generate(); } 공개 인터페이스를 각각 Sql클래스에서 파생하는 클래스로 만들었다. valueList와 같은 비공개 메서드는 해당하는 파생 클래스로 옮겼다. 모든 파생 클래스가 공통으로 사용하는 비공개 메서드는 Where와 ColumnList라는 두 유틸리티 클래스에 넣었다.\n클래스를 서로 분리하여 각 클래스는 극도로 단순하며 이해하기 쉽다. 함수 하나를 수정했다고 다른 함수가 망가질 위험도 사실상 없다. 테스트 관점에서 모든 논리를 구석구석 증명하기도 쉬워졌다. 새로운 sql을 넣을때도 기존 클래스를 변경할 필요가 전혀 없다.\n새 기능을 수정하거나 기존 기능을 변경할 때 건드릴 코드가 최소인 시스템 구조가 바람직하다. 이상적인 시스템이라면 새 기능을 추가할 때 시스템을 확장할 뿐 기존 코드를 변경하지 않는다. 변경으로부터 격리 요구사항은 변하기 마련이다. 따라서 코드도 변하기 마련이다.\n상세한 구현에 의존하는 코드는 테스트가 어렵다. 예를들어 Portfolio 클래스를 만든다고 가정하자.\nPortfolio 클래스는 외부 TokyoStockExchange API를 이용해 포트폴리오 값을 계삲나다. 따라서 테스트 코드는 시세 변화에 영향을 받게 된다.\n5분마다 값이 달라지는 API로 테스트 코드를 짜기는 어렵다.\n1 2 3 public interface StorckExchange { Money CurrentPrice(String symbol); } Portfolio에서 TokyoStockExchange API를 직접 호출하는 대신 StockExchange라는 인터페이스를 생성한 후 메서드 하나를 선언했다.\n다음으로 StockExchange 인터페이스를 구현하는 TokyoStockExchange 클래스를 구현한다.\n1 2 3 4 5 6 7 public Portfolio { private StockExchange exchange; public Portfolio(StockExchange exchange) { this.exchange = exchange; } ... } Portfolio를 수정해 StockExchange 참조자를 인수로 받는다.\n이를 통해 TokyoStockExchange 클래스를 흉내내는 테스트용 클래스를 만들 수 있게 되었다.\n테스트용 클래스는 StockExchange 인터페이스를 구현하며 고정된 주가를 반환한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public class PortfolioTest { private FixedStockExchangeStub exchange; private Portfolio portfolio; @Before protected void setUp() throws Exception { exchange = new FixedStockExchangeStub(); exchange.fix(\u0026#34;MSFT\u0026#34;, 100); protfolio = new Portfolio(exchange); } @Test public void GivenFiveMSFTTotalShouldBe500() throws Exception { portfolio.add(5, \u0026#34;MSFT\u0026#34;); Assert.assertEquals(500, portfolio.value()); } } 위와 같은 테스트가 가능할 정도로 시스템의 결함도를 낮추면 유연성과 재사용성도 더욱 높아진다.\n결합도가 낮다는 소리는 각 시스템 요소가 다른 요소로부터 그리고 변경으로부터 잘 격리되어 있다는 의미다. 시스템 요소가 서로 잘 결리되어 있으면 각 요소를 이해하기도 더 쉬워진다. 이렇게 결합도를 최소로 줄이면 자연스럽게 또 다른 클래스 설계 원칙인 DIP(Dependency Inversion Principle)를 따르는 클래스가 나온다.\n본질적으로 DIP는 클래스가 상세한 구현이 아니라 추상화에 의존해야 한다는 원칙이다.\n결론 공개가 필요할땐 최대한 캡슐화를 유지하는 방식을 강구해야 한다. 클래스도 작게 유지해야 한다. 클래스의 크기는 책임 수로 측정한다. 책임이 많은 클래스는 이름과 설명에 모호한 표현이 섞이게 된다. 응집도를 유지하면 자연스럽게 클래스가 작아진다. 변경은 여러 문제들을 수반한다. 따라서 기존 코드 변경이 없는것이 좋다. 그대로 변경은 항상 필요할 수 있으므로 변경하기 쉽게 만들어야 한다. 클래스를 깨끗하게 정리하면 변경하기 쉽다. 클래스를 작게 만들면 자연스럽게 정리된다. 결합도를 최대한 낮추면 변경으로 부터 격리할 수 있다. ","date":"2023-04-20T16:30:13+09:00","image":"https://codemario318.github.io/post/clean_code_10/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean_code_10/","title":"클린코드: 10. 클래스"},{"content":"애자일과 TDD 덕택에 단위 테스트를 자동화하는 개발자들이 이미 많아졌으며 점점 더 늘어나는 추세다.\n테스트를 급하게 서두르는 와중에 많은 개발자들이 제대로 된 테스트 케이스를 작성해야한다는 중요한 사실을 놓친다.\nTDD 법칙 세 가지 실패하는 단위 테스트를 작성할 때까지 실제 코드를 작성하지 않는다. 컴파일은 실패하지 않으면서 실행이 실패하는 정도로만 단위 테스트를 작성한다. 현재 실패하는 테스트를 통과할 정도로만 실제 코드를 작성한다. 위 세 가지 규칙을 따르면 개발과 테스트가 대략 30초 주기로 묶인다. 테스트 코드와 실제 코드가 함께 나올뿐더러 테스트 코드가 실제 코드보다 불과 몇 초 전에 나온다.\n이렇게 일하면 실제 코드를 사실상 전부 테스트하는 테스트 케이스가 나오지만, 실제 코드와 맞먹을 정도로 방대한 테스트 코드는 심각한 관리 문제를 유발하기도 한다.\n깨끗한 테스트 코드 유지하기 테스트 코드는 실제 코드 못지 않게 중요하다. 실제 코드 못지 않게 깨끗하게 짜야한다.\n지저분한 테스트 코드는 테스트를 하지 않는 것 보다 더 안좋은 결과를 만들 수 있다.\n단위 테스트는 코드 변경에 대한 안전함을 보장하여 유연성, 유지보수성, 재사용성을 지키는 버팀목이 된다.\n실제 코드가 진화하면 테스트 코드도 변해야 하는데, 테스트 코드가 지저분할수록 실제 코드를 변경하기 어려워진다. 결과적으로 테스트 코드가 복잡할수록 실제 코드를 짜는 시간보다 테스트 케이스를 추가하는 시간이 더 걸리게 된다.\n실제 코드를 변경해 기존 테스트 케이스가 실패하기 시작하면, 지저분한 코드로 인해, 실패하는 테스트 케이스를 점점 더 통과시키키 어려워진다. 그래서 테스트 코드는 계속해서 늘어나는 부담이 되고 악순환이 발생할 수 있다.\n테스트 슈트가 없으면 개발자는 자신이 수정한 코드가 제대로 도는지 확인할 방법이 없다. 시스템을 수정했을때 다른 문제가 발생하지 않는다는 것을 확인할 방법이 없다. 결함율이 높아지기 시작한다. 의도하지 않은 결함 수가 많아지면 개발자는 변경을 주저한다. 변경하면 득보다 해가 크다 생각해 더 이상 코드를 정리하지 않는다. 테스트는 유연성, 유지보수성, 재사용성을 제공한다. 테스트 코드를 깨끗하게 유지하지 않으면 결국은 잃어버린다. 그리고 테스트 케이스가 없으면 실제 코드를 유연하게 만드는 버팀목도 사라진다.\n테스트 케이스가 없다면 모든 변경이 잠정적인 버그다. 아무리 아키텍처가 유연하더라도, 설계를 아무리 잘 했어도 버그가 없다는 것을 증명할 수 없기 때문이다.\n깨끗한 테스트 코드 가독성, 가독성, 가독성!\n가독성은 실제 코드보다 테스트 코드에 더더욱 중요하다. 테스트 코드의 가독성을 높히려면 여느 코드와 마찬가지로 명료성, 단순성, 풍부한 표현력이 필요하다.\n테스트 코드는 최소 표현으로 많은 것을 나타내야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public void testGetPageHierarchyAsXml() throws Exception { makePages(\u0026#34;pageOne\u0026#34;, \u0026#34;PageOne.ChildOne\u0026#34;, \u0026#34;PageTwo\u0026#34;); submintRequest(\u0026#34;root\u0026#34;, \u0026#34;type:pages\u0026#34;); assertResponseIsXML(); assertResponseContains( \u0026#34;\u0026lt;name\u0026gt;PageOne\u0026lt;/name\u0026gt;\u0026#34;, \u0026#34;\u0026lt;name\u0026gt;PageTwo\u0026lt;/name\u0026gt;\u0026#34;, \u0026#34;\u0026lt;name\u0026gt;ChildOne\u0026lt;/name\u0026gt;\u0026#34; ); assertResponseDoesNotContain(\u0026#34;SymPage\u0026#34;); } public void testGetDataAsXml() throws Exception { makePageWithContent(\u0026#34;TestPageOne\u0026#34;, \u0026#34;test page\u0026#34;); submintRequest(\u0026#34;TestPageOne\u0026#34;, \u0026#34;type:data\u0026#34;); assertResponseIsXML(); assertResponseContains(\u0026#34;test page\u0026#34;, \u0026#34;\u0026lt;Test\u0026#34;); } BUILD-OPERATE-CHECK 패턴 Usual production patterns applied to Integration tests\n테스트를 세 부분으로 나눈다.\nBUILD: 테스트 자료를 만든다. OPERATE: 테스트 자료를 조작한다. CHECK: 조작한 결과가 올바른지 확인한다. BUILD-OPERATE-CHECK 패턴을 사용하면 잡다하고 세세한 코드를 거의 다 없앨 수 있다.\n도메인에 특화된 테스트 언어 도메인 특화 언어(DSL)에 관한 설명 | JetBrains MPS\n흔히 쓰는 시스템 조작 API를 사용하는 대신 API 위에다 함수와 유틸리티를 구현한 후 그함수와 유틸리티를 사용하므로 테스트 코드를 짜기도 읽기도 쉬어진다.\n이렇게 구현한 함수와 유틸리티는 테스트 코드에서 사용하는 특수 API가 된다. 즉 테스트를 구현하는 당사자와 나중에 테스트를 읽어볼 독자를 도와주는 테스트 언어이다.\n이중 표준 테스트 API 코드에 적용하는 표준은 실제 코드에 적용하는 표준과 확실히 다르다.\n단순하고, 간결하고, 표현력이 풍부해야 하지만, 실제 코드만큼 효율적일 필요는 없다.\n또한 실제 환경과 테스트 환경은 요구사항이 판이하게 다르다.\n1 2 3 4 5 6 7 8 9 10 @Test public void turnOnLoTempAlarmAtThreashold() throws Exception { hw.setTemp(WAY_TOO_COLD); controller.tic(); assertTrue(hw.heaterState()); assertTrue(hw.blowerState()); assertFalse(hw.coolerState()); assertFalse(hw.hiTempAlarm()); assertTrue(hw.loTempAlarm()); } 1 2 3 4 5 @Test public void turnOnLoTempAlarmAtThreashold() throws Exception { wayTooCold(); assertEquals(\u0026#34;HBchL\u0026#34;, hw.getState()); } 실제 시스템이 돌아가는 환경은 가혹할 지 모르나, 대부분 테스트 환경은 자원이 제한적일 가능성이 낮다.\n실제 환경에서는 절대로 안 되지만 테스트 환경에서는 전현 문제없는 방식이 있다. 대개 메모리나 CPU 효율과 관련있는 경우인데 코드의 깨끗함과는 철저히 무관하다.\n테스트 당 assert 하나 assert문이 단 하나인 함수는 결론이 하나라서 코드를 이해하기 쉽고 빠르다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 public void testGetPageHierarchyAsXml() throws Exception { givenPages(\u0026#34;PageOne\u0026#34;, \u0026#34;pageOne.ChildOne\u0026#34;, \u0026#34;PageTwo\u0026#34;); whenRequestIsIssued(\u0026#34;root\u0026#34;, \u0026#34;type:pages\u0026#34;); thenResponseShouldBeXML(); } public void testGetPageHiereachyHasRightTags() throws Exception { givenPages(\u0026#34;PageOne\u0026#34;, \u0026#34;pageOne.ChildOne\u0026#34;, \u0026#34;PageTwo\u0026#34;); whenRequestIsIssued(\u0026#34;root\u0026#34;, \u0026#34;type:pages\u0026#34;); thenResponseShouldContain( \u0026#34;\u0026lt;name\u0026gt;PageOne\u0026lt;/name\u0026gt;\u0026#34;, \u0026#34;\u0026lt;name\u0026gt;PageTwo\u0026lt;/name\u0026gt;\u0026#34;, \u0026#34;\u0026lt;name\u0026gt;ChildOne\u0026lt;/name\u0026gt;\u0026#34; ); } 테스트를 분리하면 위 함수 1~2처럼 중복되는 코드가 많아지는데, TEMPLATE METHOD 패턴을 사용하면 중복을 제거할 수 있다.\ngiven/when 부분을 부모 클래스에 두고 소두 부분을 자식 클래스에 두면 된다. 아니면 완전히 독자적인 테스트 클래스를 만들어 @Before 함수에 given/when 부분을 넣고 @Test 함수에 then부분을 넣어도 된다.\n하지만 두 경우 모두 과도한 느낌이 있는데 이것저것 감안해 보면 결국 assert 문을 여렷 사용하는 편이 좋을수도 있다.\n💡 단일 assert를 지원하는 해당 분야 테스트 언어를 만드는 것이 이점이 많으나, 과하다면 꼭 지킬 필요는 없다. 하지만 assert 문 개수를 최대한 줄이려고 노력해야 한다.\n테스트당 개념 하나 테스트 함수마다 한 개념만 테스트하라. 이것저것 잡다한 개념을 연속으로 테스트하는 긴 함수는 피한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 /** * addMonths() 메서드를 테스트하는 장황한 코드 */ public void testAddMonths() { SerialDate d1 = SerialDate.createInstance(31, 5, 2004); SerialDate d2 = SerialDate.addMonths(1, d1); assertEquals(30, d2.getDayOfMonth()); assertEquals(6, d2.getMonth()); assertEquals(2004, d2.getYYYY()); SerialDate d3 = SerialDate.addMonths(2, d1); assertEquals(31, d3.getDayOfMonth()); assertEquals(7, d3.getMonth()); assertEquals(2004, d3.getYYYY()); SerialDate d4 = SerialDate.addMonths(1, SerialDate.addMonths(1, d1)); assertEquals(30, d4.getDayOfMonth()); assertEquals(7, d4.getMonth()); assertEquals(2004, d4.getYYYY()); } 위 테스트 코드는 독자적인 개념 세 개를 테스트하므로 독자적인 테스트 세개로 쪼개는 것이 좋다. 새 개념을 한 함수로 몰아넣으면 독자가 각 절이 존재하는 이유와 테스트하는 개념을 모두 이해해야 하기 때문이다.\n31일로 끝나는 달의 마지막 날짜가 주어지는 경우 30일로 끝나는 한 달을 더하면 날짜는 30일이 되어야지 31일이 되어서는 안된다. 두 달을 더하면 그리고 두 번째 달이 31일로 끝나면 날짜는 31일이 되어야 한다. 30일로 끝나는 달의 마지막 날짜가 주어지는 경우 31일로 끝나는 한 달을 더하면 날짜는 30일이 되어야지 31일이 되면 안된다. 위 테스트코드는 assert문이 여렷이라는 사실이 문제가 아니라, 한 테스트 함수에서 여러 개념을 테스한다는 사실이 문제다.\n“개념 당 assert 문 수를 최소로 줄여라”와 “테스트 함수 하나는 개념 하나만 테스트하라”\nF.I.R.S.T 깨끗한 테스트는 다음 다섯가지 규칙을 따른다.\nF(Fast) 빠르게\n테스트는 빨라야 한다. 테스트가 느리면 자주 돌릴 엄두를 못 낸다, 자주 돌리지 않으면 초반에 문제를 찾아내기 힘들다. 이에 따라 코드를 마음껏 정리하지 못해지고, 결국 코드 품질이 망가지기 시작한다.\nI(Independent) 독립적으로\n각 테스트는 서로 의존하면 안된다.\n한 테스트가 다음 테스트가 실행될 환경을 준비해서는 안된다. 각 테스트는 독립적으로 그리고 어떤 순서로 실행해도 괜찮아야 한다.\n테스트가 서로에게 의존하면 하나가 실패할 때 나머지도 잇달아 실패하므로 원인을 진단하기 어려워지며 후반 테스트가 찾아내야 할 결함을 숨긴다.\nR(Repeatable) 반복가능하게\n테스트는 어떤 환경에서도 반복 가능해야 한다.\n테스트가 돌아가지 않는 환경이 하나라도 있다면 테스트가 실패한 이유를 둘러댈 변명이 생긴다. 또한 환경이 지원되지 않기에 테스트를 수행하지 못하는 상황에 직면한다.\nS(Self-Validating) 자가검증하는\n테스트는 부울 값으로 결과를 내야 한다.\n성공 아니면 실패다. 통과 여부를 알려고 로그 파일을 읽게 만들어서는 안 된다. 통과 여부르 보려고 텍스트 파일 두개를 수작업으로 비교하게 만들어서도 안 된다.\nT(Timely) 적시에\n테스트는 적시에 작성해야 한다.\n단위 테스트는 테스트하려는 실제 코드를 구현하기 직전에 구현한다.\n실제 코드를 구현한 다음에 테스트 코드를 만들면 실제 코드가 테스트에 적합하지 않은 구조로 만들어 질 수 있다.\n결론 테스트 코드는 실제 코드만큼이나 프로젝트 건강에 중요하다. 어쩌면 실제 코드보다 더 중요할지도 모른다.\n테스트 코드가 방치되어 망가지면 실제 코드도 망가진다.\n내 결론 테스트 코드는 변경에 대한 안전장치, 버팀목이며 이를 통해 코드에 유연성, 유지보수성, 재사용성을 보존하고 강화한다. 테스트 코드를 깨끗히 유지하지 않으면, 테스트 코드 작성, 유지가 어려워져 방치하게 된다. 결국 코드 변경에 대한 두려움을 만들게 되고 이러한 두려움이 실제 코드의 품질을 점점 떨어뜨린다. 테스트 코드는 가독성이 가장 중요하다. 테스트 API를 구현해 도메인 특화 언어를 만들자. 도메인에 최적화된 테스트 코드를 만들자. assert문은 최소한으로 유지하자 여러개의 assert문이 있다는 뜻은 코드 자체 가독성을 떨어뜨린다. assert 문이 여러개면 여러 개념을 테스트 하고 있을 지도 모른다. 테스트 코드 하나는 하나의 개념만 테스트 해야한다. 여러 개념을 테스트하면 테스트 코드가 어떤 동작을 테스트하는지 알기 위해 고민해야한다. (가독성이 떨어진다) 테스트 코드를 먼저 만드는 이유는 테스트에 적합한 실제 코드를 만드는데 도움이 되기 때문이다. ","date":"2023-04-20T16:22:13+09:00","image":"https://codemario318.github.io/post/clean_code_9/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean_code_9/","title":"클린코드: 9. 단위 테스트"},{"content":" 뭔가 잘못되면 바로잡을 책임은 바로 우리 프로그래머에게 있다.\n깨끗한 코드와 오류처리는 연관이 있다. 상당수 코드들은 전적으로 오류 처리 코드에 좌우된다. 여기저기 흩어진 오류 처리 코드 때문에 실제 코드가 하는 일을 파악하기가 거의 불가능하다는 의미다.\n💡 오류 처리는 중요하지만 이로 인해 프로그램 논리를 이해하기 어려워진다면 깨끗한 코드라 부르기 어렵다.\n오류 코드보다 예외를 사용하라 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public class DeviceController { ... piblic void sendShutDown() { DeviceHandle handle = getHandle(DEV1); if (handle != DeviceHandle.INVALID) { retrieveDeviceRecord(handle); if (record.getStatus() != DEVICE_SUSPENDED) { pauseDevice(handle); clearDeviceWorkQueue(handle); closeDevice(handle); } else { logger.log(\u0026#34;Device suspended. Unable to shut down\u0026#34;); } } else { logger.log(\u0026#34;Invalid handle for: \u0026#34; + DEV1.toString()); } } ... } 위와 같은 방법을 사용하면 호출자 코드가 복잡해진다. 함수를 호출한 즉시 오류를 확인해야 하기 때문이다.\n오류가 발생하면 예외를 던지면, 실제 구현 로직이 오류 처리 코드와 섞이지 않게 되어 호출자가 깔끔해진다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class DeviceController { ... public void sendShutDown() { try { tryToShutDown(); } catch (DeviceShutDownError e) { logger.log(e); } } private void tryToshutDown() throw DeviceShutDownError { DeviceHandle hadle = getHandle(DEV1); DeviceRecord record = retrieveDeviceRecord(handle); pauseDevice(handle); clearDeviceWorkQueue(handle); closeDevice(handle); } private DeviceHandle getHandle(DeviceId id) { ... throw new DeviceShutDownError(\u0026#34;Invalid handle for: \u0026#34; + id.toString()); } ... } 뒤섞여있던 디바이스 종료 로직과 오류 처리 로직이 분리되었다. 이를 통해 각 개념을 독립적으로 살펴보고 이해할 수 있다.\nTry-Catch-Finally 문부터 작성하라 예외처리는 로직 내부에 범위를 정의한다.\ntry-catch-finally 문에서 try 블록에 들어가는 코드를 실행하면 어느 시점에서든 실행이 중단된 후 catch 블록으로 넘어갈 수 있다.\ntry 블록에서 무슨 일이 생겨도 catch 블록은 프로그램 상태를 일관성 있게 유지해야 한다.\n예외가 발생할 코드를 짤 때는 try-catch-finally 문을 만들어놓고 시작하면 try 블록에서 무슨 일이 생기든지 호출자가 기대하는 상태를 정의하기 쉬워진다.\n1 2 3 4 5 // 파일이 없으면 예외를 던지는지 알아보는 단위 테스트 @Test(expcted = StroageException.class) public void retrieveSectionShouldThrowOnInvalidFileName() { sectionStore.retrieveSection(\u0026#34;invalid - file\u0026#34;); } 1 2 3 public List\u0026lt;RecordedGrip\u0026gt; retrieveSection(String sectionName) { return new ArrayList\u0026lt;RecordedGrip\u0026gt;(); } 코드가 예외를 던지지 않으므로 단위 테스트는 실패한다. 아래 코드는 예외를 던진다.\n1 2 3 4 5 6 7 8 public List\u0026lt;RecordedGrip\u0026gt; retrieveSevtion(String sectionName) { try { FileInputStream stream = new FileInputStream(sectionName); } catch (Exception e) { throw new StorageException(\u0026#34;retrieval error\u0026#34;, e); } return new ArrayList\u0026lt;RecordedGrip\u0026gt;(); } 코드가 예외를 던지므로 이제는 테스트가 성공한다. 이 시점에서 리팩터링이 가능하다.\ncatch 블록에서 예외 유형을 좁혀 실제로 FileInputStream 생성자가 던지는 FileNotFoundException을 잡아낸다.\n1 2 3 4 5 6 7 8 9 public List\u0026lt;RecordedGrip\u0026gt; retrieveSevtion(String sectionName) { try { FileInputStream stream = new FileInputStream(sectionName); stream.close(); } catch (FileNotFoundException e) { throw new StorageException(\u0026#34;retrieval error\u0026#34;, e); } return new ArrayList\u0026lt;RecordedGrip\u0026gt;(); } try-catch 구조로 범위를 정의했으므로 TDD를 이용해 필요한 나머지 논리를 추가한다. 나머지 논리는 FileInputStream을 생성하는 코드와 close 호출문 사이에 넣으며 오류나 예외가 전혀 발생하지 않는다고 가정한다.\n미확인(unchecked) 예외를 사용하라 여러 해 동안 자바 프로그래머들은 확인된(checked) 예외의 장단점을 놓고 논쟁을 벌여왔다. 안정적인 소프트웨어를 제작하는 요소로 확인된 예외가 반드시 필요하지 않다는 사실이 분명해졌다.\nJava의 체크 된 예외와 체크되지 않은 예외의 차이점\n확인된 예외: 컴파일 과정에서 발견되는 예외\nRuntimeExeption 클래스를 제외한 Exeption 클래스의 모든 하위 클래스 Error클래스와 그 하위 클래스 확인되지 않은 예외: 프로그램 실행 중 발생하는 예외, 컴파일러가 확인할 수 없는 예외\nRuntimeExeption 클래스와 해당 하위 클래스 확인된 예외는 OCP(Open Closed Principle) 를 위반한다.\n메서드에서 확인된 예외를 던졌는데 catch 블록이 세 단계 위에 있다면 그사이 메서드 모두가 선언부에 해당 예외를 정의해야 한다.\n하위 단계에서 코드를 변경하면 상위 단계 메서드 선언부를 전부 고쳐야 한다. 최하위 함수를 변경하여 새로운 오류를 던진다고 가정하고 확인된 오류를 던진다면 함수는 선언부에 throws 절을 추가해야 한다.\n변경한 함수를 호출하는 함수 모두가 catch 블록에서 새로운 예외를 처리한다. 선언부에 throw 절을 추가한다. 결과적으로 최하위 단계에서 최상위 단계까지 연쇄적인 수정이 일어난다. 다르게 해석하면, throws 경로에 위치하는 모든 함수가 최하위 함수에서 던지는 예외를 알아야 하므로 캡슐화가 깨진다.\n💡 확인된 예외를 이용하여 구현하게 되면 안전한 로직을 만들 수 있지만, 일반적인 애플리케이션은 확인된 예외를 통한 이익보다, 의존성이 더 중요하다.\n예외에 의미를 제공하라 예외를 던질 때 전후 상황을 충분히 덧붙히면, 오류가 발생한 원인과 위치를 찾기 쉬워진다.\n실패한 코드의 의도를 파악하려면 호출 수택 만으로 부족한 경우가 많아. 오류 메시지에 정보(실패한 연산 이름과 실패 유형)를 담아 예외와 함께 던진다.\n애플리케이션에서 로깅 기능을 사용한다면 catch 블록에서 오류를 기록하도록 충분한 정보를 넘겨주면 좋다.\n호출자를 고려해 예외 클래스를 정의하라 오류를 분류하는 방법은 수없이 많다.\n오류가 발생한 위치 오류가 발생한 컴포넌트 오류 유형 디바이스 실패 네트워크 실패 프로그래밍 오류 어플리케이션 오류를 정의할 때 프로그래머에게 가장 중요한 관심사는 오류를 잡아내는 방법이 되어야 한다.\n나쁜예 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ACMEport port = new ACMEport(12); try { port.open(); } catch (DeviceResponseException e) { reportPortError(e); logger.log(\u0026#34;Device response exception\u0026#34;, e); } catch (ATM1212UnlokedException e) { reportPortError(e); logger.log(\u0026#34;Unlock exception\u0026#34;. e); } catch (GMXRrror e) { reportPortError(e); logger.log(\u0026#34;Device response exeption\u0026#34;); } finally { ... } 위 코드는 중복이 심하여, 대다수 상황에서 오류를 처리하는 방식은 비교적 일정하다.\n오류를 기록한다. 프로그램을 계속 수행해도 좋은지 확인한다. 위 경우는 예외에 대응하는 방식이 예외 유형과 무관하게 거의 동일하다. 그래서 코드를 간결하게 고치기 쉽다.\n1 2 3 4 5 6 7 8 9 10 LocalPort port = new LocalPort(12); try { port.open(); } catch (portDeviceFailure e) { reportError(e); logger.log(e.getMessage(), e); } finally { ... } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class LocalPort { private ACMEPort innerPort; public LocalPort(int portNumber) { innerPort = new ACMEPort(portNumber); } public void open() { try { innerPort.open(); } catch (DeviceResponseException e) { throw new PortDeviceFailure(e); } catch (ATM1212UnlockedException e) { throw new PortDeviceFailure(e); } catch (GMXError e) { throw new PortDeviceFailure(e); } } ... } 여기서 LocalPort클래스는 단순히 ACMEport클래스가 던지는 예외를 잡아 변환하는 wrapper 클래스 일뿐이다.\n하지만 이러한 wrapper 클래스는 매우 유용하다. 실제로 외부 API를 사용할 때는 감싸기 기법이 최선이다.\n외부 API를 감싸면 외부 라이브러리와 프로그램 사이에서 의존성이 크게 줄어든다. 나중에 다른 라이브러리로 갈아타도 비용이 적다. 감싸기 클래스에서 외부 API를 호출하는 대신 테스트 코드를 넣어주는 방법으로 프로그램을 테스트하기도 쉬워진다. 특정 업체가 API를 설계한 방식에 발목 잡히지 않는다. 프로글매이 사용하기 편한 API를 정의하면 그만이다. 예외 클래스가 하나만 있어도 충분한 코드가 많다.\n예외 클래스에 포함된 정보로 오류를 구분해도 괜찮은 경우. 한 예외는 잡아내고 다른 예외는 무시해도 괜찮은 경우라면 여러 예외 클래스를 사용하는 것이 좋다.\n정상 흐름을 정의하라 정상 흐름을 정확히 정의하지 않으면 딴길로 샌다.\n1 2 3 4 5 6 try { MealExpenses expenses = expenseReportDAO.getMeals(employee.getID()); m_total += expenses.getTotal(); } catch(MealExpensesNotFound e) { m_total += getMealPerDiem(); } 위에서 식비를 비용으로 청구했다면 직원이 청구한 식비를 총계에 더한다. 식비를 비용으로 청구하지 않았다면 일일 기본 식비를 총계에 더한다. 그런데 예외가 논리를 따라가기 어렵게 만든다.\n1 2 MealExpenses expenses = expenseReportDAO.getMeals(employee.getID()); m_total += expenses.getTotal(); ExpensesReportDAO를 고쳐 청구한 식비가 없다면, 일일 기본 식비를 반환하는 MealExpense객체를 반환한다.\n1 2 3 4 5 public class PerDiemMealExpenses implements MealExpenses { public in getTotal() { // 기본값으로 일일 기본 식비를 반환 } } 이러한 형태를 **특수 사례 패턴(Special Case Pattern)**이라 부른다. 클래스를 만들거나 객체를 조작해 특수 사례를 처리하는 방식이다.\n클라이언트 코드가 예외적인 상황을 처리할 필요가 없어진다.\nnull을 반환하지 마라 null 반환은 흔히 사용되어 오류를 유발하는 경우가 많다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 public void registerItem(Item item) { if (item != null) { ItemRegistry registry = peristentStore.getItemRegistry(); if (registry != null { Item existing = registry.getItem(item.getID()); if (existing.getBillingPeriod().hasRetailOwner()) { existing.register(item); } } } } null을 반환하기 때문에 정상적인 동작 처리를 위해 null 인지 아닌지 확인하는 로직이 필요하여 일거리를 늘린다.\n이러한 코드는 호출자가 null 처리를 잊을 경우 문제가 발생하게는 구조로, 문제를 호출자에게 떠넘기기 때문에 좋은 구조라고 볼 수 없다.\n이러한 경우 특수 사례가 좋은 해결책이 될 수 있다.\n1 2 3 4 5 6 7 List\u0026lt;Empoloyee\u0026gt; employees = getEmpoloyees(); if (employees != null { for(Employee e : employees) { totalPay += e.getPay(); } } 위 getEmployees는 null도 반호나한다. 하지만 이런 경우 null반환하는 것이 아니라 빈 리스트를 반환한다면 코드가 훨씬 깔끔해진다.\n1 2 3 4 5 List\u0026lt;Employee\u0026gt; employees = getEmployees(); for ( Employees e L employees) { totalPay += e.getPay(); } 자바에는 Collections.emptyList()가 있어 미리 정의된 읽기 전용 리스트를 반환한다.\n1 2 3 4 5 public List\u0026lt;Employee\u0026gt; getEmployees() { if ( ... 직원이 없다면 ..) { return Collections.emptyList(); } } 위와 같이 코드를 변경하면 코드도 깔끔해지고 NullPointerException이 발생할 가능성도 줄어든다.\nnull을 전달하지 마라 메서드에서 null을 반환하는 방식도 나쁘지만 메서드로 null을 전달하는 방식은 더 나쁘다.\n1 2 3 4 5 6 public class MetricsCalculator { public double xProjection(Point p1, Point p2) { return (p2.x - p1.x) * 1.5; } ... } 누군가 인수로 null을 전달하면 NullPointerException이 발생한다.\n1 2 3 4 5 6 7 8 9 public class MetricsCalculator { public double xProjection(Point p1, Point p2) { if (p1 == null || p2 === null) { throw InvalidArgumentException( \u0026#34;Invalid argument for MetricsCalculator.xProjection\u0026#34;); } return (p2.x - p1.x) * 1.5; } } 위 코드로 입력이 null일 경우를 예방 했지만, InvalidArgumentException 잡아내는 처리기를 추가해야 한다.\n1 2 3 4 5 6 7 public class MetricsCalculator { public double xProjection(Point p1, Point p2) { assert p1 != null : \u0026#34;p1 should not be null\u0026#34;; assert p2 != null : \u0026#34;pw should not be null\u0026#34;; return (p2.x - p1.x) * 1.5; } } 위 코드는 assert문을 사용하여 처리했다.\nJava의 assert 키워드\n문서화가 잘 되어 코드 읽기는 편하지만 문제를 해결하지는 못한다. 누군가 null을 전달하면 여전히 실행 오류가 발생한다.\n💡 대다수 프로그래밍 언어는 호출자가 실수로 넘기는 null을 적절히 처리하는 방법이 없기 때문에, 애초에 null을 넘기지 못하도록 금지하는 정책이 합리적일 수 있다.\n결론 깨끗한 코드는 읽기도 좋아야 하지만 안정성도 높아야 한다.\n이 둘은 상충하는 목표가 아님 오류 처리를 프로그램 논리와 분리해 별도의 사안으로 고려해야함 더욱 튼튼하고 깨끗한 코드를 작성할 수 있게됨 독립적인 추론이 가능해지며 코드 유지보수성도 크게 높아짐 프로그램 논리와 분리해 별도로 고민하라. 오류 처리를 만들기 전에 흐름을 정확히 파악하면 안 쓸수도 있다. 정상 흐름을 정의하라. 특수 상황 패턴등을 이용 null은 사용자의 실수를 유발하므로 되도록 자제해야 한다. ","date":"2023-04-20T16:12:13+09:00","image":"https://codemario318.github.io/post/clean_code_7/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean_code_7/","title":"클린코드: 7. 오류 처리"},{"content":"자료 추상화 사용자가 구현을 모른 채 자료의 핵심을 조작할 수 있어야 진정한 의미의 클래스다.\n1 2 3 4 public class Point { public double x; public double y; } 1 2 3 4 5 6 7 8 public interface Point { double getX(); double getY(); void setCartesian(double x, double y); double getR(); double getTheta(); void setPolar(double r, double theta); } 두 클래스 모두 2차원 점을 표현하지만 6-1은구현을 외부로 노출하고, 6-2는 구현을 완전히 숨긴다.\n6-1은 변수를 private 으로 선언하더라도 각 값마다 getter, setter를 제공한다면 외부로 노출하는 것이다.\n변수 사이에 함수라는 계층을 넣는다고 구현이 저절로 감춰지지는 않는다. 구현을 감추려면 어느정도의 추상화가 필요하다.\n예시 1\n1 2 3 4 public interface Vehicle { double getFuelTankCapacityInGallons(); double getGallonsOfGasoline(); } 예시 2\n1 2 3 public interface Vehicle { double getPercentFuelRemaining(); } 예시 1은 연료탱크의 용량과 휘발유의 양을 받아올 수 있고, 예시 2는 백분율로 얼마나 더 남았는가를 받아올 수 있다.\n둘 중 예시 2가 더 추상적이지만, 백분율의 기준값을 알 수 없으므로 정확한 정보를 표현하지 못한다.\n인터페이스나 조회/설정 함수만으로는 추상화가 이뤄지지 않는다. 개발자는 객체가 포함하는 자료를 표현 할 가장 좋은 방법을 심각하게 고민해야 한다.\n자료/객체 비대칭 객체: 추상화 두로 자료를 숨긴 채 자료를 다루는 함수만 공개한다. 자료구조: 자료를 그대로 공개하며 별다른 함수는 제공하지 않는다. 절차적인 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 public class Square { public Point topLeft; public double side; } public class Rectangle { public Point topLeft; public double height; public double width; } public class Circle { public Point center; public double radius; } public class Geometry { public final double PI = 3.141592653589793; public double area(Object shape) throws NoSuchShapeExetion { if (shape instanceof Square) { Square s = (Square)shape; return s.side * s.side; } else if (shape instanceof Rectangle) { Rectangle r = (Rectangle)shape; return r.height * r.width; } else if (shape instanceof Circle) { Circle c = (Circle)shape; return PI * c.radious * c.radious; } throw new NoSuchShapeExcption(); } } 위 예시에서 만약 Geometry 클래스에 새로운 기능들을 추가해도 도형 클래스들은 영향을 받지 않는다. 위 예시에서 새 도형을 추가하고 싶다면 Geometry의 속한 기능들에 새로운 도형을 위한 코드를 추가해야 한다. 객체 지향적인 도형 클래스 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public class Square implements Shape { private Point topLeft; private double side; public double area() { return s.side * s.side; } } public class Rectangle implements Shape { public Point topLeft; public double height; public double width; public double area() { return height * width; } } public class Circle implements Shape { public Point center; public double radius; public double area() { return PI * radius * radius; } } 위 예시에서 만약 Shape에 새로운 기능들을 추가하면 상속받은 모든 클래스에 변경이 필요하다. 위 예시에서 Shape를 상속받은 다른 도형을 추가하고 싶다면 하나 새로 만들면 된다. 목록 절차적인 구현과 객체 지향적인 구현은 근본적으로 반대이며 상호 보완적인 특성을 가진다. 그래서 객체와 자료 구조는 근본적으로 양분된다.\n💡 객체 지향 코드에서 어려운 변경은 절차적인 코드에서 쉬우며, 절차적인 코드에서 어려운 변경은 객체지향 코드에서 쉬운 경우가 많다. 따라서, 때로는 단순한 자료 구조와 절차적인 코드가 가장 적합한 상황도 있다.\n디미터 법칙 디미터의 법칙은 잘 알려진 휴리스틱으로, 모듈은 자신이 조작하는 객체의 속사정을 몰라야 한다는 법칙이다.\n디미터 법칙(Law of Demeter)\n이러한 이유로 Don’t Talk to Strangers(낮선 이에게 말하지 마라) 또는 Principle of least Knowledge(최소 지식 원칙)으로도 알려져 있다.\n객체는 자료를 숨기고 함수를 공개한다.\n즉, 객체는 조회 함수로 내부 구조를 공개하면 안된다는 의미다. 조금 더 정확하게 표현하자면, 디미터 법칙은 “클래스 C의 메서드 f는 다음과 같은 객체의 메서드만 호출해야 한다”고 주장한다.\n클래스 C f가 생성한 객체 f 인수로 넘어온 객체 C 인스턴스 변수에 저장된 객체 위 객체에서 허용된 메서드가 반환하는 객체의 메서드는 호출하면 안된다.\n1 final String ouputDir = ctxt.getOptions().getScratchDir().getAbsolutePath(); getOptions() 함수가 반환하는 객체의 getScratchDir() 함수를 호출한 후 getScratchDir() 함수가 반환하는 객체의 getAbsolutePath() 함수를 호출하기 때문이다 디미터의 법칙을 위반했다. 클래스 C의 메서드가 지정된 메서드만을 호출하지 않고 있다. 기차 충돌 메서드를 연속적으로 호출하여 코드가 여러 객차가 한 줄로 이어진 기차처럼 보이기 때문에 기차 충돌이라 부른다.\n1 2 3 Options opts = ctxt.getOptions(); File scratchDir = opts.getScratchDir(); final String outputDir = scratchDir.getAbsolutePath(); 위와 같이 기차 충돌을 제거했을 경우 연속적으로 호출되었을 때는 반환되는 객체를 정확히 파악할 수 없었는데 나눠놓으니 더 쉽게 파악할 수 있다.\n하지만 위 구조도 아래와 같은 전제를 이미 알고 있어야 하므로 디미터 법칙을 만족한다고 볼 수 없다.\nctxt 객체가 Options를 포함한다. Options가 ScratchDir을 포함한다. ScratchDir이 AbsolutePath를 포함한다. 위 코드를 사용하는 함수는 많은 객체에 대한 정보를 알고 있기 때문에 최소 지식 원칙을 만족한다고 볼 수 없다.\n디미터 법칙을 위반하는지는 호출된 ctxt, Options, ScratchDir이 객체인지 자료 구조 인지에 달렸다.\n객체라면 내부 구조를 숨겨야 하므로 디미터 법칙을 위반한다. 자료 구조라면 내부 구조를 노출하기 때문에 디미터 법칙 대상이 아니다. 위 예제는 조회 함수를 사용하는 바람에 혼란을 일으키게 되는데, 코드를 다음과 같이 구현했다면 디미터 법칙을 거론할 필요가 없어진다.\n1 final String ouputDir = ctxt.options.scratchDir.absolutePath; 자료구조를 이용하게 되어 다른 클래스의 내부 구조에 대해 알 수 없게 되므로 디미터의 법칙 대상이 아니게 된다.\n자료 구조는 무조건 함수 없이 공개 변수만 포함하고 객체는 비공개 변수와 공개 함수를 포함한다면, 문제는 훨씬 간단해진다.\n잡종 구조 하지만 단순한 자료 구조에도 조회 함수와 설정 함수를 정의하라 요구하는 프레임워크와 표준(ex. bean)이 존재하고 있다. 이런 혼란 때문에 절반은 객체, 절반은 자료 구조인 잡종 구조가 나온다.\n잡종 구조는 중요한 기능을 수행하는 함수도 있고, 공개 변수나 공개 조회/설정 함수도 있다.\n공개/조회 함수는 비공개 변수를 그대로 노출하게 되며, 이 때문에 다른 함수가 절차적인 프로그래밍의 자료 구조 접근 방식처럼 비공개 변수를 사용하게 유혹한다.\n위처럼 만들어진 잡종 구조는 새로운 함수는 물론이고 새로운 자료 구조도 추가하기 어렵다. 그러므로 이러한 구조는 되도록 피하는 편이 좋다.\n구조체 감추기 ctxt, options, scratchDir이 진짜 객체라면 앞에서 본 예제처럼 기차 충돌을 만들면 안된다.\n1 ctxt.getAbsolutePathOfScratchDirectoryOption(); ctxt 객체가 여러 메서드를 조작해야 하므로 공개해야 하는 메서드가 너무 많아진다. 1 ctxt.getScratchDiretoryOption().getAQbsolutePath(); 객체가 아니라 자료 구조를 반환한다고 가정한다. ctxt가 객체라면 뭔가를 하라고 말해야지 속으로 드러내라고 말하면 안된다.\n고칠때는 무슨 목적으로 만들어진 기능인지 살피면 더 좋은 코드로 바꿀 수 있다. 위 기능에서 절대 경로를 얻으려는 이유는 임시 파일을 생성하기 위해서였다.\n1 2 3 String outFile = outputDir + \u0026#34;/\u0026#34; + className.replace(\u0026#39;.\u0026#39;, \u0026#39;/\u0026#39;) + \u0026#34;.class\u0026#34;; FileOutputStream fout = new FileOutputStream(outFile); BufferedOutputStream bos = new BufferedOutputStream(fout); 이러한 경우 ctxt객체에 임시 파일을 생성하는 기능을 만드는 것이 더 좋아보인다.\n1 BufferedOutputStream bos = ctxt.createScratchFileStream(classFileName); 위 코드로 변경하여 ctxt는 내부 구조를 드러내지 않으며, 모듈에서 해당 함수는 자신이 몰라야 하는 여러 객체를 탐색할 필요가 없어졌다.\n❓ 메서드 체인으로 만들었던 outputDir 은 결국 className을 함께 이용하여 파일 경로를 만들어 주게 되므로 별도로 분리하는 것이 아니라 묶어서 감추는 것이 더 좋다는 의미인가?\n자료 전달 객체 DTO 자료 구조체의 전형적인 형태는 공개 변수만 있고 함수가 없는 클래스다. 이런 자료 구조체를 때로는 자료 전달 객체(Data Transfer Object, DTO)라 한다.\nDTO는 데이터베이스와 통신하거나 소켓에서 받은 메시지의 구문을 분석할 때 유용하다.\n흔히 DTO는 데이터 베이스에 저장된 가공되지 않은 정보를 애플리케이션 코드에서 사용할 객체로 변환하는 단계에서 가장 처음으로 사용하는 구조체다.\nDTO의 좀 더 일반적인 형태는 ‘빈(bean)’구조다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 public class Address { private String street; private String streetExtra; private String city; private String state; private String zip; public Address(String street, String streetExtra, String city, String state, String zip) { this.street = street; this.streetExtra = streetExtra; this.city = city; this.state = state; this.zip = zip; } public String getStreet() { return street; } public String getStreetExtra() { return streetExtra; } public String getCity() { return city; } public String getState() { return state; } public String getZip() { return zip; } } 빈은 비공개 변수를 조회/설정 함수로 조작한다. 일종의 사이비 캡슐화로, 별 다른 이익을 제공하지는 않는다.\n활성 레코드 활성 레코드는 데이터베이스 테이블이나 다른 소스에서 자료를 직접 변환한 결과로 DTO의 특수한 형태다. 공개 변수가 있거나 비공개 변수에 조회/설정 함수가 있는 자료 구조지만, 대개 save나 find와 같은 탐색 함수도 제공한다.\n활성 레코드를 사용할 때 비즈니스 로직을 추가해 이런 자료 구조를 객체로 취급하는 개발자가 많은데, 이러한 방식은 결국 잡종 구조를 만들게 된다.\n활성 레코드는 자료 구조로 취급하여, 비즈니스 규칙을 담으면서 내부 자료를 숨기는 객체를 따로 생성해야 한다.\n결론 객체는 동작을 공개하고 자료를 숨긴다. 그래서 기존 동작을 변경하지 않으면서 새 객체 타입을 추가는 쉽다. 하지만 기존 객체에 새 동작을 추가하는 것은 어렵다. 자료구조는 별다른 동작 없이 자료를 노출한다. 그래서 기존 자료 구조에 새 동작을 추가는 쉽다. 기존 함수에 새 자료 구조를 추가하기는 어렵다. 구현할 때, 새로운 자료 타입을 추가하는 유연성이 필요하면 객체가 더 적합하다. 다른 경우로 새로운 동작을 추가하는 유연성이 필요하면 자료 구조와 절차적인 코드가 더 적합하다.\n💡 우수한 개발자는 편견 없이 이 사실을 이해해 직면한 문제에 최적인 해결책을 선택해야 한다.\n읽고 싶은 책 엘레강트 오브젝트 객체지향을 조금 다른 관점에서 접근하여 깨끗한 코드를 만드는 방법을 추천하는 책 ","date":"2023-04-20T15:53:13+09:00","image":"https://codemario318.github.io/post/clean_code_6/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean_code_6/","title":"클린코드: 6. 객체와 자료구조"},{"content":"어떤 프로그램이든 가장 기본적인 단위는 함수다\n어떻게 해야 읽기 쉽고 이해하기 쉬운 함수를 만들 수 있을까 의도를 분명히 표현하는 함수를 어떻게 구현할수 있을까 함수에 어떤 속성을 부여해야 처음 읽는 사람이 프로그램 내부를 직관적으로 파악할 수 있을까 작게 만들어라 함수를 만드는 첫번째 규칙은 ‘작게!’, 두번째 규칙은 ‘더 작게!’ 다.\n이 규칙은 증명하긴 어렵지만 작가의 경험으로 작은 함수가 좋다고 확신한다.\n함수가 작을수록 한가지 일만 처리하게 만들기 용이하고 명백해진다.\n블록과 들여쓰기 조건문을 통해 처리될 블록은 한줄로 표현해야한다. 즉 코드를 함수로 만들어야 한다.\n블록에 들어가게 되는 함수 이름을 적절하게 사용한다면 코드를 이해하기 쉬워진다.\n이 말은 중첩 구조가 생길만큼 함수가 커져서는 안 된다는 뜻으로 함수의 들여쓰기 깊이는 2단을 넘지 않게 만드는 것이 좋다.\n한가지만 해라 함수는 한 가지 일을 해야야한다. 그 한가지를 잘 해야한다. 그 한 가지만을 해야한다.\n추상화 수준이 하나인 단위로 함수를 만들면 한가지 일을 하는 함수를 만들 수 있다.\n함수를 만드는 이유는 큰 개념(기능)을 다음 추상화 수준에서 여러 단계로 나눠 수행하기 위해서다. 의미를 유지하면서 더 쪼갤 수 없는 수준까지 줄여야한다.\n단순히 다른 표현이 아니라 의미 있는 이름으로 다른 함수를 추출할 수 있다면 그 함수는 여러 작업을 하는 셈이다.\n함수 내 섹션 섹션이 여러개 만들어진다면 함수가 여러개의 일을 한다는 뜻이다. 한가지 일 만 하는 함수는 자연스럽게 섹션으로 나누기 어렵다.\n함수 내 추상화 수준은 하나로 함수가 확실히 한 가지 작업만 하려면 함수 내 모든 문장의 추상화 수준이 동일해야 한다.\n한 함수 내에 추상화 수준을 섞으면 특정 표현이 근본 개념인지 세부사항인지 구분하기 어려워, 코드를 읽는 사람이 헷갈린다.\n또한 근본 개념과 세부사항을 뒤섞기 시작하면, 깨진 유리창처럼 사람들이 함수에 세부사항을 점점 더 추가한다.\n위에서 아래로 코드 읽기: 내려가기 규칙 코드는 위에서 아래로 이야기처럼 읽혀야 좋다. 한 함수 다음에는 추상화 수준이 한 단계 낮은 함수가 온다.\n즉 위에서 아래로 읽히려면 함수 추상화 수준이 한번에 한 단계씩 낮아진다.\n💡 내려간다는건 단순히 위에서 아래로 읽힌다는 의미보다, 깊이가 깊어질수록 조금 더 낮은 수준으로 표현되야 한다는 뜻같다.\n추상화 수준이 하나인 함수를 구현하는것은 어렵다. 핵심은 짧으면서도 한가지 일만 수행하는 함수이다. 한 단계씩 깊어지는 코드를 구현하면 추상화 수준을 일관되게 유지하기 쉬워진다.\nSwitch 문 Switch문은 본질적으로 switch 문은 N 가지를 처리하기 때문에 작게, 한 가지 작업만 수행하게 만들기 어렵다.\n완전히 사용하지 않을 방법은 없기 때문에 다형성을 사용하여 저차원 클래스에 숨기고 반복하지 않게 만들어야 한다.\n","date":"2023-04-20T15:48:13+09:00","image":"https://codemario318.github.io/post/clean_code_3/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean_code_3/","title":"클린코드: 3. 함수 잘 만드는 법"},{"content":" 코드는 요구사항을 표현하는 도구이다. 고도로 추상화된 언어나 특정 응용 분야 언어로 기술하는 명세 역시 코드이다. 제대로 명시한 요구사항은 코드만큼 정형적이며 테스트 케이스로 사용해도 좋다! 나쁜 코드 잘나가던 회사를 망하게한 원인은 나쁜코드였다. 그들은 출시에 바빠 코드를 마구 짰다. 기능을 추가할수록 코드는 엉망이 되어갔고, 결국은 감당이 불가능한 수준에 이르렀다.\n언급 되었던 회사는 20년 후 망했다. 장기적으로 살피지 않아도, 빠르게 진행되던 프로젝트가 1~2년 만에 정체되는 경우가 꽤 많이 일어난다.\n이렇게 만들어진 나쁜 코드들은 생산성 악순환을 만든다.\n나쁜 (더러운) 코드를 지속적으로 개선하지 않는다면 장기적으로 부정적인 결과를 가져오게 된다.\n나중은 절대 오지 않는다 - Later == Never ( later equals never!) 나중은 절대 오지 않는다. 따라서 시간을 들여 깨끗한 코드를 만드는 노력이 비용을 절감하는 방법일 뿐만 아니라 전문가로서 꼭 필요하다.\n좋은 코드가 나쁜 코드가 되는 이유 요구사항 변경 짧은 일정 등등 외적인 요인들 💡 모두 변명임. 잘못은 사실 개발자 자신에게 있다.\n기획, 사업 등 에게 적극적으로 정보를 제공하여 사전에 방지해야 한다.\n커뮤니케이션을 통해 이러한 요인들을 예방하는 것도 개발자의 역량 나쁜 코드의 위험을 이해하지 못하는 관리자 말을 그대로 따르는 행동은 전문가답지 못하다.\n원초적 난제 개발자는 근본적인 가치에서 난제에 봉착한다.\n기한을 맞추려면 나쁜 코드를 양산할 수 밖에 없다고 느끼지만, 오히려 엉망진창인 상태로 인해 속도가 늦어지고 기한을 놓치게 된다.\n💡 기한을 맞추는 유일한 방법은, 즉 빨리 가는 유일한 방법은, 언제나 코드를 최대한 깨끗하게 유지하는 습관이다.\n깨끗한 코드는 어떻게 작성할까? 코드감각 깨끗한 코드를 작성하려면 청결이라는 힘겹게 습득한 감각을 활용해 자잘한 기법들을 적용하는 절제와 규율이 필요하다.\n깨끗한 코드란? 비야네 스트롭스트룹 - Bjarne Stroustrup (C++ 창시자) 간단한 논리\n낮은 의존성\n성능을 최적화\n깨끗한 코드는 한 가지 일을 제대로 한다.\n나쁜 코드는 나쁜 코드를 유혹한다.\n나쁜 코드를 고치면서 오히려 더 나쁜 코드를 만든다. 깨진 창문 깨진 유리창 이론 - 위키백과, 우리 모두의 백과사전\n그래디 부치 - Grady Booch : UML 창시자 깨끗한 코드는 단순하고 직접적이다. 잘 쓴 문장처럼 읽힌다. 결코 설계자의 의도를 숨기지 않는다. 명쾌한 추상화와 단순한 제어문으로 가득하다. 가독성을 강조하고 있다.\n","date":"2023-04-20T15:42:13+09:00","image":"https://codemario318.github.io/post/clean_code_1/clean_code_cover_hud03d003727d53e154227b4e2dbea4cfd_18319_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/clean_code_1/","title":"클린코드: 1. 깨끗한 코드"},{"content":"Go 언어에서 채널은 고루틴을 연결해주는 통로(파이프)다. 기본적으로 채널은 양방향이고 고루틴이 아래 이미지와 같이 동일한 채널을 통해 데이터를 보내거나 받을 수 있다.\nGo 채널은 그 채널을 통하여 데이터를 주고 받는 통로라 볼 수 있다. 채널은 make() 함수를 통해 미리 생성되어야 하며, 채널 연산자 \u0026lt;- 을 통해 데이터를 보내고 받는다.\n채널은 흔이 고루틴들 사이에 데이터를 주고 받을때 사용되는데, 상대편이 준비될 때까지 채널에서 대기함으로써 별도로 lock을 걸지 않고 데이터를 동기화하는데 사용된다.\n아래 예제는 정수형 채널을 생성하고, 한 고루틴에서 그 채널에 123이란 정수 데이터를 보낸 후, 이를 다시 메인 루틴에서 채널로부터 123 데이터를 받는 코드이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main func main() { // 정수형 채널을 생성한다 ch := make(chan int) go func() { ch \u0026lt;- 123 //채널에 123을 보낸다 }() var i int i = \u0026lt;- ch // 채널로부터 123을 받는다 println(i) } 채널을 생성할 때는 make()함수에 어떤 타입 데이터를 채널에서 주고받을 지 미리 지정해 주어야 한다. 채널로 데이터를 보낼 때는 채널명 \u0026lt;- 데이터 와 같이 사용하고, 채널로부터 데이터를 받을 경우에는 \u0026lt;- 채널명 와 같이 사용한다.\n메인 루틴은 마지막에서 채널로부터 데이터를 받고 있는데, 상대편 고루틴에서 데이터를 전송할 때까지는 계속 대기하게 된다. 따라서, 이 예제에서는 time.Sleep()이나 fmt.Scanf() 같이 고루틴이 끝날 때까지 기다리는 코드를 적지 않는다.\nGo 채널의 수신자와 송신자가 서로를 기다리는 속성을 이용하여 Go루틴이 끝날 때까지 기다리는 기능을 구현할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main import \u0026#34;fmt\u0026#34; func main() { done := make(chan bool) go func() { for i := 0; i \u0026lt; 10; i++ { fmt.Println(i) } done \u0026lt;- true }() // 위의 Go루틴이 끝날 때까지 대기 \u0026lt;-done } 익명함수를 사용한 고루틴에서 어떤 작업이 실행되고 있을 때, 메인 루틴은 ←done에서 계속 수신하며 대기하고 있게 된다.\n익명함수 고루틴에서 작업이 끝난 후, done채널에 true를 보내면, 수신자 메인루틴은 이를 받고 프로그램을 끝내게 된다.\nGo 채널 버퍼링 Go 채널은 Unbufferd Channel과 Buffered Channel 2가지 형태가 있다.\nUnbufferd Channel 위 예제에서 Go 채널은 Unbuffered Channel로서 이 채널에서는 하나의 수신자가 데이터를 받을 때까지 송신자가 데이터를 보내는 채널에 묶여있게 된다. (결과를 반환할 때까지 기다린다. blocking?)\nBufferd Channel Bufferd Channel을 사용하면 수신자가 받을 준비가 되어 있지 않아도 지정된 버퍼만큼 데이터를 보내고 계속 다른 일을 수행할 수 있다.(unblocking?) 버퍼 크기까지 입력 작업이 블락되지 않는다.\n버퍼 채널은 make(chan type, N) 함수를 통해 생성되는데, 두번째 파라미터 N에 사용할 버퍼 갯수를 넣는다.\n예를들어 make(chan int, 10)은 10개의 정수형을 갖는 버퍼 채널을 만든다.\n1 2 3 4 5 6 7 8 9 package main import \u0026#34;fmt\u0026#34; func main() { c := make(chan int) c \u0026lt;- 1 //수신루틴이 없으므로 데드락 fmt.Println(\u0026lt;-c) //코멘트해도 데드락 (별도의 Go루틴없기 때문) } 버퍼 채널을 이용하지 않는 경우, 위 코드는 데드락 에러를 발생시킨다. 메인루틴에서 채널에 1을 보내면서 상대편 수신자를 기다리고 있는데, 이 채널을 받는 수신자 고루틴이 없기 때문이다.\n1 2 3 4 5 6 7 8 9 10 11 12 package main import \u0026#34;fmt\u0026#34; func main() { ch := make(chan int, 1) //수신자가 없더라도 보낼 수 있다. ch \u0026lt;- 101 fmt.Println(\u0026lt;-ch) } 위처럼 버퍼채널을 사용하면, 수신자가 당장 없더라도 최대버퍼 수까지 데이터를 보낼 수 있으므로 에러가 발생하지 않는다\n버퍼에 적재된 데이터를 언젠가 가져갈 것이라 판단하고 최대 버퍼수를 넘지 않을때까지 보내도 데드락 에러를 발생시키지 않는다.\nUnbufferd Channel 예제코드 오류 분석 1 2 3 4 5 6 7 8 package main import \u0026#34;fmt\u0026#34; func main() { c := make(chan int) c \u0026lt;- 1 } unbuffered channel에서 발생하는 일반적인 데드락의 의미인 2개 이상의 작업 서로의 작업이 완료되기를 대기하는 교착 상태와는 약간 다르다고 볼 수도 있다.\n왜냐하면 sender와 receiver중 누군가가 먼저 작업을 끝내야지 그 다음으로 누군가가 작업을 수행할 수 있는 것이 아니라 서로 동시에 협력해야만 unbuffered channel에 대한 대기를 끝낼 수 있는데 이 경우는 동시에 협력해줄 그 누군가(receiver)가 없어 무한정 기다리게되는 상황이다.\n1 2 3 4 5 6 7 8 9 package main import \u0026#34;fmt\u0026#34; func main() { c := make(chan int) c \u0026lt;- 1 \u0026lt;- c } 그렇다고해서 위와 같이 자기 혼자 send와 receive를 하려 해도 Unbuffered channel은 sender와 receiver가 모두 ready여야 작업을 진행할 수 있기 때문에 불가능하다.\n1 2 3 4 5 6 7 8 9 10 11 package main import \u0026#34;fmt\u0026#34; func main() { c := make(chan int) go func() { \u0026lt;- c }() c \u0026lt;- 1 } 1 2 3 4 5 6 7 8 9 10 11 package main import \u0026#34;fmt\u0026#34; func main() { c := make(chan int) go func() { c \u0026lt;- 1 }() \u0026lt;- c } 따라서 다른 goroutine에서 A에 대한 receiver or sender 역할을 해주면 된다.\n채널 파라미터 채널을 함수의 파라미터로 전달할 때, 일반적으로 송수신을 모두 하는 채널을 전달하지만, 특별히 해당 채널로 송신만 할 것인지 혹은 수신만 할 것 인지를 지정할 수 있다.\n송신 파라미터: chan\u0026lt;- 수신 파라미터: \u0026lt;-chan 파라미터에 다른 송수신을 바꿔 넣게 되면 에러가 발생한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import \u0026#34;fmt\u0026#34; func main() { ch := make(chan string, 1) sendChan(ch) receiveChan(ch) } func sendChan(ch chan\u0026lt;- string) { ch \u0026lt;- \u0026#34;Data\u0026#34; // x := \u0026lt;-ch // 에러발생 } func receiveChan(ch \u0026lt;-chan string) { data := \u0026lt;-ch fmt.Println(data) } 채널 닫기 채널을 열어 데이터를 보낸 후, close()함수를 사용하여 채널을 닫을 수 있다. (송신자만 가능) 채널을 닫게되면, 해당 채널로는 더이상 송신할 수 없지만, 계속 수신은 가능하다.\n채널 수신에 사용되는 \u0026lt;- ch 은 채널 메시지, 정상 수신 여부 2개의 반환값을 갖는다. 채널이 닫혔을 경우 두번째 리턴값은 false이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main func main() { ch := make(chan int, 2) // 채널에 송신 ch \u0026lt;- 1 ch \u0026lt;- 2 // 채널을 닫는다 close(ch) // 채널 수신 println(\u0026lt;-ch) println(\u0026lt;-ch) if _, success := \u0026lt;-ch; !success { println(\u0026#34;더이상 데이타 없음.\u0026#34;) } } 채널 range 문 채널에서 송신자가 송신을 한 후, 채널을 닫을 수 있다. 수신자는 임의의 갯수의 데이터를 채널이 닫힐 때까지 수신할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package main func main() { ch := make(chan int, 2) // 채널에 송신 ch \u0026lt;- 1 ch \u0026lt;- 2 // 채널을 닫는다 close(ch) // 방법1 // 채널이 닫힌 것을 감지할 때까지 계속 수신(무한 for 루프) for { if i, success := \u0026lt;-ch; success { println(i) } else { break } } // 방법2 // 위 표현과 동일한 채널 range 문 (방법 1의 간결한 표현 세부 동작은 같음) for i := range ch { println(i) } } 채널 range문은 range 키워드 다음에 명시한 채널로부터 데이터를 계속 수신하다가 채널이 닫힌 것을 감지하면 for 루프를 종료한다.\n채널 select 문 select문은 복수 채널들을 기다리면서 준비된 채널을 실행하는 기능을 제공한다.\nselect문은 여러개의 case문에서 각각 다른 채널을 기다리다가 준비가 된 채널 case를 실행한다. select문은 case 채널들이 준비되지 않으면 계속 대기하게 되고, 가장 먼저 도착한 채널의 case를 실행한다. 만약 복수 채널에 신호가 오면, Go 런타임이 랜덤하게 그중 한 개를 선택하게 된다. select문에 default문이 있으면, case문 채널이 준비되지 않더라도 계속 대기하지 않고 바로 default문을 실행한다.\n","date":"2023-04-20T13:38:40+09:00","image":"https://codemario318.github.io/post/go_13/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/go_13/","title":"Golang: 13. Go 채널"},{"content":"Go루틴(goroutine)은 Go 런타임이 관리하는 Lightweight 논리적 (혹은 가상) 쓰레드이다. Go에서 go 키워드를 사용하여 함수를 호출하면, 런타임시 새로운 goroutine을 실행한다.\n고루틴은 비동기적으로 함수루틴을 실행하므로, 여러 코드를 동시에 실행하는데 사용된다.\n고루틴은 OS 쓰레드보다 훨씬 가볍게 비동기 Concurrent(동시성) 처리를 구현하기 위하여 만든것으로, 기본적으로 Go 런타임이 자체 관리한다.\nGo 런타임 상에서 관리되는 작업단위인 여러 고루틴들을 하나의 OS 쓰레드 1개로도 실행되곤 한다. 고루틴들은 OS 쓰레드와 1대1로 대응되지 않고, Multiplexing(다중화)으로 훨씬 적은 OS 쓰레드를 사용한다. 메모리 측면에서도 OS쓰레드가 1MB 스택을 갖는 반면, 고루틴은 이보다 훨씬 작은 몇 KB 스택을 갖는다(필요시 동적으로 증가함). Go 런타임은 Go루틴을 관리하면서 Go 채널을 통해 Go 루틴간의 통신을 쉽게 할 수 있도록 했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func say(s string) { for i := 0; i \u0026lt; 10; i++ { fmt.Println(s, \u0026#34;***\u0026#34;, i) } } func main() { // 함수를 동기적으로 실행 say(\u0026#34;Sync\u0026#34;) // 함수를 비동기적으로 실행 go say(\u0026#34;Async1\u0026#34;) go say(\u0026#34;Async2\u0026#34;) go say(\u0026#34;Async3\u0026#34;) // 3초 대기 time.Sleep(time.Second * 3) } main 함수를 보면, 먼저 say() 함수를 동기적으로 호출하고, 다음으로 동일한 say() 함수를 비동기적으로 3번 호출하고 있다. 첫번째 동기적 호출은 say() 함수가 완전히 끝났을 때 다음 문장으로 이동하고, 다음 3개의 go say() 비동기 호출은 별도 고루틴들에서 동작하면서, 메인루틴은 계속 다음 문장을 실행한다. 고루틴들은 비동기이므로 처리 순서가 일정하지 않으므로 프로그램 실행시 마다 다른 출력 결과를 나타낼 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 Sync *** 0 Sync *** 1 Sync *** 2 Sync *** 3 Sync *** 4 Sync *** 5 Sync *** 6 Sync *** 7 Sync *** 8 Sync *** 9 Async3 *** 0 Async3 *** 1 Async3 *** 2 Async3 *** 3 Async3 *** 4 Async3 *** 5 Async3 *** 6 Async3 *** 7 Async3 *** 8 Async3 *** 9 Async2 *** 0 Async2 *** 1 Async2 *** 2 Async2 *** 3 Async2 *** 4 Async1 *** 0 Async1 *** 1 Async1 *** 2 Async1 *** 3 Async1 *** 4 Async1 *** 5 Async1 *** 6 Async1 *** 7 Async1 *** 8 Async1 *** 9 Async2 *** 5 Async2 *** 6 Async2 *** 7 Async2 *** 8 Async2 *** 9 익명함수 Go루틴 고루틴은 익명함수에 대해 사용할 수도 있다. 즉, go 키워드 뒤에 익명함수를 바로 정의하는 것으로, 익명함수를 비동기로 실행하게 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func main() { // WaitGroup 생성. 2개의 Go루틴을 기다림. var wait sync.WaitGroup wait.Add(2) // 익명함수를 사용한 goroutine go func() { defer wait.Done() //끝나면 .Done() 호출 fmt.Println(\u0026#34;Hello\u0026#34;) }() // 익명함수에 파라미터 전달 go func(msg string) { defer wait.Done() //끝나면 .Done() 호출 fmt.Println(msg) }(\u0026#34;Hi\u0026#34;) wait.Wait() //Go루틴 모두 끝날 때까지 대기 } 첫번째 익명함수는 간단히 Hello를 출력하는데, 이를 고루틴으로 실행하면 비동기적으로 그 익명함수를 실행하게 된다. 두번째 익명함수는 파라미터를 전달하는 예제로 익명함수에 파라미터가 있는 경우, go 익명함수 바로 뒤에 파라미터를 함께 전달하게 된다.\n여기서 sync.WaitGroup을 사용하고 있는데, 이는 기본적으로 여러 고루틴들이 끝날 때까지 기다리는 역할을 한다. WaitGroup을 사용하기 위해서는 먼저 Add() 메소드에 몇 개의 Go루틴을 기다릴 것인지 지정하고, 각 고루틴에서 Done() 메서드를 호출한다.(여기서는 defer 사용)\n그리고 메인루틴에서는 Wait() 메서드를 호출하여, Go루틴들이 모두 끝나기를 기다린다.\n다중 CPU 처리 go는 디폴트로 CPU 1개를 사용한다. 여러 개 고루틴을 만들더라도, CPU 1개에서 작업을 시분할하여 처리한다. 만약 머신이 CPU 여러개를 가진 경우, Go 프로그램을 다중 CPU에서 병렬처리(Perallel)하게 할 수 있는데, 병렬처리를 위해서는 아래와 같이 runtime.GOMAXPROCS(CPU수) 함수를 호출하여야 한다.\nCPU 수는 Logical CPU 수를 가리킨다. 1 2 3 4 5 6 7 8 9 10 11 12 package main import ( \u0026#34;runtime\u0026#34; ) func main() { // 4개의 CPU 사용 runtime.GOMAXPROCS(4) //... } 동시성과 병렬성은 비슷하게 들리지만 전혀 다른 개념이다.\n","date":"2023-04-20T13:28:40+09:00","image":"https://codemario318.github.io/post/go_12/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/go_12/","title":"Golang: 12. Go 루틴"},{"content":"지연실행 defer Go 언어의 defer 키워드는 특정 문장 혹은 함수를 나중에 (defer를 호출하는 함수의 결과를 반환하기 직전에) 실행된다.\n일반적으로 defer는 C#, Java 같은 언어에서 finally 블럭처럼 마지막에 Clena-up 작업을 위해 사용된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import \u0026#34;os\u0026#34; func main() { f, err := os.Open(\u0026#34;1.txt\u0026#34;) if err != nil { panic(err) } // main 마지막에 파일 close 실행 defer f.Close() // 파일 읽기 bytes := make([]byte, 1024) f.Read(bytes) println(len(bytes)) } 파일을 Open 한 후 바로 파일을 Close하는 작업을 defer로 쓰고 있다. 이는 차후 문장에서 어떤 에러가 발생하더라도 항상 파일을 Close할 수 있도록 한다. panic 함수 Go 내장함수인 panic()함수는 현재 함수를 즉시 멈추고 현재 함수에 defer 함수들을 모두 실행한 후 즉시 리턴한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package main import \u0026#34;os\u0026#34; func main() { // 잘못된 파일명을 넣음 openFile(\u0026#34;Invalid.txt\u0026#34;) // openFile() 안에서 panic이 실행되면 // 아래 println 문장은 실행 안됨 println(\u0026#34;Done\u0026#34;) } func openFile(fn string) { f, err := os.Open(fn) if err != nil { panic(err) } defer f.Close() } 이러한 panic 모드 실행 방식은 다시 상위함수에도 똑같이 적용되고, 계속 콜스택을 타고 올라가며 적용된다. 마지막에는 프로그램이 에러를 내고 종료하게 된다. recover 함수 Go 내장합수인 recover() 함수는 panic 함수에 의한 패닉상태를 다시 정상상태로 되돌리는 함수이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) func main() { // 잘못된 파일명을 넣음 openFile(\u0026#34;Invalid.txt\u0026#34;) // recover에 의해 // 이 문장 실행됨 println(\u0026#34;Done\u0026#34;) } func openFile(fn string) { // defer 함수. panic 호출시 실행됨 defer func() { if r := recover(); r != nil { fmt.Println(\u0026#34;OPEN ERROR\u0026#34;, r) } }() f, err := os.Open(fn) if err != nil { panic(err) } defer f.Close() } recover 함수를 사용하면 panic 상태를 제거하고 openFile() 다음 문장인 println()을 호출하게 된다. ","date":"2023-04-20T13:26:40+09:00","image":"https://codemario318.github.io/post/go_11/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/go_11/","title":"Golang: 11. defer와 panic"},{"content":"Go는 내장 타입으로 error 라는 인터페이스 타입을 갖는다. Go 에러는 이 error 인터페이스를 통해서 주고 받게 되는데, 메서드 하나를 갖는다.\n1 2 3 type error interface { Error() string } 개발자는 error 인터페이스를 구현하는 커스텀 에러 타입을 만들 수 있다.\nGo 에러처리 Go 함수가 결과와 에러를 함께 반환한다면, 이 에러가 nil 인지 체크하여 에러를 확인할 수 있다.\n예를들어 os.Open() 함수는 func Open(name string) (file *File, err error) 과 같은 함수 원형을 갖는데, File 포인터와 error 인터페이스를 함께 반환한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import ( \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; ) func main() { f, err := os.Open(\u0026#34;C:\\\\temp\\\\1.txt\u0026#34;) if err != nil { log.Fatal(err.Error()) } println(f.Name()) } 그래서 반환된 두번째 error을 체크하여 nil 이면 에러가 없는 것이고, 아니라면 err.Error()로 부터 해당 에러를 알 수 있다.\nlog.Fatal()은 메시지를 출력하고 os.Exit(1)을 호출하여 프로그램을 종료한다. error Type을 이용한 에러처리 또 다른 에러처리로 error의 Type을 체크하여 에러 타입별로 별도의 에러 처리를 하는 방식이 있다.\n1 2 3 4 5 6 7 8 9 _, err := otherFunc() switch err.(type) { default: // no error println(\u0026#34;ok\u0026#34;) case MyError: log.Print(\u0026#34;Log my error\u0026#34;) case error: log.Fatal(err.Error()) } 예제에서 otherFunc()를 호출한 후 반환된 error의 타입을 통해 여러 유형의 에러를 처리할 수 있다.\n모든 에러는 error 인터페이스를 구현하므로 마지막 case문은 모든 에러에 적용된다. ","date":"2023-04-20T13:25:40+09:00","image":"https://codemario318.github.io/post/go_10/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/go_10/","title":"Golang: 10. 에러"},{"content":"구조체가 필드들의 집합체라면, interface는 메서드들의 집합체이다.\n인터페이스는 타입(type)이 구현해야 하는 메서드 원형(prototype)들을 정의한다. 어떠한 사용자 정의 타입이 인터페이스를 구현하려면 선언한 인터페이스가 갖는 모든 메서드들을 구현하면 된다.\n1 2 3 4 type Shape interface { area() float64 perimeter() float64 } 인터페이스는 구조체와 마찬가지로 type문을 사용하여 정의한다.\n인터페이스 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //Rect 정의 type Rect struct { width, height float64 } //Circle 정의 type Circle struct { radius float64 } //Rect 타입에 대한 Shape 인터페이스 구현 func (r Rect) area() float64 { return r.width * r.height } func (r Rect) perimeter() float64 { return 2 * (r.width + r.height) } //Circle 타입에 대한 Shape 인터페이스 구현 func (c Circle) area() float64 { return math.Pi * c.radius * c.radius } func (c Circle) perimeter() float64 { return 2 * math.Pi * c.radius } Shape 인터페이스를 구현하기 위해서는 area(), perimeter() 2개 메서드만 구현하면 된다.\n인터페이스 사용 인터페이스를 사용하는 일반적인 예로 함수가 파라미터로 인터페이스를 받을 수 있다.\n함수 파라미터가 인터페이스인 경우, 어떤 타입이든 해당 인터페이스를 구현하기만 하면 모두 입력 파라미터로 사용될 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func main() { r := Rect{10., 20.} c := Circle{10} showArea(r, c) } func showArea(shapes ...Shape) { for _, s := range shapes { a := s.area() //인터페이스 메서드 호출 p := s.perimeter() println(a, p) } } Rect 구조체와 Circle 구조체는 Shape 인터페이스에서 선언한 area, perimeter 메서드를 구현하고 있기 때문에 Shape 인터페이스를 상속했다고 볼 수 있다. 따라서 showArea에서 Shape 인터페이스 파라미터 입력으로 사용 가능하다.\n인터페이스 타입 Go 프로그래밍을 하다보면 흔히 빈 인터페이스(empty interface)를 자주 접할 수 있는데, 이는 인터페이스 타입(interface type)으로 불린다.\n여러 표준패키지들의 함수 Prototype을 살펴보면, 빈 인터페이스가 자주 등장한다.\n빈 인터페이스는 interface{} 로 표현한다. 1 2 3 func Marshal(v interface{}) ([]byte, error); func Println(a ...interface{}) (n int, err error); 빈 인터페이스는 메서드를 전혀 갖지 않는 인터페이스이다.\nGo의 모든 Type은 적어도 0개의 메서드를 구현하므로, Go에서 모든 Type을 의미한다.\n즉, 빈 인터페이스는 어떠한 타입도 담을 수 있는 컨테이너로 볼 수 있고, 여러 다른 언어에서 흔히 일컫는 Dynamin Type으로 사용할 수 있다. (C#, java ⇒ object, C,C++ ⇒ void*)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import \u0026#34;fmt\u0026#34; func main() { var x interface{} x = 1 x = \u0026#34;Tom\u0026#34; printIt(x) } func printIt(v interface{}) { fmt.Println(v) //Tom } Type Assertion 인터페이스 타입의 x와 타입 T에 대하여 x.(T)로 표현했을 때, 이는 x가 nil이 아니며, x는 T 타입에 속한다는 것을 확인한다. 이러한 표현방식을 Type Assertion 이라고 부른다.\n만약 x가 nil 이거나 x의 타입이 T가 아니라면, 런타임 에러가 발생하고, x가 T 타입인경우는 T 타입 x를 반환한다.\n","date":"2023-04-20T13:08:40+09:00","image":"https://codemario318.github.io/post/go_9/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/go_9/","title":"Golang: 9. 인터페이스"},{"content":"Go 언어는 OOP를 구조체와 메서드를 이용하는 방식으로 지원한다.\n다른 언어들이 클래스가 내부에 데이터와 메서드를 함께 갖는 것과 달리 Go 언어에서는 구조체가 데이터만을 가지고, 메서드는 별도로 분리하여 정의한다.\nGo 메서드는 특별한 형태의 함수이다. 메서드는 함수 정의에서 func 키워드와 함수명 사이에 “그 함수가 어떤 구조체를 위한 메서드인지” 표시한다.\nreceiver로 부르며 메서드가 속한 구조체 타입과 변수명을 지정하는데, 구조체 변수명은 함수 내에서 입력 파라미터처럼 사용된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main //Rect - struct 정의 type Rect struct { width, height int } //Rect의 area() 메소드 func (r Rect) area() int { return r.width * r.height } func main() { rect := Rect{10, 20} area := rect.area() //메서드 호출 println(area) } area는 Rect의 메소드로 선언 되어 메서드를 입력처럼 사용할 수 있다.\n(value VS 포인터) receiver Value receiver value receiver는 구조체의 데이터를 복사하여 전달한다. value receiver는 메서드 내의 필드값 변경되더라도 실제 데이터는 변경되지 않는다.\n1 2 3 4 5 6 7 8 9 10 func (r Rect) area2() int { r.width++ return r.width * r.height } func main() { rect := Rect{10, 20} area := rect.area2() //메서드 호출 println(rect.width, area) // 10 220 출력 } 포인터 리시버 포인터 리시버는 구조체의 포인터를 전달한다. 메서드 내의 필드값 변경이 그대로 호출자에서 반영된다.\n","date":"2023-04-20T12:34:40+09:00","image":"https://codemario318.github.io/post/go_8/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/go_8/","title":"Golang: 8. 메서드"},{"content":"Go 언어는 객체지향 프로그래밍을 고유의 방식으로 지원한다.\n클래스, 객체, 상속 개념이 없다. 전통적인 OOP의 클래스는 Go 언어에서 Custom Type을 정의하는 struct(구조체)로 표현한다.\nStruct Go에서 struct는 Custom Data Type을 표현하는데 사용된다(C 처럼)\n필드들의 집합체이며 필드들의 컨테이너이다. struct는 필드 데이터만을 가지며(자료 구조 역할), 메서드는 표현하지 않는다. 메서드는 별도로 분리하여 정의한다.\nStruct 선언 구조체를 정의하기 위해서 Custom Type을 정의하는데 사용하는 type 문을 사용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import \u0026#34;fmt\u0026#34; // struct 정의 type person struct { name string age int } func main() { // person 객체 생성 p := person{} // 필드값 설정 p.name = \u0026#34;Lee\u0026#34; p.age = 10 fmt.Println(p) } person 구조체를 패키지 외부에서 사용할 수 있게 하려면 struct명을 Person으로 변경하면 된다.\nstruct 객체 생성 선언된 struct 타입으로부터 객체를 생성하는 방법은 몇 가지 방법들이 있다.\n빈 객체를 먼저 할당하고, 나중에 그 필드값을 채워넣는 방법\n1 2 3 4 p := person{} p.name = \u0026#34;Lee\u0026#34; p.age = 10 초기값을 함께 할당하는 방법\n1 2 3 var p1 person p1 = person{\u0026#34;Bob\u0026#34;, 20} p2 := person{name: \u0026#34;Sean\u0026#34;, age: 50} 초기화가 생략된 필드들은 Zero value (정수인 경우 0, float인 경우 0.0, string인 경우 \u0026ldquo;\u0026rdquo;, 포인터인 경우 nil 등)를 갖는다.\nnew 내장함수 사용\n1 2 p := new(person) p.name = \u0026#34;Lee\u0026#34; // p가 포인터라도 . 을 사용한다 new()를 사용하면 모든 필드를 Zero value로 초기화하고 person 객체의 포인터를 반환한다.\n객체 포인터인 경우에도 필드 엑세스시 . 을 사용하는데 이 때 포인터는 자동으로 역참조된다. (c에서 → 과 동일한 동작)\nGo에서 struct는 기본적으로 mutable 개체로서 필드값이 변화할 경우 해당 개체 메모리에서 직접 변경된다. 하지만 struct 개체를 다른 함수의 파라미터로 넘기면, Pass by Value에 따라 객체를 복사해서 전달한다. 따라서 struct 개체의 값을 변경하려면 포인터를 전달해야 한다.\n생성자 함수 구조체 필드가 사용 전에 초기화되어야 하는 경우가 있다. 예를 들어 struct 필드가 map 타입인 경우 구조체 할당 후 map을 다시 할당받고 초기화 해야한다.\n이럴 때 map을 사전에 미리 초기화 하면, 외부에서 구조체를 사용할 때 별도로 초기화하는 번거로움을 줄일 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main type dict struct { data map[int]string } //생성자 함수 정의 func newDict() *dict { d := dict{} d.data = map[int]string{} return \u0026amp;d //포인터 전달 } func main() { dic := newDict() // 생성자 호출 dic.data[1] = \u0026#34;A\u0026#34; } 생성자 함수인 newDict()는 dict라는 구조체의 map 필드를 초기화한 후 구조체 포인터를 반호나한다. main 함수에서 구조체 개체를 만들 때 dict를 직접 생성하지 않고 대신 생성자 함수를 통해 이미 초기환 된 data 맵 필드를 사용할 수 있다.\n","date":"2023-04-20T12:30:40+09:00","image":"https://codemario318.github.io/post/go_7/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/go_7/","title":"Golang: 7. 구조체"},{"content":"Go는 패키지를 통해 모듈화, 재사용 기능을 제공한다. 패키지를 사용해서 작은 단위의 컴포넌트를 작성하고, 이러한 작은 패키지들을 활용해서 프로그램을 작성할 것을 권장한다.\nGo는 실제 프로그램 개발에 필요한 많음 패키지들을 표준 라이브러리로 제공한다. 이러한 표준 라이브러리 패키지들을 GOROOT/pkg 안에 존재한다. GOROOT 환경변수는 Go 설치 디렉토리를 가르키는데, 보통 Go 설치시 자동으로 추가된다.\nGo에 사용하는 표준 패키지들은 https://golang.org/pkg 에 자세히 설명되어 있다.\nMain 패키지 일반적으로 패키지는 라이브러리로서 사용되지만, main이라고 명명된 패키지는 Go Compiler에 의해 특별하게 인식된다. 패키지명이 main인 경우, 컴파일러는 해당 패키지를 공유 라이브러리가 아닌 실행 프로그램으로 만든다. 그리고 이 main 패키지 안의 main() 함수가 프로그램의 시작점, 즉 Entry Point가 된다. 패키지를 공유 라이브러리로 만들 때에는, main패키지나 main 함수를 사용해서는 안된다.\n패키지 Import 다른 패키지를 프로그램에서 사용하기 위해서는 import 키워드를 사용하여 패키지를 포함시킨다.\n예를 들어 Go 표준 라이브러리인 fmt 패키지를 사용하기 위해 import “fmt”와 같이 해당 패키지를 포함시킬 것을 선언해 준다.\n1 2 3 4 5 6 7 package main import \u0026#34;fmt\u0026#34; func main(){ fmt.Println(\u0026#34;Hello\u0026#34;) } 패키지를 import 할 때, Go 컴파일러는 GOROOT 혹은 GOPATH 환경 변수를 검색하는데, 표준 패키지는 GOROOT/pkg 에서 사용자 패키지나 3rd party 패키지의 경우 GOPATH/pkg에서 패키지를 찾는다.\nGOROOT 환경변수는 Go 설치시 자동으로 시스템에 설정되지만, GOPATH는 사용자가 지어해 주어야 한다. GOPATH 환경변수는 3rd party 패키지를 갖는 라이브러리 디렉토리나 사용자 패키지가 있는 작업 디렉토리를 지정하게 되는데, 복수 개일 경우 세미콜론을 사용하여 연결한다.\n패키지 scope 패키지 내에는 함수, 구조체, 인터페이스, 메서드 등이 존재하는데, 이름의 첫 문자를 대문자로 시작하면 public으로 사용할 수 있다. 패키지 외부에서 이들을 호출하거나 사용할 수 있게 된다.\n반면 , 이름이 소문자로 시작하면 이는 non-public으로 패키지 내부에서만 사용될 수 있다.\n패키지 init 함수와 alias 개발자가 패키지를 작성할 때, 패키지 실행시 처음으로 호출되는 init() 함수를 작성할 수 있다. init 함수는 패키지가 로드되면서 실행되는 함수로 별도의 호출 없이 자동으로 호출된다.\n1 2 3 4 5 6 7 package testlib var pop map[string]string func init() { // 패키지 로드시 map 초기화 pop = make(map[string]string) } 패키지를 import 하면서 그 패키지 안의 init() 만 호출하길 원한다면, import 시 _ 라는 alias를 지정한다.\n1 2 package main import _ \u0026#34;other/xlib\u0026#34; 패키지 이름이 동일하지만, 서로 다른 버전 혹은 서로 다른 위치에서 로딩하고자 할 때는 패키지 alias를 사용하여 구분할 수 있다.\n1 2 3 4 5 6 7 8 9 import ( mongo \u0026#34;other/mongo/db\u0026#34; mysql \u0026#34;other/mysql/db\u0026#34; ) func main() { mondb := mongo.Get() mydb := mysql.Get() //... } 사용자 정의 패키지 생성 사용자 정의 패키지를 만들어 재사용 가능한 컴포넌트를 만들어 사용할 수 있다. 사용자 정의 라이브러리 패키지는 일반적으로 폴더를 하나 만들고 그 폴더 안에 .go 파일들을 만들어 구성한다.\n하나의 서브 폴더안데 있는 .go 파일들은 동일한 패키지명을 가지며, 패키지명은 해당 폴더의 이름과 같게 한다. 해당 폴더에 있는 여러 .go 파일들은 하나의 패키지로 묶인다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package testlib import \u0026#34;fmt\u0026#34; var pop map[string]string func init() { pop = make(map[string]string) pop[\u0026#34;Adele\u0026#34;] = \u0026#34;Hello\u0026#34; pop[\u0026#34;Alicia Keys\u0026#34;] = \u0026#34;Fallin\u0026#39;\u0026#34; pop[\u0026#34;John Legend\u0026#34;] = \u0026#34;All of Me\u0026#34; } // GetMusic : Popular music by singer (외부에서 호출 가능) func GetMusic(singer string) string { return pop[singer] } func getKeys() { // 내부에서만 호출 가능 for _, kv := range pop { fmt.Println(kv) } } 패키지명은 폴더명과 동일하게 정해야 한다. 패키지 폴더 안에 여러 파일들이 있을 경우에도, 동일한게 폴더 이름을 패키지 이름으로 사용한다.\n💡 사이즈가 큰 목잡한 라이브러리 같은 경우, go install 명령을 사용하여 라이브러리를 컴파일 하여 cache 할 수 있는데, 다음 빌드시 빌드 타임을 크게 줄일 수 있다.\n1 2 cd {{package_dir}} go install 패키지를 찾기 위해 GOROOT와 GOPATH를 사용하는데, GOROOT와 GOPATH에 있는 각 루트폴더의 해당 패키지를 찾게 된다.\nGOPATH가 C:\\GoApp;C:\\GoSrc라면 지정된 라이브러리를 찾기 위해 폴더를 순차적으로 검색하게 된다.\n1 2 3 C:\\Go\\src\\testlib (from $GOROOT) C:\\GoApp\\src\\testlib (from $GOPATH) C:\\GoSrc\\src\\testlib ","date":"2023-04-20T12:28:40+09:00","image":"https://codemario318.github.io/post/go_6/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/go_6/","title":"Golang: 6. 패키지"},{"content":"배열 Array 배열은 연속적인 메모리 공간에 동일한 타입의 데이터를 순차적으로 저장하는 자료구조이다.\n배열의 선언은 var 변수명 [배열크기] 데이터타입 과 같이 한다. Go에서 배열크기는 Type을 구성하는 한 요소이다. 따라서, [3]int와 [5]int는 서로 다른 타입으로 인식된다.\n1 2 3 4 5 6 7 8 9 package main func main() { var a [3]int //정수형 3개 요소를 갖는 배열 a 선언 a[0] = 1 a[1] = 2 a[2] = 3 println(a[1]) // 2 출력 } 배열 초기화 배열을 정의할 때, 초기값을 설정할 수 있다. 초기값은 [배열크기] 데이터타입 {초기값0, 초기값1 ...} 로 할당한다.\n초기화 과정에서 [\u0026hellip;]를 사용하여 배열 크기를 생략하면 자동으로 초기화 요소 개수만큼 배열 크기가 정해진다.\n1 2 var a1 = [3]int{1, 2, 3} var a3 = [...]int{1, 2, 3} //배열크기 자동으로 다차원 배열 1 2 var multiArray [3][4][5]int // 정의 multiArray[0][1][2] = 10 다차원 배열 초기화 1 2 3 4 5 6 7 func main() { var a = [2][3]int{ {1, 2, 3}, {4, 5, 6}, //끝에 콤마 추가 } println(a[1][2]) } 슬라이스 Slice Go 배열은 고정된 크기 안에 동일한 타입의 데이터를 연속적으로 저장하지만, 배열의 크기를 동적으로 증가시키거나 부분 배열을 추출하는 등의 기능은 없다.\nGo 슬라이스는 내부적으로 배열에 기초하여 만들어졌지만 편리하고 유용한 기능들을 제공한다.\n고정된 크기를 지정하지 않을 수 있음 크기를 동적으로 변경할 수 있음 부분 배열 추출 가능 등 Go 슬라이스 선언은 배열을 선언하듯이 var v []T 처럼 하는데 배열과 달리 크기는 지정하지 않는다.\n1 2 3 4 5 6 7 8 9 package main import \u0026#34;fmt\u0026#34; func main() { var a []int //슬라이스 변수 선언 a = []int{1, 2, 3} //슬라이스에 리터럴값 지정 a[1] = 10 fmt.Println(a) // [1, 10, 3]출력 } make() 슬라이스를 생성하는 다른 방법으로 내장함수 make()를 이용할 수 있다.\nmake함수로 슬라이스를 생성하면 슬라이스 길이(length), 용량(Capacity)을 임의로 지정할 수 있는 장점이 있다.\n1 2 3 4 func main() { s := make([]int, 5, 10) println(len(s), cap(s)) // len 5, cap 10 } → 슬라이스의 길이는 len(), 용량은 cap()을 써서 확인할 수 있다.\n1 2 3 4 5 6 7 8 func main() { var s []int if s == nil { println(\u0026#34;Nil Slice\u0026#34;) } println(len(s), cap(s)) // 모두 0 } make함수로 슬라이스를 생성하면 모든 요소가 Zero value인 슬라이스를 만들게 된다. 또한 슬라이스에 별도의 길이와 용량을 지정하지 않으면, 기본적으로 길이와 용량이 0인 슬라이스를 만드는데 이를 Nill Slice 라고 하며, nil 과 비교하면 참을 반환한다.\n#$# 부분 슬라이스\n슬라이스에서 일부를 발췌하여 부분 슬라이스를 만들 수 있다. 부분 슬라이스는 슬라이스[시작인덱스:마지막인덱스] 형식으로 만든다.\n시작 인덱스는 inclusive이며 마지막 인덱스는 Exclusive이다(파이썬과 동일함)\n1 2 3 4 5 6 7 8 package main import \u0026#34;fmt\u0026#34; func main() { s := []int{0, 1, 2, 3, 4, 5} s = s[2:5] fmt.Println(s) //2,3,4 출력 } 부분 슬라이스에서 인덱스는 생략 가능하다.\n처음 인덱스 생략: 0 자동 대입 마지막 인덱스 생략: 슬라이스 길이 자동대입 따라서 모두 생략하면 전체를 가져온다.\n1 2 3 4 s := []int{0, 1, 2, 3, 4, 5} s = s[2:5] // 2, 3, 4 s = s[1:] // 3, 4 fmt.Println(s) // 3, 4 출력 슬라이스 추가 배열은 고정된 크기로 그 크기 이상의 데이터를 임의로 추가할 수 없지만, 슬라이스는 자유롭게 새로운 요소를 추가할 수 있다.\n슬라이스에 새로운 요소를 추가하려면 내장함수 append()를 사용한다.\n1 2 3 4 5 6 7 8 9 10 func main() { s := []int{0, 1} // 하나 확장 s = append(s, 2) // 0, 1, 2 // 복수 요소들 확장 s = append(s, 3, 4, 5) // 0,1,2,3,4,5 fmt.Println(s) } append 동작 과정 슬라이스 용량이 남아있는 경우 슬라이스의 길이를 변경하여 데이터를 추가 슬라이스 용량을 초과하는 경우 현재 용량의 2배에 해당하는 새로운 Underlying array를 생성하고 기존 배열 값들을 모두 새 배열에 복제한 후 다시 슬라이스를 할당. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main import \u0026#34;fmt\u0026#34; func main() { // len=0, cap=3 인 슬라이스 sliceA := make([]int, 0, 3) // 계속 한 요소씩 추가 for i := 1; i \u0026lt;= 15; i++ { sliceA = append(sliceA, i) // 슬라이스 길이와 용량 확인 fmt.Println(len(sliceA), cap(sliceA)) } fmt.Println(sliceA) // 1 부터 15 까지 숫자 출력 } 병합 한 슬라이스를 다른 슬라이스 뒤에 병합하기 위해서는 append()와 ellipsis(...)를 사용한다.\nellipsis(...) 은 파이썬 asterisk(*)와 같이 동작한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 package main import \u0026#34;fmt\u0026#34; func main() { sliceA := []int{1, 2, 3} sliceB := []int{4, 5, 6} sliceA = append(sliceA, sliceB...) //sliceA = append(sliceA, 4, 5, 6) fmt.Println(sliceA) // [1 2 3 4 5 6] 출력 } 복사 슬라이스는 내장함수 copy()를 사용하여 한 슬라이스를 다른 슬라이스로 복사할 수도 있다.\n1 2 3 4 5 6 7 func main() { source := []int{0, 1, 2} target := make([]int, len(source), cap(source)*2) copy(target, source) fmt.Println(target) // [0 1 2 ] 출력 println(len(target), cap(target)) // 3, 6 출력 } 슬라이스 내부구조 슬라이스는 내부적으로 사용하는 배열의 부분 영역인 세그먼트에 대한 메타 정보를 가지고 있다. 슬라이스는 크게 3개의 필드로 구성되어 있다.\n내부적으로 사용하는 배열에대한 포인터 세그먼트 길이 세그먼트 최대 용량 처음 슬라이스가 생성될 때, 길이와 용량이 지정되었다면, 내부적으로 용량만큼 크기의 배열을 생성하고, 슬라이스 첫번째 필드에 그 배열의 처음 메모리 위치를 지정한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main const limit = 10 func main() { slice := []int{0, 1, 2, 3, 4, 5} subSlice := slice[2:5] subSlice[0] = 20 subSlice[1] = 30 subSlice[2] = 40 for _, v := range slice { println(v) } } 1 2 3 4 5 6 0 1 20 30 40 5 서브 슬라이스를 만들면 슬라이스 생성시와 마찬가지로 지정 인덱스만큼의 길이와 용량을 설정하게 되며, 배열 포인터는 시작 슬라이스 위치로 초기화 된다.\n위의 예시의 서브 슬라이스의 길이는 3 용량은 4로 만들어진다.\n따라서 서브 슬라이스의 값을 변경하면 원본 슬라이스의 값도 변경된다.\nMap 선언 Map은 키에 대응하는 값을 신속히 찾는 해시테이블을 구현한 자료구조이다. Go 언어는 Map 타입을 내장하고 있는데, map[key 타입]값타입 로 선언할 수 있다.\n1 var idMap map[int]string 이때 선언된 idMap은 레퍼런스 타입이므로 nil 값을 갖으며, 이를 Nil Map이라고 부른다. Nil map은 어떤 데이터를 쓸 수 없는데, map을 초기화하기 위해 make()함수를 사용할 수 있다.\n초기화 make() 1 idMap = make(map[int]string) make() 함수의 첫번째 파라미터로 map 키워드와 [키타입]값타입을 지정하는데, 이때 make()함수는 해시테이블 자료구조를 메모리에 생성하고 그 메모리를 가리키는 map value를 리턴한다.\n→ map value는 내부적으로 runtime.hmap 구조체를 가리키는 포인터이다.\n따라서 idMap 변수는 이 해시테이블을 가리키는 map을 가리키게 된다.\n초기화 - 리터럴 map은 make() 함수를 써서 초기화할 수도 있지만, 리터럴을 사용해 초기화할 수도 있다. 리터럴 초기화는 map[key타입]value타입 {key:value} 와 같이 Map 타입 뒤 중괄호 안에 \u0026lsquo;키:값\u0026rsquo;들을 결거하면 된다.\n1 2 3 4 5 ticker := map[string]string{ \u0026#34;GOOG\u0026#34;: \u0026#34;Google Inc\u0026#34;, \u0026#34;MSFT\u0026#34;: \u0026#34;Microsoft\u0026#34;, \u0026#34;FB\u0026#34;: \u0026#34;FaceBook\u0026#34;, } Map 사용 처음 map이 make() 함수에 의해 초기화 되었을 때는, 아무 데이터가 없는 상태이다. 이때 새로운 데이터를 추가하기 위해서는 map변수[키] = 값 처럼 해당 키에 그 값을 할당하면 된다.\n만약 키 값이 이미 존재한다면 추가 대신 값만 갱신한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package main func main() { var m map[int]string m = make(map[int]string) m[901] = \u0026#34;Apple\u0026#34; m[134] = \u0026#34;Grape\u0026#34; m[777] = \u0026#34;Tomato\u0026#34; str := m[134] println(str) noData := m[999] println(noData) println(m[777]) delete(m, 777) println(m[777]) } 만약 map안에 찾는 키가 존재하지 않는다면 reference 타입인 경우 nil, value 타입인 경우 zero를 리턴한다.\nmap에서 특정 키와 그 값을 삭제하기 위해서는 delete() 함수를 이용한다.\nMap 키 체크 map을 사용하는 경우 종종 map안에 특정 키가 존재하는지를 체크할 필요가 있다. 이를 위해 go에선 “map 변수[키]” 읽기를 수행할 때 2개 값을 반환한다.\n키에 상응하는 값 키 존재 여부 (bool) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main func main() { tickers := map[string]string{ \u0026#34;GOOG\u0026#34;: \u0026#34;Google Inc\u0026#34;, \u0026#34;MSFT\u0026#34;: \u0026#34;Microsoft\u0026#34;, \u0026#34;FB\u0026#34;: \u0026#34;FaceBook\u0026#34;, \u0026#34;AMZN\u0026#34;: \u0026#34;Amazon\u0026#34;, } // map 키 체크 val, exists := tickers[\u0026#34;MSFT\u0026#34;] if !exists { println(\u0026#34;No MSFT ticker\u0026#34;) } } for 루프를 사용한 Map 열거 Map이 가진 모든 요소들을 출력하기 위해, for range 루프를 사용할 수 있다. Map 컬렉션에 for range를 사용하면, Map 키와 Map 값 2개 데이터를 반환한다.\n","date":"2023-04-20T12:17:40+09:00","image":"https://codemario318.github.io/post/go_5/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/go_5/","title":"Golang: 5. 컬렉션"},{"content":"func 키워드를 사용하여 정의한다.\n함수 파라미터는 0개 이상 사용할 수 있는데, 각 파라미터는 파라미터명 뒤에 int, string 등 파라미터 타입을 정의한다.\n함수의 반환 타입은 파라미터 괄호 뒤에 적는다.\n함수는 패키지 안에 정의되며 호출되는 함수가 호출하는 함수의 반드시 앞에 위치해야 할 필요는 없다.\n1 2 3 4 5 6 7 8 9 package main func main() { msg := \u0026#34;Hello\u0026#34; say(msg) } func say(msg string) { println(msg) } Pass By Reference Go에서 파라미터를 전달하는 방식은 크게 Pass By Value와 Pass By Reference로 나뉜다.\nPass By Value 함수를 사용할 때 변수를 그대로 사용하면 함수 인자로 사용된 변수들의 값이 복사되어 함수에게 전달된다.\n따라서 함수 인자로 받은 값을 함수 내부에서 변경해도 실제 변수값은 영향을 전혀 받지 않는다.\nPass By Reference 변수 앞에 \u0026amp; 를 붙이면 주소를 표시한다. 흔히 포인터라 부르며, 포인터를 사용하면 함수에서 해당 변수를 사용할 때 복사본이 아닌 실제 메모리에 접근하여 변수를 지정하므로 함수 내에서 변경이 인자에 넘겨진 주소를 가진 변수의 실제 값이 변경된다.\n1 2 3 4 5 6 7 8 9 10 11 package main func main() { msg := \u0026#34;Hello\u0026#34; say(\u0026amp;msg) println(msg) //변경된 메시지 출력 } func say(msg *string) { println(*msg) *msg = \u0026#34;Changed\u0026#34; //메시지 변경 } 함수에서 파라미터를 선언할 때 *string 처럼 포인터임을 표시하면 해당 파라미터가 문자열이 아닌 문자열을 저장하고있는 메모리의 주소를 갖는다.\n함수 내에서 포인터 파라미터의 주소에 저장된 데이터를 변경하려면 *변수명 = 값 형태로 변수 이름에 역참조 심볼인 *을 붙여 접근하고 변경할 수 있다.\n가변인자함수 함수에 여러개ㅢ 파라미터를 전달하려면 가변 파라미터를 나타내는 ... 을 타입 앞에 붙여 사용한다.\n가변 파라미터를 갖는 함수를 호출할 때 n개 동일 타입 파라미터를 전달할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 package main func main() { say(\u0026#34;This\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;book\u0026#34;) say(\u0026#34;Hi\u0026#34;) } func say(msg ...string) { for _, s := range msg { println(s) } } 함수 반환값 기본형 함수에서 반환값이 있는 경우 func 문 마지막에 리턴 타입을 정의한다. 그리고 return 키워드를 사용해야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main func main() { total := sum(1, 7, 3, 5, 9) println(total) } func sum(nums ...int) **int** { s := 0 for _, n := range nums { s += n } return **s** } 복수 개 반환값 go 언어에서 함수는 반환값이 여러개일 수 있다.\n여러개 값을 반환하기 위해서는 해당 반환 타입들을 괄호 안에 적어준다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main func main() { count, total := sum(1, 7, 3, 5, 9) println(count, total) } func sum(nums ...int) (int, int) { s := 0 // 합계 count := 0 // 요소 갯수 for _, n := range nums { s += n count++ } return count, s } Named Return Parameter Named Return Parameter라는 기능을 제공하는데, 이는 반환되는 값들을 미리 선언하며, 값들이 여러 개 일때, 코드 가독성을 높힌다.\n1 2 3 4 5 6 7 func sum(nums ...int) (**count** int, **total** int) { for _, n := range nums { total += n } count = len(nums) return } 마지막에 빈 return이 있지만 named return parameter로 정의된 형태로 반환된다. return 은 반환값에 아무런 영향을 주지 않지만 생략하면 에러가 발생한다.\n","date":"2023-04-20T12:10:40+09:00","image":"https://codemario318.github.io/post/go_4/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/go_4/","title":"Golang: 4. 함수"},{"content":"for 문 Go 언어에서 반복문은 for 하나뿐이다. 다른 언어들과 비슷하게 for 초기값; 조건식; 증감 {...} 형식을 따른다.\n초기값, 조건식, 증감식은 경우에 따라 생략할 수 있고, 초기값; 조건식; 증감 을 둘러싸는 괄호를 추가하면 에러가 발생한다.\n1 2 3 4 5 6 7 8 9 package main func main() { sum := 0 for i := 1; i \u0026lt;= 100; i++ { sum += i } println(sum) } 조건식만 사용하는 for 루프 초기값과 증감식을 생략하고 조건식만 사용하면 다른 언어의 while 루프와 같게 동작한다.\n1 2 3 4 5 6 7 8 9 10 11 12 package main func main() { n := 1 for n \u0026lt; 100 { n *= 2 //if n \u0026gt; 90 { // break //} } println(n) } 무한루프 초기값, 조건식, 증감을 모두 생략하면 무한루프로 동작한다.\n1 2 3 4 5 6 7 package main func main() { for { println(\u0026#34;Infinite loop\u0026#34;) } } range 문 for range문은 컬렉션으로 부터 한 요소씩 가져와 차례로 for 블럭 문장들을 실행한다. 다른 언어의 foreach와 비슷하다.\nfor range 문은 for 인덱스, 요소값 := range 컬렉션 형태로 for 루프를 구성하는데, range 키워드 다음에 명시한 컬렉션으로부터 하나씩 요소를 반환하여 그 요소의 인덱스와 값을 for 키워드 다음 2개 변수에 각각 할당한다.\n1 2 3 4 5 names := []string{\u0026#34;홍길동\u0026#34;, \u0026#34;이순신\u0026#34;, \u0026#34;강감찬\u0026#34;} for index, name := range names { println(index, name) } break, countinue, goto break: for 루프 내에서 즉시 빠져나옴 continue: 루프 중간에서 나머지 문장들을 실행하지 않고 다음 루프를 시작함 goto: 임의의 문장으로 이동 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package main func main() { var a = 1 for a \u0026lt; 15 { if a == 5 { a += a continue // for루프 시작으로 } a++ if a \u0026gt; 10 { break //루프 빠져나옴 } } if a == 11 { goto END //goto 사용예 } println(a) END: println(\u0026#34;End\u0026#34;) } break 레이블 break문은 보통 단독으로 사용되지만, 경우에 따라 레이블을 붙여 지정된 레이블로 이동할 수도 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main func main() { i := 0 L1: for { if i == 0 { break L1 } } println(\u0026#34;OK\u0026#34;) } break의 레이블은 현재 보통 현재 for 루프 바로 위에 적게 되는데, 현재 루프를 빠져나와 지정된 레이블로 이동하고, 동작한 break문이 속한 for 루프 전체의 다음 문장을 실행한다.\n","date":"2023-04-20T12:05:40+09:00","image":"https://codemario318.github.io/post/go_3/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/go_3/","title":"Golang: 3. 반복문"},{"content":"if 문 if 문은 해당 조건이 맞으면 {} 블럭 안의 내용을 실행한다.\n조건식을 ()로 둘러 싸지 않아도 된다. 조건 블럭 시작 { 를 if문과 같은 라인에 두어야 한다. 1 2 3 4 5 6 7 if k == 1 { println(\u0026#34;One\u0026#34;) } else if k == 2 { //같은 라인 println(\u0026#34;Two\u0026#34;) } else { //같은 라인 println(\u0026#34;Other\u0026#34;) } Optional Statement if문에서 조건식을 사용하기 이전에 간단한 문장(Optional Statement)을 함께 실행할 수 있다.\n1 2 3 4 5 6 if val := i * 2; val \u0026lt; max { println(val) } // 아래 처럼 사용하면 Scope 벗어나 에러 val++ val := i * 2 처럼 조건식 이전에 연산을 실행할 수 있는데, 정의된 변수 val의 범위는 if-else 블럭이다.\nif문 외에도 switch, for 에서도 사용할 수 있다.\nswitch 문 다른 언어들과 비슷하게 하나의 변수를 지정하고 case문에 해당 변수가 가질 수 있는 값들을 지정하여 case 블럭을 실행한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package main func main() { var name string var category = 1 switch category { case 1: name = \u0026#34;Paper Book\u0026#34; case 2: name = \u0026#34;eBook\u0026#34; case 3, 4: name = \u0026#34;Blog\u0026#34; default: name = \u0026#34;Other\u0026#34; } println(name) // Expression을 사용한 경우 switch x := category \u0026lt;\u0026lt; 2; x - 1 { //... } } 다른 언어와 차이점 C++, C#, Java 등과 조금 다르게 동작한다.\nswitch 뒤에 expression이 없을 수 있음 다른 언어는 switch 키워드 뒤에 변수나 조건식을 두지만, Go는 쓰지 않아도 된다. 이런 경우 true로 간주하고 첫번째 case문으로 이동하여 검사한다.\ncase문에 조건식 쓸 수 있음 다른 언어는 일반적으로 값을 갖지만, Go는 조건식을 쓸 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func grade(score int) { switch { case score \u0026gt;= 90: println(\u0026#34;A\u0026#34;) case score \u0026gt;= 80: println(\u0026#34;B\u0026#34;) case score \u0026gt;= 70: println(\u0026#34;C\u0026#34;) case score \u0026gt;= 60: println(\u0026#34;D\u0026#34;) default: println(\u0026#34;No Hope\u0026#34;) } } No default fall through case문에 기본적으로 break를 적용한다. 다음 case로 넘어가려면 fallthrough 키워드를 쓴다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func check(val int) { switch val { case 1: fmt.Println(\u0026#34;1 이하\u0026#34;) fallthrough case 2: fmt.Println(\u0026#34;2 이하\u0026#34;) fallthrough case 3: fmt.Println(\u0026#34;3 이하\u0026#34;) fallthrough default: fmt.Println(\u0026#34;default 도달\u0026#34;) } } Type switch switch 뒤 변수의 타입으로 분기할 수 있다.\n1 2 3 4 5 6 7 8 9 10 switch v.(type) { case int: println(\u0026#34;int\u0026#34;) case bool: println(\u0026#34;bool\u0026#34;) case string: println(\u0026#34;string\u0026#34;) default: println(\u0026#34;unknown\u0026#34;) } ","date":"2023-04-20T12:00:40+09:00","image":"https://codemario318.github.io/post/go_2/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/go_2/","title":"Golang: 2. 조건문"},{"content":"변수 기본 선언 변수는 Go 키워드 var을 사용하여 선언한다. var 키워드 뒤에 변수명을 적고 변수 타입을 적는다.\n변수를 선언하면서 초기값을 지정하지 않으면, Go는 Zero Value를 기본적으로 할당한다.\n1 var a int 초기값 지정 변수 선언문에서 변수 초기값을 할당할 수도 있다. float32 타입 변수 f에 11.0이라는 초기값을 할당하기 위해 아래같이 쓸 수 있다.\n1 var f float32 = 11. 할당 선언된 변수는 이후 해당 타입 값을 할당할 수 있다.\n1 2 a = 10 f = 12.0 →선언된 변수가 Go 프로그램 내에서 사용되지 않는다면, 에러를 발생시킨다.\nShort Assignment Statement 함수 내부라면 Short Assignment Statement를 사용할 수 있다.\n1 2 i := 1 s := \u0026#34;Hello\u0026#34; 함수 밖에서 선언시에는 var를 사용해야 한다.\n여러 개 변수 동일한 타입 변수가 여러개 있으면, 변수들을 나열하고 마지막에 타입을 한번만 지정할 수 있다.\n1 2 3 4 var i, j, k int // 선언과 동시에 초기화 가능 var i, j, k int = 1, 2, 3 타입 추론 Go 에서는 할당되는 값을 보고 그 타입을 추론하는 기능이 자주 사용된다.\n1 2 var i = 1 var s = \u0026#34;Hello\u0026#34; i는 정수형으로 1이 할당되고, s는 문자열로 Hello가 할당된다.\n상수 상수는 Go 키워드 const를 사용하여 선언한다. const 키워드 뒤에 상수명을 적고, 그 뒤에 상수 타입, 그리고 상수 값을 할당한다.\n1 2 const i int = 1 const s string = \u0026#34;Hello\u0026#34; 타입 추론 변수와 마찬가지로 타입을 생략하고 초기화 하면 Go에서 자동으로 타입을 추론한다.\n1 2 const i = 1 const s = \u0026#34;Hello\u0026#34; 여러개 상수를 묶어서 지정할 수 있다.\n1 2 3 4 5 const ( Visa = \u0026#34;Visa\u0026#34; Master = \u0026#34;Master\u0026#34; Amex = \u0026#34;American Express\u0026#34; ) iota iota 키워드를 사용하면 상수값을 0부터 순차적으로 부여할 수 있다.\n","date":"2023-04-20T11:56:40+09:00","image":"https://codemario318.github.io/post/go_1/go_cover_huf88b72bb4a5683fe0689ce424afc49ae_12554_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/go_1/","title":"Golang: 1. 변수와 상수"},{"content":"Vue? 개발자에게 더 쉽고, 가볍고, 누구나 빨리 배울 수 있는 접근성이 뛰어난 프레임워크를 목표로 개발됨\n기존 HTML 마크업 기반의 템플릿을 그대로 활용 CSS를 작성하는 스타일도 기존 문법을 그대로 따름 라우팅, 상태 관리, 빌드 도구 등 공식적으로 지원하는 라이브러리와 패키지를 통해 배포하여 복잡한 로직의 프론트엔드 개발을 비교적 단순하고 쉽게 만듦 React, Anguler에 비해서 처음 접하는 사용자들이 진입하기에 부담스럽지 않음\nVue3 개선점 가상돔 최적화 AS-IS\n기존 Vue의 렌더링을 위한 가상 DOM 설계는 HTML 기반의 템플릿을 제공하고 이 템플릿 구문을 가상 DOM 트리로 반환한 후 실제로 DOM의 어떤 영역이 업데이트 되어야 하는지 재귀적으로 탐색하는 방식\n이러한 방식은 변경사항 확인을 위해 DOM 트리를 완전 탐색을 하게 되므로, 작은 변경도 트리 전체를 확인하게 되어 비효율적임 TO-BE\n불필요한 탐색을 위한 코드를 제거하여 렌더링 성능을 향상시켜 가상돔 최적화를 진행함\n탬플릿 구문에서 정적 요소와 동적 요소를 구분하여 트리를 탐색할 때 변경이 발생하는 동적 요소만 탐색할 수 있게 변경 렌더링 시 객체가 여러 번 생성되는 것을 방지하기 위해 컴파일러가 미리 템플릿 구문 내에서 정적 요소, 서브 트리, 데이터 객체 등을 탐지해 렌더러 함수 밖으로 호이스팅함 컴파일러가 미리 템플릿 구문 내에서 동적 바인딩 요소에 대해 플래그를 생성함 특정 요소가 동적 클래스 바인딩을 가지고 있고 정적인 값이 지정된 속성을 갖고 있다면 클래스만 처리하면 되므로, 컴파일러가 미리 생성해둔 플래그로 필요한 부분만 처리하여 렌더링 속도 향상 트리쉐이킹 강화 트리쉐이킹이란?\n나무를 흔들어 잎을 떨어트리듯 모듈을 번들링하는 과정에서 사용하지 않는 코드를 제거하여 파일 크기를 줄이는 최적화 방안\nVue3는 템플릿 컴파일러가 실제 사용하는 코드만 임포트 하도록 하였음.\n양방향 데이터 바인딩을 지원하는 v-model 디렉티브와 같은 대부분의 사용자 정의 기능에서 트리쉐이킹이 가능했는데, 이를 강화하여 번들 크기를 절반 이상으로 대폭 줄일 수 있음\nCompoistion API AS-IS\n기존 Vue에서 하나의 컴포넌트에 여러 기능이 포함되면, 기능별로 데이터영역, 메소드 영역, 컴퓨티드 영역, 라이프 사이클 훅, 와치 등 많은 로직이 추가되고, 이러한 기능 별로 분리된 코드들이 각각 기능에 맞는 메서드에 포함되게 되어 필연적으로 섞이게 됨.\n이에 따라 여러 기능이 활용될수록 코드는 커지며 복잡도가 증가함\nTO-BE\n컴포지션 API는 모든 코드를 독립적으로 정의할 수 있다. 각 기능을 함수로 묶어 모듈화 하기 때문에 특정 기능의 유지 보수를 위해 해당 기능을 수행하는 함수만 확인하면 된다.\n코드 재사용 AS-IS\n기존 Vue 에서도 mixin, slot 등으로 컴포넌트 코드를 재사용 할 수 있었으나, 믹스인은 한계가 존재했음\n프로젝트가 커져 믹스인을 다중으로 상속하게 되면 이름 충돌로 인해 컨벤션 룰이 필요했음 매개변수를 믹스인을 통해 전달할 수 없어 유연성이 떨어짐 TO-BE\n컴포지션 API를 사용하면 인스턴스의 특정 기능 단위로 모듈화된 로직을 여러 컴포넌트에서 재사용 할 수 있다.\nmixin?\nVue 컴포넌트에 재사용 가능한 기능을 배포하는 유연한 방법. mixin 객체는 모든 구성요서 옵션을 포함할 수 있으며, 컴포넌트에 mixin을 사용하면 해당 mixin의 모든 옵션이 컴포넌트의 고유 옵션에 “혼합”됨\n그 외 주요 변화 텔레포트 리엑트에서 기본으로 제공하는 포털과 유사한 기능. vue가 기존에 Portal-Vue 플러그인을 통해 제공하고 있었던 기능.\n모달이나 알림 등과 같은 요소를 렌더링하려는 위치가 템플릿 구문이 속하는 컴포넌트와 다른 컴포넌트에 존재할 때, 다른 태그 위치로 모달의 위치를 조정하는 것 처럼 보이게 만드는 것을 CSS를 통해 해결하기 번거롭기 때문에, 보통 모달이 포함된 컴포넌트를 하나 더 만들어 컴포넌트의 구조를 변경하는 방식으로 구현되었음.\nvue3는 텔레포트를 사용하여 모달 컴포넌트를 분리하지 않고도 내부의 HTML을 특정 태그로 옮겨 렌더링 할 수 있게 되었음\n서스펜스 서스팬스 컴포넌트는 리액트가 지원하던 컴포넌트 종류 중 하나로, 컴포넌트 내에 있는 조건인 Async 구문이 충족되지 않으면 조건이 충족될 때까지 템플릿 내에 Fallback 구문을 렌더링함.\n컴포지션 API를 통해 setup() 함수 내에서 외부 API에 접근해 데이터를 가져오는 비동기 작업을 수행하면 데이터를 모두 가져올 때까지 로딩 표시를 해야 할 수 있다. 이럴 때 서스펜스를 사용해 컴포넌트를 감싸면 대체할 템플릿 구문을 렌더링 할 수 있다.\n데이터를 가져오는 도중 오류가 발생하면 Vue3의 새로운 라이프사이클 훅인 OnErrorCaptured를 제공하여, 에러에 대한 처리 구문을 Fallback 구문 대신 표시할 수 있다.\n리액티비티 API 이전 버전의 Vue는 인스턴스 내부에 오브젝트를 선언하고 새로운 속성을 추가하는 것을 감지할 수 없었다. 그래서 기존에는 Vue.set 메소드를 사용하여 기존 객체에 반응성을 부여했다.\nVue3는 이러한 데이터 반응성을 해결하기 위해 리액티비티 API를 지원한다. 객체에 반응성을 추가하기 위해서 리액티브 메소드를 사용하면 된다.\n단순 값이라면 ref 메소드를 사용한다. 이외에도 Readonly, ToRef 등 반응성을 지원하는 여러 API 가 추가되었다.\n","date":"2023-04-18T20:05:16+09:00","image":"https://codemario318.github.io/post/vue3/vue3_cover_huce45f5603be21a224ec2957025110a35_3700_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/vue3/","title":"Vue3 살펴보기"},{"content":" 화면이 수정될 때, 렌더링 과정을 최적화하는 방법\n재배치(Reflow)와 다시 그리기(Repainting) 처음 화면이 그려진 후 사용자의 인터렉션에 의해 요소가 변경되는 일이 발생하였을때, Render 트리가 변경되면서 발생한다.\n이 과정이 빈번하게 발생할 수록 성능 및 사용자 경험이 저하되기 때문에 이를 최소화하는 것이 좋다.\n다시 그리기(Repainting) 재배치가 발생하거나 요소의 색상등이 변경된 경우, 다시 화면에 표현하는 동작.\n화면의 구조가 변경되었을 때\nReflow 과정을 거쳐 화면 구조를 다시 계산한 후 Repaint 과정을 통해 화면을 다시 그린다.\n화면의 구조가 변경되었을 때에는 Reflow와 Repaint 모두 발생합니다. 화면의 구조가 변경되지 않는 화면 변화의 경우\nRepaint만 발생화면 구조(Layout)이 변경되었을 때, 뷰포트 내에서 렌더 트리의 노드의 정확한 위치와 크기를 계산하는 과정을 다시 수행해야 한다.\nopacity, background-color, visibility, outline 등의 스타일 변경 시에는 Repaint만 동작한다. Repaint는 변경된 화면을 실제 화면에 반영하는 과정으로 최적화할 수 있는 방법은 화면 변화를 최소화할 수 있는 방법 뿐이다.\n재배치(Reflow) 화면 구조(Layout)이 변경되었을 때, 뷰포트 내에서 렌더 트리의 노드의 정확한 위치와 크기를 계산하는 과정\nReflow가 발생하는 경우 DOM 노드의 추가, 제거 DOM 노드의 위치 변경 DOM 노드의 크기 변경(margin, padding, border, width, height 등..) CSS3 애니메이션과 트랜지션 폰트 변경, 텍스트 내용 변경 이미지 크기 변경 offset, scrollTop, scrollLeft과 같은 계산된 스타일 정보 요청 페이지 초기 렌더링 윈도우 리사이징 위의 내용에서 빠졌더라도 화면의 구조가 변경되었다면 Reflow가 발생한다.\nReflow 최적화 재배치 작업은 다시 그리기 작업을 동반하기 때문에 경우에 따라 Render 트리 전체를 재구성할 수도 있으므로 다시 그리기만 발생할 떄에 비해서 비용이 훨씬 비싸다.\n재배치, 다시 그리기 작업을 최소화 하는 과정을 통해 최적화를 한다.\nReflow는 렌더 트리의 변화를 최소화하는 등.. DOM의 depth를 최소화\nDOM의 깊이와 크기를 작게 구성하여 재배치를 더 빠르게 처리하게 만든다. 스타일 변경을 한번에 처리한다.\n1 2 3 4 5 .newstyles { width: 100px; height: 200px; margin: 10px; } 1 2 3 4 5 // 비효율적인 코드 예시 var myelement = document.getElementById(\u0026#39;myelement\u0026#39;); myelement.width = \u0026#39;100px\u0026#39;; myelement.height = \u0026#39;200px\u0026#39;; myelement.style.margin = \u0026#39;10px\u0026#39;; 1 2 3 // 개선된 예시 var myelement = document.getElementById(\u0026#39;myelement\u0026#39;); myelement.classList.add(\u0026#39;newstyles\u0026#39;); 미리 만들어놓은 스타일을 한번에 적용하여 재배치를 최소화 할 수 있다.\n주변에 영향을 주는 요소를 제한한다.\n인터렉션에 의해 크기나 위치가 변경되는 요소는 병경될 때 주변 요소들이 최대한 영향받지 않도록 정의한다.\n스타일을 변경할 경우 가장 하위 노드의 클래스를 변경 애니메이션이 있는 노드는 position을 fixed 또는 absolute 로 지정한다. 개발자 도구를 이용하여 분석\n개발자 도구를 이용하여 재배치와 다시 그리기가 얼마나 발생하는지 확인하고 해당 요소를 최적화 시도한다.\n라이브러리를 사용한다.\nReact, Vue는 트리 형태의 Object를 통해 Virtual DOM을 구성하고 요소가 변경될 때 업데이트한다. 그 후 최종 상태의 Virtual DOM을 실제 DOM에 반영하여 재배치와 다시 그리기를 최소화 시켜 렌더링 최적화를 구현한다.\n스타일을 변경할 경우 가장 하위 노드의 클래스를 변경\nDOM 노드의 크기 또는 위치가 변경되면 하위 노드와 상위 노드까지 영향을 미칠 수 있다. 따라서 가장 하위 노드의 스타일을 변경할 경우, 전체 노드가 아니니 일부 노드로 영향을 최소화 할 수 있다.\n하지만 실무에서는 보통 변경해야 할 노드들이 정해져 있기 때문에 적용 범위가 크지 않을 수 있다.\n애니메이션이 있는 노드는 position을 fixed 또는 absolute로 지정한다. 애니메이션 효과는 많은 Reflow 비용이 발생하게 됨.\nposition 속성을 fixed 또는 absolute 로 지정하면, 해당 노드를 전체 노드에서 분리시켜 일부만 Reflow가 발생하도록 제한시킬 수 있다.\n애니메이션 효과를 줘야 하는 노드에 position 속성이 적용되지 않았다면 애니메이션 시작 시 position 속성 값을 fixed 또는 absolute로 변경하였다가 애니메이션 종료 후 다시 원복 시켜 렌더링을 최적화할 수 있다.\n\u0026lt;table\u0026gt; 레이아웃을 피한다. \u0026lt;table\u0026gt; 은 점진적으로 렌더링 되지 않고, 모두 로드되고 테이블 너비가 계산된 후 화면에 그려진다. 테이블 안의 콘텐츠의 값에 따라 테이블 너비가 계산된다.\n콘텐츠의 값에 따라 테이블 너비가 계산되기 때문에, 테이블 콘텐츠의 작은 변경만 있어도 테이블 너비가 다시 계산되고 테이블의 모든 노드들이 Reflow가 발생한다.\n부득이하게 \u0026lt;table\u0026gt;을 사용할 때는 table-layout:fixed 값을 지정하는 것이 좋다.\ntable-layout:fixed는 테이블의 콘텐츠의 길이에 따라 테이블의 너비가 계산되는 것이 아니기 때문에, table-layout의 기본 값인 auto에 비해 성능이 더 좋다. \u0026lt;table\u0026gt;을 레이아웃 용도가 아닌 데이터 표시 용도로 사용할 때도 table-layout:fixed를 지정하는 것이 성능 면에서 더 좋습니다. IE의 CSS 표현식을 사용하지 않는다. CSS 표현식은 비용이 매우 높기 때문에 사용을 피해야 함\n1 2 3 .expression { width: expression(document.documentElement.clientWidth \u0026gt; 0 ? \u0026#39;1000px\u0026#39; : \u0026#39;auto\u0026#39;); } Reflow가 발생할 때마다 자바스크립트 표현식이 다시 계산되기 때문에 CSS 표현식은 비용이 비싸다.\n애니메이션이 동작한다면, 애니메이션에 의한 Reflow가 발생할 때마다 자바스크립트 표현식이 계산됨. CSS 하위 선택자를 최소화한다. 1 2 3 4 5 6 7 8 /* 잘못된 예 */ .reflow_box .reflow_list li .btn{ display:block; } /* 올바른 예 */ .reflow_list .btn { display:block; } CSS 하위 선택자를 최소화하는 것이 렌더링 성능에 더 좋다.\n렌더 트리는 DOM과 CSSOM이 합쳐져서 만들어 지는데, DOM은 HTML이 파싱 되어 만들어진 트리이고, CSSOM은 CSS가 파싱 되어 만들어진 트리이다.\n두 트리를 결합하여 렌더 트리를 만드는데, CSS 하위 선택자가 많아지만 CSSOM 트리의 깊이가 깊어지게 되고 결국 렌더 트리를 만드는 시간이 더 오래 걸릴 수 있다.\n숨겨진 노드의 스타일을 변경한다. display:none으로 숨겨진 노드를 변경할 때는 Reflow가 발생하지 않기 때문에 숨겨진 노드를 표시하기 전에 노드의 콘텐츠를 먼저 변경한 후 화면에 나타내면 Reflow를 줄일 수 있다.\n클래스를 사용하여 한 번에 스타일을 변경한다. 스타일을 변경할 때, 스타일을 각각 변경할 경우 추가 Reflow가 발생할 수 있기 때문에 한번에 스타일을 변경하는 것이 좋다.\n요약 Repaint(Redraw)는 화면에 변화가 있을 때 화면을 그리는 과정 Reflow(Layout)는 뷰포트 내에서 렌더 트리의 노드의 정확한 위치와 크기를 계산하는 과정 Repaint가 발생하는 경우는 화면이 변경되는 모든 경우 Reflow가 발생하는 경우는 화면의 구조가 바뀌었을 경우 Reflow를 최적화하는 방법 스타일을 변경할 경우 가장 하위 노드의 클래스를 변경한다. 인라인 스타일을 사용하지 않는다. 애니메이션이 있는 노드는 position을 fixed 또는 absolute로 지정한다. 퀄리티, 퍼포먼스의 타협점을 찾는다. \u0026lt;table\u0026gt; 레이아웃을 피한다. IE의 CSS 표현식을 사용하지 않는다. CSS 하위 선택자를 최소화한다. 숨겨진 노드의 스타일을 변경한다. 클래스를 혹은 cssText 사용하여 한 번에 스타일을 변경한다. DOM 사용을 최소화한다. 캐시를 활용한다. 라이브러리를 사용한다. ","date":"2023-04-18T19:48:26+09:00","image":"https://codemario318.github.io/post/rendering_optimize/browser_cover_huc408e1e4bc0026ab219b7f7573db946e_26010_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/rendering_optimize/","title":"랜더링 업데이트 최적화"},{"content":" CRP 최적화란 HTML, CSS 및 JS 간 종속성을 이해하고 최적화 하는것이다.\nCritical Rendering Path란? 브라우저가 페이지의 초기 출력을 위해 실행해야 하는 순서\nDOM 트리 구축 CSSOM 트리 구축 JS 실행 렌더트리 구축 레이아웃 생성 페인팅 CSS CSS는 렌더링 차단 리소스이므로 최초 렌더링에 걸리는 시간을 최적화하려면 클라이언트에 최대한 빠르게 다운로드되어야 한다.\n렌더 트리를 만들 때 사용되는 HTML, CSS 모두 렌더링 차단 리소스 CSS가 없는 페이지는 상대적으로 사용성이 떨어지기 때문에 브라우저는 DOM과 CSSOM을 모두 사용할 수 있게 될 때까지 렌더링을 차단한다.\nCSS를 간단하게 유지하고 가능한 빨리 제공하고 최대한 빨리 렌더링의 차단을 해제해야 한다.\n미디어 쿼리, 미디어 유형 미디어 쿼리를 사용하면 특정한 사용 사례와 동적인 조건에 맞게 렌더링이 차단되므로 효율을 높힐 수 있다.\n미디어 유형과 미디어 쿼리를 통해 일부 CSS 리소스를 렌더링 비차단 리소스로 표시할 수 있음 브라우저는 차단 동작이든 비차단 동작이든 관계없이 모든 CSS 리소스를 다운로드함 미디어 쿼리는 하나의 미디어 유형과 특정 미디어 기능의 조건을 확인하는 0개 이상의 식으로 구성된다.\n1 2 3 4 5 6 \u0026lt;!-- 1 --\u0026gt; \u0026lt;link href=\u0026#34;style.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;!-- 2 --\u0026gt; \u0026lt;link href=\u0026#34;print.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; media=\u0026#34;print\u0026#34;\u0026gt; \u0026lt;!-- 3 --\u0026gt; \u0026lt;link href=\u0026#34;other.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; media=\u0026#34;(min-width: 40em)\u0026#34;\u0026gt; 조건이 없는 경우\n미디어 유형이나 미디어 쿼리를 제공하지 않아서 모든 경우에 적용됨 즉 항상 렌더링을 차단함 미디어 유형을 적용\n콘텐츠가 인쇄될 때만 적용 처음에 로드될 때 페이지 렌더링을 차단할 필요가 없음 미디어 쿼리 적용\n조건이 일치하면 스타일시트가 다운로드되고 처리될 때까지 브라우저가 렌더링을 차단. JS 자바스크립트는 파서 차단 리소스(parser blocking resource)이며, JS를 사용하면 콘텐츠, 스타일, 사용자와의 상호작용등 거의 모든것을 수정할 수 있다.\nJS실행은 DOM 생성을 차단하고 페이지 렌더링을 지연하게 된다.\n자바스크립트를 비동기로 설정하고, CRP에서 불필요한 JS를 제거해야 한다. JS와 HTML의 종속성 HTML 파서는 script 태그를 만나면 DOM 생성 프로세스를 중지하고 자바스크립트 엔진에 권한을 넘긴다. 자바스크립트 엔진의 실행이 완료된 후 브라우저가 중지했던 시점부터 DOM 생성을 다시 시작하게 된다.\nscript 태그의 뒷부분에서 정의된 어떠한 태그들도 아직 생성되지 않았기 때문에 노드를 찾을 수 없다. 또한, 인라인 스크립트를 실행하면 DOM 생성이 차단되고, 이로 인해 초기 렌더링도 지연된다.\n이러한 이유로 인하여 자바스크립트는 화면에 그려지는 태그들이 모두 파싱 된 후인, \u0026lt;body\u0026gt; 태그를 닫기 직전에 \u0026lt;script\u0026gt; 태그를 선언하는 것이 좋다.\nJS와 CSS의 종속성 CSS를 파싱 하는 동안 자바스크립트에서 스타일 정보를 요청하는 경우, CSS가 파싱이 끝나지 않은 상태라면 자바스크립트 오류가 발생할 수 있다. CSS 파싱으로 생성되는 CSSOM과 JavaScript에서 스타일 수정 시 발생하는 CSSOM 수정 사이에 경쟁 조건(race condition)이 발생할 수 있다.\n브라우저는 이 문제를 해결하기 위해 CSSOM을 생성하는 작업이 완료할 때까지 자바스크립트 실행 및 DOM 생성을 지연시킨다. DOM, CSSOM, 자바스크립트 실행 간에 종속성 때문에 브라우저가 화면에 페이지를 처리하고 렌더링 할 때 상당한 지연이 발생할 수 있습니다.\n비동기 JS HTML을 파싱 하면서 script 태그를 만나면 DOM 생성을 중지시키고 자바스크립트 엔진에게 제어 권한을 넘겨 자바스크립트를 실행한 후, DOM 생성을 진행한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;!-- index.html --\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width,initial-scale=1\u0026#34;\u0026gt; \u0026lt;link href=\u0026#34;style.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Critical Path: Script\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Hello \u0026lt;span\u0026gt;web performance\u0026lt;/span\u0026gt; students!\u0026lt;/p\u0026gt; \u0026lt;script src=\u0026#34;app.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 1 2 3 4 5 6 7 8 9 // app.js var span = document.getElementsByTagName(\u0026#39;span\u0026#39;)[0]; span.textContent = \u0026#39;interactive\u0026#39;; // change DOM text content span.style.display = \u0026#39;inline\u0026#39;; // change CSSOM property // create a new element, style it, and append it to the DOM var loadTime = document.createElement(\u0026#39;div\u0026#39;); loadTime.textContent = \u0026#39;You loaded this page on: \u0026#39; + new Date(); loadTime.style.color = \u0026#39;blue\u0026#39;; document.body.appendChild(loadTime); 위에서 살펴본 인라인 스크립트뿐만 아니라 위의 코드와 같이 script 태그를 통해 포함된 자바스크립트 역시 파싱을 중지시킨다.\nscript 태그를 사용하여 자바스크립트를 실행할 경우, 서버에서 자바스크립트를 가져올 때까지 기다려야하며 이로 인해 수십~수천 밀리초의 지연이 추가로 발생할 수 있다.\n기본적으로 자바스크립트가 실행될 때, 스크립트가 페이지에서 무엇을 수행할지 모르기 때문에 브라우저는 최악의 대비하여 파서를 차단한다.\n브라우저에 자바스크립트를 바로 실행할 필요가 없음을 알려준다면, 브라우저는 계속해서 DOM을 생성할 수 있고 DOM 생성이 끝난 후에 자바스크립트를 실행할 수 있게 된다.\n이때 사용할 수 있는 것이 비동기 자바스크립트이다.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width,initial-scale=1\u0026#34;\u0026gt; \u0026lt;link href=\u0026#34;style.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Critical Path: Script Async\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Hello \u0026lt;span\u0026gt;web performance\u0026lt;/span\u0026gt; students!\u0026lt;/p\u0026gt; \u0026lt;script src=\u0026#34;app.js\u0026#34; async\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 위의 코드와 같이 단순히 script 태그에 async 속성을 추가해 주면 됩니다. async 속성을 script 태그에 추가하여 자바스크립트가 사용 가능해질 때까지 브라우저에게 DOM 생성을 중지하지 않아도 괜찮다는 것을 알릴 수 있다.\n리소스 우선순위 지정 브라우저는 가장 중요한 리소스(스크립트나 이미지보다 CSS 우선)를 우선 로드하기 위해 가장 중요하다 생각되는 리소스를 추측하여 먼저 로드한다. 하지만 브라우저에게 리소스의 우선순위를 전달하여 중요한 리소스를 먼저 처리하게 할 수 있다.\npreload 속성 현재 페이지에서 빠르게 가져와야 하는 리소스에 사용되는 속성이다.\n\u0026lt;link rel=\u0026quot;preload\u0026quot; as=\u0026quot;...\u0026quot;\u0026gt;는 브라우저에게 현재 리소스가 필요하며, 가능한 한 빨리 가져오기를 시도해야 한다고 알리는 역할을 한다.\n1 2 \u0026lt;link rel=\u0026#34;preload\u0026#34; as=\u0026#34;script\u0026#34; href=\u0026#34;super-important.js\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;preload\u0026#34; as=\u0026#34;style\u0026#34; href=\u0026#34;critical.css\u0026#34;\u0026gt; as 속성을 사용하여 리소스의 유형을 알려줘야 한다. 브라우저는 올바른 유형이 설정되어 있지 않으면 미리 가져온 리소스를 사용하지 않는다. preload는 브라우저가 반드시 리소스를 가져오게 만들며, 리소스를 두 번 가져오게 하거나, 필요하지 않은 것을 가져오지 않도록 주의해야한다.\npreload를 이용하여 리소스를 가져왔지만 현재 페이지에서 즉시 사용되지 않는 리소스는 위의 그림과 같이 워닝 로그가 노출된다.\nprefetch 속성 미래에 필요할 수 있는 리소스를 가져와야 할 때 사용되는 속성이다. \u0026lt;link rel=\u0026quot;prefetch\u0026quot;\u0026gt;는 현재 페이지 로딩이 마치고 다운로드할 여유가 생겼을 때 가장 낮은 우선순위로 리소스를 가져온다.\nprefetch는 사용자가 다음에 할 행동을 미리 준비하는 역할을 한다. 예를 들어, 현재 페이지가 1페이지 라면,\n1 \u0026lt;link rel=\u0026#34;prefetch\u0026#34; href=\u0026#34;page-2.html\u0026#34;\u0026gt; 위의 코드와 같이 사용하여 2페이지를 먼저 가져와 준비하게 된다.\npage-2.html의 HTML만 가져오고 page-2.html에서 사용되는 리소스는 가져오지 않는다.\n요약 CSS 최적화 방법\n미디어 유형, 미디어 쿼리를 사용 JavaScript 최적화 방법\nbody 태그 닫기 직전 \u0026lt;script\u0026gt; 태그를 선언 \u0026lt;script ... async\u0026gt;와 같이 async 속성을 사용 리소스 우선순위 지정\n현재 페이지에서 빠르게 가져와야 하는 리소스에 \u0026lt;link rel=\u0026quot;preload\u0026quot; as=\u0026quot;...\u0026quot;\u0026gt;와 같이 preload 속성을 사용 미래에 사용할 수 있는 리소스는 \u0026lt;link rel=\u0026quot;prefetch\u0026quot;\u0026gt;와 같이 prefetch 속성을 사용 ","date":"2023-04-18T19:27:35+09:00","image":"https://codemario318.github.io/post/crp_optimize/browser_cover_huc408e1e4bc0026ab219b7f7573db946e_26010_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/crp_optimize/","title":"Critical Rendering Path 최적화"},{"content":"브라우저 기본 구조 사용자 인테페이스\n요청한 페이지를 보여주는 부분의 제외한 나머지 부분\n브라우저 엔진\n사용자 인터페이스와 렌더링 엔진 사이의 동작을 제어\n랜더링 엔진\n요청한 콘텐츠 표시\nHTML을 요청하면 HTML과 CSS를 파싱하여 화면에 표시함 통신\nHTTP 요청과 같은 네트워크 호출에 사용됨.\n플랫폼 독립적인 인터페이스이고 각 플랫폼 하부에서 실행됨. UI 백앤드\n콤보 박스와 창 같은 기본적인 장치를 그림\n플랫폼에서 명시하지 않은 일반적인 인터페이스로서, OS 사용자 인터페이스 체계를 사용 자바스크립트 해석기\n자바스크립트 코드를 해석하고 실행\n자료 저장소\n자료를 저장하는 계층으로 쿠키와 같은 모든 종류의 자원을 하드디스크에 저장.\nHTML5에는 브라우저가 지원하는 \u0026lsquo;웹 데이터 베이스\u0026rsquo; 가 정의되어 있다. 크롬은 대부분의 브라우저와 달리 각 탭마다 별도의 렌더링 엔진 인스턴스를 유지하여, 각 탭이 독립된 프로세스로 처리된다.\n렌더링 렌더링 엔진 요청받은 내용을 브라우저 화면에 표시함. HTML 및 XML 문서와 이미지를 표시할 수 있다.\n렌더링 엔진의 역할은 요청받은 내용을 브라우저 화면에 나타내는 일이다. HTML, CSS JS 등의 파일을 브라우저가 화면에 표시할 수 있도록 변환하여 픽셀 단위로 나타낸다.\n렌더링 엔진 동작과정 렌더링 엔진은 요청한 문서의 내용을 얻는 것에서 시작하며, 보통 8KB 단위로 전송된다.\n렌더링 엔진은 HTML 문서를 파싱하고 콘텐츠 트리 내부에서 태그를 DOM 노드로 변환한다. 그 다음 외부 CSS 파일과 함께 포함된 스타일 요소도 파싱한다. 스타일 정보와 HTML 표시 규칙은 렌더 트리 라고 부르는 또 다른 트리를 생성한다.\n파싱 문서를 통해 파싱트리를 만드는 과정\n어휘분석: Tokenizer\n문서를 읽어 정해놓은 규칙을 통해 토큰을 추출하는 도구, 과정을 의미한다.\nToken: 의미적으로 더이상 나눌 수 없는 기본적인 언어 요소를 표현하는 데이터 단위\n구문분석: Lexer\n토큰에 약속된 의미를 부여하는 도구, 과정\n어휘분석과 구문분석의 결과물을 이용하여 파싱트리를 만드는 과정이다. 컴파일 파싱을 통해 만들어진 결과물을 기계 코드로 변환하는 과정이다\nDOM - 문서 객체 모델: Document Object Model HTML, XML 문서의 프로그래밍 인터페이스이다. HTML 문서의 객체 표현\n문서의 구조화된 표현을 제공하며 프로그래밍 언어가 DOM 구조에 접근할 수 있는 방법을 제공하여 그들이 문서 구조, 스타일, 내용 등을 변경할 수 있게 돕는다.\nnodes와 property와 method 를 갖고 있는 objects로 문서를 표현한다. 이들은 웹 페이지를 스크립트 또는 프로그래밍 언어들에서 사용될 수 있게 연결시켜주는 역할을 담당한다.\n동일한 문서를 표현, 저장, 조작하는 방법을 제공하는 웹 페이지의 객체 지향 표현\n따라서, DOM은 마크업과 1:1 관계를 맺는다.\n1 2 3 4 5 6 \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Hello World\u0026lt;/p\u0026gt; \u0026lt;div\u0026gt;\u0026lt;img src=\u0026#34;example.png\u0026#34; /\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; CSSOM(CSS Object Model) 위의 그림과 같이 DOM을 생성하는 과정 그대로 CSSOM을 생성한다.\n브라우저는 DOM을 생성하는 동안 외부 CSS를 참조하는 \u0026lt;link\u0026gt; 태그를 만나게 되면 브라우저에 리소스를 요청함. CSS의 원시 바이트가 문자열로 변환 된 후 차례로 토큰과 노드로 변환되고 마지막으로 CSSOM이라는 트리 구조를 만든다. CSSOM은 하향식으로 규칙을 적용하기 때문에 트리 구조를 가진다. 최종 스타일을 계산할 때 브라우저는 해당 노드에 적용 가능한 가장 일반적인 규칙으로 시작해 더 구체적인 규칙을 적용하는 방식으로 처리된다.\nDOM 트리를 바탕으로 CSSOM 트리를 만들기 때문에 해당 하는 스타일룰이 적용이 된 트리 형태로 구성되는 것 같다.\n어태치먼트: 랜더링 트리 생성 과정 DOM 트리와 CSSOM 트리를 결합하여, 표시해야 할 순서로 내용을 그려냏 수 있도록 하기 위해 렌더트리를 형성한다. 이 과정을 웹킷에서는 어테치먼트라고 한다.\n렌더트리는 화면에 표시되는 각 노드의 위치를 계산하는 레이아웃에 사용되고 픽셀을 하면에 그리는 페인트 과정에도 사용됨\n랜더 트리 생성 과정 DOM 트리 구축을 위한 HTML 파싱 HTML 파싱 → DOM 트리 구축 CSSOM 트리 구축을 위한 CSS 파싱 CSS → CSSOM 트리 생성 DOM 트리와 CSSOM 트리를 활용하여 랜더 트리 구축 DOM Tree + CSSOM Tree = Rendering Tree 랜더링 트리 생성 과정 2 DOM 트리의 루트에서 시작하여 표시되는 노드 각각을 탐색함 스크립트 태그, 메타 태그 등 랜더링된 출력에 반영되지 않는 트리들이 생략됨 CSS를 속성을 통해 숨겨지는 노드들이 생략됨 ex) display: none 표시된 각 노드에 대해 적절하게 일치하는 CSSOM 규칙을 찾아 적용 표시된 노드를 콘텐츠 및 계산된 스타일과 함께 내보냄 visibility: hidden은 비어 있는 상자로 렌더링되지만 display: none은 랜더링에서 제외됨. 따라서 후자는 스크린 리더기에서 읽을 수 없음, 전자는 일부 스크린 리더기에서 인식하지 않기 때문에 접근성을 위한 IR 처리시 주의해야 한다.\n최종 출력은 화면에 표시되는 모든 노드의 콘텐츠 및 스타일 정보를 포함하는 렌더링 트리\n레이아웃 렌더 트리가 생성되고, 기기의 뷰포트 내에서 렌더 트리의 노드가 정확한 위치와 크기를 계산하는 과정.\n모든 상대적인 측정값은 화면에서 절대적인 픽셀로 변환됨. 즉 CSS에 상대적인 값인 %로 할당된 값들은 절대적인 값인 PX 단위로 변환.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width,initial-scale=1\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Critial Path: Hello world!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div style=\u0026#34;width: 50%\u0026#34;\u0026gt; \u0026lt;div style=\u0026#34;width: 50%\u0026#34;\u0026gt;Hello world!\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 뷰포트 내에서 각 요소의 정확한 위치와 크기를 정확하게 캡처하는 상자 모델이 출력된다. 모든 상대적인 측정값은 화면에서 절대적인 픽셀로 변환된다.\n페인팅 렌더링 트리의 각 노드를 화면의 실제 픽셀로 변환하는 마지막 단계로 이러한 정보를 전달할 수 있습니다.\n\u0026lsquo;Layout\u0026rsquo; 이벤트는 타임라인에서 렌더링 트리 생성, 위치 및 크기 계산을 캡처. 레이아웃이 완료될 때 브라우저가 Paint Setup 및 Paint 이벤트를 발생. 이러한 작업은 렌더링 트리를 화면의 픽셀로 변환. 렌더링 트리 생성, 레이아웃 및 페인트 작업을 수행하는 데 필요한 시간은 문서의 크기, 적용된 스타일 및 실행 중인 기기에 따라 달라진다.\n문서가 클수록 브라우저가 수행해야 하는 작업도 더 많아지며, 스타일이 복잡할수록 페인팅에 걸리는 시간도 늘어나게 된다.\nex) 단색은 페인트하는 데 시간과 작업이 적게 필요한 반면, 그림자 효과는 계산하고 렌더링하는 데 시간과 작업이 더 필요하다. 요약 HTML 마크업을 처리하고 DOM 트리를 빌드 (DOM 파싱) CSS 마크업을 처리하고 CSSOM 트리를 빌드 (CSS 파싱) DOM 및 CSSOM을 결합하여 렌더 트리를 형성 (Attachment) 렌더 트리에서 레이아웃을 실행하여 각 노드의 기하학적 형태를 계산 (Layout) 개별 노드를 화면에 페인트(Painting) 참고 자료\nhttps://developers.google.com/web/fundamentals/performance/critical-rendering-path?hl=ko https://janghanboram.github.io/2018/06/06/browser-rendering/ https://d2.naver.com/helloworld/59361 http://taligarsiel.com/Projects/howbrowserswork1.htm#Render_tree_construction https://grosskurth.ca/papers/browser-refarch.pdf https://yilpe93.github.io/2018/06/18/etc/web-browser/ https://sangbui.com/sb-files/BrowserArchitecture_ClientSide.pdf https://medium.com/@monica1109/how-does-web-browsers-work-c95ad628a509 https://blog.lgcns.com/1911 https://cisctbd.github.io/Report.pdf https://blog.asamaru.net/2017/05/04/understanding-the-critical-rendering-path/ ","date":"2023-04-18T18:48:11+09:00","image":"https://codemario318.github.io/post/browser/browser_cover_huc408e1e4bc0026ab219b7f7573db946e_26010_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/browser/","title":"브라우저"},{"content":"Message Queue란? MQ란 메시지 기반의 미들웨어로 메시지를 이용하여 여러 어플리케이션, 시스템, 서비스들을 연결해주는 솔루션이다. MOM(Message Oriented Middleware)를 구현한 솔루션으로 비동기 메시지를 사용하는 서비스들 사이에서 데이터를 교환해주는 역할을 한다.\nProducer(sender)가 메시지를 큐에 전송하면 Consumer(receiver)가 처리하는 방식으로, Producer와 Consumer에 message 프로세스가 추가되는 것이 특징이다.\nMQ를 사용하여 비동기로 요청을 처리하고 Queue 에 저장하여 Consumer에게 병목을 줄여줄 수 있으나 제품마다 구현이 다르고 장단이 있다.\n대표 솔루션 IBM MQ 가장 많이 사용되는 상용 MQ 제품으로 표준 JMS 메시지 ㅇ기반으로 MQTT 프로토콜을 지원한다.\nApache ActiveMQ 자바 기반의 JMS Queue를 지원하는 오픈소스로 MQTT, AMQP, OpenWire, STOMP 프로토콜을 지원한다. 다양한 언어를 지원하며 크러스터링이 가능하다. 단 모니터링 도구는 없다. REST API를 통해 웹기반 메시징 API를 지원하며 Ajax를 통해 순수한 DHTML을 사용한 웹스트리밍을 지원 Rabbit MQ 고성능을 목표로 AMQP 프로토콜을 사용하여 개발된 MQ 로 Erlang OTP 기반으로 개발되었다. 실시간 모니터링이 용이하고 다양한 언어 및 OS 지원, RabbitMQ 서버간 클러스터링이 가능하다. Kafka Linkedin에서 구직정보들을 처리할 수 있는 플랫폼으로 개발이 시작되었다. 실시간 로그 처리에 특화되어 설계된 시스템으로 개발되어 타 MQ 대비 TPS가 매우 우수하나 특화된 솔루션이기 때문에 타 MQ솔루션에서 제공하는 다양한 기능들은 제공되지 않는다. AMQP, JMS 이 아닌 단순 메시지 헤더를 이용한 TCP 통신이다. MQ는 broker가 pruducer에게 메세지를 받아서 consumer 에게 push해주는 방식인데 반해, kafka는 consumer가 Broker로 부터 직접 메시지를 가지고 가는 pull 방식으로 동작한다. Consumer는 자신의 처리능력 만큼의 메시지만 broker로부터 가져오기 때문에 최적의 성능을 낼 수 있다. 많은 데이터 전송과 최대 처리량을 유지하기에 대량 데이터 스트리밍에 적합하다. 상태 변경이 시간순으로 기록되어야 하는 응용 프로그램인 이벤트 소싱(Event Sourcing) 저장소로 적합하다. ","date":"2023-04-18T17:19:16+09:00","image":"https://codemario318.github.io/post/message_queue/mq_cover_hu0d801aafbbe1fc1c7c4e885cbcd16691_10519_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/message_queue/","title":"Message Queue"},{"content":"What is Type Annotation in TypeScript 타입스크립트는 타입 어노테이션을 사용하여 변수, 함수, 객체 등과 같은 식별자의 유형을 명시적으로 지정한다.\n: type 구문을 식별자 뒤에 붙이는 타입 어노테이션으로 사용한다.\ntype은 모든 유효한 유형 가능 식별자에 타입 어노테이션을 추가하면 해당 타입으로만 사용할 수 있다. 식별자가 다른 유형으로 사용되면 타입스클입트 컴파일러에서 오류를 발생시킨다.\nType annotations in variables and constants 1 2 3 let variableName: type; let variableName: type = value; const constantName: type = value; 위 코드에서 타입 어노테이션은 변수, 상수 이름 뒤에 :을 붙이고 그 뒤에 타입이 온다.\n지정한 타입 외 다른 타입 값을 할당하면 컴파일 에러를 발생시킨다.\nType annotation examples 배열\n배열 타입 어노테이션은 : type[] 로 표시한다.\n1 2 let arrayName: type[]; let names: string[] = [\u0026#39;John\u0026#39;, \u0026#39;Jane\u0026#39;, \u0026#39;Peter\u0026#39;, \u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;]; 객체\n1 2 3 4 5 6 7 8 9 let person: { name: string; age: number }; person = { name: \u0026#39;John\u0026#39;, age: 25 }; 함수 인자와 반환형\n1 2 3 4 5 let greeting : (name: string) =\u0026gt; string; greeting = function (name: string) { return `Hi ${name}`; }; 위와 같이 타입 어노테이션을 사용할 경우 string을 인자로 받고, string을 반환하는 모든 함수를 greeting에 할당할 수 있다.\nBase Type number\n타입스크립트에서 모든 숫자는 부동 소수점값과 큰 정수값에 포함된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 let price: number; let price = 9.95; let counter: number = 0; let x: number = 100, y: number = 200; let bin = 0b100; let anotherBin: number = 0B010; let octal: number = 0o10; let hexadecimal: number = 0XA; let big: bigint = 9007199254740991n; string\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 let firstName: string = \u0026#39;John\u0026#39;; let title: string = \u0026#34;Web Developer\u0026#34;; let description = `This TypeScript string can span multiple lines `; let firstName: string = `John`; let title: string = `Web Developer`; let profile: string = `I\u0026#39;m ${firstName}. I\u0026#39;m a ${title}`; console.log(profile); /** result I\u0026#39;m John. I\u0026#39;m a Web Developer. **/ boolean\n1 2 3 4 5 let pending: boolean; pending = true; // after a while // .. pending = false; Object Type 타입스크립트 object 타입은 원시 타입이 아닌 모든 값을 표현할 수 있다.\n타입스크립트 원시 타입 number bigint string boolean null undfined symbol object로 선언된 변수는 원시 타입을 제외한 모든 자료형을 할당할 수 있다.\n1 2 3 4 5 6 7 8 9 10 let employee: object; employee = { firstName: \u0026#39;John\u0026#39;, lastName: \u0026#39;Doe\u0026#39;, age: 25, jobTitle: \u0026#39;Web Developer\u0026#39; }; console.log(employee); 1 2 3 4 5 6 { firstName: \u0026#39;John\u0026#39;, lastName: \u0026#39;Doe\u0026#39;, age: 25, jobTitle: \u0026#39;Web Developer\u0026#39; } 만약 object로 선언된 변수를 다른 원시 타입으로 다시 할당하려고 하면 에러가 발생한다.\n1 2 employee = \u0026#34;Jane\u0026#34;; // error TS2322: Type \u0026#39;\u0026#34;Jane\u0026#34;\u0026#39; is not assignable to type \u0026#39;object\u0026#39;. object 타입은 선언될 때 지정한 프로퍼티 목록을 고정적으로 가지게 되어, 선언되지 않은 프로퍼티를 호출하게 되면 에러를 발생시킨다.\n1 2 console.log(employee.hireDate); // error TS2339: Property \u0026#39;hireDate\u0026#39; does not exist on type \u0026#39;object\u0026#39;. 💡 자바스크립트는 보유하지 않은 프로퍼티를 호출하면 undefined를 반환한다.\nobject 타입 내부에 지정되는 프로퍼티들의 타입을 지정할 수 도 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 let employee: { firstName: string; lastName: string; age: number; jobTitle: string; }; employee = { firstName: \u0026#39;John\u0026#39;, lastName: \u0026#39;Doe\u0026#39;, age: 25, jobTitle: \u0026#39;Web Developer\u0026#39; }; 1 2 3 4 5 6 7 8 9 10 11 let employee: { firstName: string; lastName: string; age: number; jobTitle: string; } = { firstName: \u0026#39;John\u0026#39;, lastName: \u0026#39;Doe\u0026#39;, age: 25, jobTitle: \u0026#39;Web Developer\u0026#39; }; object VS Object 타입스크립트는 첫 번째 대문자만 다른 Object 타입도 있다.\nobject 타입은 원시 타입이 아닌 모든 값들을 표현하고, Object 타입은 동작하는 모든 객체들을 의미한다.\n💡 자바스크립트에 포함된 모든 생성자들은 Object를 extend한다. 따라서 Object에는 모든 객체가 할당될 수 있다. 즉 동작하는 모든 객체를 의미한다.\nThe empty type 타입스크립트에서 비어있는 타입을 표현하는 다른 방법으로 {} 가 있는데, object 타입과 유사하게 동작한다.\n1 2 3 4 let vacant: {}; vacant.firstName = \u0026#39;John\u0026#39;; //error TS2339: Property \u0026#39;firstName\u0026#39; does not exist on type \u0026#39;{}\u0026#39;. 1 2 3 4 let vacant: {} = {}; console.log(vacant.toString()); // [object Object] 1 2 3 4 5 6 7 8 9 let vacant: {} = { firstName: \u0026#39;John\u0026#39; }; console.log(vacant); // { firstName: \u0026#39;John\u0026#39; } console.log(vacant.firstName); // error TS2339: Property \u0026#39;firstName\u0026#39; does not exist on type \u0026#39;{}\u0026#39;. Array 타입스크립트의 Array는 순서가 있는 데이터 리스트를 의미한다.\n1 let 이름: 타입[]; 1 2 3 4 5 6 let skills: string[]; skills[0] = \u0026#34;Problem Solving\u0026#34;; skills[1] = \u0026#34;Programming\u0026#34;; skills.push(\u0026#39;Software Design\u0026#39;); 1 let skills = [\u0026#39;Problem Sovling\u0026#39;,\u0026#39;Software Design\u0026#39;,\u0026#39;Programming\u0026#39;]; 1 2 let skills: string[]; skills = [\u0026#39;Problem Sovling\u0026#39;,\u0026#39;Software Design\u0026#39;,\u0026#39;Programming\u0026#39;]; 위처럼 선언된 skills에 다른 타입을 넣을 경우 에러가 발생한다\n1 2 skills.push(100); // Argument of type \u0026#39;number\u0026#39; is not assignable to parameter of type \u0026#39;string\u0026#39;. 배열 타입이 가진 속성과 메소드 1 2 let series = [1, 2, 3]; console.log(series.length); // 3 유용한 메서드로 forEach(), map(), reduce(), filter() 등이 있다.\n1 2 3 4 5 let series = [1, 2, 3]; let doubleIt = series.map(e =\u0026gt; e* 2); console.log(doubleIt); [ 2, 4, 6 ] 여러 타입 섞어 저장하기 1 let scores = [\u0026#39;Programming\u0026#39;, 5, \u0026#39;Software Design\u0026#39;, 4]; 1 2 let scores : (string | number)[]; scores = [\u0026#39;Programming\u0026#39;, 5, \u0026#39;Software Design\u0026#39;, 4]; Tuple Tuple은 array에 추가 제약사항이 붙은 형태이다.\n내부 요소 갯수 고정 내부 요소의 타입을 선언할 때 같을 필요는 없음 1 2 let skill: [string, number]; skill = [\u0026#39;Programming\u0026#39;, 5]; 1 2 3 let skill: [string, number]; skill = [5, \u0026#39;Programming\u0026#39;]; // error TS2322: Type \u0026#39;string\u0026#39; is not assignable to type \u0026#39;number\u0026#39;. 1 let color: [number, number, number] = [255, 0, 0]; Optional Tuple Elements 타입스크립트 3.0부터 선택적으로 사용할 수 있는 요소 선언이 추가되었다. 선택적으로 사용할 요소 앞에 ?를 앞에 붙인다.\n1 2 3 let bgColor, headerColor: [number, number, number, number?]; bgColor = [0, 255, 255, 0.5]; headerColor = [0, 255, 255]; Enum Enum은 상수 값들을 그룹으로 묶어 이름 붙인 타입이다.\n1 enum name {constant1, constant2, ...}; constant1, constant2, ...는 enum의 멤버가 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 enum Month { Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec }; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 function isItSummer(month: Month) { let isSummer: boolean; switch (month) { case Month.Jun: case Month.Jul: case Month.Aug: isSummer = true; break; default: isSummer = false; break; } return isSummer; } isItSummer의 인자인 month는 Month enum에 해당하는 값을 받을 수 있다.\n1 2 3 console.log(isItSummer(Month.Dec)); //false console.log(isItSummer(11)); //false console.log(isItSummer(MonthSecond.Dec)); //error TS2345: Argument of type \u0026#39;MonthSecond.Dec\u0026#39; is not assignable to parameter of type \u0026#39;Month\u0026#39;. ","date":"2023-04-18T17:09:29+09:00","image":"https://codemario318.github.io/post/typescript_tutorial_2/ts_cover_hua836966f7fcda4b3d2856eb0c525b4e2_8490_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/typescript_tutorial_2/","title":"TypeScript Tutorial - 2. Basic Types"},{"content":"Setup Tools Node.js: TS 컴파일러 실행을 위해 필요함 TypeScript 컴파일러: TS를 JS로 컴파일해준다. ts-node TS 코드를 실행하고 REPL(read-eval-print loop: 스크립트 실행 환경) 사용할 수 있다. TypeScript 컴파일러 설치 npm을 이용해 Typescript를 전역 환경에 설치 1 npm install -g typescript 설치 완료 후 tsc 가 정상적으로 설치되었는지 확인 1 tsc --v ts-node 를 전역 환경에 설치 1 npm install -g ts-node What is TypeScript 타입스크립트는 자바스크립트 상위집합(superset)이다. 타입스크립트가 자바스크립트의 특성을 침범하지 않고 지원한다. 타입스크립트는 자바스크립트를 기반으로 만들어졌다. 타입스크립트 코드를 작성 후, 타입스크립트 컴파일러를 사용하여 일반 자바스크립트 코드로 컴파일 한다.\n평범한 자바스크립트 코드가 만들어지기 때문에 자바스크립트가 실행 가능한 모든 환경에서 배포할 수 있다. 기본적으로 자바스크립트 문법을 사용하고, 타입 지원을 위한 추가 구문이 있다.\n결과적으로 syntax 에러가 없는 자바스크립트 코드는 타입 스크립트 코드 이다.\n모든 자바스크립트 프로그램이 타입스크립트 프로그램이라는 뜻 기존 자바스크립트 코드 베이스를 타입 스크립트로 마이그레이션하는 경우 유용하다.\nWhy TypeScript 타입스크립트는 버그를 피할 수 있게 도와주기 때문에 생산성을 향상시킨다. 타입은 많은 실수를 피하는 데 도움을 준다. 이를 통해 런타임에 버그가 발생하지 않고 컴파일 시간에 버그를 처리할 수 있다.\n1 2 3 4 5 6 function add(x, y) { return x + y; } let result = add(input1.value, input2.value); console.log(result); // result of concatenating strings add 함수는 x+y를 반환한다. 하지만 input이 html \u0026lt;input\u0026gt;이라면 .value는 값이 숫자인 것과 상관없이 문자열로 결과를 반환하는데 자바스크립트는 문자열 + 연산을 concat으로 처리하므로 add 함수는 의도한 동작을 수행하지 않고 이어진 문자열을 반환하게 된다.\n1 2 3 4 5 function add(x: number, y: number) { return x + y; } let result = add(input1.value, input2.value); 타입스크립트는 인자에 들어올 타입을 정할 수 있어서 input.value가 number타입이 아닐 경우 컴파일 에러를 발생시킨다.\n따라서 실수로 다른 값을 넣었을 때 발생하는 오동작을 예방할 수 있다.\n타입스크립트는 미래의 자바스크립트를 현재로 가져온다.\n타입스크립트는 현재 자바스크립트 엔진에 대해 ES Next에서 계획된 기능을 미리 지원한다. 따라서 웹 브라우저나 다른 환경에서 새로운 기능을 완전히 지원하기 전에 새로운 자바스크립트 기능을 사용할 수 있다. 매년 TC39에서 자바스크립트 표준인 ECMAScript에 대한 새로운 기능을 출시하는데 일반적으로 5단계를 거쳐 완전히 적용된다.\nStage 0: Strawperson(?: Strowman, 허수아비?) Stage 1: Proposal(제안) Stage 2: Draft(초안, 신인 선발) Stage 3: Candidate(후보) Stage 4: Finished() 타입스크립트는 일반적으로 Stage 3부터 지원한다.\n\u0026ldquo;Hello, World!\u0026rdquo; in node.js 1 2 let message: string = \u0026#39;Hello, World!\u0026#34;; console.log(message); tsc 을 이용해 .ts 파일을 컴파일하여 js 파일을 만들 수 있다.\n1 tsc app.ts 1 node app.js ts-node 이용 시 컴파일 하지 않아도 실행 가능\n1 ts-node app.ts in Web Browsers 1 2 3 4 5 6 7 8 9 10 11 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;TypeScript: Hello, World!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script src=\u0026#34;app.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 1 2 3 4 5 6 const message: string = \u0026#39;Hello, World!\u0026#39;; const heading = document.createElement(\u0026#39;h1\u0026#39;); heading.textContent = message; document.body.appendChild(heading); 1 tsc app.ts ","date":"2023-04-18T16:43:29+09:00","image":"https://codemario318.github.io/post/typescript_tutorial_1_1_1/ts_cover_hua836966f7fcda4b3d2856eb0c525b4e2_8490_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/typescript_tutorial_1_1_1/","title":"TypeScript Tutorial - 1. Gettig Started(1)"},{"content":"타입 스크립트를 사용해야하는 2가지 중요한 이유가 있다.\nJS의 동적 타이핑으로 발생하는 여러 문제들을 예방할 수 있다. 앞으로 반영될 JS 문법을 당장 사용할 수 있다. (ES Next) JS 동적 타이핑으로 발생하는 문제를 예방하는 코딩은 매우 귀찮기때문에 방치되는 경우가 많다.\nJS 최신 문법은 간편한 기능을 제공하는 경우가 많고, 여러 오류들이 수정되어있다.\nJS 동적 타이핑이란 1 2 3 let box; box = \u0026#34;hello\u0026#34;; box = 100; 1 2 3 4 5 6 7 8 let box; console.log(typeof(box)); // undefined box = \u0026#34;Hello\u0026#34;; console.log(typeof(box)); // string box = 100; console.log(typeof(box)); // number 동적 타이핑은 할당 코드 수행 시점에 해석하여 자료형을 결정한다.\n이로 인해 코드가 간결해지고, 로직을 명확하게 보여줄 수 있지만. 할당 될 때 마다 자료형을 결정하는 과정으로 인해 정적 타이핑보다 상대적으로 느리고, 변수를 여러 유형 값을 사용할 여지를 만들어 코드에 혼란을 준다.\n동적 타이핑의 문제 1 2 3 4 5 6 7 function getProduct(id){ return { id: id, name: `Awesome Gadget ${id}`, price: 99.5 } } 1 2 3 4 5 const product = getProduct(1); console.log(`The product ${product.Name} costs $${product.price}`); // result // The product undefined costs $99.5 오타로 인해 undefined가 출력되었다. 1 2 3 const showProduct = (name, price) =\u0026gt; { console.log(`The product ${name} costs ${price}$.`); }; 1 2 3 4 5 const product = getProduct(1); showProduct(product.price, product.name); // result // The product 99.5 costs $Awesome Gadget 1 인자 순서 실수로 인해 다른 의도로 출력되었다. 타입스크립트는 어떤 방식으로 동적 타이핑 문제를 해결했을까? interface 1 2 3 4 5 interface Product{ id: number, name: string, price: number }; interface를 통해 Product객체의 형태를 정의했다.\n1 2 3 4 5 6 7 function getProduct(id) : Product{ return { id: id, name: `Awesome Gadget ${id}`, price: 99.5 } } getProduct함수를 Product타입을 반환하도록 선언할 수 있다.\n1 2 const product = getProduct(1); console.log(`The product ${product.Name} costs $${product.price}`); 함수에 반환 인터페이스를 설정하여 입력이 미리 정의한 타입이 아닐 경우 코드 에디터에서 잘못된 코드임을 표시한다.\n실수를 빠르게 알아차릴 수 있다. 1 2 3 const showProduct = (name: string, price:number) =\u0026gt; { console.log(`The product ${name} costs ${price}$.`); }; 인자로 받을 타입도 지정할 수 있다. 1 2 const product = getProduct(1); showProduct(product.price, product.name); 결론 자바스크립트는 동적 타이핑을 지원하여 유연하지만 그로 인해 많은 문제들이 발생한다. 타입스크립트는 타입을 통해 동적 타이핑에서 발생하는 문제들을 예방한다. ","date":"2023-04-18T16:43:29+09:00","image":"https://codemario318.github.io/post/typescript_tutorial_1_1_2/ts_cover_hua836966f7fcda4b3d2856eb0c525b4e2_8490_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://codemario318.github.io/post/typescript_tutorial_1_1_2/","title":"TypeScript Tutorial - 1.2 Why use TypeScript"},{"content":"\nStop-the-world GC를 실행하기 위해 JVM이 애플리케이션 실행을 멈추는 것이다.\nstop-the-world가 발생하면 GC를 실행하는 쓰레드를 제외한 나머지 쓰레드는 모두 작업을 멈춘다. GC작업을 완료한 이후에야 중단했던 작업을 다시 시작한다. 어떤 GC 알고리즘을 사용하더라도 Stop-the-world는 발생한다. 대개의 경우 GC 튜닝이란 이 Stop-the-world 시간을 줄이는 것이다. GC 필요성 Java의 경우 (대부분의 언어) 프로그램 코드에서 메모리를 명시적으로 지정하여 헤제하지 않는다. 따라서 가비지 컬렉터가 더이상 필요 없는 객체를 찾아 지우는 작업을 수행하여 메모리 공간을 확보한다.\n가끔 명시적으로 해제하려고 해당 객체를 null로 지정하거나 System.gc() 메서드를 호출하는 경우가 있다. null로 지정하는 것은 큰 문제가 안되지만, System.gc() 메서드를 호출하는 것은 시스템의 매우 큰 영향을 끼치므로 System.gc() 메서드는 절대 사용하면 안된다.\nGarbage Collector 가비지 컬렉터는 두 가지 가설 하에 만들어졌다.(가정 또는 전제 조건이라 표현하는 것이 맞다.)\nWeak generational hypothesis 대부분의 객체는 금방 접근 불가능 상태(Unreachable)가 된다. 오래된 객체에서 젊은 객체로의 참조는 아주 적게 존재한다. Young 영역과 Old 영역 이 가설의 장점을 최대한 살리기 위해 HotSpot VM 에서는 크게 2개로 물리적 공간을 나누었다.\nYoung 영역\n새롭게 생성한 객체의 대부분이 여기에 위치한다. 대부분의 객체가 금방 접근 불가능 상태가 되기 때문에 매우 많은 객체가 young 영역에 생성되었다가 사라진다. 이 영역에서 객체가 사라질 때 Minor GC가 발생한다고 말한다.\nOld 영역\n접근 불가능 상태로 되지 않아 Young 영역에서 살아남은 객체가 여기로 복사된다. 대부분 Young 영역보다 크게 할당하며, 크기가 큰 만큼 Young 영역보다 GC는 적게 발생한다. 이 영역에서 객체가 사라질 때 Major GC(Full GC)가 발생한다고 말한다.\nMinor GC 새로 생성된 대부분의 객체는 Eden 영역에 위치한다. Eden 영역에서 GC가 한 번 발생한 후 살아남은 객체는 Survivor 영역 중 하나로 이동된다.\n이 과정을 반복하다가 계속해서 살아남아 있는 객체는 일정시간 참조되고 있다는 뜻이므로 Old 영역으로 이동시킨다.\nMajor GC Old 영역에 있는 모든 객체들을 검사하여 참조되지 않은 객체들을 한꺼번에 삭제한다.\n가비지 컬렉션은 어떤 원리로 소멸시킬 대상을 선정하는가? 알고리즘에 따라 동작 방식이 매우 다양하지만 공통적인 원리가 있다.\n가비지 컬렉터는 힙 내의 객체 중에서 가비지를 찾아내고 찾아낸 가비지를 처리해서 힙의 메모리를 회수한다.\n참조되고 있지 않은 객체를 가비지라고 하며 객체가 가비지인지 아닌지 판단하기 위해서 reachablitiy라는 개념을 사용한다.\n어떤 힙 영역에 할당된 객체가 유효한 참조가 있으면 reachability, 없다면 unreachability로 판단한다. 하나의 객체는 다른 객체를 참조하고, 다른 객체는 또 다른 객체를 참조할 수 있기 때문에 참조 사슬이 형성되는데, 이 참조 사슬 중 최초에 참조한 것을 Root Set이라고 칭한다.\n힙 영역에 있는 객체들은 총 4가지 경우에 대하여 참조를 가지게 된다.\n힘 내의 다른 객체에 의한 참조 Java스택, 즉 Java 메서드 실행 시에 사용하는 지역변수와 파라미터들에 의한 참조 네이티브 스택에 의해 생성된 객체에 대한 참조 메서드 영역의 정적 변수에 의한 참조 2, 3, 4는 Root set이다. 즉 참조 사슬 중 최초에 참조한 것이다.\n인스턴스가 가비지 컬렉션의 대상이 되었다고 해서 바로 소멸이 되는 것은 아니다. 빈번한 가비지 컬렉션의 실행은 시스템에 부담이 될 수 있기에 성능에 영향을 미치지 않도록 가비지 컬렉션 실행 타이밍은 별도의 알고리즘을 기반으로 계산이 되며, 이 계산 결과를 기반으로 가비지 컬렉션이 수행된다.\nSerial GC 적은 메모리와 CPU 코어 개수가 적을 때 적합한 방식으로 위에서 언급한 방식으로 동작\nParallel GC 기본적인 GC 알고리즘은 Serial GC와 동일하지만 Parallel GC는 GC를 처리하는 스레드가 여러 개라서 보다 빠른 GC를 수행할 수 있다.\n메모리가 충분하고 코어의 개수가 많을 때 유리하다.\nParallel Old GC (Parallel Compacting GC) 별도로 살아있는 객체를 식별한다는 부분에서 보다 복잡한 단계로 수행된다.\n","date":"2023-04-18T16:24:22+09:00","image":"https://codemario318.github.io/post/jvm_gc/jvm_cover_hu0ae05cc4d1c0ca93a8c2f5fc57548620_55838_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/jvm_gc/","title":"JVM - Garbage Collection"},{"content":"JVM 이란? JVM이란 Java Virtual Machine, 자바 가상 머신의 약자를 따서 줄여 부르는 용어이다. JVM의 역할은 자바 애플리케이션을 클래스 로더를 통해 읽어 들여 자바 API와 함께 실행하는 것이다.\nJVM은 Java와 OS 사이에서 중개자 역할을 수행하여 JAVA가 OS에 구애받지 않고 재사용을 가능하게 해준다. 메모리 관리, Garbage collction을 수행한다. 스택기반의 가상머신이다. ARM 아키텍쳐 같은 하드웨어는 레지스터 기반으로 동작하는데 비해 JVM은 스택 기반으로 동작한다.\n자바프로그램 실행과정 프로그램이 실행되면 JVM은 OS로 부터 프로그램이 필요로 하는 메모리를 할당받는다. JVM은 이 메모리를 용도에 따라 여러 영역으로 나누어 관리한다. 자바 컴파일러(javac)가 자바 소스코드(.java)를 읽어들여 자바 바이트 코드(.class)로 변환시킨다. Class Loader를 통해 class 파일들을 JVM으로 로딩한다. 로딩된 class 파일들은 Execution engine을 통해 해석된다. 해석된 바이트 코드는 Runtime Data Areas에 배치되어 실질적인 수행이 이루어지게 된다. 이러한 실행 과정속에서 JVM은 필요에 따라 Thread Synchronizeation과 GC같은 관리작업을 수행한다.\nJVM 구성 클래스 로더 Class Loader JVM 내로 클래스 파일을 로드하고, 링크를 통해 배치하는 작업을 수행하는 모듈이다. Runtime시에 동적으로 클래스를 로드한다. jar파일 내 저장된 크래스들을 JVM위에 탑재하고 사용하지 않는 클래스들은 메모리에서 삭제한다. (컴파일러 역할)\n자바는 동적코드, 컴파일 타임이 아니라 런타임에 참조한다. 즉, 클래스를 처음으로 참조할 때, 해당 클래스를 로드하고 링크한다. 그 역할을 클래스 로더가 수행한다.\n실행 엔진 Execution Engine 클래스를 실행시킨다. 클래스 로더가 JVM내의 런타임 데이터 영역에 바이트 코드를 배치하면 실행엔진에 의해 실행된다. 자바 바이트코드는 기계가 바로 수행할 수 있는 언어보다는 비교적 인간이 보기 편한 형태로 기술된 것이다.\n그래서 실행엔진은 이와 같은 바이트코드를 실제로 JVM내부에서 기계가 실행할 수 있는 형태로 변경한다. 이 때 두가지 방식을 사용하게 된다.\n인터프리터 Interpreter\n실행 엔진은 자바 바이트 코드를 명령어 단위로 읽어서 실행한다.\n인터프리터 언어의 단점을 그대로 갖고 있다. 한 줄 씩 수행하기 때문에 느리다. JIT Just-In-Time\n인터프리터 방식의 단점을 보완하기 위해 도입된 JIT 컴파일러이다. 인터프리터 방식으로 실행하다가 적절한 시점에 바이트 코드 전체를 컴파일하여 네이트브 코드로 변경하고, 이후에는 해당 코드를 더이상 인터프리팅 하지 않고 네이티브 코드로 직접 실행한다.\n네이티브 코드는 캐시에 보관하기 때문에 한 번 컴파일 된 코드는 빠르게 수행하게 된다. JIT 컴파일러가 컴파일 하는 과정은 바이트코드를 인터프리팅하는 것보다 훨씬 오래걸리므로 한 번만 실행되는 코드라면 컴파일하지 않고 인터프리팅 하는 것이 유리하다. JIT 컴파일러를 사용하는 JVM들은 내부적으로 해당 메서드가 얼마나 자주 수행되는지 체크하고, 일정 정도를 넘을 때에만 컴파일을 수행한다. Garbage Collector GC를 수행하는 모듈 (쓰레드)를 가진다.\nRuntime Data Area 프로그램을 수행하기 위해 OS에서 할당받은 메모리 공간\nPC Register Thread가 시작될 때 생성되며 생성될 때마다 생성되는 공간으로 스레드마다 하나씩 존재한다.\n쓰레드가 어떤 부분을 어떤 명령으로 실행해야할 지에 대한 기록을 하는 부분으로 현재 수행중인 JVM 명령의 주소를 갖는다.\nJVM 스택 영역 프로그램 실행과정에서 임시로 할당되었다가 메소드를 빠져나가면 바로 소멸되는 특성의 데이터를 저장하기 위한 영역이다.\n각종 형태의 변수나 임시 데이터, 스레드나 메소드의 정보를 저장한다. 메소드 호출 시마다 각각의 스택 프레임(그 메서드 만을 위한 공간)이 생성된다. 메서드 수행이 끝나면 프레임 별로 삭제를 한다. 메소드 안에서 사용되는 값들(local variable)을 저장한다. 호출된 메소드의 매개변수, 지역변수, 리턴 값 및 연산 시 일어나는 값들을 임시로 저장한다. Native method stack 자바 프로그램이 컴파일 되어 생성되는 바이트 코드가 아닌 실제 실행할 수 있는 기계어로 작성된 프로그램을 실행시키는 영역이다.\n자바가 아닌 다른 언어로 작성된 코드를 위한 공간이다. Java Native Interface를 통해 바이트 코드로 전환하여 저장하게 된다. 일반 프로그램처럼 커널이 스택을 잡아 독자적으로 프로그램을 실행시키는 영역이다. 이 부분을 통해 C code를 실행시켜 Kernel에 접근할 수 있다. Method Area (= Class area = Static area) 클래스 정보를 처음 메모리 공간에 올릴 때 초기화되는 대상을 저장하기 위한 메모리 공간.\n올라가게 되는 메소드의 바이트코드는 프로그램의 흐름을 구성하는 바이트 코드이다. 자바 프로그램은 메인 메소드의 호출에서 부터 계속된 메소드의 호출로 흐름을 이어가기 때문이다. 대부분 인스턴스의 생성도 메소드 내에서 명령하고 호출한다. 사실상 컴파일 된 파이트코드의 대부분이 메소드 바이트코드이기 때무넹 거의 모든 바이트코드가 올라간다고 봐도 상관없다. Runtime Constat Pool이라는 별도의 관리 영역도 함께 존재하여, 상수 자료형을 저장하여 참조하고 중복을 막는 역할을 수행한다. 올라가는 정보의 종류 Feild Information 멤버 변수의 이름 데이터 타입 접근 제어자에 대한 정보 Method Information 메소드의 이름, 리턴타입, 매개변수, 접근 제어자에 대한 정보 Type Information Class인지 interface인지의 여부 저장 Type의 속성 전체 이름 Super class의 전체 이름 (interface이거나 object인 경우 제외) Heap 객체를 저장하는 가상 메모리 공간\n생성된 객체와 배열을 저장한다. class area영역에 올라온 클래스들만 객체로 생성할 수 있다. Permanent Generation 생성된 객체들의 정보 주소값이 저장된 공간이다. class loader에 의해 load되는 class, method 등에 대한 meta 정보가 저장되는 영역이고, JVM에 의해 사용된다.\nReflection을 사용하여 동적으로 클래스가 로딩되는 경우에 사용된다. 내부적으로 Reflection 기능을 자주 사용하는 Spring Framework를 이용할 경우 이 영역에 대한 고려가 필요하다.\nNew/Young 영역 Eden 객체들이 최초로 생성되는 공간 Survivor 0 / 1 Eden에서 참조되는 객체들이 저장되는 공간 Old 영역 New area에서 일정 시간 참조되고 있는, 살아남은 객체들이 저장되는 공간\nEden 영역에 객체가 가득차게 되면 첫번째 GC(minor GC)가 발생한다. Eden 영역에 있는 값들을 Survivor 1 영역에 복사하고 이 영역을 제외한 나머지 영역의 객체를 삭제한다.\n인스턴스는 소멸 방법과 소멸 시점이 지역 변수와는 다리기에 힙이라는 별도의 영역에 할당된다. 자바 가상머신은 매우 합리적으로 인스턴스를 소멸시킨다. 더이상 인스턴스의 존재 이유가 없을 때 소멸시킨다.\n","date":"2023-04-18T16:07:19+09:00","image":"https://codemario318.github.io/post/jvm/jvm_cover_hu0ae05cc4d1c0ca93a8c2f5fc57548620_55838_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/jvm/","title":"JVM"},{"content":"제너레이터는 제너레이터 함수가 호출될 때 반환되는 이터러블 객체이다. 제네레이터 함수는 일반적인 함수와 비슷하게 생겼지만 yield 구문을 사용해 데이터를 원하는 시점에 반환하고 처리를 다시 시작할 수 있다. 일반적인 함수는 진입점이 하나라면 제네레이터는 진입점이 여러개라고 생각할 수 있다. 이러한 특성때문에 제네레이터를 사용하면 원하는 시점에 원하는 데이터를 받을 수 있게된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def generator(): yield 1 yield \u0026#39;string\u0026#39; yield True gen = generator() print(gen) # \u0026lt;generator object generator at 0x10a47c678\u0026gt; next(gen) #1 next(gen) # \u0026#39;string\u0026#39; next(gen) # True next(gen) \u0026#39;\u0026#39;\u0026#39; Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; StopIteration \u0026#39;\u0026#39;\u0026#39; 동작 yield 문이 포함된 함수를 실행하면 제너레이터 객체가 반환되는데 이 때는 함수의 내용이 실행되지 않는다. next() 라는 빌트인 메서드를 통해 제네레이터를 실행시킬 수 있으며 next() 메서드 내부적으로 iterator 를 인자로 받아 이터레이터의 __next__() 메서드를 실행시킨다. 처음 __next__() 를 호출하면 함수의 내용을 실행하다 yield 문을 만났을 때 처리를 중단한다. 이 때 모든 local state는 유지되는데 변수의 상태, 명령어 포인터, 내부 스택, 예외 처리 상태를 포함한다. 그 후 제어권을 상위 컨텍스트로 양보(yield)하고 또 __next__() 가 호출되면 제네레이터는 중단된 시점부터 다시 시작한다. yield 문의 값은 어떤 메서드를 통해 제네레이터가 다시 동작했는지에 따라 다른데, __next__() 를 사용하면 None이고 send() 를 사용하면 메서드로 전달 된 값을 갖게되어 외부에서 데이터를 입력받을 수 있게 된다.\n장점 List, Set, Dict 표현식은 iterable 하기에 for 표현식 등에서 유용하게 쓰일 수 있다. 하지만 해당 객체들은 Collection 특성상 가진 데이터를 메모리에 담고 있어야 하기 때문에 큰 값을 다룰 때는 성능상 불리하다. 제너레이터는 yield 를 통해 필요한 값만 받아 쓰기 때문에 모든 값을 메모리에 들고 있을 필요가 없게 된다.\n1 2 3 4 5 6 7 import sys a = [i for i in range(100000)] sys.getsizeof(a) #824464 b = (i for i in range(100000)) sys.getsizeof(b) #88 리스트가 여러번 사용될 수 있는 반면 b 제네레이터는 한번 사용된 후 소진된다. 이는 모든 이터레이터가 마찬가지인데 List, Set 등은 이터러블하지만 이터레이터는 아니기에 소진되지 않는다.\n1 2 len(list(b)) # 100000 len(list(b)) # 0 while True 구분으로 제공받을 데이터가 무한하거나, 모든 값을 한번에 계산하기엔 시간이 많이 소요되어 그때 그때 필요한 만큼만 받아 계산하고 싶을 때 제네레이터를 활용할 수 있다.\n","date":"2023-04-18T14:31:00+09:00","image":"https://codemario318.github.io/post/python_cover/python_cover_hu071c6006b6148c050030e26fb108bd62_83564_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/python_cover/","title":"Python - Generator"},{"content":"\n엘라스틱서치는 검색 엔진인 아파치 루씬 (Apache Lucene)으로 구현한 RESTful API 기반의 검색 엔진이다. 엘라스틱서치 아키텍쳐는 클러스터 기반으로 구성되어 있다.\n클러스터 기본 특징 수평 확장\n클러스터를 사실상 무한으로 확장할 수 있다.\n인덱스 샤딩\n엘라스틱서치는 인덱스를 조각내서 \u0026ldquo;샤드 (shard)\u0026ldquo;라는 조각난 데이터로 만든다. 때문에 나누어진 데이터를 편하게 각 호스트에 분리해서 보관할 수 있다.\n엘라스틱서치 특징 Schemaless와 문서지향 엘라스틱 서치는 JSON 구조를 사용하여 기존 RDBMS와 같은 엄격한 구조를 적용하지 않는다.\n스키마가 다이나믹하게 변할 수 있다\n전통적인 관계형 구조로 구성할 경우 프로젝트 막바지에 모든 스키마르 ㄹ변경하고, 데이터를 다시 구성하는 문제에 봉착할 수 있는데 JSON 구조는 이런 문제들을 사전에 막을 수 있다. 데이터 공간을 절약할 수 있다.\n컬럼을 동적으로 정의하여, 필요한 데이터만 넣게 되어 데이터 공간 및 CPU 사용량, 네트워크 트래픽도 줄일 수 이쓴 효과를 볼 수 있다. 검색능력(Searching) 기본적인 검색 기능 뿐만 아니라 특히 Full-text 검색 능력이라는 강력한 기능을 탑재하고 있다.\n관계형 데이터베이스의 문자열 컬럼에 대해 실행되는 단순한 SQL 질의와는 다르다.\n기본적으로 엘라스틱은 검색을 할 수 있는 Term(의미의 최소한위)로 단어의 형태소 분석을 수행하고, 이 단어들과 연관된 문서를 매핑하여 검색을 시켜주는 개념으로 문서를 통쨰로 like 검색하는 DB와는 구조적으로 다르다.\n분석(Analytics) 엘라스틱 서치를 탑재하여 만든 사이트에 접속하는 사람들의 OS가 무엇인지, 어느나라에서 접속했는지 등을 알고 싶을 때 분석 기능을 사용하면 편리하게 알 수 있다.\n풍부한 API와 REST 지원 기본적으로 20개의 프로그래밍 언어를 지원하며, 기본적으로 REST API를 제공하여 REST API를 사용하는 모든 언어에서 HTTP 형식으로 사용할 수 있다.\n쉬운 작동, 쉬운 확장 Single Node Instance로 작동하며, 수백개의 스케일 아웃을 쉽게 할 수 있다. 대부분의 빅데이터 플랫폼들이 그러하듯 Horizontal Scaling을 사용한다.\nNear real-time(근접 실시간) 검색엔진은 기본적으로 형태소를 분석하고 색인을 해야 하는 시간이 다른 DBMS보다 오래 걸린다. 엘라스틱 역시 데이터를 삽입한 순간 약 몇 초 정도는 이 단계를 거친 후 검색을 할 수 있다.\nLightning-fast (빠른 속도) 엘라스틱 서치의 DNA는 루씬이기 떄문에 단어 입력후 문서를 찾는 속도가 다른 NoSQL들에 비해 매우 빠르다.\nFault-tolerant(내고장성) 노드 실패시 replicate된 다른 노드에서 데이터를 가져오며, 네트워크 실패 시 다른 마스터 복제본으로 선택한다.\n엘라스틱서치 데이터 구조 엘라스틱서치는 위와 같이 문서를 엘라스틱 인덱스로 만든 뒤, 샤드로 분리하여 보관한다.\n샤드는 논리적/물리적으로 분할 된 인덱스인데, 각각의 엘라스틱서치 샤드는 루씬 인덱스이기도 하다.\n루씬은 새로운 문서를 엘라스틱서치 인덱스에 저장할 때 \u0026ldquo;세그먼트\u0026quot;를 생성하는데, 루씬의 인덱스 조각인 이 세그먼트를 조합해 저장한 데이터의 검색을 할 수 있다.\n색인 처리량이 매우 중요할 때는 세그먼트를 더 생성하기도 한다. 루씬은 순차적으로 세그먼트를 검색하므로 세그먼트 수가 많아지면 검색속도도 따라서 느려지게 된다.\n엘라스틱서치 데이터 설명 인덱스(색인) 데이터를 저장 및 색인 하는 곳으로, 관계형 DB의 데이터베이스 개념과 유사하다.\n실제로는 각 샤드를 가리키고 있는 논리적인 네임스페이스 Shard 샤드는 엘라스틱서치에서 사용하는 검색 엔진인 루씬의 인스턴스.\n인덱스를 한 개의 샤드로 구성할 수도 있지만, 인덱스 사이즈가 증가할 경우 여러개의 물리서버에 나누어 보관하기 위해 보통은 여러개의 샤드로 구성함. Segment 각 샤드는 다수의 세그먼트를 가지고 있고, 샤드에서 검색 시, 먼저 각 세그먼트를 검색하여 결과를 조합한 최종 결과를 해당 샤드의 결과로 리턴하게 된다.\nsearchable\n엘라스틱서치에 데이터(문서)를 저장하면, 엘라스틱서치는 이것을 메모리에 모아두고 새로운 세그먼트를 디스크에 기록하여 검색을 리프레시함. 이로 인해 새로운 검색 가능한 세그먼트가 만들어지게 된다.\ncommited\n그러나 세그먼트가 fsync되지 않았으므로 여전히 데이터 손실의 위험이 남아있다. 그래서 엘라스틱서치는 세그먼트를 fsync하는 \u0026ldquo;flush\u0026quot;를 주기적으로 진행하고, 불필요한 트랜젝션 로그를 비운다.\nmerge process\n세그먼트는 불변임, 데이터(document)가 업데이트되면 실제로는 그저 삭제되었다고 마크하고 새로운 데이터(document)를 가리킬 뿐이다. 이러한 오래된 삭제된 데이터를 지우는 것\n엘라스틱서치 클러스터 구조 위 다이어그램은 3개의 엘라스틱서치 인스턴스 환경에서, 4개의 샤드를 2개의 복제본으로 구성했을 때의 구조이다.\n엘라스틱서치는 클러스터 구조로 구성되어 있으며 샤드와 복제본의 수를 설정해두면 스스로 각 노드에 샤드를 분배하여 장애발생 시 데이터 손실을 최소화한다.\n프라이머리 샤드가 손실되었을 경우에는 레플리카를 프라이머리로 승격시켜 데이터 손실을 방지한다.\n","date":"2023-04-18T14:13:19+09:00","image":"https://codemario318.github.io/post/elasticsearch/elasticsearch_cover_hu19466caf2459bbcd9b4d95bc6d53e495_16781_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/elasticsearch/","title":"Elasticsearch"},{"content":" 검색 엔진은 컴퓨터 시스템에 저장된 정보를 찾아주는 것을 도와주도록 설계된 정보 검색 시스템이다. 검색 엔진을 사용하여정보를 찾는데 필요한 시간을 최소화할 수 있다.\n웹 검색 엔진 웹 사이트를 검색하기 위한 프로그램이다.\nFTP 서버나 웹 사이트의 파일 검색 결과를 포함하며, 이미지나 토렌트 파일 또는 특정 부류의 웹 사이트에 특화된 웹 검색 엔진도 있다.\n서버에서는 \u0026lsquo;로봇\u0026rsquo;이라 불리는 특별한 프로그램을 이용하 웹 사이트들을 돌아다니며 웹 사이트들에 대한 정보를 미리 자동적으로 수집한다. 이휴 검색 엔진 사이트에서 특정 검색어를 입력하면 검색 엔진이 수집한 정보 중 그에 맞는 결과를 볼 수 있다.\n로봇이 참조와 어휘를 분헉하는 방식에 따라 검색 품질이 결정됨 원리 검색 엔진은 사용자가 검색 엔진을 사용하기 전에 미리 웹 상에서 정보를 수집하여 색인을 만들어 놓는다. 그리고 나서 사용자가 찾고자 하는 정보의 키워드를 입력하면, 미리 만들어 놓은 색인 중에서 입력된 키워드에 해당하는 정보들을 찾아서 보여준다.\n문서 수집 현재 대부분의 검색 엔진에서는 엡상의 방대한 정보들을 검색하고 색인화 하는 과정을 크롤러라고 부르는 정보 수집 프로그램을 사용하여 수행하고 있다.\n크롤러가 주기적으로 웹에 접속된 사이트들을 방문하여 해당 웹 사이트가 가지고 있는 정보에 대해 색인을 작성한 후 그것을 데이터베이스에 저장하여 검색시 활용하게된다.\n크롤러\n웹상의 문서나 이미지, 영상 등을 주기적으로 검색하고 취합하여, 자동으로 데이터베이스화 시키는 프로그램으로 봇(Bot)이라고도 부른다.\n검색 엔진의 종류 수집한 정보를 색인하는 방법에 따라 구분된다.\n로봇 검색 엔진 크롤라라고 불리는 로봇을 이용하여 웹상의 데이터를 효율적으로 수집하고, 이렇게 수집한 데이터 키워드 색인을 통해 사용자에게 제공하는 검색 엔진\nGoogle, Naver등 현재 사용되는 대부분의 검색 엔진이 이 방식을 채택하고 있다. 디렉토리 검색 엔진 주제 분류에 의한 검색을 제공하는 검색 엔진이며, 데이터의 분류를 사람이 직접 슈행해야 한다.\n현재 주류인 방식은 아니며, 1990년대 Yahoo등에서 사용되었음 메타 검색 엔진 자체적으로 정보를 보유하고 있지 않으면서 사용자가 입력한 키워드를 복수의 다른 검색 엔진으로 전송하여 결과를 얻고, 그 결과들을 종합하여 표시만 해주는 검색 엔진\n여러 검색 엔진의 결과를 동시에 보여주기 때문에 결과를 한눈에 살펴보기에는 편하지만, 메타 검색이라는 과정을 한 번 더 거쳐야 하므로 속도가 느를 수 있다.\n검색 엔진 최적화(Search Engine Optimization, SEO) 검색 결과의 상위에 자신의 웹 페이지가 노출되기 위해 검색 엔진이 자료를 수집하고 결과를 산출하는 방식에 맞춰 웹 페이지의 구성을 조정하는 것을 의미한다.\n각각의 검색 엔진에 맞처 웹 페이지 내의 키워드나 링크 등을 최적화 하는 작업을 SEO라고 한다.\n","date":"2023-04-18T14:04:15+09:00","image":"https://codemario318.github.io/post/search_engine/search_engine_cover_hu920de5c22e59a77d3210239e6515a52e_9451_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/search_engine/","title":"검색 엔진"},{"content":"용어정리 MOM (Message Oriented Middleware, 메시지 지향 미들웨어) 독립된 어플리케이션 간에 데이터를 주고받을 수 있도록 하는 시스템 디자인 함수 호출, 공유메모리 등이 방식이 아닌, 메시지 교환을 이용하는 중간 계층에 대한 인프라 아키텍쳐 개념 분산 컴퓨팅이 가능해지며, 서비스간의 결합성이 낮아짐 지동기로 메시지를 전달하는 것이 특징 Queue, Broadcast, Multicast 등의 방식으로 메시지 전달 Pub/Sub 구조 메시지를 발행하는 Publisher(Producer), 메시지를 소비하는 Subscribe(Consumer)로 구성 Message Broker 메시지처리 또는 메시지 수신자에게 메시지를 전달하는 시스템이며, 일반적으로 MOM 기반으로 구축됨 MQ (Message Queue, 메시지 큐) Message bBroker와 MOM을 구현한 소프트웨어 (RabbitMQ, ActiveMQ, Kafka 등) MOM은 메시지 전송 보장을 해야하므로 AMQP를 구현함 AMQP (Advanced Message Queueing Protocol) 메시지를 안정적으로 주고박기 위한 인터넷 프로토콜 RabbitMQ, Kafka 등은 AMQP를 구현한 MOM 시스템이다.\n메시징 시스템이란? 로그 데이터, 이벤트 메시지 등 API로 호출할 떄 보내는 데이터들을 처리하는 시스템\n예를 들어, 다음과 같이 자동 메일을 발송 시스템이 있다고 가정하면,\n회원가입을 했을 때, 이메일을 발송하는 MemberService 주문완료가 되었을 때, 이메일을 발송하는 OrderService 메일을 실제 발송하는 MailService 이렇게 서비스가 분리되었을 때 프로세스는 다음과 같이 처리될 수 있다.\nMemberService에서 회원가입, OrderService에서 주문완료 이벤트가 발생 Messaging Client로 메일 전송에 필요한 데이터( 받는/보내는 사람 이메일 주소, 메시지 제목/내용 등.. )를 API 호출 Messaging Client에서 MOM을 구현한 소프트웨어(ex. kafka)로 메시지를 생산 MailService에서 메시지가 존재하는지 구독하고 있다가 메시지가 존재하면 메시지를 소비 MailService에서 API 정보들을 통해 User에게 메일 발송 장점 서비스간의 결합성이 낮아지므로 각자의 비즈니스 로직에만 집중할 수 있다. 메시지 처리 방식은 Message Broker에 위임 각 서비스는 Client를 통해 메시지를 보내고 받기만 하면 됨 각 서비스는 비동기 방식으로 메시지를 보내기만 하면, Message Broker에서 순서 보장, 메시지 전송 보장 등을 처리 메시징 시스템이 잠깐 다운되어도 각 서비스에는 직접적인 영향을 미치지 않음 단점 Message Broker 구축, 예를 들면 kafka 클러스터 구축에 필요한 금전, 인적 자원에 대한 비용 비동기의 양면성 - 정말 메시지가 잘 전달되었는가? 함수호출, 공유 메모리 사용 방식보다 메시징 시스템을 사용했을 때 호출 구간이 늘어나므로 네트워크 비용 발생 MOM, 메세지 지향 미들웨어(Message-Oriented-Middleware) 미들웨어: 어플리케이션들을 연결해 이들이 서로 데이터를 교환할 수 있게 해주는 소프트웨어 메시지 지향(=메시징 시스템): 메시지 API를 통해 각 분산되어 있는 어플리케이션간의 다리 역할을 함으로써 데이터를 교환 할 수 있도록 하는 시스템 메시지 지향 미들웨어란? 메시지를 통해 여러 분산되어 있는 시스템 간의 Connector 역할로 결합성을 낮추고, 이들이 서로 실시간 비동기식 데이터를 교환할 수 있도록 하는 소프트웨어\nMessage Queue 기반 패턴 메시지 대기열 패턴은 일종의 지점 간(peer to peer) 메시징 시스템이다. Queue 대기열의 메시지는 Consumer가 소비하면 지워지는 형태\n소비하면 지워지는 형태라는 의미는 Producer 서버가 메시지를 Queue에 보내고 서버가 다운이 되도 Consumer가 소비하지 않았다면 Queue 대기열에 데이터가 존재한다는 걸 의미한다.\n발행(Publish)-구독(Subscribe) 메시지 패턴 메시지 큐와 마찬가지로 메시지를 생산하는 Producer와 메시지를 소비하는 Consumer로 구성되어 있다.\n차이점은 여러 소비자가 하나의 주제에서 각 메시지를 수신할 수 있다는 점. 또한 모든 Consumer가 메시지를 사용하는 경우에만 메시지가 대기열에서 지워진다.\nkafka와 같은 메시징 시스템에는 메시지가 대기열에 있어야 하는 기간을 지정한 보존 정책이 있다. 따라서 메시지는 모든 Consumer가 소비하더라도 지정된 기간 동안 대기열에 사용할 수 있다.\n언제 쓰이는가? 분산 시스템 여러 컴퓨터를 분산시켜 네트워크를 연결하여 데이터들을 나눠서 처리하면 서버의 과부하를 분산할 수 있으며, 성는개선과 장애요소를 최소화하기 위해 분산 시스템을 사용함 과거 분산 시스템의 단점과 웹 API 통신의 한계 과거 분산시스템의 단점 수많은 데이터를 처리하기 위하여 분산 시스템을 운영하였지만, 시스템이 거대해질수록 분산 시스템의 설계도의 복잡성 문제가 발생한다. 하나의 응용프로그램이 변경되면, 다른 응용프로그램에도 영향을 미쳐 분산 시스템 간의 결합도가 강한 단점을 가지고 있었다.\n웹 API 통신의 특성 MSA를 사용한 분산 시스템은 웹 API 서버로 요청 시 응답을 기다려야 한다. 여러 분산되어있는 서비스 간에는 실시간으로 비동기식으로 데이터를 처리해야 하기 떄문에 웹 API로도 비동기식 구현이 가능하지만 순서가 보장되지 않는다는 특성이 있다. 메시지를 보내는 A어플리케이션은 메시지를 보낼 때 B라는 어플리케이션의 목적지(도착점)을 알아야 통신할 수 있다. 두 어플리케이션간 불필요한 결합도가 발생되고, 응답을 취하는 B어플리케이션이 서버 장애시 요청되었던 데이터 때문에 A어플리케이션에게도 장애가 전파될 수 있다. 메시징 지향 미들웨어의 필요성 메시지 API는 비동기 프로토콜을 지원하며, 메시지 순서를 보장합니다. 메시지가 대기열에 전달되면, 응답을 즉시 할 필요가 없다. 메시지 대기열에 전달 된 상황이라면 메시지는 시스템에 보존되어 있어, 다른 어플리케이션간의 의존성이 낮게 된다. Message Broker 송신자와 수신자 사이에서 메시지의 전달을 중재하는 컴퓨터 프로그램 모듈\n메시지 브로커는 정형화된 메시지의 교환을 통해 어플리케이션간의 소통이 이뤚디는 네트워크 엘리먼드이다.\n목적 메시지의 유효성, 정송, 라우팅을 위한 아키텍처 패턴\n어플리케이션 사이의 커뮤니케이션 중재 어플리케이션간의 메시지 전달을 위한 상호 인식(mutal awareness)를 줄여 어플리케이션간의 결합성을 낮춘다(decoupling) 기능 엔드 포인트 분리 NFR(non-functional requirement) 충조 중재함수 (intermediary function)의 간편한 재사용 하나이상의 목적지로의 메시지 라우팅 메시지의 형태 변형 메시지를 수집하여 여러 메시지로 분해하고 대상으로 보내 응답을 하나의 메시지로 재구성하여 사용자에게 반환 메시지 양 증가 또는 저장을 위한 외부 저장소와 상호작용 데이터 검색을 위한 웹 서비스 호출 이벤트 또는 에러의 응담 발행-구독 패턴을 활용한 컨텐츠와 토픽 기반 메시지 라우팅 제공 설계 허브 앤 스포크(hub and spoke)\n중앙 서버가 통합 서비스를 제공하는 메커니즘으로 작동 메시지 버스(message bus)\n메시지 브로커가 버스에서 작동하는 통신 백본 또는 분산 서비스 ","date":"2023-04-17T19:42:33+09:00","image":"https://codemario318.github.io/post/messaging_system/messaging_cover_huc80ec853f6ab161a17ff43aa6052ff01_60754_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/messaging_system/","title":"메시징 시스템이란?"},{"content":"Not Only SQL: SQL만을 사용하지 않는 데이터베이스 관리 시스템을 지칭하는 단어. \u0026lsquo;데이터를 저장하는 데에는 SQL 외에 다른 방법들도 있다.\u0026rsquo;\n정의 NoSQL에 내려진 구체적인 정의는 없으나 공통적인 성향을 가지고 있다.\n대부분 클러스터에서 실행할 목적으로 만들어졌기 때문에 관계형 모델을 사용하지 않는다. 오픈 소스이다. 스키마 없이 동작하며, 구조에 대한 정의를 변경할 필요 없이 데이터베이스 레코드에 자유롭게 필드를 추가할 수 있다. 21세기 초반에 개발 된 SQL을 사용하지 않는 Schema-less 데이터베이스\n클러스터\n저렴한 상용 제품 여러 대를 조합하여 더 빠르고 안정적인 시스템을 목표로 만들어진 방법\n등장배경 여러 대의 컴퓨터에 데이터를 분산 저장하는 것을 목표로 등장했다.\n기존에는 안정적인 데이터 관리가 가장 중요했기 때문에, 트랙잭션을 통한 관리가 가능한 RDBMS가 많이 이용되었지만 웹 2.0 환경과 빅데이터가 등장하면서 RDBMS는 데이터와 트래픽 양이 기하급수적으로 증가함에 따라 한 대에서 실행되도록 설계된 RDBMS를 사용하는 데 필요한 비용 증가 이슈가 생겨났다.\nNoSQL은 데이터의 일관성을 약간 포기한 대신 여러 대의 컴퓨터에 데이터를 분산하여 저장하는 것을 목표로 등장하였고, NoSQL의 등장으로 작고 값싼 장비 여러 대로 대량의 데이터와 컴퓨팅 부하를 처리하는 것이 가능하게 되었다.\n특징 일관성과 확장성 사이의 Trade-off\n일관성이 데이터베이스의 절대적인 요소가 아님을 주장하는 움직임이 생기기 시작했다. 다수가 동시에 읽고 쓰는 상황에서의 성능 향상을 위해서. 분산 환경에서 노드들이 잘 작동하고 있음에도, 시스템의 일부가 고장나면 데이터베이스를 사용할 수 없게 되는 문제를 해결하기 위해서. 분산 저장\n데이터와 트래픽이 증가함에 따라 기존의 장비로는 원할한 데이터의 처리가 어려워졌다. 이를 해결하기 위한 방법으로 장비의 성능을 키우는 수직적 확장과 처리하는 장비 수를 늘리는 수평적 확장이 있다. 수직적 확장은 큰 비용적인 문제가 발생하므로 수평적 확장을 고려했지만, RDBMS가 클러스터 상에서 효율적으로 동작하도록 설계되지 않았다. 샤딩(Sharding)\n샤드키를 기준으로 하나의 테이블을 수평 분할하여 서로 다른 클러스터에 분산 저장하고 질의할 수 있는 기법. RDBMS에서도 사용 가능하지만 어플리케이션 레벨에서 모든 샤딩을 제어해야 한다.(어떤 데이터를 어느 클러스터에서 처리해야 하는지 등) 또한 여러 샤드에 걸치는 쿼리나 참조 정합성, 트랜잭션, 일관성 문제가 발생할 수 있다. 분산 저장을 지원하는 NoSQL 데이터베이스의 경후, 집합-지향(Aggregtae-oriented) 모델을 사용하여 이러한 문제를 해결한다. 연관된 데이터들이 함께 분산되므로, 관계형 모델에서처럼 복잡한 제어가 필요하지 않게 된다. 데이터 일치\nRDBMS에서 관계형 튜플 안의 값은 단순해야 하며 중첩된 레코드나 리스트 등 다른 구조를 포함할 수 없느 반면, 메모리 내 데이터 구조에서는 이런 제약이 없어 훨씬 복잡한 구조를 사용한다.(리스트, 딕셔너리, 중첩된 객체 구조) 그 결과 복잡한 메모리내 데이터 구조를 데이터베이스에 저장하려면 먼저 관계형 표현으로 변환해야 한다. (ORM 프레임워크등을 이용) NoSQL은 메모리 내의 데이터가 어떤 구조이든지 상관하지 않고 하나의 Aggregation으로 취급하여 저장하기 때문에 자유롭다. Impedance mismatch\n관계형 모델과 메모리 내 데이터 구조 간의 불일치\nSchema-less\nNoSQL 데이터베이스의 공통적인 특징은 스키마 없이 동작한다는 점이다. 장점 데이터 구조를 미리 정의할 필요가 없다. 시간이 지나더라도 언제든지 바꿀 수 있기 때문에 비형식적인 데이터를 저장하는 데 용이하다. 단점 단일 값에 대한 데이터 타입에서 불일치가 발생할 수 있다. 데이터베이스가 스키마를 직접 관리하지 않는 것을 의미할 뿐, 데이터 타입에 따른 암묵적인 스키마는 여전히 존재하기 때문 종류 네 가지 모델로 나눌 수 있다.\nkey-value Document Column-family Graph 이 중 그래프 모델을 제외한 나머지 세 모델은 집합-지향(Aggregate-orented)모델이다.\n집합-지향 (Agregate-orented) 모델 집합 지향 데이터베이스는 집합 자료구조로 이루어져 있다.\n집합\n연산의 한 단위로 취급되는 연관된 객체들의 집합.\n관계형 모델처람 하나의 엔티티에 대한 ACID 트랜잭션을 지원하지는 않지만, 하나의 집합에 대한 연산에서는 트랜잭션을 지원한다.\n장점\n집합 지향 데이터베이스는 여러 대의 클러스터로 이루어진 시스템에서 사용하기 적합하다. 수평적 확장이 용이하다. 이는 관계형 데이터베이스와는 달리 연관된 데이터들이 함께 움직이기 떄문이다. 메모리 내의 자료구조와 집합 간 데이터가 잘 일치하므로, 관계형 데이터베이스처럼 객체-관계 매핑 프레임워크가 필요하지 않다. 데이터의 검색도 아주 쉬운편으로, key나 ID를 사용하면 쉽게 집합 레코드를 찾아낼 수 있다. 단점\n집합 지향 데이터베이스는 조인 연산이 불가능 MongoDB나 Cassandra등의 데이터베이스에서는 맵리듀스(MapReduce) 기능을 제공함으로써 조인과 유사한 연산을 가능하도록 설계했지만 사용법이 어렵고 Hadoop의 맵 리듀스에 비하면 속도도 매우 느리다. Key-Value 키 값 저장소는 가장 단순한 형태의 NoSQL\n장점\n수평적 확장이 용이하다. 아주 간단한 API만을 제공하기 떄문에 배우는 것이 어렵지 않다. 간단한 API를 제공하는 만큼 질의의 속도가 굉장히 빠른편 어떠한 형태의 데이터라도 담을 수 있다. 이미지나 비디오도 가능 단점\n값의 내용을 사용한 쿼리가 불가능하다. 키를 사용해 값을 읽어들인 뒤, 어플리케이션 레벨에서 적절히 처리해야 한다. Document 데이터가 키와 문서 형태로 저장되는 키-값 모델의 개선 형태\n키-값 모델과의 차이점\nValue가 계층적인 형태인 도큐먼트로 저장된다. 객체지향의 객체와 유사하며, 하나의 단위로 취급되어 저장된다.\n장점\n하나의 객체를 여러 테이블에 나눠 저장할 필요가 없다. 도큐먼트 내의 item을 이용한 쿼리가 가능하다. 단, Xquery나 다른 도큐먼트 질의 언어가 필요 객체-관계 매핑이 필요하지 않다. 객체를 도큐먼트의 형태로 바로 저장 가능하기 떄문 검색에 최적화 되어있다. 단점\n사용이 번거롭고 쿼리가 SQL과 다르다. 질의의 결과가 JSON이나 XML 형태로 출력되기 때문에 사용방법이 RDBMS와 다르다. 질의 언어가 SQL과 다르기 떄문에 사용에 익숙해지기까지 다소 어려움이 있을 수 있음. 종류 MongoDB 도큐먼트 지향 데이터 베이스이다.\nbson 데이터 구조로 저장 문서를 기본 저장 단위로 이용하면서 내장 문서와 배열을 이용하여 복잡한 계층구조를 하나의 레코드로 표현한다. 스키마가 없다. 필드 추가 제거는 자유로우며 필요할 때 마다 자유자재로 변경 가능하다. RDBMS 보다 매우 빠르다. 조인과 트랜잭션을 지원하지 않으며 여러 제약조건에 대한 처리도 없다. →버전에 따라 다름 Redis(REmote DIctionary Server) 메모리 기반의 \u0026ldquo;Key-Value\u0026rdquo; 구조 데이터 관리 시스템이며, 모든 데이터를 메모리에 저장하고 조회하기에 빠른 Read, Write 속도를 보장하는 비 관계형 데이터베이스이다.\nString, set, Sorted Set, Hash, List 데이터 형식을 지원한다. Redis는 빠른 오픈 소스인 메모리 키-값 데이터 구조 스토어이며, 다양한 인메모리 데이터 구조 집합을 제공하므로 사용자 정의 어플리케이션을 손쉽게 생성 할 수 있다.\n특징 영속성을 지원하는 인메모리 데이터 저장소\n읽기 성능 증대를 위한 서버측 복제 지원\nRedis가 실행중인 서버가 충돌하는 경우 장애 조치 처리와 함께 더 높은 읽기 성능을 지원하기 위해 슬레이브가 마스터에 연결하고 전체 데이터베이스의 초기 복사본을 받는 마스터/ 슬레이브 복제를 지원. 마스터에서 쓰기가 수행되면 슬레이브 데이터 세트를 실시간으로 업데이트 하기 위해 연결된 모든 슬레이브로 전송됨 쓰기 성능 증대를 위한 클라이언트\n","date":"2023-04-17T19:24:48+09:00","image":"https://codemario318.github.io/post/nosql/nosql_cover_hu705a0f96b7606376fe264778ca77daa9_6424_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/nosql/","title":"NoSQL이란?"},{"content":"Apache 아파치는 클라이언트 요청을 받으면 MPM(Multi Processing Module : 다중처리모듈) 방식으로 처리한다.\n스레드/프로세스 기반 구조 동작 ServerSocket으로 request A가 들어오면 Thread를 할당해준다. Thread는 해당 socket을 가지고 read, write작업 등을 수행한다. 수행 도중 ServerSocket으로 request B가 들어오면, context switching이 일어난다. 새로 들어온 요청에 쓰레드를 배분하고, 또 해당 소켓을 가지고 작업을 수행한다. 아직 마무리되지 않은 A를 처리하기 위해 일정 기간마다 지속적으로 context switching을 반복하고 모든 작업을 마무리 한다. Prefork MPM 실행중인 프로세스를 복제하여 처리하는 방식\n각 프로세스는 한번에 한 연결만 처리하고, 요청량이 많아질수록 프로세스를 복제하여 동작한다.\n프로세스를 복제하는 방식이기 떄문에 메모리가 많이 소비된다\n연결 수 = 프로세스 수\nWorker MPM Prefork 방식은 한개의 프로세스가 한개의 스레드로 처리되지만, Worker 방식은 한개의 프로세스가 여러 쓰레드를 사용하여 처리한다.\n쓰레드를 사용하기 떄문에 Prefork 방식보다 메모리 소모가 적고, 통신량이 많을 때 유리하다.\n문제점 아파치는 접속마다 프로세스 또는 쓰레드를 생성하는 구조이다.\n동시 접속 요청이 많을수록 그만큰 생성 비용이 들고 대용양 요청을 처리할 수 있는 웹 서버로서 한계가 나타난다.\nNginx 한개 또는 고정된 프로세스만 생성하고, 프로세스 내부에서 비동기 방식으로 작업을 처리한다. 따라서 동시 접속 요청이 많아도 프로세스 또는 쓰레드 생성 비용이 존재하지 않는다.\nEvent-Driven 방식 Event-Driven 방식은 Reactor pattern을 사용한다.\nReactor는 이벤트가 들어오면 알맞는 handler로 dispatch 해준다. Handler는 dispatch된 이벤트를 받아서 처리하는 역할을 수행 Reactor pattern\n이벤트 처리(event handling)디자인 패턴으로 하나의 Reactor가 계속 이벤트를 찾고 이벤트가 발생(trigger)하면 해당 이벤트 처리기(event handler)에게 알린다.\nNginx와 Apache의 차이점 컨텐츠의처리 방식 정적 컨텐츠 처리\nApache: 전통적인 파일기반 방식의 정적 컨텐츠 Nginx: 이벤트 처리/비동기식/논블로킹 방식 처리로 인해 정적 컨텐츠 제공시 고속 처리 가능 동적 컨텐츠 처리\nApache: 서버 내에서 처리 기본적으로 유연성과 범용성을 갖추는 방식으로 서버 자체에서 동적 컨텐츠 처리가 가능하다. Nginx: 동적 컨텐츠를 처리하지 않음 동적 웹 페이지 컨텐츠를 가진 모든 요청을 위해 외부 자원과 연계한다. 따라서 최종적으로 동적 컨텐츠가 다시 돌아올 때까지 기다렸다가 클라이언트에게 전달하는 방식을 가지고 있다. OS 지원에 대한 범용성 Apache: 리눅스, BSD, UNIX, WINDOW 역사가 있는 만큼 지원 범위가 다양하기 때문에 일관성 있는 웹 서비스 아키텍쳐를 구현할 수 있다. Nginx: LINUX, BSD, UNIX, WIN(부분 지원) 다양한 운영체제를 지원하지만 아파치 만큼 완벽히 지원하지 않는다. 분산/중앙집중식 구성 방식 Apache: 분산/중앙집중식 구성 채택 .htaccess를 통해 디렉토리별로 추가 구성을 할 수 있다. 단일 기반 뿐만 아니라 분산형 구칙이 가능하므로 대용량 서버 아키텍쳐에서 자원만 충분하다면 여러 웹 서비스를 구현 할 수 있다. Nginx: 중앙집중식 구성 채택 아파치처럼 .htaccess를 지원하지 않는다. 따라서 추가 구성을 할 수 없는 단점이 있다. 하지만 이러한 방식은 가상화, 클라우드, MSA와 같은 아키텍쳐에서는 오히려 경량화와 성능 보장이라는 측면에서 단점이 되지 않을 수도 있다. 모듈 및 확장성/보안 Apache 60개 이상의 다양한 기능과 모듈을 지원하며, 필요에 따라 활성화 또는 비활성 시킬 수 있다. 동적 모듈을 통해 웹 서버의 사용자 지정도 가능하게 할 수 있는 등 다양한 디자인과 확장이 가능하다. 보안을 위해 다양한 Web기반 DDoS 방어 기술을 제공한다. Nginx 다른 코어 모듈을 동적으로 로딩할 수 없도록 되어있다. 옵션을 최소화 해서 태생 부터 성능에 포커싱 했다. 보안에 대한 다양한 기술 문서를 제공하며, 코드 자체가 가볍고 경량화 되어 있어서 보안에 유리한 측면도 있다.https://youngmind.tistory.com/entry/Apache-vs-Nginx ","date":"2023-04-17T19:10:21+09:00","image":"https://codemario318.github.io/post/nginx_vs_apache/web_cover_hu71ff0ea2f7ce80fa0f2ad0c2fcb44a04_52909_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/nginx_vs_apache/","title":"Nginx VS Apache"},{"content":"웹 동작 방식 클라이언트(브라우저)가 HTTP(URL)을 통해 요청을 보내면 HTML, CSS, 이미지와 같은 정적 콘텐츠를 응답으로 보내게 되고 그것을 받은 클라이언트가 해석하여 페이지로 보여준다.\nStatic pages와 Dynamic Pages Static Pages Web Server는 파일 경로의 이름을 받아 경로와 일치하는 file contents를 반환 항상 동일한 페이지를 반환 Ex) image, html, css, javascript 파일과 같이 컴퓨터에 저장되어 있는 파일들 Dynamic Pages 인자의 내용에 맞게 동적인 Contents를 반환 웹 서버에 의해서 실행되는 프로그램을 통해서 만들어진 결과물(WAS)위에서 돌아가는 프로그램 Web Server와 WAS의 차이 Web Server 소프트웨어와 하드웨어로 구분된다.\n하드웨어 Web 서버가 설치되어 있는 컴퓨터 소프트웨어 웹 브라우저 클라이언트로 부터 HTTP 요청을 받아 정적인 컨텐츠(.html, .jpeg, .css 등)을 제공하는 컴퓨터 프로그램 Web Server의 역할 HTTP 프로토콜을 기반으로 하여 클라이언트의 요청을 서비스 하는 기능 담당\n요청에 따라 두 가지 기능 중 적절하게 선택하여 수행\n정적인 컨텐츠 제공 WAS를 거치지 않고 바로 자원을 제공한다 동적인 컨텐츠 제공을 위한 요청 전달 클라이언트의 요청을 WAS에 보내고, WAS가 처리한 결과를 클라이언트에게 전달한다. Web Server의 예 Apache Server Nginx IIS 등 WAS(Web Application Server) DB 조회나 다양한 로직 처리를 요구하는 동적인 컨텐츠를 제공하기위해 만들어진 Application Server\nHTTP를 통해 컴퓨터나 장치에 어플리케이션을 수행해주는 미들웨어이다.\n웹 컨테이너(Web Container), 서블릿 컨테이너(Servlet Container)라고도 불림\nWAS의 역할 WAS = Web Server + Web Container\n웹서버 기능들을 구조적으로 분리하여 처리하고자하는 목적으로 제시됨\n분산 트랜잭션 보안 메시징 쓰레드 처리 등 DB와 서버와 같이 수행됨\nWAS의 주요 기능 프로그램 실행 환경과 DB 접속 기능 제공 여러 개의 트랜잭션 관리 기능 업무를 처리하는 비지니스 로직 수행 WAS가 필요한 이유 웹 페이지는 정적 컨텐츠와 동적 컨텐츠가 모두 존재한다.\n사용자의 요청에 맞게 적절한 동적 컨텐츠를 만들어서 제공해야 한다. 웹 서버만을 이용하게 되면 그에 맞는 결과가 정적 파일로 존재해야 한다. 따라서 WAS를 통해 요청에 맞는 데이터를 DB에서 가져와 비즈니스 로직에 맞게 결과를 만들어 제공함으로 자원을 효율적으로 사용할 수 있다.\nWAS와 Web Server를 분리하는 이유 기능 분리를 통한 서버 부하 방지\nWAS만으로도 웹서비스를 제공 가능하지만 WAS는 DB조회 등 동적인 웹 페이지를 위한 다양한 동작을 하기 때문에 바쁘다. 따라서 웹 서버를 통해 정적인 컨텐츠를 제공하여 부하를 방지한다.\n물리적으로 분리하여 보안 강화\nSSL에 대한 암복호화 처리에 웹서버를 사용한다.\n여러대의 WAS를 연결 가능\nLoad Balancing을 위해 Web Server를 사용 가능하다\nfail over(장애 극복), fail back 처리에 유리 여러대의 서버를 사용하는 대용량 웹 어플리케이션의 경우 웹 서버와 WAS를 분리하여 무중단 운영을 위한 장애 극복에 쉽게 대응할 수 있다. 여러 웹 어플리케이션 서비스 가능\n하나의 웹 서버로 다양한 WAS를 이용하게 만들 수 있다. ","date":"2023-04-17T18:28:10+09:00","image":"https://codemario318.github.io/post/web/web_cover_hu71ff0ea2f7ce80fa0f2ad0c2fcb44a04_52909_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/web/","title":"Web"},{"content":"클로저는 두개의 함수로 만들어진 환경으로 이루어진 특별한 객체의 한 종류이다. 여기서 환경이라 함은 클로저가 생성될 때 그 범위에 있던 여러 지역 변수들이 포함 된 context를 말한다. 이러한 범위는 자바스크립트 코드를 실행하기 위해 발생하는 컴파일 단계에서 결정된다.\n클로저를 통해서 자바스크립트에는 없는 비공개 속성/메소드, 공개 속성/메소드 처럼 구현 할 수 있다.\nLexical Scope 자바스크립트 코드를 실행할 때 컴파일 단계에서 몇가지 일이 일어난다. 그중 하나인 토크나이징과 렉싱이 있다.\n토크나이징 문자열을 나누어 토큰으로 만드는 과정\n1 var num = 5; 위와 같은 구문을 만나게 되면, 아래와 같은 토큰으로 나눈다.\n1 2 3 4 5 var num = 5 ; 렉스타임 토크나이징의 결과물인 토큰을 분석하여 생성된 토큰에 의미를 부여하는 것을 렉싱이라고 하며, 이 과정을 렉스타임이라고 한다.\n렉시컬 스코프 개발자가 코드를 작성할때 변수를 어디에 작성하는가를 바탕으로 렉스타임에 토큰이 분석되며 스코프가 결정된다. 이때 구성된 유효 범위를 렉스컬 스코프라고 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 var a = 10; function foo() { var b = 20; function bar() { var c = 30; colsole.log(a + b + c); // 60 } bar(); } foo(); 위의 코드가 실행될때 스코프 버블은 bar 안쪽에서 부터 시작되어 올라간다.\n코드를 해석하는 과정에서 상위에서 하위로 쌓이는 구조로 해석되기 때문에, scope에 대한 검색은 기본적으로 하위에서 상위는 되지만 상위에서 하위로의 검색은 불가능하다.\n1 2 3 4 5 6 7 8 9 var a = 10; function foo () { var b = 20; console.log(a); // 10 console.log(b); // 20 } console.log(b); // error 정리 컴파일레이션의 렉싱 단계에서 모든 변수들이 어디서 어떻게 선언되었는지 바탕으로 실행 단계에서 스코프를 구성하고, 이렇게 구성되는 스코프가 렉시컬스코프이다.\n클로저 생성하기 내부 함수가 익명 함수로 되어 외부 함수의 반환값으로 사용된다. 내부 함수는 외부 함수의 실행 환경에서 실행된다. 내부 함수에서 사용되는 변수는 외부 함수의 변수 스코프에 있다. 1 2 3 4 5 6 7 8 9 10 11 12 function outer() { var name = `closure`; function inner() { console.log(name); } inner(); } outer(); // console\u0026gt; closure outer함수를 실행시키는 context에는 name이라는 변수가 존재하지 않지만, inner함수가 outer 함수 내부에 선언된 name을 참조하기 때문에, name 변수에 대한 정보를 알 수 없는 outer 변수 외부환경에서도 정상 출력된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 var name = `Warning`; function outer() { var name = `closure`; return function inner() { console.log(name); }; } var callFunc = outer(); callFunc(); // console\u0026gt; closure 위 코드에서 callFunc 를 클로저라고 한다. callFunc 호출에 의해 name 이라는 값이 console에 출력되는데, \u0026ldquo;Warning\u0026ldquo;이 아니라 \u0026ldquo;closure\u0026ldquo;이다. 즉, outer 함수의 context 에 속해있는 변수를 참조하는 것이다. 여기서 outer 함수의 지역변수로 존재하는 name 변수를 free variable(자유변수) 라고 한다.\n이처럼 외부 함수 호출이 종료 되더라도 외부 함수의 지역 변수 및 변수 스코프 객체의 체인 관계를 유지할 수 있는 구조를 클로저라고 한다.\n","date":"2023-04-17T18:11:36+09:00","image":"https://codemario318.github.io/post/js_closure/js_cover_huc1c9a52f33db3f5dad8bb16243cf7e4e_11875_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/js_closure/","title":"Javascript - Closure"},{"content":"Javascript에서 함수의 this 키워드는 다른 언어와 조금 다르게 동작한다. 또한 엄격 모드와 비엄격 모드에서도 일부 차이가 있다.\n대부분의 경우 this의 값은 함수를 호출한 방법에 의해 결정되는데, 실행중에는 할당으로 설정할 수 없고 함수를 호출할 때 마다 다를 수 있다.\nES5는 함수를 어떻게 호출했는지 상관하지 않고 this 값을 설정할 수 있는 bind 메서드를 도입했고, ES2015는 스스로의 this 바인딩을 제공하지 않는 화살표 함수를 추가했다.(렉시컬 컨텍스트안의 this값을 유지함)\n전역 문맥 전역 실행 문맥(global execution context)에서 this는 엄격 모드 여부에 관계 없이 전역 객체를 참조한다.\n1 2 3 4 5 6 7 8 9 // 웹 브라우저에서는 window 객체가 전역 객체 console.log(this === window); // true a = 37; console.log(window.a); // 37 this.b = \u0026#34;MDN\u0026#34;; console.log(window.b) // \u0026#34;MDN\u0026#34; console.log(b) // \u0026#34;MDN\u0026#34; 함수 문맥 함수 내부에서 this의 값은 함수를 호출한 방법에 의해 결정된다.\n단순 호출 엄격 모드가 아닐경우 this의 값이 호출에 의해 설정되지 않으므로, 기본값으로 브라우저에서는 window인 전역 객체를 참조하게 된다.\n1 2 3 4 5 6 7 8 9 function f1() { return this; } // 브라우저 f1() === window; // true // Node.js f1() === global; // true 반면에 엄격 모드에서 this 값은 실행 문맥에 진입하며 설정되는 값을 유지하므로 다음 예시에서 보여지는 것 처럼 this는 undefined로 남아있게 된다.\n1 2 3 4 5 6 function f2(){ \u0026#34;use strict\u0026#34;; // 엄격 모드 참고 return this; } f2() === undefined; // true f2를 객체의 메서드나 속성(예: window.f2())이 아닌 직접 호출했기 때문에 this는 undefined여야 하지만 브라우저에서 엄격 모드를 지원하지 않는다면 window 객체를 반환한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 var prop = 1; var test = { prop: 42, func: function() { var func2 = function() { console.log(this); } console.log(this); func2(); }, }; test.func(); /* \u0026gt; Object { prop: 42, func: function() { var func2 = function() { console.log(this); } console.log(this); func2(); } } \u0026gt; [object Window] // browser \u0026gt; [object global] // node.js */ function 키워드로 선언된 함수가 전역 실행 문맥(global execution context)에서 호출되었기 때문에 this는 엄격 모드 여부에 관계 없이 전역 객체를 참조한다.\nbind 메서드 ECMAScript5는 Function.prototype.bind를 도입했다. f.bind(someObject)를 호출하면 f와 같은 본문(코드)과 범위를 가졌지만 this는 원본 함수를 가진 새로운 함수를 생성한다. 새 함수의 this는 호출 방식과 상관없이 영구적으로bind()의 첫 번째 매개변수로 고정된다.\n1 2 3 4 5 6 7 8 9 10 11 12 function f() { return this.a; } var g = f.bind({a: \u0026#39;azerty\u0026#39;}); console.log(g()); // azerty var h = g.bind({a: \u0026#39;yoo\u0026#39;}); // bind는 한 번만 동작함! console.log(h()); // azerty var o = {a: 37, f: f, g: g, h: h}; console.log(o.a, o.f(), o.g(), o.h()); // 37, 37, azerty, azerty 화살표 함수 화살표 함수에서 this는 자신을 감싼 정적 범위(lexical context)이다. 전역 코드에서는 전역 객체를 가르킨다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 var globalObject = this; var foo = (() =\u0026gt; this); console.log(foo() === globalObject); // true // 객체로서 메서드 호출 var obj = {func: foo}; console.log(obj.func() === globalObject); // true // call을 사용한 this 설정 시도 console.log(foo.call(obj) === globalObject); // true // bind를 사용한 this 설정 시도 foo = foo.bind(obj); console.log(foo() === globalObject); // true 화살표 함수를 call(), bind(), apply()를 사용해 호출할 때 this의 값을 정해주더라도 무시한다. 사용할 매개변수를 정해주는 건 문제 없지만, 첫 번째 매개변수(thisArg)는 null을 지정해야 한다.\n어떤 방법을 사용하든 foo의 this는 생성 시점의 것으로 설정된다.(위 예시에서는 global 객체). 다른 함수 내에서 생성된 화살표 함수에도 동일하게 적용된다. this는 싸여진 렉시컬 컨텍스트로 유지된다.\n1 2 3 4 5 6 7 8 9 10 11 12 var obj = { bar: function() { var x = (() =\u0026gt; this); return x; } }; var fn = obj.bar(); console.log(fn() === obj); // true var fn2 = obj.bar; console.log(fn2()() === window); // true 화살표 함수의 범위는 선언될때 결정되는데, fn은 obj.bar()로 호출된 x를 활용하게 되어 this가 obj 를 의미하게 되고, fn2는 전역 실행 문맥에서 obj.bar 자체를 fn2에 할당 하였기 때문에 window로 설정되었다.\n객체의 메서드를 호출할 때 함수를 어떤 객체의 메서드로 호출하면 this의 값은 그 객체를 사용한다.\n다음 예제에서 o.f()를 실행할 때 o 객체가 함수 내부의 this와 연결된다.\n1 2 3 4 5 6 7 8 9 10 11 12 var o = {prop: 37}; function independent() { return this.prop; } o.f = independent; console.log(o.f()); // logs 37 o.b = {g: independent, prop: 42}; console.log(o.b.g()); // logs 42 this 바인딩은 멤버 대상에 영향을 받는다. 함수를 실행할 때, 객체 o.b의 메소드 g 로서 호출하면 함수 내부의 this는 o.b를 나타낸다.\n객체의 프로토타입 체인에서의 this 메서드가 어떤 객체의 프로토타입 체인 위에 존재한다면, this의 값은 그 객체가 메서드를 가진 것 처럼 설정된다.\n1 2 3 4 5 6 7 8 var o = { f:function() { return this.a + this.b; } }; var p = Object.create(o); p.a = 1; p.b = 4; console.log(p.f()); // 5 이 예제에서, f 속성을 가지고 있지 않은 변수 p가 할당된 객체는, 프로토타입으로 부터 상속받는다. 그러나 그것은 결국 o에서 f 이름을 가진 멤버를 찾는 되는 문제가 되지 않는다 ; p.f를 찾아 참조하기 시작하므로, 함수 내부의 this는 p 처럼 나타나는 객체 값을 취한다. 즉, f는 p의 메소드로서 호출된 이후로, this는 p를 나타낸다. 이것은 JavaScript의 프로토타입 상속의 흥미로운 기능이다.\n접근자와 설정자의 this 함수를 접근자와 설정자에서 호출하더라도 동일하다. 접근자나 설정자로 사용하는 함수의 this는 접근하거나 설정하는 속겅을 가진 객체로 묶인다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 function sum() { return this.a + this.b + this.c; } var o = { a: 1, b: 2, c: 3, get average() { return (this.a + this.b + this.c) / 3; } }; Object.defineProperty(o, \u0026#39;sum\u0026#39;, { get: sum, enumerable: true, configurable: true}); console.log(o.average, o.sum); // 2, 6 생성자로서 함수를 new 키워드와 함께 생성자로 사용하면 this는 새로 생긱 객체에 묶인다.\n","date":"2023-04-17T18:11:36+09:00","image":"https://codemario318.github.io/post/js_this/js_cover_huc1c9a52f33db3f5dad8bb16243cf7e4e_11875_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/js_this/","title":"Javascript - this"},{"content":"hoist라는 단어의 사전적 정의는 \u0026ldquo;끌어올리기\u0026rdquo; 라는 뜻이다. 자바스크립트가 실행될때 선언된 모든 변수 선언부가 코드의 가장 위로 끌어올려진 것 처럼 처리된다.\n호이스트를 통해 변수의 정의가 그 범위에 따라 선언과 할당으로 분리된다. 즉, 변수가 함수 내에서 정의되었을 경우, 선언이 함수 최상위로, 함수 바깥에서 정의되었을 경우, 전역 컨텍스트의 최상위로 변경이 된다.\n1 2 3 4 5 6 7 8 9 function getX() { console.log(x); // undefined var x = 100; console.log(x); // 100 } getX(); 위와 같이 정의 되었을때, 아래와 같이 해석된다.\n1 2 3 4 5 6 7 8 9 10 11 function getX() { var x; console.log(x); x = 100; console.log(x); } getX(); 선언문은 항상 자바스크립트 엔진 구동시 가장 최우선으로 해석하므로 호이스팅 되고, 할당 구문은 런타임 과정에서 이루어지기 때문에 호이스팅 되지 않는다.\n함수의 호이스팅 함수가 자신이 위치한 코드에 상관없이 함수 선언문 형태로 정의한 함수의 유효범위는 전체 코드의 맨 처음부터 시작한다. 함수 선언이 함수 실행 부분보다 뒤에 있더라도 자바스크립트 엔진이 함수 선언을 끌어올리는 것을 의미한다.\n1 2 3 4 5 6 7 foo(); function foo(){ console.log(‘hello’); }; // console\u0026gt; hello foo 함수에 대한 선언을 호이스팅하여 global 객체에 등록시키기 때문에 hello가 제대로 출력된다.\n오류 사례 1 2 3 4 5 6 7 foo(); var foo = function() { console.log(‘hello’); }; // console\u0026gt; Uncaught TypeError: foo is not a function 예제의 함수 표현은 함수 리터럴을 할당하는 구조이기 때문에 호이스팅 되지 않으며 그렇기 때문에 아래와 같이 해석되어 런타임 환경에서 Type Error를 발생시킨다.\n1 2 3 4 5 6 7 8 var foo; foo(); // foo = undefined // console\u0026gt; Uncaught TypeError: foo is not a function foo = function( ) { console.log(‘hello’); }; ","date":"2023-04-17T18:04:35+09:00","image":"https://codemario318.github.io/post/js_hoisting/js_cover_huc1c9a52f33db3f5dad8bb16243cf7e4e_11875_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/js_hoisting/","title":"Javascript - Hoisting"},{"content":"컨테이너는 애플리케이션, 실행 라이브러리, 시스템 도구, 시스템 라이브러리 등을 포함하여 애플리케이션과 그 애플리케이션을 실행하는 환경을 패키징하여 이식성이 뛰어난 소프트웨어 패키지로 만든것이다.\n컨테이너화를 통해 더욱 신속하게 작업을 진행할 수 있다. 효율적으로 소프트웨어를 배포할 수 있다. 매우 높은 수준의 확장성을 확보할 수 있다. 서버 가상화 컴퓨터의 성능을 효율적으로 사용하기 위해 가상화 기술이 등장하였다. 서버 관리자 입장에서 리소스 사용률이 적은 서버들은 낭비라고 생각할 수 있다. 그렇다고 모든 서비스를 한 서버 안에 올린다면 안정성에 문제가 생길 수 있다. 이에 따라 안정성을 높이며 리소스를 최대한 활용할 수 있는 방법으로 고안된 방법이 서버 가상화이다.\n컨테이너 정의 컨테이너는 소프트웨어 서비스를 실행하는 데 필요한 특정 버전의 프로그래밍 언어 런타임 및 라이브러리와 같은 종속 항목과 애플리케이션 코드를 함께 포함하는 경량 패키지이다.\n컨테이너는 운영체제 수준에서 CPU, 메모리, 스토리지, 네트워크 리소스를 쉽게 공유할 수 있게 해주며 컨테이너가 실제로 실행되는 환경에서 애플리케이션을 추상화 할 수 있는 논리 패키징 매커니즘을 제공한다.\n컨테이너의 이점 책임 분리 컨테이너화를 통해 책임을 깔끔하게 분리할 수 있다.\n개발자는 애플리케이션의 로직과 종속 항목에 집중하고, IT 운영팀은 특정 소프트웨어 버전 및 구성과 같은 애플리케이션의 세부 요소 대신 배포 및 관리에 집중할 수 있다.\n워크로드 이동성 컨테이너는 Linux, Windows, Mac 등 운영체제를 가리지 않고, 가상머신 물리적 서버, 개발자 컴퓨터, 데이터 센터, 온프레미스 환경, 퍼블릭 클라우드 등 사실상 어느 환경에서나 구동되므로 개발 및 배포가 크게 쉬워진다.\n애플리케이션 격리 컨테이너는 운영체제 수준에서 CPU, 메모리, 스토리지, 네트워크 리소스를 가상화 하므로 개발자에게 다른 애플리케이션으로부터 논리적으로 격리된 OS 환경을 제공한다.\n컨테이너와 VM의 차이 VM은 기본 하드웨어에 대한 엑세스 권한을 갖는 호스트 운영체제 위에서 Linux또는 Windows 같은 게스트 운영체제를 실행하기 때문에 컨테이너와 비교되는 경우가 많다.\n컨테이너는 가상 머신과 마찬가지로 애필리케이션을 관련 라이브러리 및 종속 항목과 함께 패키지로 묶어 소프트웨어 서비스 구동을 위한 격리 환경을 마련해준다. 그러나 컨테이너를 사용하면 훨씬 작은 단위로 업무를 수행할 수 있어 이점이 훨씬 많다.\nVM 보다 훨씬 더 가볍다. OS 수준에서 가상화되고, VM은 하드웨어 수준에서 가상화된다. OS 커널을 공유하며 VM에서 필요한 것 보다 훨씬 적은 메모리를 사용한다. 컨테이너의 용도 애플리케이션을 실제 구동 환경으로부터 추상화할 수 있는 논리 패키징 메커니즘을 제공함. 이러한 분리를 통해 어떤 환경에서도 컨테이너 기반 애플리케이션을 쉽게 지속적으로 배포할 수 있음.\n민첩한 개발 컨테이너를 사용하면 개발자가 종속 항목과 환경에 미치는 영향을 신경쓰지 않고 훨씬 더 빠르게 개발을 진행할 수 있다.\n효율적인 운영 컨테이너는 경량이며 필요한 컴퓨팅 리소스만 사용하면 된다. 따라서 애플리케이션을 효율적으로 구동할 수 있다.\n폭넓은 구동 환경 컨테이너는 거의 모든 곳에서 구동할 수 있어 환경에 영향 없이 사용할 수 있다.\n컨테이너 기술 Namespaces VM에서는 각 게스트 머신별로 독립적인 공간을 제공하고 서로가 충돌하지 않도록 하는 기능을 갖추고 있다.\n리눅스에서는 이와 동일한 역할을 하는 namespaces 기능을 커널에 내장하고 있다.\nmnt(파일시스템 마운트): 호스트 파일 시스템에 구애받지 않고 독립적으로 파일 시스템을 마운트하거나 언마운트 가능 pid(프로세스): 독립적은 프로세스 공간을 할당 net(네트워크): namespace간 network 충돌 방지(중복 포트 바인딩 등) ipc(SystemV IPC): 프로세스간의 독립적인 통신통로 할당 uts(hostname): 독립적인 hostname 할당 user(UID): 독립적인 사용자 할당 namespaces를 지원하는 리눅스 커널을 사용하고 있다면 다음 명령어를 통해 바로 namespace를 만들어 실행할 수 있다.\n1 $ sudo unshare --fork --pid --mount-proc bash 1 2 3 4 # ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 4.0 0.0 17656 6924 pts/9 S 22:06 0:00 bash root 2 0.0 0.0 30408 1504 pts/9 R+ 22:06 0:00 ps aux PID namespace에 실행한 bash가 PID 1로 할당되어 있고, 바로 다음 실행한 ps aux 명령어가 PID 2를 배정 받았다.\ncgroups - Control Groups cgrups는 자원(resources)에 대한 제어를 가능하게 해주는 리눅스 커널 기능으로 아래와 같은 자원들을 제어할 수 있다.\n메모리 CPU I/O 네트워크 device 노드 (/dev/) 1 2 3 // \u0026#34;dhlee\u0026#34; 유저가 소유하하며, // 메모리를 제어할 그룹 \u0026#34;testgrp\u0026#34; 생성 $ sudo cgcreate -a dhlee -g memory:testgrp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 $ ls -alh /sys/fs/cgroup/memory/testgrp 합계 0 drwxr-xr-x 2 ssut root 0 8월 8 23:19 . dr-xr-xr-x 8 root root 0 7월 7 15:30 .. -rw-r--r-- 1 ssut root 0 8월 8 23:19 cgroup.clone_children --w--w--w- 1 ssut root 0 8월 8 23:19 cgroup.event_control -rw-r--r-- 1 ssut root 0 8월 8 23:19 cgroup.procs -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.failcnt --w------- 1 ssut root 0 8월 8 23:19 memory.force_empty -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.failcnt -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.limit_in_bytes -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.max_usage_in_bytes -r--r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.slabinfo -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.tcp.failcnt -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.tcp.limit_in_bytes -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.tcp.max_usage_in_bytes -r--r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.tcp.usage_in_bytes -r--r--r-- 1 ssut root 0 8월 8 23:19 memory.kmem.usage_in_bytes -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.limit_in_bytes -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.max_usage_in_bytes -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.move_charge_at_immigrate -r--r--r-- 1 ssut root 0 8월 8 23:19 memory.numa_stat -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.oom_control ---------- 1 ssut root 0 8월 8 23:19 memory.pressure_level -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.soft_limit_in_bytes -r--r--r-- 1 ssut root 0 8월 8 23:19 memory.stat -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.swappiness -r--r--r-- 1 ssut root 0 8월 8 23:19 memory.usage_in_bytes -rw-r--r-- 1 ssut root 0 8월 8 23:19 memory.use_hierarchy -rw-r--r-- 1 ssut root 0 8월 8 23:19 notify_on_release -rw-r--r-- 1 root root 0 8월 8 23:19 tasks /sys/fs/cgroup/*/groupname 경로에 있는 파일을 통해 그룹의 여러 옵션들을 변경 가능\n1 2 // testgrp 최대 메모리 사용량을 2MB로 제한 $ echo 2000000 \u0026gt; /sys/fs/cgroup/memory/testgrp/memory.kmem.limit_in_bytes 정리 LXC, LibContainer, runC 등은 위에서 설명한 cgrups, namespaces를 표준으로 정의해둔 OCI(Open Container Initative) 스펙을 구현한 컨테이너 기술의 구현체이다.\nLXC는 캐노니컬(Canonical)이 지원하고 있는 리눅스 컨테이너 프로젝트로 Docker의 경우 1.8 이전 버전까지 LXC를 이용해 구현해서 사용되었다.\n이후 Docker는 libcontainer → runC(libcontainer의 리팩토링 구현체)로 자체 구현체를 갖게 되었다.\nDocker 도커는 1.11버전부터 위와 같은 구조로 작동한다.\ncontainerd는 OCI 구현체(주로 runC)를 이용해 container를 관리해주는 deamon이다.\n도커 엔진 자체는 이미지, 네트워크, 디스크 등의 리소스 관리 역할을 수행하며, 여기서 도커 엔진과 containerd 각각이 완전히 분리되어 도커 엔진을 재시작 해도, 컨테이너 재시작 없이 사용할 수 있다.\n위와 같이 각각 역할이 분리됨에 따라 도커는 4개의 독립적인 프로세스로 작동하고 있다. (docker, docker-containerd, docker-containerd-shim, docker-runc)\n","date":"2023-04-15T16:30:25+09:00","image":"https://codemario318.github.io/post/docker_0/docker_cover_hue12353db563619e41ee3a11307d3cf25_62602_120x120_fill_box_smart1_3.png","permalink":"https://codemario318.github.io/post/docker_0/","title":"0. 컨테이너와 도커"},{"content":"MySQL 서버는 서버의 상태를 진단할 수 있는 많은 도구들을 지원하지만 이러한 기능은 많은 지식을 필요로 하는 경우가 많다. 로그 파일을 활용하면 MySQL 서버에 대해 깊은 지식이 없어도 서버의 상태나 부하를 일으키는 원인을 쉽게 찾아 해결할 수 있다.\n에러 로그 파일 MySQL이 실행되는 도중 발생하는 에러나 경고 메시지가 출력되는 로그 파일이다. MySQL 설정 파일(my.cnf)에서 log_error라는 이름의 파라미터로 정의된 경로에 생성되는데 설정 파일에 별도로 정의도지 않은 경우 데이터 디렉터리(datadir 파라미터에 설정된 디렉터리)에 .err라는 확장자가 붙은 파일로 생성된다.\nMySQL이 시작하는 과정과 곤련된 정보성 및 에러 메시지 MySQL 설정 파일을 변경하거나 데이터베이스가 비정상적으로 종료된 후 다시 시작하는 경우에는 반드시 MySQL 에러 로그 파일을 통해 설정된 변수의 이름이나 값이 명확하게 설정되고 의도한 대로 적용됐는지 확인해야 한다.\nMySQL 서버가 정상적으로 기동했고(mysqld: ready for commections 메시지 확인), 새로 변경하거나 추가된 파라미터에 대한 에러나 경고성 메시지가 없다면 정상적으로 적용된 것으로 판단하면 된다.\n무시 ignore: 서버는 정상적으로 기동하지만 해당 파라미터가 적용안됨. 실패: 에러 메시지를 출력하고 시작하지 못했다는 메시지가 노출됨 마지막으로 종료할 때 비정상적으로 종료된 경우 나타나는 트랜잭션 복구 메시지 InnoDB의 경우 MySQL 서버가 비정상적으로 종료됐다면 다시 시작되면서 완료하지 못한 트랜잭션을 정리하고 디스크에 기록되지 못한 데이터가 있다면 다시 기록하는데, 이 과정에서 간단한 메시지가 출력된다.\n복구가 불가능한 경우 해당 에러메시지를 출력하고 MySQL은 다시 종료되며, 이 단계에서 발생하는 문제는 해결하기 어려운 문제점 일 때가 많고, innodb_force_recovery 파라미터를 설정하여 재시작 해야 할 수 있다.\n쿼리 처리 도중에 발생하는 문제에 대한 에러 메시지 쿼리 도중 발생하는 문제점은 사전 예방이 어려우며, 에러 로그 파일을 검토하는 과정에서 알게 된다.\n쿼리 실행 도중 발생한 에러나 복제에서 문제가 될 만한 쿼리에 대한 경고 메시지가 에러 로그에 기록되므로 자주 검토하는 것이 숨겨진 문제점을 해결하는데 많은 도움이 될 수 있다.\n비정상적으로 종료된 커넥션 메시지(Aborted connection) 클라이언트 애플리케이션에서 정상적으로 접속 종료를 하지 못하고 프로그램이 종료된 경우 MySQL 서버의 에러 로그 파일에 이런 내용이 기록된다. (네트워크 문제로 인한 겨우 포함)\n이런 메시지가 아주 많이 기록된다면 애플리케이션의 커넥션 종료 로직을 한번 검토해볼 필요가 있다.\nmax_connect_errors 시스템 변숫값이 너무 낮게 설정된 경우 클라이언트 프로그램이 MySQL 서버에 접속하지 못하고 Host 'host_name' is blocked라는 에러가 발생 가능하며, 이러한 경우는 에러가 어떻게 발생하게 됐는지 원인을 확인하고, 문제가 없다면 해당 시스템 변수의 값을 증가시키면 해결된다.\nInnoDB의 모니터링 또는 상태 조회 명령의 결과 메시지 InnoDB 테이블 모니터링이나 락 모니터링, 또는 엔진 상태를 조회하는 명령은 상대적으로 큰 메시지를 로그 파일에 기록한다.\nInnoDB의 모니터링을 활성화 상태로 유지하는 경우에는 에러 로그 파일이 매우 커져서 파일 시스템의 공간을 많이 사용할 수 있으므로, 다시 비활성화하여 에러 로그 파일이 커지지 않게 만들어야 한다.\nMySQL의 종료 메시지 가끔 MySQL이 아무도 모르게 종료돼 있거나 재시작 되는 경우가 있는데, 이러한 경우 에러 로그 파일에서 MySQL이 마지막으로 종료되면서 출력한 메시지를 확는 것이 서버가 종료된 이유를 확인하는 유일한 방법이다.\nReceived SHOUTDOWN from user ... 메시지: 특정 유저가 종료한 경우 없거나 스택 트레이스(16진수 주소값이 잔뜩 출력되는) 메시지: 세그멘테이션 폴트로 비정상 종료된 경우 세그멘테이션 폴트로 종료된 경우 스택 트레이스 내용을 최대한 참조하여 MySQL의 버그와 연관이 있는지 조사 후 버전을 업그레이드 하거나 회피책(WorkAround)를 찾는다. MySQL \u0026ldquo;The Error Log\u0026quot;절을 확인한다. 제너럴 쿼리 로그 파일 MySQL 서버에서 실행되는 쿼리 목륵을 검토하고 싶다면, 쿼리 로그를 활성화하여 실행 쿼리를 쿼리 로그 파일로 기록하게 한 다음, 해당 파일을 검토한다.\n슬로우 쿼리 로그와는 다르게 제너럴 로그는 실행되기 전에 MySQL이 요청을 받으면 바로 기록하기 때문에 쿼리 실행 중에 에러가 발생해도 일단 로그 파일에 기록된다.\n쿼리 로그 파일의 경로는 general_log_file 파라미터에 설정되있으며, 쿼리 로그를 파일이 아닌 테이블에 저장하도록 설정할 수 있으므로 이 경우에는 테이블을 SQL로 조회해 검토해야 한다.\n1 SHOW GLOBAL VARIABLES LIKE \u0026#39;general_log_file\u0026#39;; 슬로우 쿼리 로그 서비스 운영 중에 MySQL 서버의 전체적인 성능 저하를 검사하거나 정기적인 점검을 위한 튜닝이 필요할 때 어떤 쿼리가 문제인지를 판단하는데 많은 도움을 준다.\nlong_query_time: 해당 시스템 변수에 설정한 시간 이상의 시간이 소요된 쿼리가 모두 기록된다. log_output: 슬로우 커리 로그를 파일 또는 테이블에 기록할지 설정할 수 있다. TABLE: mysql DB의 테이블에 제너럴로그나 슬로우 쿼리 로그를 테이블(slow_log, general_log)에 저장한다. FILE: 로그 내용을 디스크의 파일로 저장한다. 로그 파일의 분석이 완료되면 그 결과는 3개의 그룹으로 나뉘어 저장된다.\n슬로우 쿼리 통계 분석 결과의 최상단에 표시되며, 모든 쿼리를 대상으로 슬로우 쿼리 로그의 실행 시간(Exec time), 잠금 대기 시간(Lock time) 등에 대해 평균 및 최소/최대 값을 표시한다.\n실행 빈도 및 누적 실행 시간순 랭킹 각 쿼리별로 응답 시간과 실행 횟수를 보여주는데, pt-query-digest 명령 실행 시 --oorder-by옵션으로 정렬 순서를 변경할 수 있다.\nQuery ID는 실행된 쿼리 문장을 정규화(쿼리에 사용된 리터럴을 제거)해서 만들어진 해시 값을 의미하는데, 같은 모양의 쿼리라면 동일한 Query ID를 가지게 된다.\n쿼리별 실행 횟수 및 누적 실행 시간 상세 정보 Query ID별 쿼리를 쿼리 랭킹에 표시된 순서대로 자세한 내용을 보여준다. 랭킹별 쿼리에서는 대상 테이블에 대해 어떤 쿼리인지만을 표시하는데, 실제 상세한 쿼리 내용은 개별 쿼리의 정보를 확인해보면 된다.\n","date":"2023-04-14T16:31:23+09:00","image":"https://codemario318.github.io/post/real_mysql_4_4/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_4_4/","title":"4.4 MySQL 로그 파일"},{"content":"\n키 캐시 키 버퍼라고도 불리는 MyISAM의 키 캐시는 InnoDB의 버퍼풀과 비슷한 역할을 수행한다. 키 캐시는 인덱스만을 대상으로 작동하며, 인덱스의 디스크 쓰기 작업에 대해서만 부분적으로 버퍼링 역할을 한다.\n키 캐시 효율성 수식\n키 캐시 히트율 = 100 - (Key_reads / Key_read_requests * 100)\nKey_reads: 인덱스를 디스크에서 읽어들인 횟수를 저장하는 상태 변수 Key_read_requests: 키 캐시로부터 인덱스를 읽은 횟수를 저장하는 상태변수 1 SHOW GLOBAL STATUS LIKE \u0026#39;Key%\u0026#39;; 메뉴얼에서는 키 캐시를 이용한 쿼리의 비율(Hit rate)을 99% 로 유지할 것을 권장하며, 99% 미만이라면 키 캐시를 조금 더 크게 설정하는 것이 좋다.\n32비트 운영체제에서는 하나의 키 캐시에 4GB 이상 메모리 공간을 설정할 수 없고, 64비트 운영체제에서는 OS_PER_PROCESS_LIMIT 값에 설정된 크기만큼의 메모리를 할당할 수 있다.\n제한 값 이상의 키 캐시를 할당하고 싶다면 기본 키 캐시 이외에 별도 명명된 키 캐시 공간을 설정해야 하며, 기본 키 캐시 공간을 설정하는 파라미터는 key_buffer_size다.\n1 2 3 key_buffer_size = 4GB kbuf_board.key_buffer_size = 2GB kbuf_comment.key_buffer_size = 2GB 위 같이 설정하면 기본 키 캐시 4GB와 kbuf_board, kbuf_comment라는 이름의 키 캐시가 각각 2GB씩 생성된다.\n기본 키 캐시 영역외 키 캐시 영역은 아무런 설정이 없다면 할당만 해두고 사용하지 않아 추가된 키 캐시는 어떤 인덱스를 캐시할지 MySQL에 알려야 한다.\n1 2 CACHE INDEX db1.board, db2.board IN kbuf_board; CACHE INDEX db1.comment, db2.comment IN kbuf_comment; 운영체제의 캐시 및 버퍼 MyISAM 테이블의 인덱스는 키 캐시를 이용해 디스크를 검색하지 않고도 빠르게 검색될 수 있으나, 디스크 데이터 I/O를 성능을 위한 캐시나 버퍼링 기능은 없다. 따라서 MyISAM 테이블의 데이터 읽기나 쓰기 작업은 항상 운영체제의 디스크 읽기 또는 쓰기 작업으로 요청된다.\n운영체제의 캐시 공간은 남는 메모리를 사용하는 것이 기본 원칙이어서 남는 공간이 없다면 MyISAM 스토리지 엔진 데이터 I/O에 사용될 메모리를 확보할 수 없어 느려진다.\nMyISAM이 주로 사용되는 MySQL에서 일반적으로 키 캐시는 최대 물리 메모리의 40% 이상을 넘지 않게 설정하고, 나머지 메모리 공간은 운영체제가 자체적은 파일 시스템을 위한 캐시 공간을 마련할 수 있게 해주는 것이 좋다.\n데이터 파일과 프라이머리 키(인덱스) 구조 InnoDB 스토리지 엔진을 사용하는 테이블은 프라이머리 키에 의해서 클러스터링되어 저장되지만, MyISAM 테이블은 프라이머리 키에 의한 클러스터링 없이 데이터 파일이 Heap 공간처럼 활용된다. (프라이머리 키 값과 무관하게 INSERT되는 순서대로 데이터 파일에 저장된다.)\nMyISAM 테이블에 저장되는 레크드는 모두 ROWID라는 물리적인 주소값을 가지는데, 프라이머리 키와 세컨더리 인덱스는 모두 데이터 파일에 저장된 레코드의 ROWID 값을 포인터로 가진다.\nROWID는 두가지 방법으로 저장된다.\n고정 길이 ROWID\n자주 사용되지는 않지만 MyISAM 테이블을 생성할 때 MAX_ROWS 옵션을 사용해 명시하면 MySQL 서버는 쵀대로 가질 수 있는 레코드가 한정된 테이블을 생성한다. 레코드 개수가 한정되면 MyISAM 테이블은 ROWID값으로 4바이트 정수를 사용하여 INSERT된 순번으로 ROWID를 사용한다.\n가변 길이 ROWID\nMAX_ROWS 옵션을 사용하지 않으면 MyISAM 테이블의 ROWID는 최대 myisam_data_pointer_size 시스템 변수에 설정된 바이트 수 만큼에 공간을 사용할 수 있다. 기본값은 7이며 최소 2byte 부터 7byte 까지 가변적인 ROWID를 갖게 된다. 첫 바이트는 ROWID의 길이를 저장하는 용도로 사용되고 나머지 공간은 실제 ROWID를 저장하는데 사용한다. 가변적인 ROWID를 가지면 데이터 파일에서 레코드의 위치가 ROWID로 사용되어, 가변 길이 ROWID인 테이블일때 최대 크기 256TB(2**(8(7-1)))가질 수 있다.\n","date":"2023-04-14T15:52:19+09:00","image":"https://codemario318.github.io/post/real_mysql_4_3/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_4_3/","title":"4.3 MyISAM 스토리지 엔진 아키텍처"},{"content":"Double Write Buffer InnoDB 스토리지 엔진의 리두 로그는 공간 낭비를 막기 위해 페이지의 변경된 내용만 기록한다. 이로인해 더티 페이지를 디스크 파일로 플러시할 때 일부만 기록되는 문제가 발생하면 그 페이지의 내용은 복구할 수 없을 가능성이 있다.\n이런 현상을 파셜 페이지 또는 톤 페이지라고 하는데, 하드웨어의 오작동이나 시스템의 비정상 종료 등으로 발생할 수 있다.\n파셜 페이지(Partial-Page)\n데이터 베이스 페이지 중에서 데이터가 일부만 채워진 페이지. 레코드가 페이지의 크기보다 작을 때 발생하며, 레코드가 페이지를 벗어나지 않은 상태에서 페이지의 일부만 사용하게 된다. 톤 페이지(Tone-Page)\n디스크에 기록 중인 페이지의 기록 작업이 중간에 중단되어 발생하는 문제. 페이지 일부가 디스크에 기록되지 않아 데이터 무결성이 손상되는 문제를 일으킬 수 있다. InnoDB 스토리지 엔진은 이러한 문제를 막기 위해 Double-Write 기법을 활용한다.\nInnoDB 스토리지 엔진은 실제 데이터 파일에 변경 내용을 기록하기 전에 기록될 더티 페이지들을 묶어 시스템 테이블 스페이스의 DoubleWrite 버퍼에 기록한다.\n더티 페이지 플러싱 중 오류등으로 서버가 종료되었다면 InnoDB 스토리지 엔진은 재시작 될 때 항상 Double Write 버퍼의 내용을 데이터 파일의 페이지로 복사하게 된다.\nDoubleWrite 기능을 사용할지 여부는 Innodb_doublewrite 시스템 변수로 제어할 수 있다.\n이처럼 DoubleWrite 버퍼는 데이터의 안정성을 위해 사용되는데, HDD처럼 자기 원판이 회전하는 저장 시스템에서는 한 번의 순차 디스크 쓰기를 하는 것으로 부담스럽지 않지만 SSD처럼 랜덤 IO와 순차 IO의 비용이 비슷한 저장 시스템에서는 부담스럽다.\nSSD는 HDD와 다르게 내부적으로 물리적인 섹터 단위로 데이터를 읽고 쓰지 않는다. 따라서 메모리에 복사된 내용이 SSD의 섹터 크기보다 작은 경우에도(순차 디스크 쓰기) 여러번 기록되어야 한다.\n하지만 데이터의 무결성이 매우 중요한 서비스에서는 DoubleWrite의 활성화를 고려하는 것도 좋다. 만약 데이터베이스 서버의 성능을 위해 InnoDB 리두 로그 동기화 설정(innodb_flush_log_at_trx_commit 시스템 변수)을 1이 아닌 값으로 설정했다면, DoubleWrite도 비활성화 하는 것이 좋다.\ninnodb_flush_log_at_trx_commit\n0: 커밋 후 로그 버퍼를 디스크에 즉시 플러시 하지 않고, 로그 버퍼가 일정 수준 채워지거나 데이터베이스 서버가 종료될 때 플러시한다. 데이터 일관성이 보장되지 않을 수 있다. 1(default): 컷밋 후 로그 버퍼를 디스크에 즉시 플러시한다. 데이터 일관성은 보장하지만 디스크 IO가 부하를 발생시킬 수 있다. 2: 커밋 후 로그 버퍼를 디스크에 즉시 플러시 하지 않고, 로그를 별도 파일에 쓴 후 파일을 주기적으로 플러시 한다. 0과 1의 중간 정도로 데이터 일관성과 디스크 IO 부하 감소를 균형있게 유지할 수 있다. 언두 로그 InnoDB 스토리지 엔진은 트랜잭션과 격리 수준을 보장하기 위해 DML로 변경되기 이전 버전의 데이터를 별도로 백업한다. 이렇게 백업된 데이터를 언두 로그(Undo Log)라고 한다.\n트랜잭션 보장\n트랜잭션이 롤백되면 트랜잭션 도중 변경된 데이터를 변경 전 데이터로 복구해야 하는데, 이때 언두 로그에 백업해둔 이전 버전의 데이터를 이용해 복구한다. 격리 수준 보장\n특정 커넥션에서 데이터를 변경하는 도중 다른 커넥션에서 데이터를 조회하면 트랜잭션 격리 수준에 맞게 변경중인 레코드를 읽지 않고 언두 로그에 백업해둔 데이터를 읽어서 반환하기도 한다. 언두 로그 모니터링 언두 로그로 인해 여러가지 성능 이슈가 발생할 수 있어 모니터링이 필요하다.\n대용량 처리 트랜잭션\n1억 건의 레코드가 저장된 100GB 크기의 테이블을 DELETE로 삭제한다고 가정했을때, 언두 로그에 삭제전 값을 저장해야 하므로 언두 로그 공간은 100GB가 된다.\n장시간 활성화된 트랜잭션\n트랜잭션이 완료됐다고 해서 해당 트랜잭션이 생성한 언두 로그를 즉시 삭제할수 없을 수 있다. 먼저 시작된 트랜잭션보다 이후 발생한 트랜잭션이 완료된 경우, 먼저 시작된 완료된 트랜적션이 완료되기 전 까지 언두 로그는 삭제되지 않는다. 이러한 경우 언두 로그의 이력을 필요한 만큼 스캔해야만 필요한 레코드를 찾을 수 있기 때문에 쿼리의 성능이 전반적으로 떨어질 수 있다.\n1 2 3 4 5 6 7 8 9 /* MySQL 모든 버전 */ SHOW ENGING INNODB STATUS \\G /* MySQL 8.0 */ SELECT count FROM information_schema.innodb_metrics WHERE SUBSYSTEM=\u0026#39;transaction\u0026#39; AND NAME=\u0026#39;trx_rseg_history_len\u0026#39; ; MySQL 서버에서 실행되는 INSERT, UPDATE, DELETE 문장이 얼마나 많은 데이터를 변경하느냐에 따라 평상시 언두 로그 건수는 상이할 수 있어, 안정적인 시점의 언두 로그 건수를 확인하고 이를 기중으로 언두 로그의 급증 여부를 모니터링하는 것이 좋다.\nMySQL 서버에서 INSERT 문장으로 인한 언두 로그와 UPDATE, DELETE 문장으로 인한 언두 로그는 별도로 관리된다. UPDATE, DELETE 문장으로 인한 언두 로그는 MVCC와 데이터 복구(롤백 등)에 모두 사용되지만, INSERT 문장으로 인한 언두 로그는 롤백, 데이터 복구만을 위해 사용된다.\n언두 테이블스페이스 관리 언두 로그가 저장되는 공간을 언두 테이블스페이스(Undo Tablespace)라고 한다.\nMySQL 5.6 이전 버전에서는 언두 로그가 모두 시스템 테이블스페이스(ibdata.idb)에 저장되었었지만, 시스템 테이블스페이스의 언두 로그는 MySQL서버가 초기화될 때 생성되기 때문에 확장의 한계가 있었다. 이에 따라 5.6 버전에서는 innodb_undo_tablespaces 시스템 변수가 도입되어 별도 로그 파일을 사용할 수 있게 되었고, 8.0으로 업그레이드되면서 언두 로그는 항상 시스템 테이블스페이스 외부의 별도 로그 파일에만 기록되도록 개선되었다.\n하나의 언두 테이블스페이스는 1~128개의 롤백 세그먼트를 가지며, 롤백 세크먼트는 1개 이상의 언두 슬롯(Undo Slot)을 가진다.\n최대 동시 처리 가능한 트랜잭션의 개수는 다음 수식으로 예측할 수 있다.\n(InnDB 페이지 크기) / 16 * (롤백 세그먼트 개수) * (언두 테이블 스페이스 수)\nInnoDB 기본 설정(innodb_undo_tablespace=2, innodb_rollback_segments=128)을 사용한다면 131,072개 정도의 트랜잭션이 동시에 처리 가능해진다. 일반적인 서비스에서 이 정도까지 동시 트랜잭션이 필요하진 않겠지만 기본값으로 해서 크게 문제될 건 없다.\n언두 로그 슬롯이 부족한 경우에는 트래잭션을 시작할 수 없는 심각한 문제가 발생하기 때문에 적절히 정해야 한다.\nMySQL 8.0 부터 CREATE UNDO TABLESPACE나 DROP TABLESPACE같은 명령으로 새로운 언두 테이블 스페이스를 동적으로 추가하고 삭제할 수 있게 개선되었다.\n언두 테이블스페이스 공간을 필요한 만큼만 남기고 불필요하거나 과도하게 할당된 공간을 운영체제로 반납하는 것을 \u0026lsquo;Undo tablespace truncate\u0026rsquo;라고 하며 자동, 수동 두가지 방법이 있다.\n체인지 버퍼 RDBMS에서 레코드가 추가, 변경될 때 데이터 파일을 변경하는 작업뿐 아니라 해당 테이블에 포함된 인덱스를 업데이트하는 작업도 필요하다. 인덱스를 업데이트하는 작업은 랜덤하게 디스크를 읽는 작업이 필요하므로 테이블에 인덱스가 많다면 상당히 많은 자원을 소모하게 된다. 따라서 InnoDB는 변경해야 할 인덱스 페이지가 버퍼풀에 있으면 바로 업데이트를 수행하지만, 그렇지 않고 디스크로부터 읽어와서 업데이트해야 한다면 이를 즉시 실행하지 않고 임시 공간에 저장해 두고 바로 사용자에게 결과를 반환하는 형태로 성능을 향상시키게 되는데, 이때 사용하는 임시 메모리 공간을 체인지 버퍼(Change Buffer)라고 한다.\n사용자에게 결과를 반환하기 전에 반드시 중복 여부를 체크해야 하는 유니크 인덱스는 체인지 버퍼를 사용할 수 없다.\n체인지 버퍼에 임시로 저장된 인덱스 레코드 조각은 이후 백그라운드 스레드에 의해 병합되는데, 이 스레드를 체인지 버퍼 머지 스레드라고 한다.\nMySQL 5.5 이전 버전까지는 INSERT 작업에 대해서만 이러한 버퍼링이 가능했는데, 이후 조금씩 개선되며 INSERT, UPDATE, DELETE로 인해 키를 추가하거나 삭제하는 작업에 대해서도 버퍼링이 될 수 있게 개선되었다.\n또한 innodb_change_buffering이라는 시스템 변수가 새로 도입되어 작업의 종류별로 체인지 버퍼를 활성화할 수 있으며, 체인지 버퍼가 비효일적일 때는 체인지 버퍼를 사용하지 않게 설정할 수 있게 개선되었다.\nall: 모든 인덱스 관련 작업을 버퍼링(inserts + deletes + purges) none: 버퍼링 안함 inserts: 인덱스에 새로운 아이템을 추가하는 작업만 버퍼링 deletes: 인덱스에서 기존 아이템을 삭제하는 작업(삭제됐다는 마킹 작업)만 버퍼링 changes: 인덱스에 추가하고 삭제하는 작업만(inserts + deletes) 버퍼링 purges: 인덱스 아이템을 영구적으로 삭제하는 작업만 버퍼링(백그라운드 작업) 체인지 버퍼는 기본적으로 InnoDB 버퍼풀로 설정된 메모리 공간의 25%까지 활용할 수 있게 설정돼있으며, 필요하다면 50%까지 설정할 수 있다. innodb_change_buffer_max_size 시스템 변수에 비율을 조정하여 바꿀 수 있다.\n리두 로그 및 로그 버퍼 리두 로그는 트랜잭션의 4가지 요소인 ACID 중에서 D(Durable)에 해당하는 영속성과 가장 밀점하게 연관돼 있다. 리두 로그는 하드웨어나 소프트웨어 등 문제로 인해 MySQL 서버가 비정상적으로 종료됐을 때 데이터 파일에 기록되지 못한 데이터를 잃지 않게 해주는 안전장치이다.\n대부분 데이터베이스 서버는 데이터 변경 내용을 로그로 먼저 기록한다. 대부분 DBMS에서 데이터 파일은 쓰기보다 읽기 성능을 고려한 자료 구조를 가지고 있기 때문에 데이터 파일 쓰기는 디스크의 랜덤 액세스가 필요하여 상대적으로 큰 비용이 필요하다.\n이로 인한 성능 저하를 막기 위해 쓰기 비용이 낮은 자료구조인 리두 로그를 가지고 있으며, 비정상 종료가 발생하면 리두 로그의 내용을 이용해 데이터 파일을 다시 서버가 종료되기 직전 상태로 복구한다.\n또한 성능을 위해 리두 로그를 버퍼링 할 수 있는 InnoDB 버퍼풀이나, 리두 로그를 버퍼링할 수 있는 로그 버퍼와 같은 자료 구조도 가지고 있다.\nMySQL 서버가 비정상으로 종료되는 경우 InnoDB 스토리지 엔진의 데이터 파일은 두 가지 일관되지 않은 데이터를 가질 수 있다.\n커밋됐지만 데이터 파일에 기록되지 않은 데이터 롤백됐지만 데이터 파일에 이미 기록된 데이터 리두로그를 활용하여 변경이 커밋, 롤백, 트랜잭션의 실행 중간 상태였는지 확인하고, 적절히 처리한다.\n데이터베이스 서버에서 리두 로그는 트랜잭션이 커밋되면 즉시 디스크로 기록되도록 시스템 변수를 설정하는 것을 권장한다. 그래야만 서버가 비정상적으로 종료되었을때 직전까지의 트랜잭션 커밋 내용이 리두 로그에 기록될 수 있고, 그 리두 로그를 이용해 장애 직전 시점까지 복구가 가능해진다.\n하지만 트랜잭션이 커밋될 때마다 리두 로그를 디스크에 기록하면 부하가 생길 수 있어, InnoDB 스토리지 엔진에서 리두 로그를 어느 주기로 디스크에 동기화할지를 결정하는 innodb_flush_log_trx_commit 시스템 변수를 제공한다.\n0: 1초에 한 번씩 리두 로그를 디스크로 기록하고 동기화를 실행한다. 서버가 비정상 종료되면 최대 1초 동안의 트랜잭션은 커밋됐더라도 데이터는 사라질 수 있다. 1: 매번 트랜잭션이 커밋될 때마다 디스크로 기록되고 동기화까지 수행한다. 2: 트랜잭션이 커밋될 때마다 디스크로 기록은 되지만 실질적인 동기화는 1초에 한번씩 실행된다. 커밋이 되면 변경 내용이 운영체제의 메모리 버퍼로 기롤되는 것이 보장되기 때문에 MySQL 서버가 비정상 종료되더라도 트랜잭션 데이터는 사라지지 않는다. 리두 로그 파일들의 전체 크기는 버버풀의 효율성을 결정하기 때문에 신중히 결정해야한다. 리두 로그 파일의 크기는 innodb_log_file_size 시스템 변수로 결정하며, innodb_log_files_in_group 시스템 뼌수는 리두 로그 파일 개수를 결정한다.\n리두 로그 파일의 전체 크기를 버퍼풀의 크기에 맞게 설정해야 적절히 변경된 내용을 버퍼풀에 모아 한번에 디스크에 기록할 수 있다.\nACID는 데이터베이스에서 트랜잭션의 무결성을 보장하기 위해 꼭 필요한 4가지 요소(기능)을 의미한다.\nA(Atomic): 트랜잭션은 원자성 작업이어야 함. C(Consistent): 일관성 I(Isolated): 격리성 D(Durable): 영속성. 한 번 저장된 데이터는 지속적으로 유지되어야 함. 리두 로그 아카이빙 MySQL 8.0부터 InnoDB 스토리지 엔진의 리두 로그를 아카이빙 할 수 있는 기능이 추가됐다.\n백업 툴이 리두 로그 아카이빙을 사용하려면 먼저 MySQL 서버에서 아카이빙된 리두 로그가 저장될 디렉터리를 innodb_redo_log_archive_dirs 시스템 변수에 설정해야 하며, 디렉터리는 운영체제의 MySQL 서버를 실행하는 유저만 접근이 가능해야 한다.\n1 2 3 4 mkdir /var/log/mysql_redo_archive cd /var/log/mysql_redo_archive mkdir 20230413 chmod 700 20230413 1 SET GLOBAL innodb_redo_log_archive_dirs=\u0026#39;backup:/var/log/mysql_redo_archive\u0026#39;; 디렉터리가 준비되면 리두 로그 아카이빙을 시작하도록 innodb_redo_log_archive_start UDF(사용자 정의 함수)를 실행한다. 해당 UDF는 리두 로그를 아카이빙할 디렉터리에 대한 레이블과 선택적으로 서브 디렉터리 이름 총 두가지의 매개 변수를 받는다.\n1 DO innodb_redo_log_archive_start(\u0026#39;backup\u0026#39;, \u0026#39;20230413\u0026#39;); 리두 아카이빙을 종료할 때는 innodb_redo_log_archive_stop UDF를 실행한다.\n1 DO innodb_redo_log_archive_stop(); innodb_redo_log_archive_start UDF를 실행한 세션의 연결이 끊어지면 InnoDB 스토리지 엔진은 리두 로그 아카이빙을 멈추고 아카이빙 파일도 자동으로 삭제하므로 커넥션을 유지해야 하고, innodb_redo_log_archive_stop UDF를 호출하여 정상적으로 종료돼야 한다.\n리두 로그 활성화 및 비활성화 InnoDB 스토리지 엔진의 리두 로그는 MySQL 서버가 비정상 종료됐을때 데이터 파일에 기록되지 못한 트랜잭션을 복구하기 위해 항상 활성화되어있다. MySQL 서버에서 트랜잭션이 커밋돼도 데이터 파일은 즉시 디스크로 동기화되지 않는 반면, 리두 로그는 항상 디스크로 기록된다.\nMySQL 8.0 버전부터 수동으로 리두 로그를 비활성화 할 수 있어, 대용량 데이터를 한번에 적재하는 경우 사용하여 적재 시간을 단축할 수 있다.\n어댑티브 해시 인덱스 어댑티브 해시 인덱스는 InnoDB 스토리지 엔진에서 사용자가 자주 요청하는 데이터에 대해 자동으로 생성하는 인덱스로, innodb_adaptive_hash_index 시스템 변수를 이용하여 활성화, 비활성화 할 수 있다.\nInnoDB 스토리지 엔진의 대표적인 인덱스는 B-Tree로 데이터는 PK 순으로 정렬되어 관리되고, Secondary Key는 인덱스키 + PK 조합으로 정렬되어 있다. 특정 데이터를 찾기 위해 Secondary Key에서 PK를 찾고, 찾은 PK를 통해 원하는 데이터를 찾는 형태로 처리된다.\nPK 사용시 데이터에 접근되는 비용은 O(logN)이고, Secondary Key를 사용해 데이터에 접근은 PK에 대한 접근도 필요하므로 2 * O(logN)이다.\n따라서 B-Tree 자료구조 특성으로 데이터가 많아진다 하더라도 탐색 비용이 크게 증가하지 않지만, 동시에 많은 스레드에서 탐색 작업이 발생할 경우 Lock 등으로 인해 성능 저하가 발생할 수 있다.\n어댑티브 해시 인덱스는 B-Tree의 검색 시간을 줄여주기 위해 도입된 기능으로, 자주 읽히는 데이터 페이지의 키 값을 이용해 해시 인덱스를 만들고, 필요할 때마다 어댑티브 해시 인덱스를 검색해서 레코드가 저장된 데이터 페이지를 즉시 찾아갈 수 있다.\n구조 해시 인덱스는 인덱스 키 값과 해당 인덱스 키 값이 저장된 데이터 페이지 주소의 쌍으로 관리된다.\n인덱스 키 값:\nB-Tree 인덱스의 고유번호 + B-Tree 인덱스의 실제 키 값 인덱스의 고유번호가 포함되는 이유는 InnoDB 스토리지 엔진에서 어댑티브 해시 인덱스는 하나만 존재하기 때문이다. 데이터 페이지 주소:\n실제 키 값이 저장된 데이터 페이지의 메모리 주소, 버퍼풀에 로딩된 페이지의 주소를 의미 어댑티브 해시 인덱스는 버퍼풀에 올려진 데이터 페이지에 대해서만 괸리되고, 버퍼풀에서 해당 데이터 페이지가 없어지면 어댑티브 해시 인덱스에서도 해당 페이지의 정보는 사라진다.\n성능 어댑티브 해시 인덱스를 활성화 후 처리량은 2배 가까이 늘었음에도 불구하고 CPU 사용량은 오히려 떨어진다.\nInnoDB 내부잠금(세마포어)의 횟수도 획기적으로 줄어든다.\n추가로 MySQL 8.0 부터는 내부 잠금을 줄이기 위해 어댑티브 해시 인덱스의 파티션 기능을 제공하며 innodb_adaptive_hash_index_parts 시스템 변수를 통해 파티션 개수를 변경할 수 있다(기본값 8개).\n어댑티브 해시 인덱스가 성능에 많은 도음이 된다면 파티션 개수를 더 많이 설정하는 것도 도움이 될 수 있다.\n한계 상황에 따라 어댑티브 해시 인덱스가 성능 향상에 크게 도움이 되지 않는 경우도 있다.\n성능 향상에 도움이 되는 경우 디스크의 데이터가 InnoDB 버퍼풀 크기와 비슷한 경우(디스크 읽기가 많지 않은 경우) 동등 조건 검색(동등 비교 및 IN 연산)이 많은 경우 쿼리가 일부 데이터에만 집중 되는 경우 성능 향상에 크게 도움이 되지 않는 경우 디스크 읽기가 많은 경우 특정 패턴의 쿼리가 많은 경우(JOIN, LIKE 패턴 검색) 매우 큰 데이터를 가진 테이블의 레코드를 폭넓게 읽는 경우 어댑티브 해시 인덱스는 데이터 페이지를 메모리(버퍼풀) 내에서 접근하는 것을 더 빠르게 만드는 기능으로 데이터 페이지를 디스크에서 읽어오는 경우가 많은 경우 데이터베이스 서버에서는 큰 도움이 되지 않는다.\n어댑티브 해시 인덱스 또한 메모리를 사용하며, 때로는 상당히 큰 메모리 공간을 사용할 수 있다. 데이터 페이지의 인덱스 키가 해시 인덱스로 만들어져야 하기 때문에 불필요한 경우 제거되어야 한다. 활성화되면 InnoDB 스토리지 엔진이 필수적으로 검색에 활용해야 하기 때문에 불필요한 접근이 발생할 수 있다. 주의할 점 테이블 삭제(DROP), 변경(ALTER)시 해당 테이블이 가진 모든 데이터 페이지의 내용을 어댑티브 해시 인덱스에서 제거해야한다. 이로 인해 테이블이 삭제되거나 스키마가 변경되는 동안 상당히 많은 CPU 자원을 사용하게되어 데이터베이스 서버의 처리 성능이 떨어진다.\n모니터링 MySQL 서버의 상태 값들을 통해 어댑티브 해시 인덱스가 불필요한 오버헤드만 만들고 있는지 확인할 수 있다.\n1 2 3 4 5 6 7 8 SHOW ENGINE INNODB STATUS\\G /* Hash table size 8747, node heap has 1 buffers(s) Hash table size 8747, node heap has 0 buffers(s) ... 1.03 hash searches/s, 2.64 non-hash searches/s /* searches: 쿼리가 처리되기 위해 내부적으로 키 값의 검색이 몇 번 실행되었는지를 의미함\n어댑티브 해시 인덱스의 효율은 검색 횟수가 아니라 해시 인덱스 히트율과 인덱스가 사용 중인 메모리 공간, 서버의 CPU 사용량을 종합해서 판단해야 한다.\n위 실행 쿼리 결과에서는 28% 정도가 어댑티브 해시 인덱스를 이용했다는 것을 알 수 있는데, 서버의 CPU 사용량이 100%에 근접한다면 효율적이라고 볼 수 있다. 하지만 CPU 사용량이 낮고 어댑티브 해시 인덱스의 메모리 사용량이 높다면 비활성화하여 버퍼풀이 더 많은 메모리를 사용할 수 있게 유도하는 것도 좋은 방법이다.\n어댑티브 해시 인덱스의 메모리 사용량은 performance_schema를 이용해서 확인 가능하다.\n1 2 3 4 5 SELECT EVENT_NAME ,CURRENT_NUMBER_OF_BYTES_USED FROM performance_schema.memory_summary_global_by_event_name WHERE EVENT_NAME=\u0026#39;memory/innodb/adaptive hash index\u0026#39; ; MyISAM, MEMORY 스토리지 엔진 비교 MyISAM MySQL 5.5부터는 InnoDB 스토리지 엔진이 기본 스토리지 엔진으로 채택 되었지만, 이전까지는 MyISAM이 기본 스토리지 엔진으로 사용되는 경우가 많았다.\nMySQL 서버의 시스템 테이블의 기본 스토리지 엔진 MySQL 8.0 부터는 MyISAM이 기본 설정되었던 서버의 시스템 테이블(사용자 인증 관련 정보, 복제 관련 정보가 저장된 mysql DB의 테이블) 등 서버의 모든 기능을 InnoDB 스토리지 엔진으로 교체되었다. 전문 검색 및 공간 좌표 검색 기능 제공. InnoDB 스토리지 엔진에서도 전문 검색과 공간 좌표 검색 기능을 모두 지원하도록 개선되었다. 이러한 이유로 MyISAM 스토리지 엔진은 InnoDB 스토리지 엔진으로 대체될 것으로 예상된다.\nMEMORY MEMORY 스토리지 엔진이 메모리라는 이름 때문에 과대 평가를 받는 경우가 있다.\n단일 스레드 처리 성능 단일 스레드 처리 성능은 MEMORY 스토리지 엔진이 빠를 수 있으나, MySQL 서버는 일반적으로 온라인 트랜잭션 처리를 위한 목적으로 사용되어 동시 처리 성능이 매우 중요하다. MEMORY 스토리지 엔진에서 동시에 많은 클라이언트 쿼리 요청이 실행되는 상황이라면 테이블 수준의 잠금으로 인해 InnoDB 스토리지 엔진을 따라갈 수 없다.\n임시 테이블 용도로 활용 MySQL 5.7 버전까지 내부 임시 테이블 용도로 활용되었으나, 가변 길이 타입의 컬럼을 지원하지 않는다는 문제점으로 MySQL 8.0 부터는 TempTable 스토리지 엔진이 대체되어 사용된다.\n이러한 이유로 MEMORY 스토리지 엔진을 선택해서 얻을 수 있는 장점이 없어져, 향후 버전에서는 제거될 것으로 예상된다.\n","date":"2023-04-13T12:39:01+09:00","image":"https://codemario318.github.io/post/real_mysql_4_2_3/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_4_2_3/","title":"4.2 InnoDB 스토리지 엔진 아키텍처(3)"},{"content":"버퍼풀은 InnoDB 스토리지 엔진의 핵심으로 디스크에서 데이터를 읽어 메모리에 보관하고, 필요할 때 메모리에서 데이터를 읽어와 처리하는 역할을 수행한다. 또한 디스크와 메모리 사이에서 데이터 읽기 및 쓰기를 관리하여 데이터 베이스의 성능을 향상시킨다.\n디스크의 데이터 파일이나 인덱스 정보를 메모리에 캐시해 두는 공간이다. 쓰기 작업을 지연시켜 일괄 작업으로 처리할 수 있게 해주는 버퍼 역할도 수행한다. 일반적인 애플리케이션에서는 INSERT, UPDATE, DELETE처럼 데이터를 변경하는 쿼리는 데이터 파일의 흩어져있는 레코드를 변경하기 때문에 랜덤한 디스크 작업을 발생시킨다. 변경을 모아 처리하여 랜덤 디스크 접근 작업 수를 줄일 수 있다.\n버퍼풀의 크기 설정 운영체제와 각 클라이언트 스레드가 사용할 메모리도 충분히 고려하여 설정한다. MySQL 서버 내에서 메모리를 필요로 하는 부분은 크게 없지만 아주 독특한 경우 레코드 버퍼가 상당한 메모리를 사용하기도 한다.\n레코드버퍼\n각 클라이언트 세션에서 테이블의 레코드를 읽고 쓸 때 버퍼로 사용하는 공간으로 커넥션이 많고 사용하는 테이블도 많다면 레코드 버퍼 용도로 사용되는 메모리 공간이 많이 필요할 수 있다.\nMySQL 서버가 사용하는 레코드 버퍼 공간은 별도로 설정할 수 없어, 전체 커넥션 개수와 각 커넥션에서 읽고 쓰는 테이블의 개수에 따라 결정되고, 동적으로 해제되기도 하므로 정확히 필요한 메모리의 크기를 계산할 수 없다.\n버퍼풀 동적 크기 조절 MySQL 5.7 버전부터 InnoDB 버퍼풀의 크기를 동적으로 조절할 수 있게 개선되어 가능하면 InnoDB 버퍼풀의 크기를 적절히 작은 값으로 설정하고 상황을 봐가며 증가시키는 방법이 최적이다.\ninnodb_buffer_pool_size 시스템 변수로 크기를 설정할 수 있으며, 동적으로 버퍼풀의 크기를 확장할 수 있다.\n크리티컬한 변경이므로 가능하며 MySQL 서버가 한가한 시점을 골라 실행한다. 버퍼풀의 크기를 줄이는 작업은 서비스 영향도가 매우 크므로 주의해야한다. 버퍼풀은 내부적으로 128MB 청크 단위로 쪼개어 관리되어 조절된다. 버퍼풀 나누기 InnoDB 버퍼풀은 정통적으로 버퍼풀 전체를 관리하는 잠금(세마포어)으로 인해 내부 잠금 경합을 많이 유발해왔는데, 이런 경함을 줄이기 위해 버퍼풀을 여러개로 쪼개어 관리할 수 있게 개선되었다.\n버퍼풀이 여러 개의 작은 버퍼풀로 쪼개지면서 개별 버퍼풀을 관리하는 잠금 자체도 경합이 분산되는 효과를 얻을 수 있게 된다.\ninnodb_buffer_pool_instances 시스템 변수를 이용해 버퍼풀을 여러개로 분리하여 관리할 수 있다.\n버퍼풀 구조 InnoDB 스토리지 엔진은 버퍼풀이라는 거대한 메모리 공간을 페이지 크기(innodb_page_size 시스템 변수에 설정된)의 조각으로 쪼개어 InnoDB 스토리지 엔진이 데이터를 필요로 할 때 해당 데이터 페이지를 읽어서 각 조각에 저장한다.\n버퍼풀의 페이지 조각을 관리하기 위해 LRU(least Recently Used) 리스트와, 플러시(Flush) 리스트, 프리(Free) 리스트라는 3개의 자료 구조를 관리한다.\nLRU 리스트: 디스크로부터 읽어온 페이지 저장하는 자료구조. 읽어온 페이지를 최대한 오랫동안 버퍼풀의 메모리에 유지하여 디스크 읽기를 최소화 한다.\n플러시 리스트: 디스크로 동기화되지 않은 데이터를 가진 데이터 페이지(더티 페이지)의 변경 시점 기준의 페이지 목록을 관리한다.\n프리리스트: 버퍼풀에서 실제 사용자 데이터로 채워지지 않은 비어있는 페이지들의 목록. 사용자의 쿼리가 새로벡 디스크의 데이터 페이지를 읽어와야 하는 경우 사용된다.\nLRU 리스트 구조 LRU 리스트는, Old 서브리스트 영역은 LRU, New 서브리스트 영역은 MRU(Most Recently Used)가 합쳐진 방식으로 동작한다.\nNew 서브리스트의 Tail과 Old 서브리스트의 Head가 만나는 지점을 MidPoint라 하며 버퍼풀에 새로운 페이지가 들어올 경우 Old 서브리스트의 Head 부분에 저장한다.\nNew 서브리스트와 Old 서브리스트로 나눈 이유는?\n하나의 큐를 사용하여 페이지를 관리할 경우에 Head 또는 Tail에 페이지를 저장하는 방식을 생각해 볼 수 있다. Head에 저장될 경우 새로 관리되는 페이지가 사용되지 않더라도 오랜 시간동안 리스트에 남아 있게되고, Tail에 저장될경우 해당 페이지를 즉시 읽지 않는다면 리스트에 남아있지 않게 되어 의미가 없어질 수 있다.\n이를 위해 두개의 서브리스트로 나누고, 경험적으로 얻은 5/8 지점을 활용한 중간점 삽입 전략을 사용하는 것 같다.\n데이터를 찾는 과정 필요한 레코드가 저장된 데이터 페이지가 버퍼풀에 있는지 검사\nInnoDB 어댑티브 해시 인덱스를 이용해 페이지를 검색 해당 테이블의 인덱스(B-Tree)를 이용해 버퍼풀에서 페이지를 검색 버퍼풀에 이미 데이터 페이지가 있다면 해당 페이지의 포인터를 MRU 방향으로 승급 디스크에서 필요한 데이터 페이지를 버퍼풀에 적재하고, 적재된 페이지에 대한 포인터를 LRU 헤더 부분에 추가\n버퍼풀의 LRU 헤더에 적재된 데이터 페이지가 실제로 읽히면 MRU 헤더 부분으로 이동(Read Ahead와 같이 대량 읽기의 경우 디스크 페이지가 버퍼풀로 적재는 되지만 실제 쿼리에서 사용되지는 않을수도 있으며, 이련 경우는 MRU로 이동되지 않음)\n버퍼풀에 상주하는 데이터 페이지는 사용자 쿼리가 얼마나 최근에 접근했었는지에 따라 나이가 부여되며, 버퍼풀에 상주하는 동안 쿼리에서 오랫동안 사용되지 않으면 오래된 페이지는 버퍼풀에서 제거됨. 버퍼풀의 페이지가 쿼리에 의해 사용되면 나이가 초기회되고 MRU헤더 부분으로 옮겨진다.\n필요한 데이터가 자주 접근됐다면 해당 페이지의 인덱스 키를 어댑티브 해시 인덱스에 추가\n처음 한번 읽힌 데이터 페이지가 이후 자주 사용된다면 버퍼풀의 MRU 영역에서 살아남게 되고, 그렇지 않은 경우 새롭게 읽히는 데이터 페이지에 밀려 결과적으로 버퍼풀에서 제거된다.\n버퍼풀과 리두 로그 버퍼풀은 데이터 베이스 서버의 성능 향상을 위해 데이터 캐시와 쓰기 버퍼링이라는 두가지 용도가 있다. 따라서 메모리가 허용하는 만큼 크게 설정하면 데이터 캐시 공간을 키워 쿼리의 성능 높힐 수 있지만, 쓰기 버퍼링 성능 향상을 위해서는 버퍼풀과 리두 로그의 관계에 대해 이해하는 것이 중요하다.\n리두 로그(Redo Log)란? 리두 로그는 데이터베이스에 대한 모든 변경 내용을 기록하는 파일셋이다. 시스템 장애나 충돌이 발생했을 때 데이터의 내구성과 일관성을 보장하기 위해 사용된다.\n데이터베이스에 변경 사항이 발생하면, 먼저 선로깅(write-ahead logging)프로세스로 리두 로그에 쓰여지고 변경 내용이 리두 로그에 기록되면 데이터베이스에 적용된다.\n리두 로그 파일은 일반적으로 디스크에 저장되며, MySQL은 변경 사항을 순차적으로 기록한다. 리두 로그 파일이 가득 차면 MySQL은 \u0026ldquo;체크포인트(checkpoint)\u0026ldquo;를 수행하여 데이터베이스의 모든 더티 페이지(버퍼풀에서 수정된 페이지)를 디스크에 기록한 후, 로그 파일을 잘라내어 공간을 확보하게 된다.\nMySQL에서 리두 로그는 원형 버퍼 형식으로 저장되며, 채워지면 다음 변경 사항은 순환 방식으로 다음 사용 가능한 리두 로그 파일에 기록된다. 이를 통해 리두 로그에 변경 사항이 지속적으로 기록되어 시스템 충돌이나 장애가 발생하더라도 변경 사항을 손실하지 않도록 보장한다.\n버퍼풀과 리두 로그의 관계 버퍼풀은 디스크에서 읽은 상태로 전혀 변경되지 않은 클린 페이지(Clean Page)와 함께 INSERT, UPDATE, DELETE를 통해 변경된 데이터를 가진 더티 페이지(Duty Page)를 가지고 있다.\n데이터 변경이 발생하면 먼저 리두 로그에 기록되고 리두 로그는 더티 페이지와 대응하게 된다.\n데이터 변경이 반복되면 결국 리두 로그 파일을 기록할 수 없거나 버퍼풀 용량이 부족해지는데, 이를 대응하기 위해 체크포인트를 수행하여 모든 더티페이지를 디스크에 기록한 후, 리두 로그 파일을 잘라내어 공간을 확보한다.\n이러한 방식이 버퍼풀이 쓰기 버퍼의 역할을 수행하게 만들게 되는데, 이에 따라서 리두 로그 파일의 크기가 작을수록 버퍼풀의 크기가 크더라도 대응되는 더티 페이지가 적으므로 버퍼링으로 얻을 수 있는 효과가 적어지고, 리두 로그 파일이 클수록 체크포인트를 통해 디스크에 기록되는 데이터가 많아져 갑자기 많은 디스크 I/O를 발생 시킬 수 있다.\n따라서, 리두 로그 파일의 크기를 적절히 선택해야하며, 어렵다면 버퍼풀의 크기가 100GB 이하의 MySQL 서버에서는 리두 로그 파일의 전체 크기를 대략 5~10GB 수준으로 선택하고 필요할 때마다 조금씩 늘려가며 최적값을 찾는 것이 좋다.\n버퍼풀 플러시(Buffer Pool Flush) 버퍼풀에서 수정된 데이터 페이지를 디스크로 쓰는 과정으로 MySQL 5.6 버전까지는 InnoDB 스토리지 더티 페이지 플러시 기능이 급작스럽게 디스크 기록이 폭증해서 MySQL 서버의 사용자 쿼리 처리 성능에 영향을 받는 등 그다지 부드럽게 처리되지 않았다.\nMySQL 5.7 버전을 거쳐 8.0 버전으로 업그레이드되면서 대부분의 서비스에서는 더티 페이지 프러시에서 예전과 같이 폭증 현상은 발생하지 않았다. 따라서 InnoDB 스토리지 엔진의 더티 페이지 플러시 성능 문제가 발생하지 않는다면 관련 시스템 변수는 조절하지 않아도 괜찮다.\nInnoDB 스토리지 엔진은 버퍼풀에서 아직 디스크로 기록되지 않은 더티 페이지들을 성능상 악영향 없이 디스크에 동기화하기 위해 다음과 같이 2개의 플러시 기능을 백그라운드로 실행한다.\n플러시 리스트(Flush_list) 플러시 LRU 리스트(LRU_list) 플러시 플러시 리스트 플러시 InnoDB 스토리지 엔진은 리두 로그 공간의 재활용을 위해 주기적으로 오래된 리두 로그 엔트리가 사용하는 공간을 비운다. 이때 오래된 리두 로그 공간이 지워지려면 반드시 InnoDB 버퍼풀의 더티 페이지가 먼저 디스크로 동기화 돼야 한다.\n이를 위해 InnoDB 스토리지 엔진은 주기적으로 플러시 리스트(Flush_list) 플러시 함수를 호출하여 플러시 리스트에서 오래전에 변경된 데이터 페이지 순서대로 디스크에 동기화 하는 작업을 수행한다.\n이때 언제부터 얼마나 많은 더티 페이지를 한번에 디스크로 기록하느냐에 따라 사용자의 쿼리 처리가 악영향을 받지 않으면서 부드럽게 처리된다. 리를 위해 InnoDB 스토리지 엔진은 여러 시스템 변수를 제공한다.\ninnodb_page_cleaners InnoDB 스토리지 엔진에서 더티 페이지를 디스크로 동기화하는 스레드를 클리너 스레드(Cleaner Thread)라고 하고, 클리너 스레드의 개수를 조정할 수 있게 해준다.\n설정값이 버퍼풀 인스턴스 개수보다 많은 경우 innodb_buffer_pool_instances 설정값으로 자동으로 변경하여, 하나의 클리너 스레드가 하나의 버퍼풀 인스턴스를 처리하도록 한다.\n시스템 변수의 설정값이 버퍼풀 인스턴스 개수보다 적은 경우 하나의 클리너 스레드가 여러 개의 버퍼풀 인스턴스를 처리하므로, innodb_page_cleaners 설정값은 innodb_buffer_pool_instances 설정 값과 동일하게 설정하는 것이 좋다.\ninnodb_max_dirty_pages_pct InnoDB 버퍼풀은 클린 페이지와 더티 페이지를 함께 가지고 있어 뭏란정 더티 페이지를 그대로 유지할 수 없다. 기본적으로 InnoDB 스토리지 엔진은 전체 버퍼풀이 가진 페이지의 90%까지 더티페이지를 가질 수 있는데, innodb_max_dirty_pct 시스템 변수를 이용해 더티페이지의 비율을 조절할 수 있다.\n일반적으로 버퍼풀이 더티페이지를 많이 가지고 있을수록 디스크 쓰기 작업을 버퍼링함으로써 I/O 작업을 줄일 수 있으므로 기본값으로 유지하는 것이 좋다.\ninnodb_max_dirty_pages_pct_lwm InnoDB 스토리지 엔진은 innodb_io_capacity 시스템 변수에 설정된 값을 기준으로 더티 페이지 쓰기를 실행하는데, 디스크로 기록되는 더티페이지 개수보다 더 많은 더티페이지가 발생하면 버퍼풀에 더티페이지가 계속 증가하게 되고, 지정한 비율이 넘어가면 더티페이지를 디스크로 기록하여 디스크 쓰기 폭발(Dist IO Bust) 현상이 발생할 가능성이 있다.\n이런 문제를 완화하기 위해 innodb_max_dirty_pages_pct_lwm 시스템 설정 변수를 이용해 일정 수준 이상의 더티페이지가 발생하면 조금씩 더티 페이지를 디스크로 기록한다.\n기본값은 10% 정도로, 디스크 쓰기가 많이 발생하고 더티 페이지 비율이 낮은 상태를 유지한다면 높은 값으로 조정할 수 있다.\ninnodb_io_capacity, innodb_io_capacity_max 데이터베이스 서버에서 어느정도의 디스크 IO가 가능한지 설정하는 값이다. innodb_io_capacity는 일반적인 상황에서 디스크가 적절히 처리할 수 있는 수준의 값을 설정하며, innodb_io_capacity_max는 디스크가 최대 성능을 발휘할 때 어느 정도 IO가 가능한지를 설정한다.\n여기서 언급되는 IO는 InnoDB 스토리지 엔진의 백그라운드 스레드가 수행하는 디스크 작업을 의미하며, 대부분 더티페이지 쓰기이다.\n스토리지 엔진은 사용자의 쿼리를 처리하기 위해 디스크를 읽기도 해야하므로 하드웨어 성능에 무조건 맞추는 것은 좋지 않다.\ninnodb_adaptive_flushing, innodb_adaptive_flushing_lwm 어댑티브 플러시를 활성화 하면 InnoDB 스토리지 엔진은 버퍼불의 더티 페이지 비율이나 innodb_io_capacity, innodb_io_capacity_max 설정 값에 의존하지 않고 알고리즘을 사용한다.\n더티 페이지는 리두 로그와 대응하므로, 리두 로그가 어느정도 증가하는지 분석하여 확인할 수 있다. 어댑티브 플러시 알고리즘은 리두 로그의 증가 속도를 분석하여 적절한 수준의 더티 페이지가 버퍼풀에 유지될 수 있도록 디스크 쓰기를 실행한다.\ninnodb_adaptive_flushing는 기본값이 활성이며, innodb_adaptive_flushing_lwm는 어댑티브 플러시 알고리즘 활성을 위한 활성 리두 공간의 하한 비율을 의미한다.\ninnodb_flush_neighbors 더티페이지를 디스크에 기록할 때 디스크에서 근접한 페이지 중 더티페이지가 있다면 InnoDB 스토리지 엔진이 함께 묶어 디스크로 기록하게 해주는 기능을 활성화 할지 결정한다.\n과거에는 HDD의 경우 IO 비용이 높아 최대한 줄이기 위해 만들어졌다.\n데이터 저장을 하드디스크로 하고있다면 1, 2 정도로 활성화 하고, SSD를 사용한다면 기본값인 비활성으로 유지하는 것이 좋다.\nLRU 리스트 플러시 InnoDB 스토리지 엔진은 LRU 리스트에서 사용 빈도가 낮은 데이터 페이지들을 제거하여 새로운 페이지들을 읽어올 공간을 만들어야 하는데, 이를 위해 LRU 리스트 플러시 함수가 사용된다.\n리스트 끝부분 부터 시작하여 최대 innodb_lru_scan_depth 시스템 변수에 설정된 수만큼의 페이지들을 스캔하는데, 이때 더티체이지는 디스크에 동기화하고, 클린 페이지는 즉시 프리 리스트로 페이지를 옮긴다.\nInnoDB 스토리지 엔진은 버퍼풀 인스턴스 별로 최대 innodb_lru_scan_depth 개수만큼 스캔하기 때문에 실질적으로 LRU 리스트의 스캔은 (innodb_buffer_pool_instances * innodb_lru_scan_depth) 수만큼 수행하게 된다.\n버퍼풀 상태 백업 및 복구 InnoDB 서버의 버퍼풀은 쿼리의 성능에 매우 밀접하게 연결돼 있다. 서버 재실행시 버퍼풀에 쿼리들이 사용할 데이터가 없어 성능이 매우 떨어지게 된다.\n디스크의 데이터가 버퍼풀에 적재돼 있는 상태를 위밍업(Warming Up)이라고 표현하는데, 워밍업 상태에 따라 몇십 배 쿼리 처리속도 차이가 발생하게 된다.\nMySQL 5.5 버전은 재실행시 강제 워밍업을 위해 주요 테이블과 인덱스에 대해 풀스캔을 실행하고 서비스를 오픈했었다. 하지만 5.6 버전부터는 버퍼풀 덤프 및 적재 기능이 도입되어 MySQL 서버 셧다운 전 innodb_buffer_pool_dump_now 시스템 변수를 이용해 현재 InnoDB 버퍼풀 상태를 백업할 수 있다.\n1 2 3 4 5 /* 버퍼풀 상태 백업 */ SET GLOBAL innodb_buffer_pool_dump_now=ON; /* 백업된 버퍼풀 상태 복구 */ SET GLOBAL innodb_buffer_pool_load_now=ON; 버퍼풀 백업을 수행하면 데이터 디렉터리에 ib_buffer_pool이라는 파일로 생성되는데, InnoDB 스토리지 엔진이 버퍼풀의 LRU 리스트에서 적재된 데이터 페이지의 메타 정보만 가져와 저장하여, 버퍼풀이 크다고 하더라도 몇십 MB 이하로 작다.\n하지만 버퍼풀로 복구하는 과정에서 각 테이블의 데이터 페이지를 디스크에서 다시 읽어와야 하기 때문에 버퍼풀의 크기에 따라 매우 오래 걸릴 수 있다.\n1 SHOW STATUS LIKE \u0026#39;Innodb_buffer_pool_dump_status\u0026#39;\\G InnoDB의 버퍼풀을 복구하는 작업은 상당히 많은 디스크 읽기를 필요로 하기 때문에, 복구중 서비스 재개하는 것은 좋지 않을 수 있다. 버퍼풀 적재 작업을 중지하려면 innodb_buffer_pool_load_abort 시스템 변수를 통해 중지하여 재개하는 것을 권장한다.\n1 SET GLOBAL innodb_buffer_pool_load_abort=ON; 백업 및 복구 자동화 InnoDB 스토리지 엔진은 innodb_buffer_pool_dump_at_shutdown, innodb_buffer_pool_load_at_shutdown 설정을 MySQL 설정 파일에 넣으면 서버가 셧다운 되기 직전에 버퍼풀의 백업을 실행하고, MySQL 서버가 시작되면 자동으로 백업된 버퍼풀의 상태를 복구할 수 있는 기능을 제공한다.\n버퍼풀의 적재 내용 확인 MySQL 5.6 버전부터 MySQL 서버의 information_schema 데이터베이스의 innodb_buffer_page 테이블을 이용해 InnoDB 버퍼풀의 메모리에 어떤 테이블의 페이지들이 적재돼 있는지 확인할 수 있었다. 하지만 버퍼풀이 큰 경우에는 테이블 조회가 상당히 큰 부하를 일으키면서 서비스 쿼리가 많이 느려지는 문제가 있어, 실제 서비스용으로 사용되는 MySQL 서버에서는 버퍼풀의 상태를 확인하는 것이 거의 불가능했다.\nMySQL 8.0 버전에서는 information_schema 데이터베이스에 innodb_cached_indexes 테이블이 새로 추가되어, 테이블의 인덱스별로 데이터 페이지가 얼마나 InnoDB 버퍼풀에 적재돼 있는지 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 SELECT it.name table_name ,ii.name index_name ,ici.n_cached_pages n_cached_pages FROM information_schema.innodb_tables it JOIN information_schema.innodb_indexes ii ON ii.table_id = it.table_id JOIN information_schema.innodb_cached_indexes ici ON ici.index_id = ii.index_id WHERE it.name=CONCAT(\u0026#39;employees\u0026#39;, \u0026#39;/\u0026#39;, \u0026#39;employees\u0026#39;) ; 아직 MySQL 서버는 개별 인덱스별로 전체 페이지 개수가 몇 개인지는 사용자에게 알려주지 않기 때문에 information_schema의 테이블을 이용해도 테이블의 인덱스별로 페이지가 InnoDB 버퍼풀에 적재된 비율은 확인할 수가 없다.\n","date":"2023-04-12T14:27:41+09:00","image":"https://codemario318.github.io/post/real_mysql_4_2_2/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_4_2_2/","title":"4.2 InnoDB 스토리지 엔진 아키텍처(2) - InnoDB 버퍼풀"},{"content":"\nInnoDB는 MySQL에서 사용할 수 있는 스토리지 엔진 중 거의 유일하게 레코드 기반 잠금을 제공하며, 그 때문에 높은 동시성 처리가 가능하고 안정적이며 성능이 뛰어나다.\n프라이머리 키에 의한 클러스터링 InnoDB의 모든 테이블은 기본적으로 프라이머리 키를 기준으로 클러스터링되어 자장된다.\n프라이머리 키 값의 순서대로 디스크에 저장되며, 모든 세컨더리 인덱스는 레코드의 주소 대신 프라이머리 키의 값을 논리적인 주소로 사용한다. 프라이머리 키가 클러스터링 인덱스이기 때문에 프라이머리 키를 이용한 레인지 스캔은 상당히 빨리 처리될 수 있다. 쿼리의 실행계획에서 프라이머리 키는 기본적으로 다른 보조 인덱스에 비해 비중이 높게 설정된다. 외래 키 지원 외래 키에 대한 지원은 InnoDB 스토리지 엔진 레벨에서 지원하는 기능으로 MyISAM이나 MEMORY 테이블에서는 사용할 수 없다.\n외래 키는 데이터베이스 서버 운영의 불편함 때문에 서비스용 데이터베이스에서는 생성하지 않는 경우도 자주 있다. 그렇다 하더라도 개발 환경의 데이터베이스에서는 좋은 가이드 역할을 할 수 있다.\nInnoDB에서 외래 키는 부모 테이블과 자식 테이블 모두 해당 칼럼에 인덱스 생성이 필요함 변경 시에는 반드시 부모 테이블이나 자식 테이블에 데이터가 있는지 체크하는 작업이 필요하므로, 잠금이 여러 테이블로 전파됨 그로인한 데드락이 발생할 때가 많으므로 개발할때도 외래 키의 존재에 주의하는 것이 좋음 수동으로 데이터를 적재하거나 스키마 변경 등의 관리 작업이 실패할 수 있다. 부모 테이블과 자식 테이블의 관계를 명확히 파악해서 순서대로 작업한다면 문제없이 실행될 수 있지만 외래키가 복잡하게 얽힌 경우에는 간단하지 않다.\nforeign_key_checks 시스템 변수를 OFF로 설정하면 외래키 관계에 대한 체크 작업을 일시적으로 멈출 수 있다. 외래키 체크 작업을 일시적으로 멈추면 대략 레코드 적재나 삭제 등의 작업도 부가적인 체크가 필요 없기 때문에 훨씬 빠르게 처리할 수 있다.\n1 2 3 4 5 SET foreign_key_checks=OFF; /* 작업 수행 ... */ SET foreign_key_checks=ON; 외래키 체크를 일시적으로 중지한 상태에서 외래키 관계를 가진 부모 테이블의 레코드를 삭제했다면 반드시 자식 테이블의 레코드도 살제하여 일관성을 맞춰준 후 다시 외래키 체크 기능을 활성화 해야 한다.\nforeign_key_checks가 비활성화되면 외래키 관계의 부모 테이블에 대한 작업도 무시한다.(ON DELETE CASCADE, ON UPDATE CASCADE)\nMVCC - Multi Version Concurrency Control 일반적으로 레코드 레벨의 트랜잭션을 지원하는 DBMS가 제공하는 기능이며, MVCC의 가장 큰 목적은 잠금을 사용하지 않는 일관된 읽기를 제공하는 데 있다.\nInnoDB는 언두 로그(Undo log)를 이용해 이 기능을 구현한다.\n멀티 버전: 하나의 레코드에 대해 여러 개의 버전이 동시에 관리 1 2 3 4 5 6 7 CREATE TABLE member ( m_id INT NOT NULL, m_name VARCHAR(20) NOT NULL, m_area VARCHAR(100) NOT NULL, PRIMARY KEY (m_id), INDEX ix_area (m_area) ); 1 2 INSERT INTO member (m_id, m_name, m_area) VALUES (12, \u0026#39;홍길동\u0026#39;, \u0026#39;서울\u0026#39;); COMMIT; 1 UPDATE member SET m_area=\u0026#39;경기\u0026#39; WHERE m_id=12; UPDATE 문장이 실행되면 커밋 실행 여부와 관계 없이 InnoDB의 버퍼풀은 새로운 값인 ‘경기’로 업데이트 된다. 그리고 디스크의 데이터 파일에는 체크포인트나 InnoDB의 Write 스레드에 의해 새로운 값으로 업데이트돼 있을 수도 있고 아닐 수도 있다.(InnoDB가 ACID를 보장하기 때문에 일반적으로는 InnoDB의 버퍼풀과 데이터 파일은 동일한 상태라고 가정해도 무방함)\n아직 COMMIT이나 ROLLBACK이 되지 않은 상태에서 다른 사용자가 다음 같은 쿼리로 작업 중인 레코드를 조회한다면, MySQL 서버의 시스템 변수(transaction_isolation)에 설정된 격리 수준(Isolation level)에 따라 다르다.\nREAD_UNCOMMITED: InnoDB 버퍼풀이 현재 가지고 있는 변경된 데이터를 읽어서 반환한다. READ_COMMITTED, REPEATABLE_READ, SERIALIZABLE: 아직 커밋되지 않았기 때문에 InnoDB 버퍼풀이나 데이터 파일에 있는 내용 대신 변경되기 이전의 내용을 보관하고 있는 언두 영역의 데이터를 반환한다. 이러한 과정을 DBMS에서는 MVCC라고 표현한다. 즉 하나의 레코드(회원번호가 12인 레코드)에 대해 2개의 버전이 유지되고, 필요에 따라 어느 데이터가 보여지는지 여러 가지 상황에 따라 다르다.\n트랜잭션이 길어지면 언두에서 관리하는 예전 데이터가 삭제되지 못하고 오랫동안 관리되어야 하며, 자연히 언두 영역이 저장되는 시스템 테이블 스페이스의 공간이 많이 늘어나는 상황이 발생할 수 있다.\nUPDATE 쿼리가 실행되면 InnoDB 버퍼 풀은 즉시 새로운 데이터로 변경되고 기존 데이터는 언두영역으로 복사된다.\nCOMMIT: InnoDB는 더 이상의 변경 작업 없이 지금의 상태를 영구적인 데이터로 만들어 버린다. ROLLBACK: 언두 영역에 있는 백업된 데이터를 InnoDB 버퍼 풀로 다시 복구하고, 언두 영역의 내용을 삭제한다. 커밋이 된다고 언두 영역의 백업 데이터가 항상 바로 삭제되지는 않고, 언두 영역을 필요로 하는 트랜잭션이 없을때 삭제된다.\n잠금 없는 일관된 읽기 - Non-Locking Consistent Read InnoDB 스토리지 엔진은 MVCC 기술을 이용해 감금을 걸지 않고 읽기 작업을 수행한다. 잠금을 걸지 않기 때문에 InnoDB에서 읽기 작업은 다른 트랜잭션이 가지고 있는 잠금을 기다리지 않고, 읽기 작업이 가능하다.\n격리수준이 SERIALIZABLE이 아닌 READ_UNCOMMITED나 READ_COMMITED, REPEATEABLE_READ 수준인 경우 INSERT와 연결되지 않은 순수한 읽기(SELECT) 작업은 다른 트랜잭션의 변경 작업과 관계 없이 항상 잠금을 대기하지 않고 바로 실행된다.\n특정 사용자가 레코드를 변경하고 아직 커밋을 수행하지 않았다 하더라도 변경 트랜잭션이 다른 사용자의 SELECT 작업을 방해하지 않는다. 이를 ‘잠금 없는 일관된 읽기’ 라고 표현하며, InnoDB에서는 변경되기 전의 데이터를 읽기 위해 언두 로그를 사용한다.\n오랜 시간 동안 활성 상태인 트랜잭션으로 인해 MySQL 서버가 느려지거나 문제가 발생할 때가 가끔 있는데, 일관된 읽기를 위해 언두 로그를 삭제하지 못하고 계속 유지해야 하기 때문에 발생하는 문제이다.\n따라서 트랜잭션이 시작됐다면 가능한 빨리 롤백이나 커밋을 통해 트랜잭션을 완료하는 것이 좋다.\n자동 데드락 감지 InnoDB 스토리지 엔진은 내부적으로 잠금이 교착 상태에 빠지지 않았는지 체크하기 위해 잠금 대기 목록(Wait-for List)을 그래프 형태로 관리한다. InnoDB 스토리지 엔진은 데드락 감지 스레드를 통해 주기적으로 잠금 대기를 그래프를 검사해 교착 상태에 빠진 트랜잭션들을 찾아서 그중 하나를 강제 종료한다.\n트랜잭션의 언두 로그양이 적은 트랜잭션이 롤백 해도 처리한 내용이 적기 때문에 선택된다.\nInnoDB 스토리지 엔진은 상위 레이어인 MySQL 엔진에서 관리되는 테이블 잠금(LOCK TABLES 명령으로 잠긴 테이블)은 볼 수가 없어 데드락 감지가 불확실 할 수 있는데, innodb_table_locks 시스템 변수를 활성화 하면 InnoDB 스토리지 엔진 내부의 레코드 잠금뿐만 아니라 테이블 레벨의 잠금 까지 감지할 수 있게 된다.\n일반적인 서비스에서는 데드락 감지 스레드가 데드락을 찾아내는 작업은 부담되지 않지만, 동시 처리 스레드가 매우 많아지거나 트랜잭션이 가진 잠금 개수가 많아지면 데드락 감지 스레드가 느려진다.\n데드락 감지 스레드는 잠금 목록을 검사해야 하기 때문에 잠금 상태가 변경되지 않도록 잠금 목록이 저장된 리스트(잠금 테이블)에 새로운 잠금을 걸고 데드락 스레드를 찾게 되는데, 데드락 감시 스레드가 느려지면 서비스 쿼리를 처리중인 스레드는 더는 작업을 진행하지 못하고 대기하며 서비스에 악영항을 미치게 된다. 이렇게 동시 처리 스레드가 매우 많은 경우 데드락 감지 스레드는 더 많은 CPU 자원을 소모할 수도 있다.\ninnodb_deadlock_detect 시스템 변수를 활용하여 데드락 감지 스레드를 비활성화 할 수 있다. 이럴 경우 데드락 상황 발생시 무한정 대기할 수도 있지만, innodb_lock_wait_timeout 시스템 변수를 활성화하면 일정 시간이 지났을 경우 요청 실패하고 에러 메시지를 반환하게 만들 수 있다.\n데드락 감시 스레드가 부담되어 innodb_deadlock_detect를 OFF로 설장해서 비활성화 하는 경우에는 innodb_lock_time_wait_timeout을 기본값인 50초보다 훨씬 낮은 시간으로 변경하여 사용할 것을 권장한다.\n자동화된 장애 복구 InnoDB에는 손실이나 장애로 부터 데이터를 보호하기 위한 여러가지 메커니즘이 탑재돼있다. 그러한 메커니즘을 이용해 MySQL 서버가 시작될 때 완료되지 못한 트랜잭션이나 디스크에 일부만 기록된(Partial write)데이터 페이지 등에 대한 인련의 복구 작업이 자동으로 진행된다.\nInnoDB 스토리지 엔진은 매우 견고해서 데이터 파일이 손상되거나 MySQL 서버가 시작되지 못하는 경우는 거의 발생하지 않지만, 디스크나 하드웨어 이슈로 InnoDB 스토리지 엔진이 자동으로 복구를 못 하는 경우도 발생할 수 있는데, 한번 문제가 생기면 복구하기 쉽지 않다.\nInnoDB 데이터 파일은 기본적으로 서버가 시작될 때 자동 복구를 수행하며, 자동으로 복구될 수 없는 손상이 있다면 서버가 종료된다.\n장애 복구 대응 MySQL 서버의 설정 파일에 innodb_force_recovery 시스템 변수를 설정하여 시작해야 한다.\n6: 로그 파일 손상 1: 테이블의 데이터 파일이 손상 어떤 부분이 문제인지 알 수 없다면 1~6까지 변경하며 재실행 이후 서버가 가동되고 InnoDB 테이블이 인식된다면 mysqldump를 이용해 데이터를 가능한 만큼 백업하고 그 데이터로 MySQL 서버의 DB와 테이블을 다시 생성하는 것이 좋다.\nInnoDB_force_recovery 옵션 1(SRV_FORCE_IGNORE_CORRUPT):\n테이블스페이스의 데이터나 인덱스 페이지에서 손상된 부분이 발견되도 무시하고 서버를 시작한다. \u0026lsquo;Database page corruption on disk or a failed\u0026rsquo; 출력되는 경우가 많다. mysqldump나 SELECT INTO OUTFILE ...를 이용해 덤프하여 데이터베이스를 다시 구축하는 것이 좋다. 2(SRV_FORCE_NO_BACKGROUND):\n백그라운드 스레드 가운데 메인 스레드를 시작하지 않고 MySQL 서버를 시작한다. 메인 스레드가 언두 데이터를 삭제하는 과정에서 장애가 발생했을때 사용 3(SRV_FORCE_NO_TRX_UNDO):\n일반적으로 MySQL 서버는 재실행시 언두 영역의 데이터를 먼저 파일에 적용하고 리두 로그의 내용을 다시 덮어써서 장애 시점의 데이터 상태를 만들어 낸 후, 최종적으로 커밋되지 않은 트랜잭션의 작업을 롤백하지만 3으로 설정시 롤백하지 않고 그대로 나둔다. 커밋되지 않고 종료된 트랜잭션은 계속 그 상태로 남아있게 된다. 백업 후 데이터베이스를 다시 구축하는 것이 좋다. 4(SRV_FORCE_NO_IBUF_MERGE):\nInnoDB는 INSERT, UPDATE, DELETE 등의 데이터 변경으로 인한 인덱스 변경 작업을 상황에 따라 즉시처리 혹은 버퍼에 두고 나중에 처리할 수 있다. 인서트 버퍼를 통해 처리가 될 경우, 비정상 종료시 병합 될지 알 수 없기 때문에, 인서트 버퍼의 손상을 감지하면 에러를 발생시켜 MySQL 서버의 실행을 막는다. 인서트 버퍼의 내용을 무시하고 강제로 MySQL을 실행시킨다. 인서트 버퍼는 실제 데이터와 관련된 부분이 아니라, 인덱스에 관련된 부분이므로 테이블을 텀프한 후 다시 데이터베이스를 구축하면 데이터의 손실 없이 복구할 수 있다. 5(SRV_FORCE_NO_UNDO_LOG_SCAN):\nMySQL 서버가 종료되는 시점에 처리중인 트랜잭션이 있을 경우 별도의 처리 없이 커넥션을 강제로 끊어버리고 종료된다. MySQL 서버가 재실행되면 InnoDB 엔진은 언두 레코드를 이용해 데이터 페이지를 복구하고 리두 로그를 적용해 종료 시점의 상태로 만들고, 커밋되지 않은 트랜잭션에서 변경한 작업은 모두 롤백 처리한다. 이때 InnoDB 스토리지 엔진이 언두 로그를 사용할 수 없다면 에러가 발생하여 MySQL 서버가 실행될 수 없다. 언두 로그를 모두 무시하고 실행한다. MySQL 서버가 종료되던 시점에 커밋되지 않았던 작업도 모두 커밋된 것처럼 처리되어 잘못된 데이터가 남을 수 있다. 데이터를 백업하고, 데이터베이스를 새로 구축해야한다. 6(SRV_FORCE_NO_LOG_REDO):\nInnoDB 스토리지 엔진의 리두 로그가 손상되면 MySQL 서버가 실행되지 못한다. 해당 복구 모드로 실행하면 리두 로그를 무시하고 서버가 실행된다.\n트랜잭션이 커밋됐다 하더라도 리두 로그에만 기록되고 데이터 파일에 기록되지 않은 데이터는 모두 무시되므로 마지막 체크 포인트시점의 데이터만 남게 된다. 기존 InnoDB의 리두 로그는 모두 삭제 또는 백업하고 MySQL 서버를 시작하는 것이 좋다. 데이터를 백업하고 MySQL 서버를 새로 구축하는 것이 좋다. 위 방법을 수행해도 MySQL서버가 시작되지 않으면 백업을 이용해 다시 구축하는 방법밖에 없다. 백업이 있다면 마지막 백업으로 데이터베이스를 다시 구축하고, 바이너리 로그를 사용해 최대한 장애 시점까지의 데이터를 복구할 수도 있다.\n마지막 풀 백업 시점부터 장애 시점까지의 바이너리 로그가 있다면 이용하는 것이 데이터 손실이 더 적을 수 있다.\n백업은 있지만 복제의 바이너리 로그가 없거나 손실되었다면, 마지막 백업 시점가지만 복구할 수 있다.\n","date":"2023-04-11T19:15:11+09:00","image":"https://codemario318.github.io/post/real_mysql_4_2_1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_4_2_1/","title":"4.2 InnoDB 스토리지 엔진 아키텍처(1)"},{"content":"MySQL의 전체 구조 MySQL 서버는 크게 MySQL 엔진과 스토리지 엔진으로 구분할 수 있다.\n사람으로 비유하면 MySQL 엔진은 머리 역할을 담담하고, 스토리지 엔진은 손과 발의 역할을 담당한다.\nMySQL 엔진 MySQL 엔진은 요청된 SQL 문장을 분석하거나 최적화하는 등 DBMS의 두뇌에 해당하는 처리를 수행한다.\n커넥션 핸들러: 클라이언트 요청에 따라 새로운 연결을 생성하고 관리 SQL 파서 및 전처리기: SQL 쿼리를 최적화 및 실행하기 전에 구문 분석 및 전처리를 담당 옵티마이저: 쿼리의 최적화 MySQL은 표준 SQL(ANSI SQL) 문법을 지원하기 때문에 표준 문법에 따라 작성된 쿼리는 타 DBMS와 호환되어 실행될 수 있다.\n스토리지 엔진 스토리지 엔진은 실제 데이터를 디스크 스토리지에 저장하거나 디스크 스토리지로부터 데이터를 읽어오는 역할 수행한다.\nMySQL 서버에서 MySQL엔진은 하나지만 스토리지 엔진은 여러 개를 동시에 사용할 수 있다.\n1 CREATE TABLE test_table (fd1 INT, fd2 INT) ENGINE=INNODB; 위처럼 테이블이 사용할 스토리지 엔진을 지정하면 해당 테이블의 모든 읽기 작업과 변경 작업은 정의된 스토리지 엔진이 처리한다.\n각 스토리지 엔진은 성능 향상을 위해 키 캐시(MyISAM), 버퍼풀(InnoDB) 같은 기능을 내장하고 있다.\n핸들러 API MySQL 엔진의 쿼리 실행기에서 데이터를 쓰거나 읽어야 할 때는 스토리지 엔진에 쓰기 또는 읽기를 요청하는데, 이러한 요청을 핸들러 요청이라고 하며, 사용되는 API를 핸들러 API라고 한다.\nInnoDB 스토리지 엔진 또한 이 핸들러 API를 이용해 MySQL 엔진과 데이터를 주고 받는다.\n핸들러 API를 통해 발생한 작업은 아래 쿼리로 확인 가능하다.\n1 SHOW GLOBAL STATUS LIKE \u0026#39;Handler%\u0026#39;; MySQL 스레딩 구조 MySQL 서버는 프로세스 기반이 아니라 스레드 기반으로 동작한다.\nMySQL 서버에서 실행 중인 스레드 목록은 performance_schema 데이터베이스에 threads 테이블을 통해 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 SELECT thread_id ,name ,type ,processlist_user ,processlist_host FROM performance_schema.threads ORDER BY type,thread_id ; 백그라운드 스레드의 개수는 MySQL 서버의 설정 내용에 따라 가변적일 수 있다. 동일한 스레드가 2개 이상씩 보이는 것은 MySQL 서버의 설정 내용에 의해 여러 스레드가 동일 작업을 병렬로 처리하는 경우이다.\n포그라운드 스레드(클라이언트 스레드) 포그라운드 스레드는 클라이언트 연결 요청을 처리하고 데이터베이스 작업을 수행한다. 이러한 스레드는 쿼리 실행 중에 CPU 및 I/O 리소스를 사용하므로, 성능에 중요한 역할을 한다.\n포그라운드 스레드는 최소한 MySQL 서버에 접속된 클라이언트의 수만큼 존재하며, 주로 각 클라이언트 사용자가 요청하는 쿼리 문장을 처리한다.\n클라이언트 사용자가 작업을 마치고 커넥션을 종료하면, 해당 커넥션을 담당하던 스레드는 다시 스레드 캐시로 돌아간다.\n이때 이미 스레드 캐시에 일정 개수 이상의 대기중인 스레드가 있으면 스레드 캐시에 넣지 않고 스레드를 종료시켜 일정 개수의 스레드만 스레드 캐시에 존재하게 한다.\n스레드 캐시에 유지할 수 있는 최대 스레드 개수는 thread_cache_size 시스템 변수로 설정한다.\n포그라운드 스레드는 데이터를 MySQL 데이터 버퍼나 캐시로 부터 가져오며, 버퍼나 캐시에 없는 경우 직접 디스크의 데이터나 인덱스 파일로부터 데이터를 읽어와서 작업을 처리한다.\nMyISAM: 디스크 쓰기 작업까지 포그라운드 스레드가 처리 InnoDB: 데이터 버퍼나 캐시까지만 포그라운드 스레드가 처리 백그라운드 스레드 MyISAM의 경우 해당 사항이 별로 없지만, InnoDB는 다음과 같이 여러가지 작업이 백그라운드로 처리된다.\n인서트 버퍼(Insert Buffer)를 병합하는 스레드 로그를 디스크로 기록하는 스레드 InnoDB 버퍼풀의 데이터를 디스크에 기록하는 스레드 데이터 버퍼로 읽어 오는 스레드 잠금이나 데드락을 모니터링 하는 스레드 모두 중요한 역할을 수행하지만 로그 스레드와 버퍼의 데이터를 디스크로 내려쓰는 작업을 처리하는 쓰기 스레드(Write thread)가 특히 중요하다.\nMySQL 5.5 버전부터 데이터 쓰기 스레드와 데이터 읽기 스레드의 개수를 2개 이상 지정할 수 있게 됐으며, innodb_write_io_thread, innodb_read_io_threads 시스템 변수로 스레드의 개수를 설정한다.\nInnoDB에서도 데이터를 읽는 작업은 주로 클라이언트 스레드에서 처리되기 때문에 읽기 스레드는 많이 설정할 필요는 없지만, 쓰기 스레드는 아주 많은 작업을 백그라운드로 처리하기 때문에 일반적인 내장 디스크를 사용할때는 2~4 정도, DAS, SAN과 같은 스토리지를 사용할 때는 디스크를 최적으로 사용할 수 있을 만큼 충분히 설정하는 것이 좋다.\n사용자의 요청을 처리하는 도중 데이터의 쓰기 작업은 지연(버퍼링)되어 처리될 수 있지만 데이터의 읽기 작업은 절대 지연될 수 없다. 일반적인 상용 DBMS에는 대부분 쓰기 작업을 버퍼링해서 일괄 처리하는 기능이 있다.\nInnoDB: INSERT, UPDATE, DELETE 쿼리로 데이터가 변경되는 경우 데이터가 디스크의 데이터 파일로 완전히 저장될 때까지 기다리지 않아도 된다. MyISAM: 사용자 스레드가 쓰기 작업까지 함께 처리하도록 설계되어, 일반적인 쿼리는 쓰기 버퍼링 기능을 사용할 수 없다. 메모리 할당 및 사용 구조 글로벌 메모리 영역과 로컬 메모리 영역으로 구분되며, 서버 내에 존재하는 많은 스레드가 공유해서 사용하는 공간인지 여부에 따라 구분된다.\n글로벌 메모리 영역 일반적으로 클라이언트 스레드의 수와 무관하게 하나의 메모리 공간만 할당된다. 필요에 따라 2개 이상의 메모리 공간을 할당받을 수도 있지만 클라이언트의 스레드 수와 무관하며, 생성된 글로벌 영역이 N개라 하더라도 모든 스레드에 의해 공유된다.\n테이블 캐시 InnoDB 버퍼풀 InnoDB 어댑티드 해시 인덱스 InnoDB 리두 로그 버퍼 등이 대표적인 글로벌 메모리 영역이다.\n로컬 메모리 영역 세션 메모리 영역이라고도 표현하며, MySQL 서버상에 존재하는 클라이언트 스레드가 쿼리를 처리하는 데 사용하는 메모리 영역이다.\n정렬 버퍼 조인 버퍼 바이너리 로그 캐시 네트워크 버퍼 MySQL 서버에 클라이언트가 접속하면, 클라이언트 커넥션(세션)으로부터의 요청을 처리하기 위해 스레드를 하나씩 할당하게 되는데, 클라이언트 스레드가 사용하는 메모리 공간이라고 해서 클라이언트 메모리 영역이라고도 한다.\n로컬 메모리는 각 클라이언트 스레드별로 독립적으로 할당되며 절대 공유되어 사용되지 않는다.\n일반적으로 글로벌 메모리 영역의 크기는 주의해서 설정하지만 소트 버퍼와 같은 오컬 메모리 영역은 크게 신경 쓰지 않고 설정하는데, 최악의 경우 MySQL 서버가 메모리 부족으로 멈춰 버릴수도 있으므로 적절한 메모리 공간을 설정하는 것이 중요하다.\n커넥션이 열러있는 동안 계속 할당된 상태로 남아있는 경우: 커넥션 버퍼, 결과 버퍼 쿼리를 실행하는 순간에만 할당: 소트 버퍼, 조인 버퍼 플러그인 스토리지 엔진 모델 MySQL의 독특한 구조 중 대표적인 중 하나가 플러그인 모델이다.\n스토리지 엔진 검색 엔진을 위한 검색어 파서 사용자의 인증을 위한 Native Authentication, Caching SHA-2 Authentication 등 MySQL은 이미 기본적으로 많은 스토리지 엔진을 가지고 있지만, 필요에 의해 직접 스토리지 엔진을 만드는 것도 가능하다.\nMySQL에서 쿼리가 실행되는 과정을 보면 대부분 작업이 MySQL엔진에서 처리되고, 마지막 데이터 읽기, 쓰기 작업만 스토리지 엔진에 의해 처리한다.\nGROUP BY, ORDER BY 등 복잡한 처리는 스토리지 엔진 영역이 아니라 MySQL 엔진의 처리 영역인 쿼리 실행기에서 처리된다.\n스토리지 엔진에 따라 데이터 읽기/쓰기 작업 처리 방식이 크게 달라질 수 있다.\n하나의 쿼리 작업은 여러 하위 작업으로 나뉘는데, 각 하위 작업이 MySQL 엔진 영역에서 처리되는지 스토리지 엔진 영역에서 처리되는지 구분할 줄 알아야 한다.\n1 2 /* 스토리지 엔진 조회 */ SHOW ENGINES; 서버에 포함되지 않은 스토리지 엔진을 사용하려면 MySQL 서버를 다시 빌드해야 한다. 준비만 되어있다면 플러그인 형태로 빌드된 스토리지 엔진 라이브러리를 다운로드해서 끼워넣기만 하면 사용할 수 있다.\n1 2 /* 플러그인 조회 */ SHOW PLUGINS; 컴포넌트 플러그인 아키텍처는 다음과 같은 단점이 있다.\n오직 MySQL 서버와 인테페이스할 수 있고, 플러그인끼리는 통신할 수 없음 MySQL 서버의 변수나 함수를 직접 호출하기 때문에 안전하지 않음(캡슐화 안됨) 플러그인은 상호 의존 관계를 설정할 수 없어 초기화가 어려움 이러한 문제를 개선하기 위해 MySQL 8.0 부터는 기존의 플러그인 아키텍처를 대체하기 위해 컴포넌트 아키텍처가 지원된다.\n예를 들면, MySQL 5.7 버전까지는 비밀번호 검증 기능이 플러그인 형태로 제공됐지만 MySQL8.0의 비밀번호 검증 기능은 컴포넌트로 개선됐다.\n1 2 3 4 5 /* validate_password 설치 */ INSTALL COMPONENT \u0026#39;file://component_validate_password\u0026#39;; /* 설치된 컴포넌트 확인 */ SELECT * FROM mysql.component; 쿼리 실행 구조 쿼리 파서 쿼리 파서는 사용자 요청으로 들어온 쿼리 문장을 토큰(MySQL이 인식할 수 있는 최소 단위의 어휘나 기호)으로 분리해 트리 형태의 구조로 만들어 내는 작업을 의미한다.\n쿼리 문장의 기본 문법 오류는 이 과정에서 발견되고 사용자에게 오류 메시지를 전달하게 된다. 전처리기 파서 과정에서 만들어진 파서 트리를 기반으로 쿼리 문장에 구조적인 문제점이 있는지 확인한다.\n각 토큰을 테이블 이름이나 컬럼 이름, 또는 내장 함수와 같은 개체를 매핑해 해당 객체의 존재 여부와 객체의 접근 권한 등을 확인하는 과정을 수행한다.\n실제 존재하지 않거나 권한상 사용할 수 없는 개체의 토큰(컬럼, 내장 함수)은 이 단계에서 걸러진다. 옵티마이저 사용자의 요청으로 들어온 쿼리 문장을 저렴한 비용으로 가장 빠르게 처리할지를 결정하는 역할을 담당한다.\nDBMS의 두뇌에 비유되며, 옵티마이저가 더 나은 선택을 하도록 유도하는 것이 매우 중요하다.\n실행 엔진 옵티마이저가 두뇌라면 실행 엔진과 핸들러는 손과 발에 비유할 수 있다.\n옵티마이저가 GROUP BY를 처리하기 위해 임시 테이블을 사용하기로 결정했다면 아래 과정을 거칠 수 있다.\n실행 엔진이 핸들러에게 임시 테이블을 만들라고 요청 실행 엔진은 WHERE 절에 일치하는 레코드를 읽어오라고 핸들러에게 요청 읽어온 레코드들을 1번에서 준비한 임시 테이블로 저장하라고 핸들러에게 요청 데이터가 준비된 임시 테이블에서 필요한 방식으로 데이터를 읽어 오라고 핸들러에게 요청 결과를 사용자나 다른 모듈로 넘김 즉, 만들어진 계획대로 각 핸들러에게 요청해서 받은 결과를 또 다른 핸들러 요청의 입력으로 연결하는 역할을 수행한다.\n핸들러(스토리지 엔진) MySQL 서버의 가장 밑단에서 실행 엔진의 요청에 따라 데이터를 디스크로 저장하고 디스크로부터 읽어 오는 역할을 담당한다.\n핸들러는 결국 스토리지 엔진을 의미하며, MyISAM 테이블을 조작하는 경우 핸들러가 MyISAM 스토리지 엔진이 되고, InnoDB 테이블을 조작하는 경우 InnoDB 스토리지 엔진이 된다.\n복제 MySQL 서버에서 복제(Replication)는 매우 중요한 역할을 담당하며, 지금까지 MySQL 서버에서 복제는 많은 발전을 거듭해왔다.(16장)\n쿼리 캐시 쿼리 캐시는 빠른 응답을 필요로 하는 웹 기반의 응용 프로그램에서 매우 중요한 역할을 담당했다. 쿼리 캐시는 SQL의 실행 결과를 메모리에 캐시하고, 동일 SQL 쿼리가 실행되면 테이블을 읽지 않고 즉시 결과를 반환하기 때문에 매우 빠른 성능을 보였다.\n하지만 쿼리 캐시는 테이블의 데이터가 변경되면 캐시에 저장된 결과 중에서 변경된 테이블과 관련된 테이블과 관련된 것들은 모두 삭제(Invalidate)해야 하므로, 심각한 동시 처리 성능 저하를 유발한다. 또한 MySQL 서버가 발전하면서 성능이 개선되는 과정에서 쿼리 캐시는 계속된 동시 처리 성능 저하와 많은 버그의 원인이 되기도 했다.\n다수의 클라이언트가 동시에 같은 쿼리를 실행하는 경우 쿼리 캐시 락(query cache lock)이 발생 가능하다. 이는 쿼리 캐시에 새로운 결과를 저장하거나 기존 결과를 반환하기 위해 필요한 락(lock)으로, 동시 처리가 많은 시스템에서는 쿼리 캐시를 사용하지 않는 것이 더 나은 성능을 보일 수 있다.\nMySQL 5.6 이하 버전에서는 쿼리 캐시가 InnoDB 또는 NDB Cluster 스토리지 엔진을 사용하는 경우에만 동작하는데 MyISAM 스토리지 엔진을 사용하는 경우에도 쿼리 캐시를 켜면 쿼리 결과가 무한정 캐시될 수 있는 버그가 있었다. 이러한 버그는 시스템의 부하를 높일 뿐만 아니라, 캐시 메모리의 공간을 차지해 다른 쿼리의 실행에 영향을 미칠 수 있다.\n이러한 이유로 MySQL 8.0으로 올라오면서 완전히 제거되고, 관련 시스템 변수도 모두 제거되었다.\n스레드 풀 MySQL 서버 엔터프라이즈 에디션은 스레드풀 기능을 제공하지만 커뮤니티 에디션은 지원하지 않는다. 따라서 Percona Server 플러그인에서 제공하는 스레드풀 기능을 살펴본다.\n스레드풀은 내부적으로 사용자의 요청을 처리하는 스레드 개수를 줄여서 동시 처리되는 요청이 많다 하더라도 MySQL 서버의 CPU가 제한된 개수의 스레드 처리에만 집중할 수 있게 하여 서버의 자원 소모를 줄이는것이 목적이다.\n하지만 스레드풀이 실제 서비스에서 눈에띄는 성능 향상을 보여준 경우는 드물다.\n실행 중인 스레드들을 CPU가 최대한 잘 처리해낼 수 있는 수준으로 줄여서 빨리 처리하게 하는 기능으므로 스케줄링 과정에서 CPU 시간을 제대로 확보하지 못하는 경우 쿼리 처리가 더 느려지는 사례도 발생할 수 있다.\n제한된 수의 스레드만으로 CPU가 처리하도록 적절히 유도하면 CPU의 프로세서 친화도(Processor affinity)도 높히고 불필요한 컨텍스트 스위치를 줄여 오버헤드를 낮출 수 있다.\n스레드 그룹 개수 Percona Server의 스레드 풀은 기본적으로 CPU 코어의 개수만큼 스레드 그룹을 생성하며 일반적으로 CPU 코어의 개수와 맞추는것이 CPU 프로세서 친화도를 높이는 데 좋다.\nMySQL 서버가 처리해야할 요청이 생기면 스레드풀로 처리를 이관하는데, 이미 스레드풀이 처리중인 작업이 있는 경우 시스템 변수에 설정된 개수만큼 추가로 더 받아들여서 처리한다. 너무 많으면 스케줄링해야 할 수레드가 많아져 비효율적으로 작동할 수 있다.\n타이머 스레드 스레드 그룹의 모든 스레드가 일을 처리하고 있다면 스레드 풀은 해당 스레드 그룹에 새로운 작업 스레드를 추가할지, 기존 작업 스레드가 처리를 완료할 때가지 기다릴지 여부를 판단해야 한다.\n주기적으로 스레드 그룹의 상태를 체크해서 thread_pool_stall_limit 시스템 변수에 정의된 시간에 작업을 끝내지 못했다면 새로운 스레드를 생성해 스레드 그룹에 추가한다.\n모든 스레드 그룹의 스레드가 작업을 수행중이라면 시스템 변수에 설정된 개수를 넘어설 수 없어 대기해야 한다.\n응답 시간이 아주 민감한 서비스라면 시스템 변수를 적절히 낮춰 설정해야하며, 0에 가까운 값으로 설정하는 것은 좋지 않고 이런 경우는 스레드풀을 사용하지 않는 것이 좋을 수 있다.\n우선순위 큐 선순위 큐와 후순위 큐를 이용해 특정 트랜잭션이나 쿼리를 우선적으로 처리할 수 있는 기능도 제공한다. 먼저 시작된 트랜잭션 내에 속한 SQL을 빨리 처리해주면 해당 트랜잭션이 가지고 있던 잠금이 빨리 해제되고 잠금 경합을 낮춰 전체적인 처리 성능을 향상시킬 수 있다.\n트랜잭션 지원 메타데이터 데이터베이스 서버에서 테이블의 구조 정보와 스토어드 프로그램 등의 정보를 데이터 딕셔너리 또는 메타데이터라고 하는데, MySQL 서버는 5.7 버전까지 테이블의 구조를 FRM 파일에 저장하고 일부 스토어드 프로그램 또한 파일 기반으로 관리 되었다.\n이러한 파일 기반의 메타데이터는 생성 및 변경 작업이 트랜잭션을 지원하지 않기 때문에 테이블의 생성 또는 변경 도중에 MySQL 서버가 비정상적으로 종료되면 일관되지 않은 상태로 남게되는 문제가 있었다.\n이에따라 8버전 부터는 테이블의 구조 정보나 스토어드 프로그램의 코드 관련 정보를 모두 InnoDB의 테이블에 저장하도록 개선되었다.\n","date":"2023-04-10T00:00:00Z","image":"https://codemario318.github.io/post/real_mysql_4_1/real_mysql_hu03b8ff0acb6f52b3ec9ae95e616f82c2_25714_120x120_fill_q75_box_smart1.jpeg","permalink":"https://codemario318.github.io/post/real_mysql_4_1/","title":"4.1 MySQL 엔진 아키텍처"}]